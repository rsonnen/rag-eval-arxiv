{
  "corpus": "robotics",
  "source": "arxiv",
  "search_query": "cat:cs.RO AND (robot OR manipulation OR navigation OR grasping OR motion planning)",
  "curated_at": "2025-12-27T00:10:20.349004+00:00",
  "total_papers": 200,
  "papers_evaluated": 232,
  "acceptance_rate": 0.8620689655172413,
  "papers": [
    {
      "arxiv_id": "2512.21293v1",
      "title": "Quadrupped-Legged Robot Movement Plan Generation using Large Language Model",
      "authors": [
        {
          "name": "Muhtadin"
        },
        {
          "name": "Vincentius Gusti Putu A. B. M."
        },
        {
          "name": "Ahmad Zaini"
        },
        {
          "name": "Mauridhi Hery Purnomo"
        },
        {
          "name": "I Ketut Eddy Purnama"
        },
        {
          "name": "Chastine Fatichah"
        }
      ],
      "abstract": "Traditional control interfaces for quadruped robots often impose a high barrier to entry, requiring specialized technical knowledge for effective operation. To address this, this paper presents a novel control framework that integrates Large Language Models (LLMs) to enable intuitive, natural language-based navigation. We propose a distributed architecture where high-level instruction processing is offloaded to an external server to overcome the onboard computational constraints of the DeepRobotics Jueying Lite 3 platform. The system grounds LLM-generated plans into executable ROS navigation commands using real-time sensor fusion (LiDAR, IMU, and Odometry). Experimental validation was conducted in a structured indoor environment across four distinct scenarios, ranging from single-room tasks to complex cross-zone navigation. The results demonstrate the system's robustness, achieving an aggregate success rate of over 90\\% across all scenarios, validating the feasibility of offloaded LLM-based planning for autonomous quadruped deployment in real-world settings.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "published": "2025-12-24T17:22:00+00:00",
      "updated": "2025-12-24T17:22:00+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21293v1",
      "file": "papers/2512.21293v1.pdf"
    },
    {
      "arxiv_id": "2512.21243v1",
      "title": "LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation",
      "authors": [
        {
          "name": "Anatoly O. Onishchenko"
        },
        {
          "name": "Alexey K. Kovalev"
        },
        {
          "name": "Aleksandr I. Panov"
        }
      ],
      "abstract": "Methods that use Large Language Models (LLM) as planners for embodied instruction following tasks have become widespread. To successfully complete tasks, the LLM must be grounded in the environment in which the robot operates. One solution is to use a scene graph that contains all the necessary information. Modern methods rely on prebuilt scene graphs and assume that all task-relevant information is available at the start of planning. However, these approaches do not account for changes in the environment that may occur between the graph construction and the task execution. We propose LookPlanGraph - a method that leverages a scene graph composed of static assets and object priors. During plan execution, LookPlanGraph continuously updates the graph with relevant objects, either by verifying existing priors or discovering new entities. This is achieved by processing the agents egocentric camera view using a Vision Language Model. We conducted experiments with changed object positions VirtualHome and OmniGibson simulated environments, demonstrating that LookPlanGraph outperforms methods based on predefined static scene graphs. To demonstrate the practical applicability of our approach, we also conducted experiments in a real-world setting. Additionally, we introduce the GraSIF (Graph Scenes for Instruction Following) dataset with automated validation framework, comprising 514 tasks drawn from SayPlan Office, BEHAVIOR-1K, and VirtualHome RobotHow. Project page available at https://lookplangraph.github.io .",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-24T15:36:21+00:00",
      "updated": "2025-12-24T15:36:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21243v1",
      "file": "papers/2512.21243v1.pdf"
    },
    {
      "arxiv_id": "2512.21235v1",
      "title": "RoboCade: Gamifying Robot Data Collection",
      "authors": [
        {
          "name": "Suvir Mirchandani"
        },
        {
          "name": "Mia Tang"
        },
        {
          "name": "Jiafei Duan"
        },
        {
          "name": "Jubayer Ibn Hamid"
        },
        {
          "name": "Michael Cho"
        },
        {
          "name": "Dorsa Sadigh"
        }
      ],
      "abstract": "Imitation learning from human demonstrations has become a dominant approach for training autonomous robot policies. However, collecting demonstration datasets is costly: it often requires access to robots and needs sustained effort in a tedious, long process. These factors limit the scale of data available for training policies. We aim to address this scalability challenge by involving a broader audience in a gamified data collection experience that is both accessible and motivating. Specifically, we develop a gamified remote teleoperation platform, RoboCade, to engage general users in collecting data that is beneficial for downstream policy training. To do this, we embed gamification strategies into the design of the system interface and data collection tasks. In the system interface, we include components such as visual feedback, sound effects, goal visualizations, progress bars, leaderboards, and badges. We additionally propose principles for constructing gamified tasks that have overlapping structure with useful downstream target tasks. We instantiate RoboCade on three manipulation tasks -- including spatial arrangement, scanning, and insertion. To illustrate the viability of gamified robot data collection, we collect a demonstration dataset through our platform, and show that co-training robot policies with this data can improve success rate on non-gamified target tasks (+16-56%). Further, we conduct a user study to validate that novice users find the gamified platform significantly more enjoyable than a standard non-gamified platform (+24%). These results highlight the promise of gamified data collection as a scalable, accessible, and engaging method for collecting demonstration data.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-24T15:20:54+00:00",
      "updated": "2025-12-24T15:20:54+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21235v1",
      "file": "papers/2512.21235v1.pdf"
    },
    {
      "arxiv_id": "2512.21233v1",
      "title": "UniTacHand: Unified Spatio-Tactile Representation for Human to Robotic Hand Skill Transfer",
      "authors": [
        {
          "name": "Chi Zhang"
        },
        {
          "name": "Penglin Cai"
        },
        {
          "name": "Haoqi Yuan"
        },
        {
          "name": "Chaoyi Xu"
        },
        {
          "name": "Zongqing Lu"
        }
      ],
      "abstract": "Tactile sensing is crucial for robotic hands to achieve human-level dexterous manipulation, especially in scenarios with visual occlusion. However, its application is often hindered by the difficulty of collecting large-scale real-world robotic tactile data. In this study, we propose to collect low-cost human manipulation data using haptic gloves for tactile-based robotic policy learning. The misalignment between human and robotic tactile data makes it challenging to transfer policies learned from human data to robots. To bridge this gap, we propose UniTacHand, a unified representation to align robotic tactile information captured by dexterous hands with human hand touch obtained from gloves. First, we project tactile signals from both human hands and robotic hands onto a morphologically consistent 2D surface space of the MANO hand model. This unification standardizes the heterogeneous data structures and inherently embeds the tactile signals with spatial context. Then, we introduce a contrastive learning method to align them into a unified latent space, trained on only 10 minutes of paired data from our data collection system. Our approach enables zero-shot tactile-based policy transfer from humans to a real robot, generalizing to objects unseen in the pre-training data. We also demonstrate that co-training on mixed data, including both human and robotic demonstrations via UniTacHand, yields better performance and data efficiency compared with using only robotic data. UniTacHand paves a path toward general, scalable, and data-efficient learning for tactile-based dexterous hands.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-24T15:18:54+00:00",
      "updated": "2025-12-24T15:18:54+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21233v1",
      "file": "papers/2512.21233v1.pdf"
    },
    {
      "arxiv_id": "2512.21226v1",
      "title": "Relative Localization System Design for SnailBot: A Modular Self-reconfigurable Robot",
      "authors": [
        {
          "name": "Shuhan Zhang"
        },
        {
          "name": "Tin Lun Lam"
        }
      ],
      "abstract": "This paper presents the design and implementation of a relative localization system for SnailBot, a modular self reconfigurable robot. The system integrates ArUco marker recognition, optical flow analysis, and IMU data processing into a unified fusion framework, enabling robust and accurate relative positioning for collaborative robotic tasks. Experimental validation demonstrates the effectiveness of the system in realtime operation, with a rule based fusion strategy ensuring reliability across dynamic scenarios. The results highlight the potential for scalable deployment in modular robotic systems.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "published": "2025-12-24T15:07:09+00:00",
      "updated": "2025-12-24T15:07:09+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21226v1",
      "file": "papers/2512.21226v1.pdf"
    },
    {
      "arxiv_id": "2512.21220v1",
      "title": "RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic",
      "authors": [
        {
          "name": "Le Wang"
        },
        {
          "name": "Zonghao Ying"
        },
        {
          "name": "Xiao Yang"
        },
        {
          "name": "Quanchen Zou"
        },
        {
          "name": "Zhenfei Yin"
        },
        {
          "name": "Tianlin Li"
        },
        {
          "name": "Jian Yang"
        },
        {
          "name": "Yaodong Yang"
        },
        {
          "name": "Aishan Liu"
        },
        {
          "name": "Xianglong Liu"
        }
      ],
      "abstract": "Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a promising solution due to their flexibility. However, existing defenses often rely on static rule filters or prompt-level control, which struggle to address implicit risks arising in dynamic, temporally dependent, and context-rich environments. To address this, we propose RoboSafe, a hybrid reasoning runtime safeguard for embodied agents through executable predicate-based safety logic. RoboSafe integrates two complementary reasoning processes on a Hybrid Long-Short Safety Memory. We first propose a Backward Reflective Reasoning module that continuously revisits recent trajectories in short-term memory to infer temporal safety predicates and proactively triggers replanning when violations are detected. We then propose a Forward Predictive Reasoning module that anticipates upcoming risks by generating context-aware safety predicates from the long-term safety memory and the agent's multimodal observations. Together, these components form an adaptive, verifiable safety logic that is both interpretable and executable as code. Extensive experiments across multiple agents demonstrate that RoboSafe substantially reduces hazardous actions (-36.8% risk occurrence) compared with leading baselines, while maintaining near-original task performance. Real-world evaluations on physical robotic arms further confirm its practicality. Code will be released upon acceptance.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-12-24T15:01:26+00:00",
      "updated": "2025-12-24T15:01:26+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21220v1",
      "file": "papers/2512.21220v1.pdf"
    },
    {
      "arxiv_id": "2512.21219v1",
      "title": "Wireless Center of Pressure Feedback System for Humanoid Robot Balance Control using ESP32-C3",
      "authors": [
        {
          "name": "Muhtadin"
        },
        {
          "name": "Faris Rafi Pramana"
        },
        {
          "name": "Dion Hayu Fandiantoro"
        },
        {
          "name": "Moh Ismarintan Zazuli"
        },
        {
          "name": "Atar Fuady Babgei"
        }
      ],
      "abstract": "Maintaining stability during the single-support phase is a fundamental challenge in humanoid robotics, particularly in dance robots that require complex maneuvers and high mechanical freedom. Traditional tethered sensor configurations often restrict joint movement and introduce mechanical noises. This study proposes a wireless embedded balance system designed to maintain stability on uneven surfaces. The system utilizes a custom-designed foot unit integrated with four load cells and an ESP32-C3 microcontroller to estimate the Center of Pressure (CoP) in real time. The CoP data were transmitted wirelessly to the main controller to minimize the wiring complexity of the 29-DoF VI-ROSE humanoid robot. A PID control strategy is implemented to adjust the torso, hip, and ankle roll joints based on CoP feedback. Experimental characterization demonstrated high sensor precision with an average measurement error of 14.8 g. Furthermore, the proposed control system achieved a 100% success rate in maintaining balance during single-leg lifting tasks at a 3-degree inclination with optimized PID parameters (Kp=0.10, Kd=0.005). These results validate the efficacy of wireless CoP feedback in enhancing the postural stability of humanoid robots, without compromising their mechanical flexibility.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "published": "2025-12-24T15:00:23+00:00",
      "updated": "2025-12-24T15:00:23+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21219v1",
      "file": "papers/2512.21219v1.pdf"
    },
    {
      "arxiv_id": "2512.21201v1",
      "title": "Schrödinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation",
      "authors": [
        {
          "name": "Yu He"
        },
        {
          "name": "Da Huang"
        },
        {
          "name": "Zhenyang Liu"
        },
        {
          "name": "Zixiao Gu"
        },
        {
          "name": "Qiang Sun"
        },
        {
          "name": "Guangnan Ye"
        },
        {
          "name": "Yanwei Fu"
        }
      ],
      "abstract": "Zero-shot object navigation (ZSON) requires a robot to locate a target object in a previously unseen environment without relying on pre-built maps or task-specific training. However, existing ZSON methods often struggle in realistic and cluttered environments, particularly when the scene contains heavy occlusions, unknown risks, or dynamically moving target objects. To address these challenges, we propose \\textbf{Schrödinger's Navigator}, a navigation framework inspired by Schrödinger's thought experiment on uncertainty. The framework treats unobserved space as a set of plausible future worlds and reasons over them before acting. Conditioned on egocentric visual inputs and three candidate trajectories, a trajectory-conditioned 3D world model imagines future observations along each path. This enables the agent to see beyond occlusions and anticipate risks in unseen regions without requiring extra detours or dense global mapping. The imagined 3D observations are fused into the navigation map and used to update a value map. These updates guide the policy toward trajectories that avoid occlusions, reduce exposure to uncertain space, and better track moving targets. Experiments on a Go2 quadruped robot across three challenging scenarios, including severe static occlusions, unknown risks, and dynamically moving targets, show that Schrödinger's Navigator consistently outperforms strong ZSON baselines in self-localization, object localization, and overall Success Rate in occlusion-heavy environments. These results demonstrate the effectiveness of trajectory-conditioned 3D imagination in enabling robust zero-shot object navigation.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-12-24T14:28:17+00:00",
      "updated": "2025-12-24T14:28:17+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21201v1",
      "file": "papers/2512.21201v1.pdf"
    },
    {
      "arxiv_id": "2512.21196v1",
      "title": "Flocking phase transition and threat responses in bio-inspired autonomous drone swarms",
      "authors": [
        {
          "name": "Matthieu Verdoucq"
        },
        {
          "name": "Dari Trendafilov"
        },
        {
          "name": "Clément Sire"
        },
        {
          "name": "Ramón Escobedo"
        },
        {
          "name": "Guy Theraulaz"
        },
        {
          "name": "Gautier Hattenberger"
        }
      ],
      "abstract": "Collective motion inspired by animal groups offers powerful design principles for autonomous aerial swarms. We present a bio-inspired 3D flocking algorithm in which each drone interacts only with a minimal set of influential neighbors, relying solely on local alignment and attraction cues. By systematically tuning these two interaction gains, we map a phase diagram revealing sharp transitions between swarming and schooling, as well as a critical region where susceptibility, polarization fluctuations, and reorganization capacity peak. Outdoor experiments with a swarm of ten drones, combined with simulations using a calibrated flight-dynamics model, show that operating near this transition enhances responsiveness to external disturbances. When confronted with an intruder, the swarm performs rapid collective turns, transient expansions, and reliably recovers high alignment within seconds. These results demonstrate that minimal local-interaction rules are sufficient to generate multiple collective phases and that simple gain modulation offers an efficient mechanism to adjust stability, flexibility, and resilience in drone swarms.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY",
        "nlin.AO"
      ],
      "published": "2025-12-24T14:20:19+00:00",
      "updated": "2025-12-24T14:20:19+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21196v1",
      "file": "papers/2512.21196v1.pdf"
    },
    {
      "arxiv_id": "2512.21109v1",
      "title": "Robust and Efficient MuJoCo-based Model Predictive Control via Web of Affine Spaces Derivatives",
      "authors": [
        {
          "name": "Chen Liang"
        },
        {
          "name": "Daniel Rakita"
        }
      ],
      "abstract": "MuJoCo is a powerful and efficient physics simulator widely used in robotics. One common way it is applied in practice is through Model Predictive Control (MPC), which uses repeated rollouts of the simulator to optimize future actions and generate responsive control policies in real time. To make this process more accessible, the open source library MuJoCo MPC (MJPC) provides ready-to-use MPC algorithms and implementations built directly on top of the MuJoCo simulator. However, MJPC relies on finite differencing (FD) to compute derivatives through the underlying MuJoCo simulator, which is often a key bottleneck that can make it prohibitively costly for time-sensitive tasks, especially in high-DOF systems or complex scenes. In this paper, we introduce the use of Web of Affine Spaces (WASP) derivatives within MJPC as a drop-in replacement for FD. WASP is a recently developed approach for efficiently computing sequences of accurate derivative approximations. By reusing information from prior, related derivative calculations, WASP accelerates and stabilizes the computation of new derivatives, making it especially well suited for MPC's iterative, fine-grained updates over time. We evaluate WASP across a diverse suite of MJPC tasks spanning multiple robot embodiments. Our results suggest that WASP derivatives are particularly effective in MJPC: it integrates seamlessly across tasks, delivers consistently robust performance, and achieves up to a 2$\\mathsf{x}$ speedup compared to an FD backend when used with derivative-based planners, such as iLQG. In addition, WASP-based MPC outperforms MJPC's stochastic sampling-based planners on our evaluation tasks, offering both greater efficiency and reliability. To support adoption and future research, we release an open-source implementation of MJPC with WASP derivatives fully integrated.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-24T11:13:41+00:00",
      "updated": "2025-12-24T11:13:41+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21109v1",
      "file": "papers/2512.21109v1.pdf"
    },
    {
      "arxiv_id": "2512.21085v1",
      "title": "Global End-Effector Pose Control of an Underactuated Aerial Manipulator via Reinforcement Learning",
      "authors": [
        {
          "name": "Shlok Deshmukh"
        },
        {
          "name": "Javier Alonso-Mora"
        },
        {
          "name": "Sihao Sun"
        }
      ],
      "abstract": "Aerial manipulators, which combine robotic arms with multi-rotor drones, face strict constraints on arm weight and mechanical complexity. In this work, we study a lightweight 2-degree-of-freedom (DoF) arm mounted on a quadrotor via a differential mechanism, capable of full six-DoF end-effector pose control. While the minimal design enables simplicity and reduced payload, it also introduces challenges such as underactuation and sensitivity to external disturbances, including manipulation of heavy loads and pushing tasks. To address these, we employ reinforcement learning, training a Proximal Policy Optimization (PPO) agent in simulation to generate feedforward commands for quadrotor acceleration and body rates, along with joint angle targets. These commands are tracked by an incremental nonlinear dynamic inversion (INDI) attitude controller and a PID joint controller, respectively. Flight experiments demonstrate centimeter-level position accuracy and degree-level orientation precision, with robust performance under external force disturbances. The results highlight the potential of learning-based control strategies for enabling contact-rich aerial manipulation using simple, lightweight platforms.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-24T10:00:01+00:00",
      "updated": "2025-12-24T10:00:01+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21085v1",
      "file": "papers/2512.21085v1.pdf"
    },
    {
      "arxiv_id": "2512.21065v1",
      "title": "Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation",
      "authors": [
        {
          "name": "Zebin Jiang"
        },
        {
          "name": "Tianle Jin"
        },
        {
          "name": "Xiangtong Yao"
        },
        {
          "name": "Alois Knoll"
        },
        {
          "name": "Hu Cao"
        }
      ],
      "abstract": "Grasping is one of the most fundamental challenging capabilities in robotic manipulation, especially in unstructured, cluttered, and semantically diverse environments. Recent researches have increasingly explored language-guided manipulation, where robots not only perceive the scene but also interpret task-relevant natural language instructions. However, existing language-conditioned grasping methods typically rely on shallow fusion strategies, leading to limited semantic grounding and weak alignment between linguistic intent and visual grasp reasoning.In this work, we propose Language-Guided Grasp Detection (LGGD) with a coarse-to-fine learning paradigm for robotic manipulation. LGGD leverages CLIP-based visual and textual embeddings within a hierarchical cross-modal fusion pipeline, progressively injecting linguistic cues into the visual feature reconstruction process. This design enables fine-grained visual-semantic alignment and improves the feasibility of the predicted grasps with respect to task instructions. In addition, we introduce a language-conditioned dynamic convolution head (LDCH) that mixes multiple convolution experts based on sentence-level features, enabling instruction-adaptive coarse mask and grasp predictions. A final refinement module further enhances grasp consistency and robustness in complex scenes.Experiments on the OCID-VLG and Grasp-Anything++ datasets show that LGGD surpasses existing language-guided grasping methods, exhibiting strong generalization to unseen objects and diverse language queries. Moreover, deployment on a real robotic platform demonstrates the practical effectiveness of our approach in executing accurate, instruction-conditioned grasp actions. The code will be released publicly upon acceptance.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published": "2025-12-24T09:16:42+00:00",
      "updated": "2025-12-24T09:16:42+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21065v1",
      "file": "papers/2512.21065v1.pdf"
    },
    {
      "arxiv_id": "2512.21043v1",
      "title": "Tracing Energy Flow: Learning Tactile-based Grasping Force Control to Prevent Slippage in Dynamic Object Interaction",
      "authors": [
        {
          "name": "Cheng-Yu Kuo"
        },
        {
          "name": "Hirofumi Shin"
        },
        {
          "name": "Takamitsu Matsubara"
        }
      ],
      "abstract": "Regulating grasping force to reduce slippage during dynamic object interaction remains a fundamental challenge in robotic manipulation, especially when objects are manipulated by multiple rolling contacts, have unknown properties (such as mass or surface conditions), and when external sensing is unreliable. In contrast, humans can quickly regulate grasping force by touch, even without visual cues. Inspired by this ability, we aim to enable robotic hands to rapidly explore objects and learn tactile-driven grasping force control under motion and limited sensing. We propose a physics-informed energy abstraction that models the object as a virtual energy container. The inconsistency between the fingers' applied power and the object's retained energy provides a physically grounded signal for inferring slip-aware stability. Building on this abstraction, we employ model-based learning and planning to efficiently model energy dynamics from tactile sensing and perform real-time grasping force optimization. Experiments in both simulation and hardware demonstrate that our method can learn grasping force control from scratch within minutes, effectively reduce slippage, and extend grasp duration across diverse motion-object pairs, all without relying on external sensing or prior object knowledge.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-24T08:19:25+00:00",
      "updated": "2025-12-24T08:19:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21043v1",
      "file": "papers/2512.21043v1.pdf"
    },
    {
      "arxiv_id": "2512.20992v1",
      "title": "Multimodal Sensing for Robot-Assisted Sub-Tissue Feature Detection in Physiotherapy Palpation",
      "authors": [
        {
          "name": "Tian-Ao Ren"
        },
        {
          "name": "Jorge Garcia"
        },
        {
          "name": "Seongheon Hong"
        },
        {
          "name": "Jared Grinberg"
        },
        {
          "name": "Hojung Choi"
        },
        {
          "name": "Julia Di"
        },
        {
          "name": "Hao Li"
        },
        {
          "name": "Dmitry Grinberg"
        },
        {
          "name": "Mark R. Cutkosky"
        }
      ],
      "abstract": "Robotic palpation relies on force sensing, but force signals in soft-tissue environments are variable and cannot reliably reveal subtle subsurface features. We present a compact multimodal sensor that integrates high-resolution vision-based tactile imaging with a 6-axis force-torque sensor. In experiments on silicone phantoms with diverse subsurface tendon geometries, force signals alone frequently produce ambiguous responses, while tactile images reveal clear structural differences in presence, diameter, depth, crossings, and multiplicity. Yet accurate force tracking remains essential for maintaining safe, consistent contact during physiotherapeutic interaction. Preliminary results show that combining tactile and force modalities enables robust subsurface feature detection and controlled robotic palpation.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-24T06:35:48+00:00",
      "updated": "2025-12-24T06:35:48+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20992v1",
      "file": "papers/2512.20992v1.pdf"
    },
    {
      "arxiv_id": "2512.20951v1",
      "title": "From Human Bias to Robot Choice: How Occupational Contexts and Racial Priming Shape Robot Selection",
      "authors": [
        {
          "name": "Jiangen He"
        },
        {
          "name": "Wanqi Zhang"
        },
        {
          "name": "Jessica Barfield"
        }
      ],
      "abstract": "As artificial agents increasingly integrate into professional environments, fundamental questions have emerged about how societal biases influence human-robot selection decisions. We conducted two comprehensive experiments (N = 1,038) examining how occupational contexts and stereotype activation shape robotic agent choices across construction, healthcare, educational, and athletic domains. Participants made selections from artificial agents that varied systematically in skin tone and anthropomorphic characteristics. Our study revealed distinct context-dependent patterns. Healthcare and educational scenarios demonstrated strong favoritism toward lighter-skinned artificial agents, while construction and athletic contexts showed greater acceptance of darker-toned alternatives. Participant race was associated with systematic differences in selection patterns across professional domains. The second experiment demonstrated that exposure to human professionals from specific racial backgrounds systematically shifted later robotic agent preferences in stereotype-consistent directions. These findings show that occupational biases and color-based discrimination transfer directly from human-human to human-robot evaluation contexts. The results highlight mechanisms through which robotic deployment may unintentionally perpetuate existing social inequalities.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "published": "2025-12-24T05:15:26+00:00",
      "updated": "2025-12-24T05:15:26+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20951v1",
      "file": "papers/2512.20951v1.pdf"
    },
    {
      "arxiv_id": "2512.20940v1",
      "title": "ETP-R1: Evolving Topological Planning with Reinforcement Fine-tuning for Vision-Language Navigation in Continuous Environments",
      "authors": [
        {
          "name": "Shuhao Ye"
        },
        {
          "name": "Sitong Mao"
        },
        {
          "name": "Yuxiang Cui"
        },
        {
          "name": "Xuan Yu"
        },
        {
          "name": "Shichao Zhai"
        },
        {
          "name": "Wen Chen"
        },
        {
          "name": "Shunbo Zhou"
        },
        {
          "name": "Rong Xiong"
        },
        {
          "name": "Yue Wang"
        }
      ],
      "abstract": "Vision-Language Navigation in Continuous Environments (VLN-CE) requires an embodied agent to navigate towards target in continuous environments, following natural language instructions. While current graph-based methods offer an efficient, structured approach by abstracting the environment into a topological map and simplifying the action space to waypoint selection, they lag behind methods based on Large Vision-Language Models (LVLMs) in leveraging large-scale data and advanced training paradigms. In this paper, we try to bridge this gap by introducing ETP-R1, a framework that applies the paradigm of scaling up data and Reinforcement Fine-Tuning (RFT) to a graph-based VLN-CE model. To build a strong foundation, we first construct a high-quality, large-scale pretraining dataset using the Gemini API. This dataset consists of diverse, low-hallucination instructions for topological trajectories, providing rich supervision for our graph-based policy to map language to topological paths. This foundation is further strengthened by unifying data from both R2R and RxR tasks for joint pretraining. Building on this, we introduce a three-stage training paradigm, which culminates in the first application of closed-loop, online RFT to a graph-based VLN-CE model, powered by the Group Relative Policy Optimization (GRPO) algorithm. Extensive experiments demonstrate that our approach is highly effective, establishing new state-of-the-art performance across all major metrics on both the R2R-CE and RxR-CE benchmarks. Our code is available at https://github.com/Cepillar/ETP-R1.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-24T04:53:03+00:00",
      "updated": "2025-12-24T04:53:03+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20940v1",
      "file": "papers/2512.20940v1.pdf"
    },
    {
      "arxiv_id": "2512.20931v1",
      "title": "Certifiable Alignment of GNSS and Local Frames via Lagrangian Duality",
      "authors": [
        {
          "name": "Baoshan Song"
        },
        {
          "name": "Matthew Giamou"
        },
        {
          "name": "Penggao Yan"
        },
        {
          "name": "Chunxi Xia"
        },
        {
          "name": "Li-Ta Hsu"
        }
      ],
      "abstract": "Estimating the absolute orientation of a local system relative to a global navigation satellite system (GNSS) reference often suffers from local minima and high dependency on satellite availability. Existing methods for this alignment task rely on abundant satellites unavailable in GNSS-degraded environments, or use local optimization methods which cannot guarantee the optimality of a solution. This work introduces a globally optimal solver that transforms raw pseudo-range or Doppler measurements into a convexly relaxed problem. The proposed method is certifiable, meaning it can numerically verify the correctness of the result, filling a gap where existing local optimizers fail. We first formulate the original frame alignment problem as a nonconvex quadratically constrained quadratic program (QCQP) problem and relax the QCQP problem to a concave Lagrangian dual problem that provides a lower cost bound for the original problem. Then we perform relaxation tightness and observability analysis to derive criteria for certifiable optimality of the solution. Finally, simulation and real world experiments are conducted to evaluate the proposed method. The experiments show that our method provides certifiably optimal solutions even with only 2 satellites with Doppler measurements and 2D vehicle motion, while the traditional velocity-based VOBA method and the advanced GVINS alignment technique may fail or converge to local optima without notice. To support the development of GNSS-based navigation techniques in robotics, all code and data are open-sourced at https://github.com/Baoshan-Song/Certifiable-Doppler-alignment.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-24T04:24:33+00:00",
      "updated": "2025-12-24T04:24:33+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20931v1",
      "file": "papers/2512.20931v1.pdf"
    },
    {
      "arxiv_id": "2512.20888v1",
      "title": "Stretchable and High-Precision Optical Tactile Sensor for Trajectory Tracking of Parallel Mechanisms",
      "authors": [
        {
          "name": "Yiding Nie"
        },
        {
          "name": "Dongliang Fan"
        },
        {
          "name": "Jiatai Huang"
        },
        {
          "name": "Chunyu Liu"
        },
        {
          "name": "Jian S. Dai"
        }
      ],
      "abstract": "Stretchable sensors indicate promising prospects for soft robotics, medical devices, and human-machine interactions due to the high compliance of soft materials. Discrete sensing strategies, including sensor arrays and distributed sensors, are broadly involved in tactile sensors across versatile applications. However, it remains a challenge to achieve high spatial resolution with self-decoupled capacity and insensitivity to other off-axis stimuli for stretchable tactile sensors. Herein, we develop a stretchable tactile sensor based on the proposed continuous spectral-filtering principle, allowing superhigh resolution for applied stimuli. This proposed sensor enables a high-linear spatial response (0.996) even during stretching and bending, and high continuous spatial (7 μm) and force (5 mN) resolutions with design scalability and interaction robustness to survive piercing and cutting. We further demonstrate the sensors' performance by integrating them into a planar parallel mechanism for precise trajectory tracking (rotational resolution: 0.02°) in real time.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-24T02:13:16+00:00",
      "updated": "2025-12-24T02:13:16+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20888v1",
      "file": "papers/2512.20888v1.pdf"
    },
    {
      "arxiv_id": "2512.20876v1",
      "title": "Proprioception Enhances Vision Language Model in Generating Captions and Subtask Segmentations for Robot Task",
      "authors": [
        {
          "name": "Kanata Suzuki"
        },
        {
          "name": "Shota Shimizu"
        },
        {
          "name": "Tetsuya Ogata"
        }
      ],
      "abstract": "From the perspective of future developments in robotics, it is crucial to verify whether foundation models trained exclusively on offline data, such as images and language, can understand the robot motion. In particular, since Vision Language Models (VLMs) do not include low-level motion information from robots in their training datasets, video understanding including trajectory information remains a significant challenge. In this study, we assess two capabilities of VLMs through a video captioning task with low-level robot motion information: (1) automatic captioning of robot tasks and (2) segmentation of a series of tasks. Both capabilities are expected to enhance the efficiency of robot imitation learning by linking language and motion and serve as a measure of the foundation model's performance. The proposed method generates multiple \"scene\" captions using image captions and trajectory data from robot tasks. The full task caption is then generated by summarizing these individual captions. Additionally, the method performs subtask segmentation by comparing the similarity between text embeddings of image captions. In both captioning tasks, the proposed method aims to improve performance by providing the robot's motion data - joint and end-effector states - as input to the VLM. Simulator experiments were conducted to validate the effectiveness of the proposed method.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-24T01:36:12+00:00",
      "updated": "2025-12-24T01:36:12+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20876v1",
      "file": "papers/2512.20876v1.pdf"
    },
    {
      "arxiv_id": "2512.20847v1",
      "title": "YCB-Handovers Dataset: Analyzing Object Weight Impact on Human Handovers to Adapt Robotic Handover Motion",
      "authors": [
        {
          "name": "Parag Khanna"
        },
        {
          "name": "Karen Jane Dsouza"
        },
        {
          "name": "Chunyu Wang"
        },
        {
          "name": "Mårten Björkman"
        },
        {
          "name": "Christian Smith"
        }
      ],
      "abstract": "This paper introduces the YCB-Handovers dataset, capturing motion data of 2771 human-human handovers with varying object weights. The dataset aims to bridge a gap in human-robot collaboration research, providing insights into the impact of object weight in human handovers and readiness cues for intuitive robotic motion planning. The underlying dataset for object recognition and tracking is the YCB (Yale-CMU-Berkeley) dataset, which is an established standard dataset used in algorithms for robotic manipulation, including grasping and carrying objects. The YCB-Handovers dataset incorporates human motion patterns in handovers, making it applicable for data-driven, human-inspired models aimed at weight-sensitive motion planning and adaptive robotic behaviors. This dataset covers an extensive range of weights, allowing for a more robust study of handover behavior and weight variation. Some objects also require careful handovers, highlighting contrasts with standard handovers. We also provide a detailed analysis of the object's weight impact on the human reaching motion in these handovers.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "published": "2025-12-23T23:50:55+00:00",
      "updated": "2025-12-23T23:50:55+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20847v1",
      "file": "papers/2512.20847v1.pdf"
    },
    {
      "arxiv_id": "2512.20778v1",
      "title": "Towards Optimal Performance and Action Consistency Guarantees in Dec-POMDPs with Inconsistent Beliefs and Limited Communication",
      "authors": [
        {
          "name": "Moshe Rafaeli Shimron"
        },
        {
          "name": "Vadim Indelman"
        }
      ],
      "abstract": "Multi-agent decision-making under uncertainty is fundamental for effective and safe autonomous operation. In many real-world scenarios, each agent maintains its own belief over the environment and must plan actions accordingly. However, most existing approaches assume that all agents have identical beliefs at planning time, implying these beliefs are conditioned on the same data. Such an assumption is often impractical due to limited communication. In reality, agents frequently operate with inconsistent beliefs, which can lead to poor coordination and suboptimal, potentially unsafe, performance. In this paper, we address this critical challenge by introducing a novel decentralized framework for optimal joint action selection that explicitly accounts for belief inconsistencies. Our approach provides probabilistic guarantees for both action consistency and performance with respect to open-loop multi-agent POMDP (which assumes all data is always communicated), and selectively triggers communication only when needed. Furthermore, we address another key aspect of whether, given a chosen joint action, the agents should share data to improve expected performance in inference. Simulation results show our approach outperforms state-of-the-art algorithms.",
      "primary_category": "cs.MA",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.RO"
      ],
      "published": "2025-12-23T21:25:53+00:00",
      "updated": "2025-12-23T21:25:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20778v1",
      "file": "papers/2512.20778v1.pdf"
    },
    {
      "arxiv_id": "2512.20769v1",
      "title": "A General Purpose Method for Robotic Interception of Non-Cooperative Dynamic Targets",
      "authors": [
        {
          "name": "Tanmay P. Patel"
        },
        {
          "name": "Erica L. Tevere"
        },
        {
          "name": "Erik H. Kramer"
        },
        {
          "name": "Rudranarayan M. Mukherjee"
        }
      ],
      "abstract": "This paper presents a general purpose framework for autonomous, vision-based interception of dynamic, non-cooperative targets, validated across three distinct mobility platforms: an unmanned aerial vehicle (UAV), a four-wheeled ground rover, and an air-thruster spacecraft testbed. The approach relies solely on a monocular camera with fiducials for target tracking and operates entirely in the local observer frame without the need for global information. The core contribution of this work is a streamlined and general approach to autonomous interception that can be adapted across robots with varying dynamics, as well as our comprehensive study of the robot interception problem across heterogenous mobility systems under limited observability and no global localization. Our method integrates (1) an Extended Kalman Filter for relative pose estimation amid intermittent measurements, (2) a history-conditioned motion predictor for dynamic target trajectory propagation, and (3) a receding-horizon planner solving a constrained convex program in real time to ensure time-efficient and kinematically feasible interception paths. Our operating regime assumes that observability is restricted by partial fields of view, sensor dropouts, and target occlusions. Experiments are performed in these conditions and include autonomous UAV landing on dynamic targets, rover rendezvous and leader-follower tasks, and spacecraft proximity operations. Results from simulated and physical experiments demonstrate robust performance with low interception errors (both during station-keeping and upon scenario completion), high success rates under deterministic and stochastic target motion profiles, and real-time execution on embedded processors such as the Jetson Orin, VOXL2, and Raspberry Pi 5. These results highlight the framework's generalizability, robustness, and computational efficiency.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-23T21:14:03+00:00",
      "updated": "2025-12-23T21:14:03+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20769v1",
      "file": "papers/2512.20769v1.pdf"
    },
    {
      "arxiv_id": "2512.20748v1",
      "title": "Fixed-time control with prescribed performance for path following of underwater gliders",
      "authors": [
        {
          "name": "Hanzhi Yang"
        },
        {
          "name": "Nina Mahmoudian"
        }
      ],
      "abstract": "Underwater gliders are increasingly deployed in challenging missions - such as hurricane-season observations and long-endurance environmental monitoring - where strong currents and turbulence pose significant risks to navigation safety. To address these practical challenges, this paper presents a fixed-time prescribed performance control scheme for the 3D path following of underwater gliders subject to model uncertainties and environmental disturbances. The primary contribution is the integration of a finite-time performance function within a fixed-time control framework. This synthesis ensures that the tracking errors are constrained within prescribed performance bounds and converge to a compact set within a fixed time, independent of initial conditions. A second key contribution is the development of a fixed-time sliding mode disturbance observer that provides accurate finite-time estimation of lumped disturbances, enhancing the system's robustness. Integrated with an iLOS guidance law, the proposed controller enables precise and safe waypoint following. Numerical simulations demonstrate that the proposed method outperforms conventional sliding mode and prescribed performance controllers in tracking accuracy, convergence speed, and control effort smoothness, validating its efficacy for robust underwater navigation.",
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY",
        "cs.RO",
        "math.OC"
      ],
      "published": "2025-12-23T20:11:25+00:00",
      "updated": "2025-12-23T20:11:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20748v1",
      "file": "papers/2512.20748v1.pdf"
    },
    {
      "arxiv_id": "2512.20711v1",
      "title": "Anytime Metaheuristic Framework for Global Route Optimization in Expected-Time Mobile Search",
      "authors": [
        {
          "name": "Jan Mikula"
        },
        {
          "name": "Miroslav Kulich"
        }
      ],
      "abstract": "Expected-time mobile search (ETS) is a fundamental robotics task where a mobile sensor navigates an environment to minimize the expected time required to locate a hidden object. Global route optimization for ETS in static 2D continuous environments remains largely underexplored due to the intractability of objective evaluation, stemming from the continuous nature of the environment and the interplay of motion and visibility constraints. Prior work has addressed this through partial discretization, leading to discrete-sensing formulations tackled via utility-greedy heuristics. Others have taken an indirect approach by heuristically approximating the objective using minimum latency problems on fixed graphs, enabling global route optimization via efficient metaheuristics. This paper builds on and significantly extends the latter by introducing Milaps (Minimum latency problems), a model-based solution framework for ETS. Milaps integrates novel auxiliary objectives and adapts a recent anytime metaheuristic for the traveling deliveryman problem, chosen for its strong performance under tight runtime constraints. Evaluations on a novel large-scale dataset demonstrate superior trade-offs between solution quality and runtime compared to state-of-the-art baselines. The best-performing strategy rapidly generates a preliminary solution, assigns static weights to sensing configurations, and optimizes global costs metaheuristically. Additionally, a qualitative study highlights the framework's flexibility across diverse scenarios.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-23T19:19:27+00:00",
      "updated": "2025-12-23T19:19:27+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20711v1",
      "file": "papers/2512.20711v1.pdf"
    },
    {
      "arxiv_id": "2512.20591v1",
      "title": "LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing",
      "authors": [
        {
          "name": "Changyi Lin"
        },
        {
          "name": "Boda Huo"
        },
        {
          "name": "Mingyang Yu"
        },
        {
          "name": "Emily Ruppel"
        },
        {
          "name": "Bingqing Chen"
        },
        {
          "name": "Jonathan Francis"
        },
        {
          "name": "Ding Zhao"
        }
      ],
      "abstract": "Contact often occurs without macroscopic surface deformation, such as during interaction with liquids, semi-liquids, or ultra-soft materials. Most existing tactile sensors rely on deformation to infer contact, making such light-contact interactions difficult to perceive robustly. To address this, we present LightTact, a visual-tactile fingertip sensor that makes contact directly visible via a deformation-independent, optics-based principle. LightTact uses an ambient-blocking optical configuration that suppresses both external light and internal illumination at non-contact regions, while transmitting only the diffuse light generated at true contacts. As a result, LightTact produces high-contrast raw images in which non-contact pixels remain near-black (mean gray value < 3) and contact pixels preserve the natural appearance of the contacting surface. Built on this, LightTact achieves accurate pixel-level contact segmentation that is robust to material properties, contact force, surface appearance, and environmental lighting. We further integrate LightTact on a robotic arm and demonstrate manipulation behaviors driven by extremely light contact, including water spreading, facial-cream dipping, and thin-film interaction. Finally, we show that LightTact's spatially aligned visual-tactile images can be directly interpreted by existing vision-language models, enabling resistor value reasoning for robotic sorting.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-23T18:38:25+00:00",
      "updated": "2025-12-23T18:38:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20591v1",
      "file": "papers/2512.20591v1.pdf"
    },
    {
      "arxiv_id": "2512.20475v1",
      "title": "Drift-Corrected Monocular VIO and Perception-Aware Planning for Autonomous Drone Racing",
      "authors": [
        {
          "name": "Maulana Bisyir Azhari"
        },
        {
          "name": "Donghun Han"
        },
        {
          "name": "Je In You"
        },
        {
          "name": "Sungjun Park"
        },
        {
          "name": "David Hyunchul Shim"
        }
      ],
      "abstract": "The Abu Dhabi Autonomous Racing League(A2RL) x Drone Champions League competition(DCL) requires teams to perform high-speed autonomous drone racing using only a single camera and a low-quality inertial measurement unit -- a minimal sensor set that mirrors expert human drone racing pilots. This sensor limitation makes the system susceptible to drift from Visual-Inertial Odometry (VIO), particularly during long and fast flights with aggressive maneuvers. This paper presents the system developed for the championship, which achieved a competitive performance. Our approach corrected VIO drift by fusing its output with global position measurements derived from a YOLO-based gate detector using a Kalman filter. A perception-aware planner generated trajectories that balance speed with the need to keep gates visible for the perception system. The system demonstrated high performance, securing podium finishes across multiple categories: third place in the AI Grand Challenge with top speed of 43.2 km/h, second place in the AI Drag Race with over 59 km/h, and second place in the AI Multi-Drone Race. We detail the complete architecture and present a performance analysis based on experimental data from the competition, contributing our insights on building a successful system for monocular vision-based autonomous drone flight.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-23T16:12:10+00:00",
      "updated": "2025-12-23T16:12:10+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20475v1",
      "file": "papers/2512.20475v1.pdf"
    },
    {
      "arxiv_id": "2512.20391v1",
      "title": "Contingency Model-based Control (CMC) for Communicationless Cooperative Collision Avoidance in Robot Swarms",
      "authors": [
        {
          "name": "Georg Schildbach"
        }
      ],
      "abstract": "Cooperative collision avoidance between robots in swarm operations remains an open challenge. Assuming a decentralized architecture, each robot is responsible for making its own control decisions, including motion planning. To this end, most existing approaches mostly rely some form of (wireless) communication between the agents of the swarm. In reality, however, communication is brittle. It may be affected by latency, further delays and packet losses, transmission faults, and is subject to adversarial attacks, such as jamming or spoofing. This paper proposes Contingency Model-based Control (CMC) as a communicationless alternative. It follows the implicit cooperation paradigm, under which the design of the robots is based on consensual (offline) rules, similar to traffic rules. They include the definition of a contingency trajectory for each robot, and a method for construction of mutual collision avoidance constraints. The setup is shown to guarantee the recursive feasibility and collision avoidance between all swarm members in closed-loop operation. Moreover, CMC naturally satisfies the Plug \\& Play paradigm, i.e., for new robots entering the swarm. Two numerical examples demonstrate that the collision avoidance guarantee is intact and that the robot swarm operates smoothly under the CMC regime.",
      "primary_category": "math.OC",
      "categories": [
        "math.OC",
        "cs.RO",
        "eess.SY"
      ],
      "published": "2025-12-23T14:28:42+00:00",
      "updated": "2025-12-23T14:28:42+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20391v1",
      "file": "papers/2512.20391v1.pdf"
    },
    {
      "arxiv_id": "2512.20355v1",
      "title": "FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration",
      "authors": [
        {
          "name": "Hao Wei"
        },
        {
          "name": "Peiji Wang"
        },
        {
          "name": "Qianhao Wang"
        },
        {
          "name": "Tong Qin"
        },
        {
          "name": "Fei Gao"
        },
        {
          "name": "Yulin Si"
        }
      ],
      "abstract": "Underwater environments impose severe challenges to visual-inertial odometry systems, as strong light attenuation, marine snow and turbidity, together with weakly exciting motions, degrade inertial observability and cause frequent tracking failures over long-term operation. While tightly coupled acoustic-visual-inertial fusion, typically implemented through an acoustic Doppler Velocity Log (DVL) integrated with visual-inertial measurements, can provide accurate state estimation, the associated graph-based optimization is often computationally prohibitive for real-time deployment on resource-constrained platforms. Here we present FAR-AVIO, a Schur-Complement based, tightly coupled acoustic-visual-inertial odometry framework tailored for underwater robots. FAR-AVIO embeds a Schur complement formulation into an Extended Kalman Filter(EKF), enabling joint pose-landmark optimization for accuracy while maintaining constant-time updates by efficiently marginalizing landmark states. On top of this backbone, we introduce Adaptive Weight Adjustment and Reliability Evaluation(AWARE), an online sensor health module that continuously assesses the reliability of visual, inertial and DVL measurements and adaptively regulates their sigma weights, and we develop an efficient online calibration scheme that jointly estimates DVL-IMU extrinsics, without dedicated calibration manoeuvres. Numerical simulations and real-world underwater experiments consistently show that FAR-AVIO outperforms state-of-the-art underwater SLAM baselines in both localization accuracy and computational efficiency, enabling robust operation on low-power embedded platforms. Our implementation has been released as open source software at https://far-vido.gitbook.io/far-vido-docs.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-23T13:36:47+00:00",
      "updated": "2025-12-23T13:36:47+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20355v1",
      "file": "papers/2512.20355v1.pdf"
    },
    {
      "arxiv_id": "2512.20342v1",
      "title": "Design and Modeling of a Simple-Structured Continuously Variable Transmission Utilizing Shape Memory Alloy Superelasticity for Twisted String Actuator",
      "authors": [
        {
          "name": "Chanchan Xu"
        },
        {
          "name": "Shuai Dong"
        },
        {
          "name": "Xiaojie Wang"
        }
      ],
      "abstract": "Twisted String Actuators (TSAs) are widely used in robotics but suffer from a limited range of Transmission Ratio (TR) variation, restricting their efficiency under varying loads.To overcome this, we propose a novel lightweight, simple-structured Continuously Variable Transmission (CVT) mechanism for TSA utilizing Shape Memory Alloy (SMA) superelasticity. The CVT mechanism consists solely of a pair of highly lightweight superelastic SMA rods connecting the ends of twisted strings. These rods deform under external loads, adjusting the inter-string distance to enable continuous TR variation.We develop a comprehensive theoretical model that integrates three critical nonlinearities",
      "primary_category": "physics.ins-det",
      "categories": [
        "physics.ins-det",
        "cs.RO"
      ],
      "published": "2025-12-23T13:21:17+00:00",
      "updated": "2025-12-23T13:21:17+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20342v1",
      "file": "papers/2512.20342v1.pdf"
    },
    {
      "arxiv_id": "2512.20322v1",
      "title": "Pneumatic bladder links with wide range of motion joints for articulated inflatable robots",
      "authors": [
        {
          "name": "Katsu Uchiyama"
        },
        {
          "name": "Ryuma Niiyama"
        }
      ],
      "abstract": "Exploration of various applications is the frontier of research on inflatable robots. We proposed an articulated robots consisting of multiple pneumatic bladder links connected by rolling contact joints called Hillberry joints. The bladder link is made of a double-layered structure of tarpaulin sheet and polyurethane sheet, which is both airtight and flexible in shape. The integration of the Hilberry joint into an inflatable robot is also a new approach. The rolling contact joint allows wide range of motion of $\\pm 150 ^{\\circ}$, the largest among the conventional inflatable joints. Using the proposed mechanism for inflatable robots, we demonstrated moving a 500 g payload with a 3-DoF arm and lifting 3.4 kg and 5 kg payloads with 2-DoF and 1-DoF arms, respectively. We also experimented with a single 3-DoF inflatable leg attached to a dolly to show that the proposed structure worked for legged locomotion.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-23T12:44:01+00:00",
      "updated": "2025-12-23T12:44:01+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20322v1",
      "file": "papers/2512.20322v1.pdf"
    },
    {
      "arxiv_id": "2512.20276v1",
      "title": "ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge",
      "authors": [
        {
          "name": "Yuntao Dai"
        },
        {
          "name": "Hang Gu"
        },
        {
          "name": "Teng Wang"
        },
        {
          "name": "Qianyu Cheng"
        },
        {
          "name": "Yifei Zheng"
        },
        {
          "name": "Zhiyong Qiu"
        },
        {
          "name": "Lei Gong"
        },
        {
          "name": "Wenqi Lou"
        },
        {
          "name": "Xuehai Zhou"
        }
      ],
      "abstract": "Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "published": "2025-12-23T11:29:03+00:00",
      "updated": "2025-12-23T11:29:03+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20276v1",
      "file": "papers/2512.20276v1.pdf"
    },
    {
      "arxiv_id": "2512.20229v1",
      "title": "Finite-Time Control Based on Differential Flatness for Wheeled Mobile Robots with Experimental Validation",
      "authors": [
        {
          "name": "Imtiaz Ur Rehman"
        },
        {
          "name": "Moussa Labbadi"
        },
        {
          "name": "Amine Abadi"
        },
        {
          "name": "Lew Lew Yan Voon"
        }
      ],
      "abstract": "A robust tracking control strategy is designed to empower wheeled mobile robots (WMRs) to track predetermined routes while operating in diverse fields and encountering disturbances like strong winds or uneven path conditions, which affect tracking performance. Ensuring the applicability of this tracking method in real-world scenarios is essential. To accomplish this, the WMR model is initially transformed into a linear canonical form by leveraging the differential flatness of its kinematic model, facilitating controller design. Subsequently, a novel integral nonlinear hyperplane-based sliding mode control (INH-SMC) technique is proposed for WMR under disturbances. The stability of the technique is analyzed and verified. Finally, its practical viability is demonstrated through a comparative real-world indoor experiment on a TurtleBot3 WMR subjected to disturbances, confirming the feasibility and efficacy of the proposed approach.",
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY",
        "cs.RO"
      ],
      "published": "2025-12-23T10:41:04+00:00",
      "updated": "2025-12-23T10:41:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20229v1",
      "file": "papers/2512.20229v1.pdf"
    },
    {
      "arxiv_id": "2512.20188v1",
      "title": "Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation",
      "authors": [
        {
          "name": "Teqiang Zou"
        },
        {
          "name": "Hongliang Zeng"
        },
        {
          "name": "Yuxuan Nong"
        },
        {
          "name": "Yifan Li"
        },
        {
          "name": "Kehui Liu"
        },
        {
          "name": "Haotian Yang"
        },
        {
          "name": "Xinyang Ling"
        },
        {
          "name": "Xin Li"
        },
        {
          "name": "Lianyang Ma"
        }
      ],
      "abstract": "Most Vision-Language-Action (VLA) systems integrate a Vision-Language Model (VLM) for semantic reasoning with an action expert generating continuous action signals, yet both typically run at a single unified frequency. As a result, policy performance is constrained by the low inference speed of large VLMs. This mandatory synchronous execution severely limits control stability and real-time performance in whole-body robotic manipulation, which involves more joints, larger motion spaces, and dynamically changing views. We introduce a truly asynchronous Fast-Slow VLA framework (DuoCore-FS), organizing the system into a fast pathway for high-frequency action generation and a slow pathway for rich VLM reasoning. The system is characterized by two key features. First, a latent representation buffer bridges the slow and fast systems. It stores instruction semantics and action-reasoning representation aligned with the scene-instruction context, providing high-level guidance to the fast pathway. Second, a whole-body action tokenizer provides a compact, unified representation of whole-body actions. Importantly, the VLM and action expert are still jointly trained end-to-end, preserving unified policy learning while enabling asynchronous execution. DuoCore-FS supports a 3B-parameter VLM while achieving 30 Hz whole-body action-chunk generation, approximately three times as fast as prior VLA models with comparable model sizes. Real-world whole-body manipulation experiments demonstrate improved task success rates and significantly enhanced responsiveness compared to synchronous Fast-Slow VLA baselines. The implementation of DuoCore-FS, including training, inference, and deployment, is provided to commercial users by Astribot as part of the Astribot robotic platform.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2025-12-23T09:28:20+00:00",
      "updated": "2025-12-23T09:28:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20188v1",
      "file": "papers/2512.20188v1.pdf"
    },
    {
      "arxiv_id": "2512.20166v1",
      "title": "LoLA: Long Horizon Latent Action Learning for General Robot Manipulation",
      "authors": [
        {
          "name": "Xiaofan Wang"
        },
        {
          "name": "Xingyu Gao"
        },
        {
          "name": "Jianlong Fu"
        },
        {
          "name": "Zuolei Li"
        },
        {
          "name": "Dean Fortier"
        },
        {
          "name": "Galen Mullins"
        },
        {
          "name": "Andrey Kolobov"
        },
        {
          "name": "Baining Guo"
        }
      ],
      "abstract": "The capability of performing long-horizon, language-guided robotic manipulation tasks critically relies on leveraging historical information and generating coherent action sequences. However, such capabilities are often overlooked by existing Vision-Language-Action (VLA) models. To solve this challenge, we propose LoLA (Long Horizon Latent Action Learning), a framework designed for robot manipulation that integrates long-term multi-view observations and robot proprioception to enable multi-step reasoning and action generation. We first employ Vision-Language Models to encode rich contextual features from historical sequences and multi-view observations. We further introduces a key module, State-Aware Latent Re-representation, which transforms visual inputs and language commands into actionable robot motion space. Unlike existing VLA approaches that merely concatenate robot proprioception (e.g., joint angles) with VL embeddings, this module leverages such robot states to explicitly ground VL representations in physical scale through a learnable \"embodiment-anchored\" latent space. We trained LoLA on diverse robotic pre-training datasets and conducted extensive evaluations on simulation benchmarks (SIMPLER and LIBERO), as well as two real-world tasks on Franka and Bi-Manual Aloha robots. Results show that LoLA significantly outperforms prior state-of-the-art methods (e.g., pi0), particularly in long-horizon manipulation tasks.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-23T08:45:24+00:00",
      "updated": "2025-12-23T08:45:24+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20166v1",
      "file": "papers/2512.20166v1.pdf"
    },
    {
      "arxiv_id": "2512.20083v1",
      "title": "Detecting Non-Optimal Decisions of Embodied Agents via Diversity-Guided Metamorphic Testing",
      "authors": [
        {
          "name": "Wenzhao Wu"
        },
        {
          "name": "Yahui Tang"
        },
        {
          "name": "Mingfei Cheng"
        },
        {
          "name": "Wenbing Tang"
        },
        {
          "name": "Yuan Zhou"
        },
        {
          "name": "Yang Liu"
        }
      ],
      "abstract": "As embodied agents advance toward real-world deployment, ensuring optimal decisions becomes critical for resource-constrained applications. Current evaluation methods focus primarily on functional correctness, overlooking the non-functional optimality of generated plans. This gap can lead to significant performance degradation and resource waste. We identify and formalize the problem of Non-optimal Decisions (NoDs), where agents complete tasks successfully but inefficiently. We present NoD-DGMT, a systematic framework for detecting NoDs in embodied agent task planning via diversity-guided metamorphic testing. Our key insight is that optimal planners should exhibit invariant behavioral properties under specific transformations. We design four novel metamorphic relations capturing fundamental optimality properties: position detour suboptimality, action optimality completeness, condition refinement monotonicity, and scene perturbation invariance. To maximize detection efficiency, we introduce a diversity-guided selection strategy that actively selects test cases exploring different violation categories, avoiding redundant evaluations while ensuring comprehensive diversity coverage. Extensive experiments on the AI2-THOR simulator with four state-of-the-art planning models demonstrate that NoD-DGMT achieves violation detection rates of 31.9% on average, with our diversity-guided filter improving rates by 4.3% and diversity scores by 3.3 on average. NoD-DGMT significantly outperforms six baseline methods, with 16.8% relative improvement over the best baseline, and demonstrates consistent superiority across different model architectures and task complexities.",
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.RO"
      ],
      "published": "2025-12-23T06:27:18+00:00",
      "updated": "2025-12-23T06:27:18+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20083v1",
      "file": "papers/2512.20083v1.pdf"
    },
    {
      "arxiv_id": "2512.20052v1",
      "title": "Learning Skills from Action-Free Videos",
      "authors": [
        {
          "name": "Hung-Chieh Fang"
        },
        {
          "name": "Kuo-Han Hung"
        },
        {
          "name": "Chu-Rong Chen"
        },
        {
          "name": "Po-Jung Chou"
        },
        {
          "name": "Chun-Kai Yang"
        },
        {
          "name": "Po-Chen Ko"
        },
        {
          "name": "Yu-Chiang Wang"
        },
        {
          "name": "Yueh-Hua Wu"
        },
        {
          "name": "Min-Hung Chen"
        },
        {
          "name": "Shao-Hua Sun"
        }
      ],
      "abstract": "Learning from videos offers a promising path toward generalist robots by providing rich visual and temporal priors beyond what real robot datasets contain. While existing video generative models produce impressive visual predictions, they are difficult to translate into low-level actions. Conversely, latent-action models better align videos with actions, but they typically operate at the single-step level and lack high-level planning capabilities. We bridge this gap by introducing Skill Abstraction from Optical Flow (SOF), a framework that learns latent skills from large collections of action-free videos. Our key idea is to learn a latent skill space through an intermediate representation based on optical flow that captures motion information aligned with both video dynamics and robot actions. By learning skills in this flow-based latent space, SOF enables high-level planning over video-derived skills and allows for easier translation of these skills into actions. Experiments show that our approach consistently improves performance in both multitask and long-horizon settings, demonstrating the ability to acquire and compose skills directly from raw visual data.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "published": "2025-12-23T05:03:33+00:00",
      "updated": "2025-12-23T05:03:33+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20052v1",
      "file": "papers/2512.20052v1.pdf"
    },
    {
      "arxiv_id": "2512.20014v1",
      "title": "Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting",
      "authors": [
        {
          "name": "Sangoh Lee"
        },
        {
          "name": "Sangwoo Mo"
        },
        {
          "name": "Wook-Shin Han"
        }
      ],
      "abstract": "While Vision-Language-Action (VLA) models generalize well to generic instructions, they struggle with personalized commands such as \"bring my cup\", where the robot must act on one specific instance among visually similar objects. We study this setting of manipulating personal objects, in which a VLA must identify and control a user-specific object unseen during training using only a few reference images. To address this challenge, we propose Visual Attentive Prompting (VAP), a simple-yet-effective training-free perceptual adapter that equips frozen VLAs with top-down selective attention. VAP treats the reference images as a non-parametric visual memory, grounds the personal object in the scene through open-vocabulary detection and embedding-based matching, and then injects this grounding as a visual prompt by highlighting the object and rewriting the instruction. We construct two simulation benchmarks, Personalized-SIMPLER and Personalized-VLABench, and a real-world tabletop benchmark to evaluate personalized manipulation across multiple robots and tasks. Experiments show that VAP consistently outperforms generic policies and token-learning baselines in both success rate and correct-object manipulation, helping to bridge the gap between semantic understanding and instance-level control.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2025-12-23T03:13:39+00:00",
      "updated": "2025-12-23T03:13:39+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20014v1",
      "file": "papers/2512.20014v1.pdf"
    },
    {
      "arxiv_id": "2512.19914v1",
      "title": "A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones",
      "authors": [
        {
          "name": "Sujan Warnakulasooriya"
        },
        {
          "name": "Andreas Willig"
        },
        {
          "name": "Xiaobing Wu"
        }
      ],
      "abstract": "Drone applications continue to expand across various domains, with flocking offering enhanced cooperative capabilities but introducing significant challenges during initial formation. Existing flocking algorithms often struggle with efficiency and scalability, particularly when potential collisions force drones into suboptimal trajectories. This paper presents a time-efficient prioritised scheduling algorithm that improves the initial formation process of drone flocks. The method assigns each drone a priority based on its number of potential collisions and its likelihood of reaching its target position without permanently obstructing other drones. Using this hierarchy, each drone computes an appropriate delay to ensure a collision-free path. Simulation results show that the proposed algorithm successfully generates collision-free trajectories for flocks of up to 5000 drones and outperforms the coupling-degree-based heuristic prioritised planning method (CDH-PP) in both performance and computational efficiency.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "published": "2025-12-22T22:37:58+00:00",
      "updated": "2025-12-22T22:37:58+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19914v1",
      "file": "papers/2512.19914v1.pdf"
    },
    {
      "arxiv_id": "2512.19629v2",
      "title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry",
      "authors": [
        {
          "name": "Jiaqi Peng"
        },
        {
          "name": "Wenzhe Cai"
        },
        {
          "name": "Yuqiang Yang"
        },
        {
          "name": "Tai Wang"
        },
        {
          "name": "Yuan Shen"
        },
        {
          "name": "Jiangmiao Pang"
        }
      ],
      "abstract": "Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation. We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the https://steinate.github.io/logoplanner.github.io.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published": "2025-12-22T18:03:08+00:00",
      "updated": "2025-12-23T05:37:16+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19629v2",
      "file": "papers/2512.19629v2.pdf"
    },
    {
      "arxiv_id": "2512.19583v1",
      "title": "Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations",
      "authors": [
        {
          "name": "Yinhuai Wang"
        },
        {
          "name": "Runyi Yu"
        },
        {
          "name": "Hok Wai Tsui"
        },
        {
          "name": "Xiaoyi Lin"
        },
        {
          "name": "Hui Zhang"
        },
        {
          "name": "Qihan Zhao"
        },
        {
          "name": "Ke Fan"
        },
        {
          "name": "Miao Li"
        },
        {
          "name": "Jie Song"
        },
        {
          "name": "Jingbo Wang"
        },
        {
          "name": "Qifeng Chen"
        },
        {
          "name": "Ping Tan"
        }
      ],
      "abstract": "We present a system for learning generalizable hand-object tracking controllers purely from synthetic data, without requiring any human demonstrations. Our approach makes two key contributions: (1) HOP, a Hand-Object Planner, which can synthesize diverse hand-object trajectories; and (2) HOT, a Hand-Object Tracker that bridges synthetic-to-physical transfer through reinforcement learning and interaction imitation learning, delivering a generalizable controller conditioned on target hand-object states. Our method extends to diverse object shapes and hand morphologies. Through extensive evaluations, we show that our approach enables dexterous hands to track challenging, long-horizon sequences including object re-arrangement and agile in-hand reorientation. These results represent a significant step toward scalable foundation controllers for manipulation that can learn entirely from synthetic data, breaking the data bottleneck that has long constrained progress in dexterous manipulation.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.GR"
      ],
      "published": "2025-12-22T17:08:54+00:00",
      "updated": "2025-12-22T17:08:54+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19583v1",
      "file": "papers/2512.19583v1.pdf"
    },
    {
      "arxiv_id": "2512.19567v1",
      "title": "LIMOncello: Revisited IKFoM on the SGal(3) Manifold for Fast LiDAR-Inertial Odometry",
      "authors": [
        {
          "name": "Carlos Pérez-Ruiz"
        },
        {
          "name": "Joan Solà"
        }
      ],
      "abstract": "This work introduces LIMOncello, a tightly coupled LiDAR-Inertial Odometry system that models 6-DoF motion on the $\\mathrm{SGal}(3)$ manifold within an iterated error-state Kalman filter backend. Compared to state representations defined on $\\mathrm{SO}(3)\\times\\mathbb{R}^6$, the use of $\\mathrm{SGal}(3)$ provides a coherent and numerically stable discrete-time propagation model that helps limit drift in low-observability conditions.\n  LIMOncello also includes a lightweight incremental i-Octree mapping backend that enables faster updates and substantially lower memory usage than incremental kd-tree style map structures, without relying on locality-restricted search heuristics. Experiments on multiple real-world datasets show that LIMOncello achieves competitive accuracy while improving robustness in geometrically sparse environments. The system maintains real-time performance with stable memory growth and is released as an extensible open-source implementation at https://github.com/CPerezRuiz335/LIMOncello.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-22T16:50:10+00:00",
      "updated": "2025-12-22T16:50:10+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19567v1",
      "file": "papers/2512.19567v1.pdf"
    },
    {
      "arxiv_id": "2512.19562v1",
      "title": "REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation",
      "authors": [
        {
          "name": "Martin Sedlacek"
        },
        {
          "name": "Pavlo Yefanov"
        },
        {
          "name": "Georgy Ponimatkin"
        },
        {
          "name": "Jai Bardhan"
        },
        {
          "name": "Simon Pilc"
        },
        {
          "name": "Mederic Fourmy"
        },
        {
          "name": "Evangelos Kazakos"
        },
        {
          "name": "Cees G. M. Snoek"
        },
        {
          "name": "Josef Sivic"
        },
        {
          "name": "Vladimir Petrik"
        }
      ],
      "abstract": "Vision-Language-Action (VLA) models empower robots to understand and execute tasks described by natural language instructions. However, a key challenge lies in their ability to generalize beyond the specific environments and conditions they were trained on, which is presently difficult and expensive to evaluate in the real-world. To address this gap, we present REALM, a new simulation environment and benchmark designed to evaluate the generalization capabilities of VLA models, with a specific emphasis on establishing a strong correlation between simulated and real-world performance through high-fidelity visuals and aligned robot control. Our environment offers a suite of 15 perturbation factors, 7 manipulation skills, and more than 3,500 objects. Finally, we establish two task sets that form our benchmark and evaluate the π_{0}, π_{0}-FAST, and GR00T N1.5 VLA models, showing that generalization and robustness remain an open challenge. More broadly, we also show that simulation gives us a valuable proxy for the real-world and allows us to systematically probe for and quantify the weaknesses and failure modes of VLAs. Project page: https://martin-sedlacek.com/realm",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2025-12-22T16:44:23+00:00",
      "updated": "2025-12-22T16:44:23+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19562v1",
      "file": "papers/2512.19562v1.pdf"
    },
    {
      "arxiv_id": "2512.19453v1",
      "title": "MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation",
      "authors": [
        {
          "name": "Zhenglong Guo"
        },
        {
          "name": "Yiming Zhao"
        },
        {
          "name": "Feng Jiang"
        },
        {
          "name": "Heng Jin"
        },
        {
          "name": "Zongbao Feng"
        },
        {
          "name": "Jianbin Zhou"
        },
        {
          "name": "Siyuan Xu"
        }
      ],
      "abstract": "Embodied robotic AI systems designed to manage complex daily tasks rely on a task planner to understand and decompose high-level tasks. While most research focuses on enhancing the task-understanding abilities of LLMs/VLMs through fine-tuning or chain-of-thought prompting, this paper argues that defining the planned skill set is equally crucial. To handle the complexity of daily environments, the skill set should possess a high degree of generalization ability. Empirically, more abstract expressions tend to be more generalizable. Therefore, we propose to abstract the planned result as a set of meta-actions. Each meta-action comprises three components: {move/rotate, end-effector status change, relationship with the environment}. This abstraction replaces human-centric concepts, such as grasping or pushing, with the robot's intrinsic functionalities. As a result, the planned outcomes align seamlessly with the complete range of actions that the robot is capable of performing. Furthermore, to ensure that the LLM/VLM accurately produces the desired meta-action format, we employ the Retrieval-Augmented Generation (RAG) technique, which leverages a database of human-annotated planning demonstrations to facilitate in-context learning. As the system successfully completes more tasks, the database will self-augment to continue supporting diversity. The meta-action set and its integration with RAG are two novel contributions of our planner, denoted as MaP-AVR, the meta-action planner for agents composed of VLM and RAG. To validate its efficacy, we design experiments using GPT-4o as the pre-trained LLM/VLM model and OmniGibson as our robotic platform. Our approach demonstrates promising performance compared to the current state-of-the-art method. Project page: https://map-avr.github.io/.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-22T14:58:52+00:00",
      "updated": "2025-12-22T14:58:52+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19453v1",
      "file": "papers/2512.19453v1.pdf"
    },
    {
      "arxiv_id": "2512.19402v1",
      "title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface",
      "authors": [
        {
          "name": "Yujie Zhao"
        },
        {
          "name": "Hongwei Fan"
        },
        {
          "name": "Di Chen"
        },
        {
          "name": "Shengcong Chen"
        },
        {
          "name": "Liliang Chen"
        },
        {
          "name": "Xiaoqi Li"
        },
        {
          "name": "Guanghui Ren"
        },
        {
          "name": "Hao Dong"
        }
      ],
      "abstract": "Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.GR"
      ],
      "published": "2025-12-22T13:53:25+00:00",
      "updated": "2025-12-22T13:53:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19402v1",
      "file": "papers/2512.19402v1.pdf"
    },
    {
      "arxiv_id": "2512.19390v1",
      "title": "TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation",
      "authors": [
        {
          "name": "Hongwei Fan"
        },
        {
          "name": "Hang Dai"
        },
        {
          "name": "Jiyao Zhang"
        },
        {
          "name": "Jinzhou Li"
        },
        {
          "name": "Qiyang Yan"
        },
        {
          "name": "Yujie Zhao"
        },
        {
          "name": "Mingju Gao"
        },
        {
          "name": "Jinghang Wu"
        },
        {
          "name": "Hao Tang"
        },
        {
          "name": "Hao Dong"
        }
      ],
      "abstract": "The robotics field is evolving towards data-driven, end-to-end learning, inspired by multimodal large models. However, reliance on expensive real-world data limits progress. Simulators offer cost-effective alternatives, but the gap between simulation and reality challenges effective policy transfer. This paper introduces TwinAligner, a novel Real2Sim2Real system that addresses both visual and dynamic gaps. The visual alignment module achieves pixel-level alignment through SDF reconstruction and editable 3DGS rendering, while the dynamic alignment module ensures dynamic consistency by identifying rigid physics from robot-object interaction. TwinAligner improves robot learning by providing scalable data collection and establishing a trustworthy iterative cycle, accelerating algorithm development. Quantitative evaluations highlight TwinAligner's strong capabilities in visual and dynamic real-to-sim alignment. This system enables policies trained in simulation to achieve strong zero-shot generalization to the real world. The high consistency between real-world and simulated policy performance underscores TwinAligner's potential to advance scalable robot learning. Code and data will be released on https://twin-aligner.github.io",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.GR"
      ],
      "published": "2025-12-22T13:38:11+00:00",
      "updated": "2025-12-22T13:38:11+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19390v1",
      "file": "papers/2512.19390v1.pdf"
    },
    {
      "arxiv_id": "2512.19347v1",
      "title": "OMP: One-step Meanflow Policy with Directional Alignment",
      "authors": [
        {
          "name": "Han Fang"
        },
        {
          "name": "Yize Huang"
        },
        {
          "name": "Yuheng Zhao"
        },
        {
          "name": "Paul Weng"
        },
        {
          "name": "Xiao Li"
        },
        {
          "name": "Yutong Ban"
        }
      ],
      "abstract": "Robot manipulation, a key capability of embodied AI, has turned to data-driven generative policy frameworks, but mainstream approaches like Diffusion Models suffer from high inference latency and Flow-based Methods from increased architectural complexity. While simply applying meanFlow on robotic tasks achieves single-step inference and outperforms FlowPolicy, it lacks few-shot generalization due to fixed temperature hyperparameters in its Dispersive Loss and misaligned predicted-true mean velocities. To solve these issues, this study proposes an improved MeanFlow-based Policies: we introduce a lightweight Cosine Loss to align velocity directions and use the Differential Derivation Equation (DDE) to optimize the Jacobian-Vector Product (JVP) operator. Experiments on Adroit and Meta-World tasks show the proposed method outperforms MP1 and FlowPolicy in average success rate, especially in challenging Meta-World tasks, effectively enhancing few-shot generalization and trajectory accuracy of robot manipulation policies while maintaining real-time performance, offering a more robust solution for high-precision robotic manipulation.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-22T12:45:35+00:00",
      "updated": "2025-12-22T12:45:35+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19347v1",
      "file": "papers/2512.19347v1.pdf"
    },
    {
      "arxiv_id": "2512.19269v1",
      "title": "Translating Flow to Policy via Hindsight Online Imitation",
      "authors": [
        {
          "name": "Yitian Zheng"
        },
        {
          "name": "Zhangchen Ye"
        },
        {
          "name": "Weijun Dong"
        },
        {
          "name": "Shengjie Wang"
        },
        {
          "name": "Yuyang Liu"
        },
        {
          "name": "Chongjie Zhang"
        },
        {
          "name": "Chuan Wen"
        },
        {
          "name": "Yang Gao"
        }
      ],
      "abstract": "Recent advances in hierarchical robot systems leverage a high-level planner to propose task plans and a low-level policy to generate robot actions. This design allows training the planner on action-free or even non-robot data sources (e.g., videos), providing transferable high-level guidance. Nevertheless, grounding these high-level plans into executable actions remains challenging, especially with the limited availability of high-quality robot data. To this end, we propose to improve the low-level policy through online interactions. Specifically, our approach collects online rollouts, retrospectively annotates the corresponding high-level goals from achieved outcomes, and aggregates these hindsight-relabeled experiences to update a goal-conditioned imitation policy. Our method, Hindsight Flow-conditioned Online Imitation (HinFlow), instantiates this idea with 2D point flows as the high-level planner. Across diverse manipulation tasks in both simulation and physical world, our method achieves more than $2\\times$ performance improvement over the base policy, significantly outperforming the existing methods. Moreover, our framework enables policy acquisition from planners trained on cross-embodiment video data, demonstrating its potential for scalable and transferable robot learning.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "published": "2025-12-22T11:06:06+00:00",
      "updated": "2025-12-22T11:06:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19269v1",
      "file": "papers/2512.19269v1.pdf"
    },
    {
      "arxiv_id": "2512.19245v1",
      "title": "Vision-Aided Relative State Estimation for Approach and Landing on a Moving Platform with Inertial Measurements",
      "authors": [
        {
          "name": "Tarek Bouazza"
        },
        {
          "name": "Alessandro Melis"
        },
        {
          "name": "Soulaimane Berkane"
        },
        {
          "name": "Robert Mahony"
        },
        {
          "name": "Tarek Hamel"
        }
      ],
      "abstract": "This paper tackles the problem of estimating the relative position, orientation, and velocity between a UAV and a planar platform undergoing arbitrary 3D motion during approach and landing. The estimation relies on measurements from Inertial Measurement Units (IMUs) mounted on both systems, assuming there is a suitable communication channel to exchange data, together with visual information provided by an onboard monocular camera, from which the bearing (line-of-sight direction) to the platform's center and the normal vector of its planar surface are extracted. We propose a cascade observer with a complementary filter on SO(3) to reconstruct the relative attitude, followed by a linear Riccati observer for relative position and velocity estimation. Convergence of both observers is established under persistently exciting conditions, and the cascade is shown to be almost globally asymptotically and locally exponentially stable. We further extend the design to the case where the platform's rotation is restricted to its normal axis and show that its measured linear acceleration can be exploited to recover the remaining unobservable rotation angle. A sufficient condition to ensure local exponential convergence in this setting is provided. The performance of the proposed observers is validated through extensive simulations.",
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY",
        "cs.RO"
      ],
      "published": "2025-12-22T10:28:20+00:00",
      "updated": "2025-12-22T10:28:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19245v1",
      "file": "papers/2512.19245v1.pdf"
    },
    {
      "arxiv_id": "2512.19178v1",
      "title": "Vision-Language-Policy Model for Dynamic Robot Task Planning",
      "authors": [
        {
          "name": "Jin Wang"
        },
        {
          "name": "Kim Tien Ly"
        },
        {
          "name": "Jacques Cloete"
        },
        {
          "name": "Nikos Tsagarakis"
        },
        {
          "name": "Ioannis Havoutis"
        }
      ],
      "abstract": "Bridging the gap between natural language commands and autonomous execution in unstructured environments remains an open challenge for robotics. This requires robots to perceive and reason over the current task scene through multiple modalities, and to plan their behaviors to achieve their intended goals. Traditional robotic task-planning approaches often struggle to bridge low-level execution with high-level task reasoning, and cannot dynamically update task strategies when instructions change during execution, which ultimately limits their versatility and adaptability to new tasks. In this work, we propose a novel language model-based framework for dynamic robot task planning. Our Vision-Language-Policy (VLP) model, based on a vision-language model fine-tuned on real-world data, can interpret semantic instructions and integrate reasoning over the current task scene to generate behavior policies that control the robot to accomplish the task. Moreover, it can dynamically adjust the task strategy in response to changes in the task, enabling flexible adaptation to evolving task requirements. Experiments conducted with different robots and a variety of real-world tasks show that the trained model can efficiently adapt to novel scenarios and dynamically update its policy, demonstrating strong planning autonomy and cross-embodiment generalization. Videos: https://robovlp.github.io/",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2025-12-22T09:12:48+00:00",
      "updated": "2025-12-22T09:12:48+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19178v1",
      "file": "papers/2512.19178v1.pdf"
    },
    {
      "arxiv_id": "2512.19148v1",
      "title": "A Flexible Field-Based Policy Learning Framework for Diverse Robotic Systems and Sensors",
      "authors": [
        {
          "name": "Jose Gustavo Buenaventura Carreon"
        },
        {
          "name": "Floris Erich"
        },
        {
          "name": "Roman Mykhailyshyn"
        },
        {
          "name": "Tomohiro Motoda"
        },
        {
          "name": "Ryo Hanai"
        },
        {
          "name": "Yukiyasu Domae"
        }
      ],
      "abstract": "We present a cross robot visuomotor learning framework that integrates diffusion policy based control with 3D semantic scene representations from D3Fields to enable category level generalization in manipulation. Its modular design supports diverse robot camera configurations including UR5 arms with Microsoft Azure Kinect arrays and bimanual manipulators with Intel RealSense sensors through a low latency control stack and intuitive teleoperation. A unified configuration layer enables seamless switching between setups for flexible data collection training and evaluation. In a grasp and lift block task the framework achieved an 80 percent success rate after only 100 demonstration episodes demonstrating robust skill transfer between platforms and sensing modalities. This design paves the way for scalable real world studies in cross robotic generalization.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-22T08:45:33+00:00",
      "updated": "2025-12-22T08:45:33+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19148v1",
      "file": "papers/2512.19148v1.pdf"
    },
    {
      "arxiv_id": "2512.19083v2",
      "title": "CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models",
      "authors": [
        {
          "name": "Pengyu Chen"
        },
        {
          "name": "Tao Ouyang"
        },
        {
          "name": "Ke Luo"
        },
        {
          "name": "Weijie Hong"
        },
        {
          "name": "Xu Chen"
        }
      ],
      "abstract": "Autonomous navigation for Unmanned Aerial Vehicles faces key challenges from limited onboard computational resources, which restrict deployed deep neural networks to shallow architectures incapable of handling complex environments. Offloading tasks to remote edge servers introduces high latency, creating an inherent trade-off in system design. To address these limitations, we propose CoDrone - the first cloud-edge-end collaborative computing framework integrating foundation models into autonomous UAV cruising scenarios - effectively leveraging foundation models to enhance performance of resource-constrained unmanned aerial vehicle platforms. To reduce onboard computation and data transmission overhead, CoDrone employs grayscale imagery for the navigation model. When enhanced environmental perception is required, CoDrone leverages the edge-assisted foundation model Depth Anything V2 for depth estimation and introduces a novel one-dimensional occupancy grid-based navigation method - enabling fine-grained scene understanding while advancing efficiency and representational simplicity of autonomous navigation. A key component of CoDrone is a Deep Reinforcement Learning-based neural scheduler that seamlessly integrates depth estimation with autonomous navigation decisions, enabling real-time adaptation to dynamic environments. Furthermore, the framework introduces a UAV-specific vision language interaction module incorporating domain-tailored low-level flight primitives to enable effective interaction between the cloud foundation model and the UAV. The introduction of VLM enhances open-set reasoning capabilities in complex unseen scenarios. Experimental results show CoDrone outperforms baseline methods under varying flight speeds and network conditions, achieving a 40% increase in average flight distance and a 5% improvement in average Quality of Navigation.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-22T06:48:12+00:00",
      "updated": "2025-12-24T02:49:47+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19083v2",
      "file": "papers/2512.19083v2.pdf"
    },
    {
      "arxiv_id": "2512.19043v1",
      "title": "EGM: Efficiently Learning General Motion Tracking Policy for High Dynamic Humanoid Whole-Body Control",
      "authors": [
        {
          "name": "Chao Yang"
        },
        {
          "name": "Yingkai Sun"
        },
        {
          "name": "Peng Ye"
        },
        {
          "name": "Xin Chen"
        },
        {
          "name": "Chong Yu"
        },
        {
          "name": "Tao Chen"
        }
      ],
      "abstract": "Learning a general motion tracking policy from human motions shows great potential for versatile humanoid whole-body control. Conventional approaches are not only inefficient in data utilization and training processes but also exhibit limited performance when tracking highly dynamic motions. To address these challenges, we propose EGM, a framework that enables efficient learning of a general motion tracking policy. EGM integrates four core designs. Firstly, we introduce a Bin-based Cross-motion Curriculum Adaptive Sampling strategy to dynamically orchestrate the sampling probabilities based on tracking error of each motion bin, eficiently balancing the training process across motions with varying dificulty and durations. The sampled data is then processed by our proposed Composite Decoupled Mixture-of-Experts (CDMoE) architecture, which efficiently enhances the ability to track motions from different distributions by grouping experts separately for upper and lower body and decoupling orthogonal experts from shared experts to separately handle dedicated features and general features. Central to our approach is a key insight we identified: for training a general motion tracking policy, data quality and diversity are paramount. Building on these designs, we develop a three-stage curriculum training flow to progressively enhance the policy's robustness against disturbances. Despite training on only 4.08 hours of data, EGM generalized robustly across 49.25 hours of test motions, outperforming baselines on both routine and highly dynamic tasks.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-22T05:25:24+00:00",
      "updated": "2025-12-22T05:25:24+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19043v1",
      "file": "papers/2512.19043v1.pdf"
    },
    {
      "arxiv_id": "2512.19024v1",
      "title": "IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments",
      "authors": [
        {
          "name": "Xu Liu"
        },
        {
          "name": "Yu Liu"
        },
        {
          "name": "Hanshuo Qiu"
        },
        {
          "name": "Yang Qirong"
        },
        {
          "name": "Zhouhui Lian"
        }
      ],
      "abstract": "Vision-Language Navigation (VLN) enables agents to navigate in complex environments by following natural language instructions grounded in visual observations. Although most existing work has focused on ground-based robots or outdoor Unmanned Aerial Vehicles (UAVs), indoor UAV-based VLN remains underexplored, despite its relevance to real-world applications such as inspection, delivery, and search-and-rescue in confined spaces. To bridge this gap, we introduce \\textbf{IndoorUAV}, a novel benchmark and method specifically tailored for VLN with indoor UAVs. We begin by curating over 1,000 diverse and structurally rich 3D indoor scenes from the Habitat simulator. Within these environments, we simulate realistic UAV flight dynamics to collect diverse 3D navigation trajectories manually, further enriched through data augmentation techniques. Furthermore, we design an automated annotation pipeline to generate natural language instructions of varying granularity for each trajectory. This process yields over 16,000 high-quality trajectories, comprising the \\textbf{IndoorUAV-VLN} subset, which focuses on long-horizon VLN. To support short-horizon planning, we segment long trajectories into sub-trajectories by selecting semantically salient keyframes and regenerating concise instructions, forming the \\textbf{IndoorUAV-VLA} subset. Finally, we introduce \\textbf{IndoorUAV-Agent}, a novel navigation model designed for our benchmark, leveraging task decomposition and multimodal reasoning. We hope IndoorUAV serves as a valuable resource to advance research on vision-language embodied AI in the indoor aerial navigation domain.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2025-12-22T04:42:35+00:00",
      "updated": "2025-12-22T04:42:35+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19024v1",
      "file": "papers/2512.19024v1.pdf"
    },
    {
      "arxiv_id": "2512.19021v1",
      "title": "VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation",
      "authors": [
        {
          "name": "Sihao Lin"
        },
        {
          "name": "Zerui Li"
        },
        {
          "name": "Xunyi Zhao"
        },
        {
          "name": "Gengze Zhou"
        },
        {
          "name": "Liuyi Wang"
        },
        {
          "name": "Rong Wei"
        },
        {
          "name": "Rui Tang"
        },
        {
          "name": "Juncheng Li"
        },
        {
          "name": "Hanqing Wang"
        },
        {
          "name": "Jiangmiao Pang"
        },
        {
          "name": "Anton van den Hengel"
        },
        {
          "name": "Jiajun Liu"
        },
        {
          "name": "Qi Wu"
        }
      ],
      "abstract": "Despite remarkable progress in Vision-Language Navigation (VLN), existing benchmarks remain confined to fixed, small-scale datasets with naive physical simulation. These shortcomings limit the insight that the benchmarks provide into sim-to-real generalization, and create a significant research gap. Furthermore, task fragmentation prevents unified/shared progress in the area, while limited data scales fail to meet the demands of modern LLM-based pretraining. To overcome these limitations, we introduce VLNVerse: a new large-scale, extensible benchmark designed for Versatile, Embodied, Realistic Simulation, and Evaluation. VLNVerse redefines VLN as a scalable, full-stack embodied AI problem. Its Versatile nature unifies previously fragmented tasks into a single framework and provides an extensible toolkit for researchers. Its Embodied design moves beyond intangible and teleporting \"ghost\" agents that support full-kinematics in a Realistic Simulation powered by a robust physics engine. We leverage the scale and diversity of VLNVerse to conduct a comprehensive Evaluation of existing methods, from classic models to MLLM-based agents. We also propose a novel unified multi-task model capable of addressing all tasks within the benchmark. VLNVerse aims to narrow the gap between simulated navigation and real-world generalization, providing the community with a vital tool to boost research towards scalable, general-purpose embodied locomotion agents.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-12-22T04:27:26+00:00",
      "updated": "2025-12-22T04:27:26+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19021v1",
      "file": "papers/2512.19021v1.pdf"
    },
    {
      "arxiv_id": "2512.19010v1",
      "title": "PalpAid: Multimodal Pneumatic Tactile Sensor for Tissue Palpation",
      "authors": [
        {
          "name": "Devi Yuliarti"
        },
        {
          "name": "Ravi Prakash"
        },
        {
          "name": "Hiu Ching Cheung"
        },
        {
          "name": "Amy Strong"
        },
        {
          "name": "Patrick J. Codd"
        },
        {
          "name": "Shan Lin"
        }
      ],
      "abstract": "The tactile properties of tissue, such as elasticity and stiffness, often play an important role in surgical oncology when identifying tumors and pathological tissue boundaries. Though extremely valuable, robot-assisted surgery comes at the cost of reduced sensory information to the surgeon; typically, only vision is available. Sensors proposed to overcome this sensory desert are often bulky, complex, and incompatible with the surgical workflow. We present PalpAid, a multimodal pneumatic tactile sensor equipped with a microphone and pressure sensor, converting contact force into an internal pressure differential. The pressure sensor acts as an event detector, while the auditory signature captured by the microphone assists in tissue delineation. We show the design, fabrication, and assembly of sensory units with characterization tests to show robustness to use, inflation-deflation cycles, and integration with a robotic system. Finally, we show the sensor's ability to classify 3D-printed hard objects with varying infills and soft ex vivo tissues. Overall, PalpAid aims to fill the sensory gap intelligently and allow improved clinical decision-making.",
      "primary_category": "eess.SP",
      "categories": [
        "eess.SP",
        "cs.RO"
      ],
      "published": "2025-12-22T03:53:09+00:00",
      "updated": "2025-12-22T03:53:09+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19010v1",
      "file": "papers/2512.19010v1.pdf"
    },
    {
      "arxiv_id": "2512.18987v1",
      "title": "Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation",
      "authors": [
        {
          "name": "Ryosuke Korekata"
        },
        {
          "name": "Quanting Xie"
        },
        {
          "name": "Yonatan Bisk"
        },
        {
          "name": "Komei Sugiura"
        }
      ],
      "abstract": "In this study, we address the problem of open-vocabulary mobile manipulation, where a robot is required to carry a wide range of objects to receptacles based on free-form natural language instructions. This task is challenging, as it involves understanding visual semantics and the affordance of manipulation actions. To tackle these challenges, we propose Affordance RAG, a zero-shot hierarchical multimodal retrieval framework that constructs Affordance-Aware Embodied Memory from pre-explored images. The model retrieves candidate targets based on regional and visual semantics and reranks them with affordance scores, allowing the robot to identify manipulation options that are likely to be executable in real-world environments. Our method outperformed existing approaches in retrieval performance for mobile manipulation instruction in large-scale indoor environments. Furthermore, in real-world experiments where the robot performed mobile manipulation in indoor environments based on free-form instructions, the proposed method achieved a task success rate of 85%, outperforming existing methods in both retrieval performance and overall task success.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CL",
        "cs.CV"
      ],
      "published": "2025-12-22T02:55:25+00:00",
      "updated": "2025-12-22T02:55:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18987v1",
      "file": "papers/2512.18987v1.pdf"
    },
    {
      "arxiv_id": "2512.18938v1",
      "title": "A Framework for Deploying Learning-based Quadruped Loco-Manipulation",
      "authors": [
        {
          "name": "Yadong Liu"
        },
        {
          "name": "Jianwei Liu"
        },
        {
          "name": "He Liang"
        },
        {
          "name": "Dimitrios Kanoulas"
        }
      ],
      "abstract": "Quadruped mobile manipulators offer strong potential for agile loco-manipulation but remain difficult to control and transfer reliably from simulation to reality. Reinforcement learning (RL) shows promise for whole-body control, yet most frameworks are proprietary and hard to reproduce on real hardware. We present an open pipeline for training, benchmarking, and deploying RL-based controllers on the Unitree B1 quadruped with a Z1 arm. The framework unifies sim-to-sim and sim-to-real transfer through ROS, re-implementing a policy trained in Isaac Gym, extending it to MuJoCo via a hardware abstraction layer, and deploying the same controller on physical hardware. Sim-to-sim experiments expose discrepancies between Isaac Gym and MuJoCo contact models that influence policy behavior, while real-world teleoperated object-picking trials show that coordinated whole-body control extends reach and improves manipulation over floating-base baselines. The pipeline provides a transparent, reproducible foundation for developing and analyzing RL-based loco-manipulation controllers and will be released open source to support future research.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-22T01:19:26+00:00",
      "updated": "2025-12-22T01:19:26+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18938v1",
      "file": "papers/2512.18938v1.pdf"
    },
    {
      "arxiv_id": "2512.18922v1",
      "title": "Optimizing Robotic Placement via Grasp-Dependent Feasibility Prediction",
      "authors": [
        {
          "name": "Tianyuan Liu"
        },
        {
          "name": "Richard Dazeley"
        },
        {
          "name": "Benjamin Champion"
        },
        {
          "name": "Akan Cosgun"
        }
      ],
      "abstract": "In this paper, we study whether inexpensive, physics-free supervision can reliably prioritize grasp-place candidates for budget-aware pick-and-place. From an object's initial pose, target pose, and a candidate grasp, we generate two path-aware geometric labels: path-wise inverse kinematics (IK) feasibility across a fixed approach-grasp-lift waypoint template, and a transit collision flag from mesh sweeps along the same template. A compact dual-output MLP learns these signals from pose encodings, and at test time its scores rank precomputed candidates for a rank-then-plan policy under the same IK gate and planner as the baseline. Although learned from cheap labels only, the scores transfer to physics-enabled executed trajectories: at a fixed planning budget the policy finds successful paths sooner with fewer planner calls while keeping final success on par or better. This work targets a single rigid cuboid with side-face grasps and a fixed waypoint template, and we outline extensions to varied objects and richer waypoint schemes.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-21T23:47:09+00:00",
      "updated": "2025-12-21T23:47:09+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18922v1",
      "file": "papers/2512.18922v1.pdf"
    },
    {
      "arxiv_id": "2512.18712v1",
      "title": "DSO-VSA: a Variable Stiffness Actuator with Decoupled Stiffness and Output Characteristics for Rehabilitation Robotics",
      "authors": [
        {
          "name": "Maozeng Zhang"
        },
        {
          "name": "Ke Shi"
        },
        {
          "name": "Huijun Li"
        },
        {
          "name": "Tongshu Chen"
        },
        {
          "name": "Jiejun Yan"
        },
        {
          "name": "Aiguo Song"
        }
      ],
      "abstract": "Stroke-induced motor impairment often results in substantial loss of upper-limb function, creating a strong demand for rehabilitation robots that enable safe and transparent physical human-robot interaction (pHRI). Variable stiffness actuators are well suited for such applications. However, in most existing designs, stiffness is coupled with the deflection angle, complicating both modeling and control. To address this limitation, this paper presents a variable stiffness actuator featuring decoupled stiffness and output behavior for rehabilitation robotics. The system integrates a variable stiffness mechanism that combines a variable-length lever with a hypocycloidal straight-line mechanism to achieve a linear torque-deflection relationship and continuous stiffness modulation from near zero to theoretically infinite. It also incorporates a differential transmission mechanism based on a planetary gear system that enables dual-motor load sharing. A cascade PI controller is further developed on the basis of the differential configuration, in which the position-loop term jointly regulates stiffness and deflection angle, effectively suppressing stiffness fluctuations and output disturbances. The performance of prototype was experimentally validated through stiffness calibration, stiffness regulation, torque control, decoupled characteristics, and dual-motor load sharing, indicating the potential for rehabilitation exoskeletons and other pHRI systems.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-21T12:13:17+00:00",
      "updated": "2025-12-21T12:13:17+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18712v1",
      "file": "papers/2512.18712v1.pdf"
    },
    {
      "arxiv_id": "2512.18619v1",
      "title": "ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning",
      "authors": [
        {
          "name": "Zhenhao Zhou"
        },
        {
          "name": "Dan Negrut"
        }
      ],
      "abstract": "We present ChronoDreamer, an action-conditioned world model for contact-rich robotic manipulation. Given a history of egocentric RGB frames, contact maps, actions, and joint states, ChronoDreamer predicts future video frames, contact distributions, and joint angles via a spatial-temporal transformer trained with MaskGIT-style masked prediction. Contact is encoded as depth-weighted Gaussian splat images that render 3D forces into a camera-aligned format suitable for vision backbones. At inference, predicted rollouts are evaluated by a vision-language model that reasons about collision likelihood, enabling rejection sampling of unsafe actions before execution. We train and evaluate on DreamerBench, a simulation dataset generated with Project Chrono that provides synchronized RGB, contact splat, proprioception, and physics annotations across rigid and deformable object scenarios. Qualitative results demonstrate that the model preserves spatial coherence during non-contact motion and generates plausible contact predictions, while the LLM-based judge distinguishes collision from non-collision trajectories.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "published": "2025-12-21T06:36:03+00:00",
      "updated": "2025-12-21T06:36:03+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18619v1",
      "file": "papers/2512.18619v1.pdf"
    },
    {
      "arxiv_id": "2512.18477v1",
      "title": "STORM: Search-Guided Generative World Models for Robotic Manipulation",
      "authors": [
        {
          "name": "Wenjun Lin"
        },
        {
          "name": "Jensen Zhang"
        },
        {
          "name": "Kaitong Cai"
        },
        {
          "name": "Keze Wang"
        }
      ],
      "abstract": "We present STORM (Search-Guided Generative World Models), a novel framework for spatio-temporal reasoning in robotic manipulation that unifies diffusion-based action generation, conditional video prediction, and search-based planning. Unlike prior Vision-Language-Action (VLA) models that rely on abstract latent dynamics or delegate reasoning to language components, STORM grounds planning in explicit visual rollouts, enabling interpretable and foresight-driven decision-making. A diffusion-based VLA policy proposes diverse candidate actions, a generative video world model simulates their visual and reward outcomes, and Monte Carlo Tree Search (MCTS) selectively refines plans through lookahead evaluation. Experiments on the SimplerEnv manipulation benchmark demonstrate that STORM achieves a new state-of-the-art average success rate of 51.0 percent, outperforming strong baselines such as CogACT. Reward-augmented video prediction substantially improves spatio-temporal fidelity and task relevance, reducing Frechet Video Distance by over 75 percent. Moreover, STORM exhibits robust re-planning and failure recovery behavior, highlighting the advantages of search-guided generative world models for long-horizon robotic manipulation.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published": "2025-12-20T19:40:25+00:00",
      "updated": "2025-12-20T19:40:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18477v1",
      "file": "papers/2512.18477v1.pdf"
    },
    {
      "arxiv_id": "2512.18474v1",
      "title": "When Robots Say No: The Empathic Ethical Disobedience Benchmark",
      "authors": [
        {
          "name": "Dmytro Kuzmenko"
        },
        {
          "name": "Nadiya Shvai"
        }
      ],
      "abstract": "Robots must balance compliance with safety and social expectations as blind obedience can cause harm, while over-refusal erodes trust. Existing safe reinforcement learning (RL) benchmarks emphasize physical hazards, while human-robot interaction trust studies are small-scale and hard to reproduce. We present the Empathic Ethical Disobedience (EED) Gym, a standardized testbed that jointly evaluates refusal safety and social acceptability. Agents weigh risk, affect, and trust when choosing to comply, refuse (with or without explanation), clarify, or propose safer alternatives. EED Gym provides different scenarios, multiple persona profiles, and metrics for safety, calibration, and refusals, with trust and blame models grounded in a vignette study. Using EED Gym, we find that action masking eliminates unsafe compliance, while explanatory refusals help sustain trust. Constructive styles are rated most trustworthy, empathic styles -- most empathic, and safe RL methods improve robustness but also make agents more prone to overly cautious behavior. We release code, configurations, and reference policies to enable reproducible evaluation and systematic human-robot interaction research on refusal and trust. At submission time, we include an anonymized reproducibility package with code and configs, and we commit to open-sourcing the full repository after the paper is accepted.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "published": "2025-12-20T19:35:08+00:00",
      "updated": "2025-12-20T19:35:08+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18474v1",
      "file": "papers/2512.18474v1.pdf"
    },
    {
      "arxiv_id": "2512.18396v1",
      "title": "AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation",
      "authors": [
        {
          "name": "Yulu Wu"
        },
        {
          "name": "Jiujun Cheng"
        },
        {
          "name": "Haowen Wang"
        },
        {
          "name": "Dengyang Suo"
        },
        {
          "name": "Pei Ren"
        },
        {
          "name": "Qichao Mao"
        },
        {
          "name": "Shangce Gao"
        },
        {
          "name": "Yakun Huang"
        }
      ],
      "abstract": "Recent advances in Vision-Language-Action (VLA) and world-model methods have improved generalization in tasks such as robotic manipulation and object interaction. However, Successful execution of such tasks depends on large, costly collections of real demonstrations, especially for fine-grained manipulation of articulated objects. To address this, we present AOMGen, a scalable data generation framework for articulated manipulation which is instantiated from a single real scan, demonstration and a library of readily available digital assets, yielding photoreal training data with verified physical states. The framework synthesizes synchronized multi-view RGB temporally aligned with action commands and state annotations for joints and contacts, and systematically varies camera viewpoints, object styles, and object poses to expand a single execution into a diverse corpus. Experimental results demonstrate that fine-tuning VLA policies on AOMGen data increases the success rate from 0% to 88.7%, and the policies are tested on unseen objects and layouts.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-20T15:21:25+00:00",
      "updated": "2025-12-20T15:21:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18396v1",
      "file": "papers/2512.18396v1.pdf"
    },
    {
      "arxiv_id": "2512.18368v1",
      "title": "Learning Semantic Atomic Skills for Multi-Task Robotic Manipulation",
      "authors": [
        {
          "name": "Yihang Zhu"
        },
        {
          "name": "Weiqing Wang"
        },
        {
          "name": "Shijie Wu"
        },
        {
          "name": "Ye Shi"
        },
        {
          "name": "Jingya Wang"
        }
      ],
      "abstract": "While imitation learning has shown impressive results in single-task robot manipulation, scaling it to multi-task settings remains a fundamental challenge due to issues such as suboptimal demonstrations, trajectory noise, and behavioral multi-modality. Existing skill-based methods attempt to address this by decomposing actions into reusable abstractions, but they often rely on fixed-length segmentation or environmental priors that limit semantic consistency and cross-task generalization. In this work, we propose AtomSkill, a novel multi-task imitation learning framework that learns and leverages a structured Atomic Skill Space for composable robot manipulation. Our approach is built on two key technical contributions. First, we construct a Semantically Grounded Atomic Skill Library by partitioning demonstrations into variable-length skills using gripper-state keyframe detection and vision-language model annotation. A contrastive learning objective ensures the resulting skill embeddings are both semantically consistent and temporally coherent. Second, we propose an Action Generation module with Keypose Imagination, which jointly predicts a skill's long-horizon terminal keypose and its immediate action sequence. This enables the policy to reason about overarching motion goals and fine-grained control simultaneously, facilitating robust skill chaining. Extensive experiments in simulated and real-world environments show that AtomSkill consistently outperforms state-of-the-art methods across diverse manipulation tasks.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-20T13:46:08+00:00",
      "updated": "2025-12-20T13:46:08+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18368v1",
      "file": "papers/2512.18368v1.pdf"
    },
    {
      "arxiv_id": "2512.18213v1",
      "title": "Fractional-order Modeling for Nonlinear Soft Actuators via Particle Swarm Optimization",
      "authors": [
        {
          "name": "Wu-Te Yang"
        },
        {
          "name": "Masayoshi Tomizuka"
        }
      ],
      "abstract": "Modeling soft pneumatic actuators with high precision remains a fundamental challenge due to their highly nonlinear and compliant characteristics. This paper proposes an innovative modeling framework based on fractional-order differential equations (FODEs) to accurately capture the dynamic behavior of soft materials. The unknown parameters within the fractional-order model are identified using particle swarm optimization (PSO), enabling parameter estimation directly from experimental data without reliance on pre-established material databases or empirical constitutive laws. The proposed approach effectively represents the complex deformation phenomena inherent in soft actuators. Experimental results validate the accuracy and robustness of the developed model, demonstrating improvement in predictive performance compared to conventional modeling techniques. The presented framework provides a data-efficient and database-independent solution for soft actuator modeling, advancing the precision and adaptability of soft robotic system design.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-20T04:46:25+00:00",
      "updated": "2025-12-20T04:46:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18213v1",
      "file": "papers/2512.18213v1.pdf"
    },
    {
      "arxiv_id": "2512.18206v1",
      "title": "Alternating Minimization for Time-Shifted Synergy Extraction in Human Hand Coordination",
      "authors": [
        {
          "name": "Trevor Stepp"
        },
        {
          "name": "Parthan Olikkal"
        },
        {
          "name": "Ramana Vinjamuri"
        },
        {
          "name": "Rajasekhar Anguluri"
        }
      ],
      "abstract": "Identifying motor synergies -- coordinated hand joint patterns activated at task-dependent time shifts -- from kinematic data is central to motor control and robotics. Existing two-stage methods first extract candidate waveforms (via SVD) and then select shifted templates using sparse optimization, requiring at least two datasets and complicating data collection. We introduce an optimization-based framework that jointly learns a small set of synergies and their sparse activation coefficients. The formulation enforces group sparsity for synergy selection and element-wise sparsity for activation timing. We develop an alternating minimization method in which coefficient updates decouple across tasks and synergy updates reduce to regularized least-squares problems. Our approach requires only a single data set, and simulations show accurate velocity reconstruction with compact, interpretable synergies.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "math.OC"
      ],
      "published": "2025-12-20T04:09:37+00:00",
      "updated": "2025-12-20T04:09:37+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18206v1",
      "file": "papers/2512.18206v1.pdf"
    },
    {
      "arxiv_id": "2512.18146v1",
      "title": "On Swarm Leader Identification using Probing Policies",
      "authors": [
        {
          "name": "Stergios E. Bachoumas"
        },
        {
          "name": "Panagiotis Artemiadis"
        }
      ],
      "abstract": "Identifying the leader within a robotic swarm is crucial, especially in adversarial contexts where leader concealment is necessary for mission success. This work introduces the interactive Swarm Leader Identification (iSLI) problem, a novel approach where an adversarial probing agent identifies a swarm's leader by physically interacting with its members. We formulate the iSLI problem as a Partially Observable Markov Decision Process (POMDP) and employ Deep Reinforcement Learning, specifically Proximal Policy Optimization (PPO), to train the prober's policy. The proposed approach utilizes a novel neural network architecture featuring a Timed Graph Relationformer (TGR) layer combined with a Simplified Structured State Space Sequence (S5) model. The TGR layer effectively processes graph-based observations of the swarm, capturing temporal dependencies and fusing relational information using a learned gating mechanism to generate informative representations for policy learning. Extensive simulations demonstrate that our TGR-based model outperforms baseline graph neural network architectures and exhibits significant zero-shot generalization capabilities across varying swarm sizes and speeds different from those used during training. The trained prober achieves high accuracy in identifying the leader, maintaining performance even in out-of-training distribution scenarios, and showing appropriate confidence levels in its predictions. Real-world experiments with physical robots further validate the approach, confirming successful sim-to-real transfer and robustness to dynamic changes, such as unexpected agent disconnections.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "published": "2025-12-20T00:02:58+00:00",
      "updated": "2025-12-20T00:02:58+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18146v1",
      "file": "papers/2512.18146v1.pdf"
    },
    {
      "arxiv_id": "2512.18081v2",
      "title": "Towards Autonomous Navigation in Endovascular Interventions",
      "authors": [
        {
          "name": "Tudor Jianu"
        }
      ],
      "abstract": "Cardiovascular diseases remain the leading cause of global mortality, with minimally invasive treatment options offered through endovascular interventions. However, the precision and adaptability of current robotic systems for endovascular navigation are limited by heuristic control, low autonomy, and the absence of haptic feedback. This thesis presents an integrated AI-driven framework for autonomous guidewire navigation in complex vascular environments, addressing key challenges in data availability, simulation fidelity, and navigational accuracy.\n  A high-fidelity, real-time simulation platform, CathSim, is introduced for reinforcement learning based catheter navigation, featuring anatomically accurate vascular models and contact dynamics. Building on CathSim, the Expert Navigation Network is developed, a policy that fuses visual, kinematic, and force feedback for autonomous tool control. To mitigate data scarcity, the open-source, bi-planar fluoroscopic dataset Guide3D is proposed, comprising more than 8,700 annotated images for 3D guidewire reconstruction. Finally, SplineFormer, a transformer-based model, is introduced to directly predict guidewire geometry as continuous B-spline parameters, enabling interpretable, real-time navigation.\n  The findings show that combining high-fidelity simulation, multimodal sensory fusion, and geometric modelling substantially improves autonomous endovascular navigation and supports safer, more precise minimally invasive procedures.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-19T21:38:52+00:00",
      "updated": "2025-12-23T21:55:55+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18081v2",
      "file": "papers/2512.18081v2.pdf"
    },
    {
      "arxiv_id": "2512.18068v1",
      "title": "SurgiPose: Estimating Surgical Tool Kinematics from Monocular Video for Surgical Robot Learning",
      "authors": [
        {
          "name": "Juo-Tung Chen"
        },
        {
          "name": "XinHao Chen"
        },
        {
          "name": "Ji Woong Kim"
        },
        {
          "name": "Paul Maria Scheikl"
        },
        {
          "name": "Richard Jaepyeong Cha"
        },
        {
          "name": "Axel Krieger"
        }
      ],
      "abstract": "Imitation learning (IL) has shown immense promise in enabling autonomous dexterous manipulation, including learning surgical tasks. To fully unlock the potential of IL for surgery, access to clinical datasets is needed, which unfortunately lack the kinematic data required for current IL approaches. A promising source of large-scale surgical demonstrations is monocular surgical videos available online, making monocular pose estimation a crucial step toward enabling large-scale robot learning. Toward this end, we propose SurgiPose, a differentiable rendering based approach to estimate kinematic information from monocular surgical videos, eliminating the need for direct access to ground truth kinematics. Our method infers tool trajectories and joint angles by optimizing tool pose parameters to minimize the discrepancy between rendered and real images. To evaluate the effectiveness of our approach, we conduct experiments on two robotic surgical tasks: tissue lifting and needle pickup, using the da Vinci Research Kit Si (dVRK Si). We train imitation learning policies with both ground truth measured kinematics and estimated kinematics from video and compare their performance. Our results show that policies trained on estimated kinematics achieve comparable success rates to those trained on ground truth data, demonstrating the feasibility of using monocular video based kinematic estimation for surgical robot learning. By enabling kinematic estimation from monocular surgical videos, our work lays the foundation for large scale learning of autonomous surgical policies from online surgical data.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-19T21:15:26+00:00",
      "updated": "2025-12-19T21:15:26+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18068v1",
      "file": "papers/2512.18068v1.pdf"
    },
    {
      "arxiv_id": "2512.18048v1",
      "title": "Design of a Polymer-based Steerable Cannula for Neurosurgical Applications",
      "authors": [
        {
          "name": "Nidhi Malhotra"
        },
        {
          "name": "Amber K. Rothe"
        },
        {
          "name": "Revanth Konda"
        },
        {
          "name": "Jaydev P. Desai"
        }
      ],
      "abstract": "Robotically steerable compliant surgical tools offer several advantages over rigid tools, including enhanced dexterity, reduced tissue damage, and the ability to generate non-linear trajectories in minimally invasive neurosurgical procedures. Many existing robotic neurosurgical tools are designed using stainless steel or nitinol materials. Using polymer-based materials instead can offer advantages such as reduced interference in magnetic resonance imaging, enhanced safety for guiding electrically powered instruments, and reduced tissue damage due to inherent compliance. Several polymer materials have been used in robotic surgical applications, such as polyimide, polycarbonate, and elastic resin. Various fabrication strategies have also been proposed, including standard microfabrication techniques, thermal drawing, and 3-D printing. In our previous work, a tendon-driven, notched-tube was designed for several neurosurgical robotic tools, utilizing laser micromachining to reduce the stiffness of the tube in certain directions. This fabrication method is desirable because it has a single-step process, has high precision, and does not require a cleanroom or harsh chemicals. Past studies have explored laser-micromachining of polymer material for surgical applications such as stent fabrication. In this work, we explore extending the use of the laser micromachining approach to the fabrication of polyimide (PI) robotically steerable cannulas for neurosurgical applications. Utilizing the method presented in this work, we fabricated joints as small as 1.5 mm outer diameter (OD). Multiple joints were fabricated using PI tubes of different ODs, and the loading behavior of the fabricated joints was experimentally characterized.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-19T20:36:20+00:00",
      "updated": "2025-12-19T20:36:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18048v1",
      "file": "papers/2512.18048v1.pdf"
    },
    {
      "arxiv_id": "2512.18032v1",
      "title": "Design and Integration of Thermal and Vibrotactile Feedback for Lifelike Touch in Social Robots",
      "authors": [
        {
          "name": "Jacqueline Borgstedt"
        },
        {
          "name": "Jake Bhattacharyya"
        },
        {
          "name": "Matteo Iovino"
        },
        {
          "name": "Frank E. Pollick"
        },
        {
          "name": "Stephen Brewster"
        }
      ],
      "abstract": "Zoomorphic Socially Assistive Robots (SARs) offer an alternative source of social touch for individuals who cannot access animal companionship. However, current SARs provide only limited, passive touch-based interactions and lack the rich haptic cues, such as warmth, heartbeat or purring, that are characteristic of human-animal touch. This limits their ability to evoke emotionally engaging, life-like physical interactions.\n  We present a multimodal tactile prototype, which was used to augment the established PARO robot, integrating thermal and vibrotactile feedback to simulate feeling biophysiological signals. A flexible heating interface delivers body-like warmth, while embedded actuators generate heartbeat-like rhythms and continuous purring sensations. These cues were iteratively designed and calibrated with input from users and haptics experts. We outline the design process and offer reproducible guidelines to support the development of emotionally resonant and biologically plausible touch interactions with SARs.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "published": "2025-12-19T19:52:44+00:00",
      "updated": "2025-12-19T19:52:44+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18032v1",
      "file": "papers/2512.18032v1.pdf"
    },
    {
      "arxiv_id": "2512.18028v1",
      "title": "Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation",
      "authors": [
        {
          "name": "Tin Stribor Sohn"
        },
        {
          "name": "Maximilian Dillitzer"
        },
        {
          "name": "Jason J. Corso"
        },
        {
          "name": "Eric Sax"
        }
      ],
      "abstract": "Vision-language navigation requires agents to reason and act under constraints of embodiment. While vision-language models (VLMs) demonstrate strong generalization, current benchmarks provide limited understanding of how embodiment -- i.e., the choice of physical platform, sensor configuration, and modality alignment -- influences perception, reasoning, and control. We introduce Embodied4C, a closed-loop benchmark designed as a Turing test for embodied reasoning. The benchmark evaluates the core embodied capabilities of VLMs across three heterogeneous embodiments -- autonomous vehicles, aerial drones, and robotic manipulators -- through approximately 1.1K one-shot reasoning questions and 58 goal-directed navigation tasks. These tasks jointly assess four foundational dimensions: semantic, spatial, temporal, and physical reasoning. Each embodiment presents dynamic sensor configurations and environment variations to probe generalization beyond platform-specific adaptation. To prevent embodiment overfitting, Embodied4C integrates domain-far queries targeting abstract and cross-context reasoning. Comprehensive evaluation across ten state-of-the-art VLMs and four embodied control baselines shows that cross-modal alignment and instruction tuning matter more than scale, while spatial and temporal reasoning remains the primary bottleneck for reliable embodied competence.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published": "2025-12-19T19:47:55+00:00",
      "updated": "2025-12-19T19:47:55+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18028v1",
      "file": "papers/2512.18028v1.pdf"
    },
    {
      "arxiv_id": "2512.18007v1",
      "title": "Robotic VLA Benefits from Joint Learning with Motion Image Diffusion",
      "authors": [
        {
          "name": "Yu Fang"
        },
        {
          "name": "Kanchana Ranasinghe"
        },
        {
          "name": "Le Xue"
        },
        {
          "name": "Honglu Zhou"
        },
        {
          "name": "Juntao Tan"
        },
        {
          "name": "Ran Xu"
        },
        {
          "name": "Shelby Heinecke"
        },
        {
          "name": "Caiming Xiong"
        },
        {
          "name": "Silvio Savarese"
        },
        {
          "name": "Daniel Szafir"
        },
        {
          "name": "Mingyu Ding"
        },
        {
          "name": "Michael S. Ryoo"
        },
        {
          "name": "Juan Carlos Niebles"
        }
      ],
      "abstract": "Vision-Language-Action (VLA) models have achieved remarkable progress in robotic manipulation by mapping multimodal observations and instructions directly to actions. However, they typically mimic expert trajectories without predictive motion reasoning, which limits their ability to reason about what actions to take. To address this limitation, we propose joint learning with motion image diffusion, a novel strategy that enhances VLA models with motion reasoning capabilities. Our method extends the VLA architecture with a dual-head design: while the action head predicts action chunks as in vanilla VLAs, an additional motion head, implemented as a Diffusion Transformer (DiT), predicts optical-flow-based motion images that capture future dynamics. The two heads are trained jointly, enabling the shared VLM backbone to learn representations that couple robot control with motion knowledge. This joint learning builds temporally coherent and physically grounded representations without modifying the inference pathway of standard VLAs, thereby maintaining test-time latency. Experiments in both simulation and real-world environments demonstrate that joint learning with motion image diffusion improves the success rate of pi-series VLAs to 97.5% on the LIBERO benchmark and 58.0% on the RoboTwin benchmark, yielding a 23% improvement in real-world performance and validating its effectiveness in enhancing the motion reasoning capability of large-scale VLAs.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published": "2025-12-19T19:07:53+00:00",
      "updated": "2025-12-19T19:07:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18007v1",
      "file": "papers/2512.18007v1.pdf"
    },
    {
      "arxiv_id": "2512.17992v1",
      "title": "Unifying Deep Predicate Invention with Pre-trained Foundation Models",
      "authors": [
        {
          "name": "Qianwei Wang"
        },
        {
          "name": "Bowen Li"
        },
        {
          "name": "Zhanpeng Luo"
        },
        {
          "name": "Yifan Xu"
        },
        {
          "name": "Alexander Gray"
        },
        {
          "name": "Tom Silver"
        },
        {
          "name": "Sebastian Scherer"
        },
        {
          "name": "Katia Sycara"
        },
        {
          "name": "Yaqi Xie"
        }
      ],
      "abstract": "Long-horizon robotic tasks are hard due to continuous state-action spaces and sparse feedback. Symbolic world models help by decomposing tasks into discrete predicates that capture object properties and relations. Existing methods learn predicates either top-down, by prompting foundation models without data grounding, or bottom-up, from demonstrations without high-level priors. We introduce UniPred, a bilevel learning framework that unifies both. UniPred uses large language models (LLMs) to propose predicate effect distributions that supervise neural predicate learning from low-level data, while learned feedback iteratively refines the LLM hypotheses. Leveraging strong visual foundation model features, UniPred learns robust predicate classifiers in cluttered scenes. We further propose a predicate evaluation method that supports symbolic models beyond STRIPS assumptions. Across five simulated and one real-robot domains, UniPred achieves 2-4 times higher success rates than top-down methods and 3-4 times faster learning than bottom-up approaches, advancing scalable and flexible symbolic world modeling for robotics.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-19T18:59:56+00:00",
      "updated": "2025-12-19T18:59:56+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17992v1",
      "file": "papers/2512.17992v1.pdf"
    },
    {
      "arxiv_id": "2512.17853v1",
      "title": "AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning",
      "authors": [
        {
          "name": "Ran Gong"
        },
        {
          "name": "Xiaohan Zhang"
        },
        {
          "name": "Jinghuan Shang"
        },
        {
          "name": "Maria Vittoria Minniti"
        },
        {
          "name": "Jigarkumar Patel"
        },
        {
          "name": "Valerio Pepe"
        },
        {
          "name": "Riedana Yan"
        },
        {
          "name": "Ahmet Gundogdu"
        },
        {
          "name": "Ivan Kapelyukh"
        },
        {
          "name": "Ali Abbas"
        },
        {
          "name": "Xiaoqiang Yan"
        },
        {
          "name": "Harsh Patel"
        },
        {
          "name": "Laura Herlant"
        },
        {
          "name": "Karl Schmeckpeper"
        }
      ],
      "abstract": "Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data. We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards. We train behavior cloning policies on generated data, validate them in simulation, and deploy them directly on real robot hardware. The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks. Our project website is at https://anytask.rai-inst.com .",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2025-12-19T17:55:48+00:00",
      "updated": "2025-12-19T17:55:48+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17853v1",
      "file": "papers/2512.17853v1.pdf"
    },
    {
      "arxiv_id": "2512.17846v1",
      "title": "Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes",
      "authors": [
        {
          "name": "Carlos Vélez García"
        },
        {
          "name": "Miguel Cazorla"
        },
        {
          "name": "Jorge Pomares"
        }
      ],
      "abstract": "We present Planning as Descent (PaD), a framework for offline goal-conditioned reinforcement learning that grounds trajectory synthesis in verification. Instead of learning a policy or explicit planner, PaD learns a goal-conditioned energy function over entire latent trajectories, assigning low energy to feasible, goal-consistent futures. Planning is realized as gradient-based refinement in this energy landscape, using identical computation during training and inference to reduce train-test mismatch common in decoupled modeling pipelines.\n  PaD is trained via self-supervised hindsight goal relabeling, shaping the energy landscape around the planning dynamics. At inference, multiple trajectory candidates are refined under different temporal hypotheses, and low-energy plans balancing feasibility and efficiency are selected.\n  We evaluate PaD on OGBench cube manipulation tasks. When trained on narrow expert demonstrations, PaD achieves state-of-the-art 95\\% success, strongly outperforming prior methods that peak at 68\\%. Remarkably, training on noisy, suboptimal data further improves success and plan efficiency, highlighting the benefits of verification-driven planning. Our results suggest learning to evaluate and refine trajectories provides a robust alternative to direct policy learning for offline, reward-free planning.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2025-12-19T17:49:13+00:00",
      "updated": "2025-12-19T17:49:13+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17846v1",
      "file": "papers/2512.17846v1.pdf"
    },
    {
      "arxiv_id": "2512.17764v1",
      "title": "UniStateDLO: Unified Generative State Estimation and Tracking of Deformable Linear Objects Under Occlusion for Constrained Manipulation",
      "authors": [
        {
          "name": "Kangchen Lv"
        },
        {
          "name": "Mingrui Yu"
        },
        {
          "name": "Shihefeng Wang"
        },
        {
          "name": "Xiangyang Ji"
        },
        {
          "name": "Xiang Li"
        }
      ],
      "abstract": "Perception of deformable linear objects (DLOs), such as cables, ropes, and wires, is the cornerstone for successful downstream manipulation. Although vision-based methods have been extensively explored, they remain highly vulnerable to occlusions that commonly arise in constrained manipulation environments due to surrounding obstacles, large and varying deformations, and limited viewpoints. Moreover, the high dimensionality of the state space, the lack of distinctive visual features, and the presence of sensor noises further compound the challenges of reliable DLO perception. To address these open issues, this paper presents UniStateDLO, the first complete DLO perception pipeline with deep-learning methods that achieves robust performance under severe occlusion, covering both single-frame state estimation and cross-frame state tracking from partial point clouds. Both tasks are formulated as conditional generative problems, leveraging the strong capability of diffusion models to capture the complex mapping between highly partial observations and high-dimensional DLO states. UniStateDLO effectively handles a wide range of occlusion patterns, including initial occlusion, self-occlusion, and occlusion caused by multiple objects. In addition, it exhibits strong data efficiency as the entire network is trained solely on a large-scale synthetic dataset, enabling zero-shot sim-to-real generalization without any real-world training data. Comprehensive simulation and real-world experiments demonstrate that UniStateDLO outperforms all state-of-the-art baselines in both estimation and tracking, producing globally smooth yet locally precise DLO state predictions in real time, even under substantial occlusions. Its integration as the front-end module in a closed-loop DLO manipulation system further demonstrates its ability to support stable feedback control in complex, constrained 3-D environments.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-19T16:35:02+00:00",
      "updated": "2025-12-19T16:35:02+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17764v1",
      "file": "papers/2512.17764v1.pdf"
    },
    {
      "arxiv_id": "2512.17680v1",
      "title": "A Dual Quaternion based RRT* Path Planning Approach for Satellite Rendezvous and Docking",
      "authors": [
        {
          "name": "Ana Stankovic"
        },
        {
          "name": "Mohamed Khalil Ben-Larbi"
        },
        {
          "name": "Wolfgang H. Müller"
        }
      ],
      "abstract": "This paper proposes a sampling-based motion planner that employs a dual quaternion representation to generate smooth, collision-free six-degree-of-freedom pose trajectories for satellite rendezvous and docking under keep-out zone constraints. The proposed planner integrates the dual quaternion algebra directly into an RRT* framework, thereby enabling natural screw motion interpolation in SE(3). The dual quaternion-based RRT* has been implemented in Python and demonstrated on a representative multi-obstacle scenario. A comparison with a standard RRT* using separate translation and quaternion steering highlights the enhanced pose continuity and obstacle avoidance of the proposed method. The present approach is purely kinematic in nature and does not take into account relative orbital dynamics. Consequently, the resulting path provides a preliminary estimate for a subsequent optimisation-based trajectory planner, which will refine the motion with dynamic constraints for the purpose of practical satellite rendezvous and docking missions.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-19T15:17:46+00:00",
      "updated": "2025-12-19T15:17:46+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17680v1",
      "file": "papers/2512.17680v1.pdf"
    },
    {
      "arxiv_id": "2512.17661v1",
      "title": "Vidarc: Embodied Video Diffusion Model for Closed-loop Control",
      "authors": [
        {
          "name": "Yao Feng"
        },
        {
          "name": "Chendong Xiang"
        },
        {
          "name": "Xinyi Mao"
        },
        {
          "name": "Hengkai Tan"
        },
        {
          "name": "Zuyue Zhang"
        },
        {
          "name": "Shuhe Huang"
        },
        {
          "name": "Kaiwen Zheng"
        },
        {
          "name": "Haitian Liu"
        },
        {
          "name": "Hang Su"
        },
        {
          "name": "Jun Zhu"
        }
      ],
      "abstract": "Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "published": "2025-12-19T15:04:24+00:00",
      "updated": "2025-12-19T15:04:24+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17661v1",
      "file": "papers/2512.17661v1.pdf"
    },
    {
      "arxiv_id": "2512.17584v1",
      "title": "Optimized Scheduling and Positioning of Mobile Manipulators in Collaborative Applications",
      "authors": [
        {
          "name": "Christian Cella"
        },
        {
          "name": "Sole Ester Sonnino"
        },
        {
          "name": "Marco Faroni"
        },
        {
          "name": "Andrea Zanchettin"
        },
        {
          "name": "Paolo Rocco"
        }
      ],
      "abstract": "The growing integration of mobile robots in shared workspaces requires efficient path planning and coordination between the agents, accounting for safety and productivity. In this work, we propose a digital model-based optimization framework for mobile manipulators in human-robot collaborative environments, in order to determine the sequence of robot base poses and the task scheduling for the robot. The complete problem is treated as black-box, and Particle Swarm Optimization (PSO) is employed to balance conflicting Key-Performance Indicators (KPIs). We demonstrate improvements in cycle time, task sequencing, and adaptation to human presence in a collaborative box-packing scenario.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-19T13:50:07+00:00",
      "updated": "2025-12-19T13:50:07+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17584v1",
      "file": "papers/2512.17584v1.pdf"
    },
    {
      "arxiv_id": "2512.17579v1",
      "title": "On Using Neural Networks to Learn Safety Speed Reduction in Human-Robot Collaboration: A Comparative Analysis",
      "authors": [
        {
          "name": "Marco Faroni"
        },
        {
          "name": "Alessio Spanò"
        },
        {
          "name": "Andrea M. Zanchettin"
        },
        {
          "name": "Paolo Rocco"
        }
      ],
      "abstract": "In Human-Robot Collaboration, safety mechanisms such as Speed and Separation Monitoring and Power and Force Limitation dynamically adjust the robot's speed based on human proximity. While essential for risk reduction, these mechanisms introduce slowdowns that makes cycle time estimation a hard task and impact job scheduling efficiency. Existing methods for estimating cycle times or designing schedulers often rely on predefined safety models, which may not accurately reflect real-world safety implementations, as these depend on case-specific risk assessments. In this paper, we propose a deep learning approach to predict the robot's safety scaling factor directly from process execution data. We analyze multiple neural network architectures and demonstrate that a simple feed-forward network effectively estimates the robot's slowdown. This capability is crucial for improving cycle time predictions and designing more effective scheduling algorithms in collaborative robotic environments.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-19T13:46:56+00:00",
      "updated": "2025-12-19T13:46:56+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17579v1",
      "file": "papers/2512.17579v1.pdf"
    },
    {
      "arxiv_id": "2512.17568v1",
      "title": "Kinematics-Aware Diffusion Policy with Consistent 3D Observation and Action Space for Whole-Arm Robotic Manipulation",
      "authors": [
        {
          "name": "Kangchen Lv"
        },
        {
          "name": "Mingrui Yu"
        },
        {
          "name": "Yongyi Jia"
        },
        {
          "name": "Chenyu Zhang"
        },
        {
          "name": "Xiang Li"
        }
      ],
      "abstract": "Whole-body control of robotic manipulators with awareness of full-arm kinematics is crucial for many manipulation scenarios involving body collision avoidance or body-object interactions, which makes it insufficient to consider only the end-effector poses in policy learning. The typical approach for whole-arm manipulation is to learn actions in the robot's joint space. However, the unalignment between the joint space and actual task space (i.e., 3D space) increases the complexity of policy learning, as generalization in task space requires the policy to intrinsically understand the non-linear arm kinematics, which is difficult to learn from limited demonstrations. To address this issue, this letter proposes a kinematics-aware imitation learning framework with consistent task, observation, and action spaces, all represented in the same 3D space. Specifically, we represent both robot states and actions using a set of 3D points on the arm body, naturally aligned with the 3D point cloud observations. This spatially consistent representation improves the policy's sample efficiency and spatial generalizability while enabling full-body control. Built upon the diffusion policy, we further incorporate kinematics priors into the diffusion processes to guarantee the kinematic feasibility of output actions. The joint angle commands are finally calculated through an optimization-based whole-body inverse kinematics solver for execution. Simulation and real-world experimental results demonstrate higher success rates and stronger spatial generalizability of our approach compared to existing methods in body-aware manipulation policy learning.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-19T13:34:29+00:00",
      "updated": "2025-12-19T13:34:29+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17568v1",
      "file": "papers/2512.17568v1.pdf"
    },
    {
      "arxiv_id": "2512.17560v1",
      "title": "Learning-Based Safety-Aware Task Scheduling for Efficient Human-Robot Collaboration",
      "authors": [
        {
          "name": "M. Faroni"
        },
        {
          "name": "A. Spano"
        },
        {
          "name": "A. M. Zanchettin"
        },
        {
          "name": "P. Rocco"
        }
      ],
      "abstract": "Ensuring human safety in collaborative robotics can compromise efficiency because traditional safety measures increase robot cycle time when human interaction is frequent. This paper proposes a safety-aware approach to mitigate efficiency losses without assuming prior knowledge of safety logic. Using a deep-learning model, the robot learns the relationship between system state and safety-induced speed reductions based on execution data. Our framework does not explicitly predict human motions but directly models the interaction effects on robot speed, simplifying implementation and enhancing generalizability to different safety logics. At runtime, the learned model optimizes task selection to minimize cycle time while adhering to safety requirements. Experiments on a pick-and-packaging scenario demonstrated significant reductions in cycle times.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-19T13:29:32+00:00",
      "updated": "2025-12-19T13:29:32+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17560v1",
      "file": "papers/2512.17560v1.pdf"
    },
    {
      "arxiv_id": "2512.17553v1",
      "title": "Deep Learning-based Robust Autonomous Navigation of Aerial Robots in Dense Forests",
      "authors": [
        {
          "name": "Guglielmo Del Col"
        },
        {
          "name": "Väinö Karjalainen"
        },
        {
          "name": "Teemu Hakala"
        },
        {
          "name": "Yibo Zhang"
        },
        {
          "name": "Eija Honkavaara"
        }
      ],
      "abstract": "Autonomous aerial navigation in dense natural environments remains challenging due to limited visibility, thin and irregular obstacles, GNSS-denied operation, and frequent perceptual degradation. This work presents an improved deep learning-based navigation framework that integrates semantically enhanced depth encoding with neural motion-primitive evaluation for robust flight in cluttered forests. Several modules are incorporated on top of the original sevae-ORACLE algorithm to address limitations observed during real-world deployment, including lateral control for sharper maneuvering, a temporal consistency mechanism to suppress oscillatory planning decisions, a stereo-based visual-inertial odometry solution for drift-resilient state estimation, and a supervisory safety layer that filters unsafe actions in real time. A depth refinement stage is included to improve the representation of thin branches and reduce stereo noise, while GPU optimization increases onboard inference throughput from 4 Hz to 10 Hz.\n  The proposed approach is evaluated against several existing learning-based navigation methods under identical environmental conditions and hardware constraints. It demonstrates higher success rates, more stable trajectories, and improved collision avoidance, particularly in highly cluttered forest settings. The system is deployed on a custom quadrotor in three boreal forest environments, achieving fully autonomous completion in all flights in moderate and dense clutter, and 12 out of 15 flights in highly dense underbrush. These results demonstrate improved reliability and safety over existing navigation methods in complex natural environments.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-19T13:19:33+00:00",
      "updated": "2025-12-19T13:19:33+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17553v1",
      "file": "papers/2512.17553v1.pdf"
    },
    {
      "arxiv_id": "2512.17505v1",
      "title": "Adaptive Covariance and Quaternion-Focused Hybrid Error-State EKF/UKF for Visual-Inertial Odometry",
      "authors": [
        {
          "name": "Ufuk Asil"
        },
        {
          "name": "Efendi Nasibov"
        }
      ],
      "abstract": "This study presents an innovative hybrid Visual-Inertial Odometry (VIO) method for Unmanned Aerial Vehicles (UAVs) that is resilient to environmental challenges and capable of dynamically assessing sensor reliability. Built upon a loosely coupled sensor fusion architecture, the system utilizes a novel hybrid Quaternion-focused Error-State EKF/UKF (Qf-ES-EKF/UKF) architecture to process inertial measurement unit (IMU) data. This architecture first propagates the entire state using an Error-State Extended Kalman Filter (ESKF) and then applies a targeted Scaled Unscented Kalman Filter (SUKF) step to refine only the orientation. This sequential process blends the accuracy of SUKF in quaternion estimation with the overall computational efficiency of ESKF. The reliability of visual measurements is assessed via a dynamic sensor confidence score based on metrics, such as image entropy, intensity variation, motion blur, and inference quality, adapting the measurement noise covariance to ensure stable pose estimation even under challenging conditions. Comprehensive experimental analyses on the EuRoC MAV dataset demonstrate key advantages: an average improvement of 49% in position accuracy in challenging scenarios, an average of 57% in rotation accuracy over ESKF-based methods, and SUKF-comparable accuracy achieved with approximately 48% lower computational cost than a full SUKF implementation. These findings demonstrate that the presented approach strikes an effective balance between computational efficiency and estimation accuracy, and significantly enhances UAV pose estimation performance in complex environments with varying sensor reliability.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published": "2025-12-19T12:14:37+00:00",
      "updated": "2025-12-19T12:14:37+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17505v1",
      "file": "papers/2512.17505v1.pdf"
    },
    {
      "arxiv_id": "2512.17435v1",
      "title": "ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination",
      "authors": [
        {
          "name": "Teng Wang"
        },
        {
          "name": "Xinxin Zhao"
        },
        {
          "name": "Wenzhe Cai"
        },
        {
          "name": "Changyin Sun"
        }
      ],
      "abstract": "Visual navigation is a fundamental capability for autonomous home-assistance robots, enabling long-horizon tasks such as object search. While recent methods have leveraged Large Language Models (LLMs) to incorporate commonsense reasoning and improve exploration efficiency, their planning remains constrained by textual representations, which cannot adequately capture spatial occupancy or scene geometry--critical factors for navigation decisions. We explore whether Vision-Language Models (VLMs) can achieve mapless visual navigation using only onboard RGB/RGB-D streams, unlocking their potential for spatial perception and planning. We achieve this through an imagination-powered navigation framework, ImagineNav++, which imagines future observation images from candidate robot views and translates navigation planning into a simple best-view image selection problem for VLMs. First, a future-view imagination module distills human navigation preferences to generate semantically meaningful viewpoints with high exploration potential. These imagined views then serve as visual prompts for the VLM to identify the most informative viewpoint. To maintain spatial consistency, we develop a selective foveation memory mechanism, which hierarchically integrates keyframe observations via a sparse-to-dense framework, constructing a compact yet comprehensive memory for long-term spatial reasoning. This approach transforms goal-oriented navigation into a series of tractable point-goal navigation tasks. Extensive experiments on open-vocabulary object and instance navigation benchmarks show that ImagineNav++ achieves SOTA performance in mapless settings, even surpassing most map-based methods, highlighting the importance of scene imagination and memory in VLM-based spatial reasoning.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-19T10:40:16+00:00",
      "updated": "2025-12-19T10:40:16+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17435v1",
      "file": "papers/2512.17435v1.pdf"
    },
    {
      "arxiv_id": "2512.17425v1",
      "title": "Personalized Gait Patterns During Exoskeleton-Aided Training May Have Minimal Effect on User Experience. Insights from a Pilot Study",
      "authors": [
        {
          "name": "Beatrice Luciani"
        },
        {
          "name": "Katherine Lin Poggensee"
        },
        {
          "name": "Heike Vallery"
        },
        {
          "name": "Alex van den Berg"
        },
        {
          "name": "Severin David Woernle"
        },
        {
          "name": "Mostafa Mogharabi"
        },
        {
          "name": "Stefano Dalla Gasperina"
        },
        {
          "name": "Laura Marchal-Crespo"
        }
      ],
      "abstract": "Robot-aided gait rehabilitation facilitates high-intensity and repeatable therapy. However, most exoskeletons rely on pre-recorded, non-personalized gait trajectories constrained to the sagittal plane, potentially limiting movement naturalness and user comfort. We present a data-driven gait personalization framework for an exoskeleton that supports multi-planar motion, including hip abduction/adduction and pelvic translation and rotation. Personalized trajectories to individual participants were generated using regression models trained on anthropometric, demographic, and walking speed data from a normative database. In a within-subject experiment involving ten unimpaired participants, these personalized trajectories were evaluated in regard to comfort, naturalness, and overall experience and compared against two standard patterns from the same database: one averaging all the trajectories, and one randomly selected. We did not find relevant differences across pattern conditions, despite all trajectories being executed with high accuracy thanks to a stiff position-derivative controller. We found, however, that pattern conditions in later trials were rated as more comfortable and natural than those in the first trial, suggesting that participants might have adapted to walking within the exoskeleton, regardless of the enforced gait pattern. Our findings highlight the importance of integrating subjective feedback when designing personalized gait controllers and accounting for user adaptation during experimentation.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-19T10:23:29+00:00",
      "updated": "2025-12-19T10:23:29+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17425v1",
      "file": "papers/2512.17425v1.pdf"
    },
    {
      "arxiv_id": "2512.17349v1",
      "title": "Flying in Clutter on Monocular RGB by Learning in 3D Radiance Fields with Domain Adaptation",
      "authors": [
        {
          "name": "Xijie Huang"
        },
        {
          "name": "Jinhan Li"
        },
        {
          "name": "Tianyue Wu"
        },
        {
          "name": "Xin Zhou"
        },
        {
          "name": "Zhichao Han"
        },
        {
          "name": "Fei Gao"
        }
      ],
      "abstract": "Modern autonomous navigation systems predominantly rely on lidar and depth cameras. However, a fundamental question remains: Can flying robots navigate in clutter using solely monocular RGB images? Given the prohibitive costs of real-world data collection, learning policies in simulation offers a promising path. Yet, deploying such policies directly in the physical world is hindered by the significant sim-to-real perception gap. Thus, we propose a framework that couples the photorealism of 3D Gaussian Splatting (3DGS) environments with Adversarial Domain Adaptation. By training in high-fidelity simulation while explicitly minimizing feature discrepancy, our method ensures the policy relies on domain-invariant cues. Experimental results demonstrate that our policy achieves robust zero-shot transfer to the physical world, enabling safe and agile flight in unstructured environments with varying illumination.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-19T08:44:06+00:00",
      "updated": "2025-12-19T08:44:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17349v1",
      "file": "papers/2512.17349v1.pdf"
    },
    {
      "arxiv_id": "2512.17321v1",
      "title": "Neuro-Symbolic Control with Large Language Models for Language-Guided Spatial Tasks",
      "authors": [
        {
          "name": "Momina Liaqat Ali"
        },
        {
          "name": "Muhammad Abid"
        }
      ],
      "abstract": "Although large language models (LLMs) have recently become effective tools for language-conditioned control in embodied systems, instability, slow convergence, and hallucinated actions continue to limit their direct application to continuous control. A modular neuro-symbolic control framework that clearly distinguishes between low-level motion execution and high-level semantic reasoning is proposed in this work. While a lightweight neural delta controller performs bounded, incremental actions in continuous space, a locally deployed LLM interprets symbolic tasks. We assess the suggested method in a planar manipulation setting with spatial relations between objects specified by language. Numerous tasks and local language models, such as Mistral, Phi, and LLaMA-3.2, are used in extensive experiments to compare LLM-only control, neural-only control, and the suggested LLM+DL framework. In comparison to LLM-only baselines, the results show that the neuro-symbolic integration consistently increases both success rate and efficiency, achieving average step reductions exceeding 70% and speedups of up to 8.83x while remaining robust to language model quality. The suggested framework enhances interpretability, stability, and generalization without any need of reinforcement learning or costly rollouts by controlling the LLM to symbolic outputs and allocating uninterpreted execution to a neural controller trained on artificial geometric data. These outputs show empirically that neuro-symbolic decomposition offers a scalable and principled way to integrate language understanding with ongoing control, this approach promotes the creation of dependable and effective language-guided embodied systems.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-19T08:08:40+00:00",
      "updated": "2025-12-19T08:08:40+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17321v1",
      "file": "papers/2512.17321v1.pdf"
    },
    {
      "arxiv_id": "2512.17309v1",
      "title": "RecipeMasterLLM: Revisiting RoboEarth in the Era of Large Language Models",
      "authors": [
        {
          "name": "Asil Kaan Bozcuoglu"
        },
        {
          "name": "Ziyuan Liu"
        }
      ],
      "abstract": "RoboEarth was a pioneering initiative in cloud robotics, establishing a foundational framework for robots to share and exchange knowledge about actions, objects, and environments through a standardized knowledge graph. Initially, this knowledge was predominantly hand-crafted by engineers using RDF triples within OWL Ontologies, with updates, such as changes in an object's pose, being asserted by the robot's control and perception routines. However, with the advent and rapid development of Large Language Models (LLMs), we believe that the process of knowledge acquisition can be significantly automated. To this end, we propose RecipeMasterLLM, a high-level planner, that generates OWL action ontologies based on a standardized knowledge graph in response to user prompts. This architecture leverages a fine-tuned LLM specifically trained to understand and produce action descriptions consistent with the RoboEarth standardized knowledge graph. Moreover, during the Retrieval-Augmented Generation (RAG) phase, environmental knowledge is supplied to the LLM to enhance its contextual understanding and improve the accuracy of the generated action descriptions.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-19T07:47:19+00:00",
      "updated": "2025-12-19T07:47:19+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17309v1",
      "file": "papers/2512.17309v1.pdf"
    },
    {
      "arxiv_id": "2512.17241v1",
      "title": "A Service Robot's Guide to Interacting with Busy Customers",
      "authors": [
        {
          "name": "Suraj Nukala"
        },
        {
          "name": "Meera Sushma"
        },
        {
          "name": "Leimin Tian"
        },
        {
          "name": "Akansel Cosgun"
        },
        {
          "name": "Dana Kulic"
        }
      ],
      "abstract": "The growing use of service robots in hospitality highlights the need to understand how to effectively communicate with pre-occupied customers. This study investigates the efficacy of commonly used communication modalities by service robots, namely, acoustic/speech, visual display, and micromotion gestures in capturing attention and communicating intention with a user in a simulated restaurant scenario. We conducted a two-part user study (N=24) using a Temi robot to simulate delivery tasks, with participants engaged in a typing game (MonkeyType) to emulate a state of busyness. The participants' engagement in the typing game is measured by words per minute (WPM) and typing accuracy. In Part 1, we compared non-verbal acoustic cue versus baseline conditions to assess attention capture during a single-cup delivery task. In Part 2, we evaluated the effectiveness of speech, visual display, micromotion and their multimodal combination in conveying specific intentions (correct cup selection) during a two-cup delivery task. The results indicate that, while speech is highly effective in capturing attention, it is less successful in clearly communicating intention. Participants rated visual as the most effective modality for intention clarity, followed by speech, with micromotion being the lowest ranked.These findings provide insights into optimizing communication strategies for service robots, highlighting the distinct roles of attention capture and intention communication in enhancing user experience in dynamic hospitality settings.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "published": "2025-12-19T05:05:14+00:00",
      "updated": "2025-12-19T05:05:14+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17241v1",
      "file": "papers/2512.17241v1.pdf"
    },
    {
      "arxiv_id": "2512.17215v1",
      "title": "Research on Dead Reckoning Algorithm for Self-Propelled Pipeline Robots in Three-Dimensional Complex Pipelines",
      "authors": [
        {
          "name": "Yan Gao"
        },
        {
          "name": "Jiliang Wang"
        },
        {
          "name": "Minghan Wang"
        },
        {
          "name": "Xiaohua Chen"
        },
        {
          "name": "Demin Chen"
        },
        {
          "name": "Zhiyong Ren"
        },
        {
          "name": "Tian-Yun Huang"
        }
      ],
      "abstract": "In the field of gas pipeline location, existing pipeline location methods mostly rely on pipeline location instruments. However, when faced with complex and curved pipeline scenarios, these methods often fail due to problems such as cable entanglement and insufficient equipment flexibility. To address this pain point, we designed a self-propelled pipeline robot. This robot can autonomously complete the location work of complex and curved pipelines in complex pipe networks without external dragging. In terms of pipeline mapping technology, traditional visual mapping and laser mapping methods are easily affected by lighting conditions and insufficient features in the confined space of pipelines, resulting in mapping drift and divergence problems. In contrast, the pipeline location method that integrates inertial navigation and wheel odometers is less affected by pipeline environmental factors. Based on this, this paper proposes a pipeline robot location method based on extended Kalman filtering (EKF). Firstly, the body attitude angle is initially obtained through an inertial measurement unit (IMU). Then, the extended Kalman filtering algorithm is used to improve the accuracy of attitude angle estimation. Finally, high-precision pipeline location is achieved by combining wheel odometers. During the testing phase, the roll wheels of the pipeline robot needed to fit tightly against the pipe wall to reduce slippage. However, excessive tightness would reduce the flexibility of motion control due to excessive friction. Therefore, a balance needed to be struck between the robot's motion capability and positioning accuracy. Experiments were conducted using the self-propelled pipeline robot in a rectangular loop pipeline, and the results verified the effectiveness of the proposed dead reckoning algorithm.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2025-12-19T03:58:02+00:00",
      "updated": "2025-12-19T03:58:02+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17215v1",
      "file": "papers/2512.17215v1.pdf"
    },
    {
      "arxiv_id": "2512.17212v1",
      "title": "Design and Research of a Self-Propelled Pipeline Robot Based on Force Analysis and Dynamic Simulation",
      "authors": [
        {
          "name": "Yan Gao"
        },
        {
          "name": "Jiliang Wang"
        },
        {
          "name": "Ming Cheng"
        },
        {
          "name": "Tianyun Huang"
        }
      ],
      "abstract": "In pipeline inspection, traditional tethered inspection robots are severely constrained by cable length and weight, which greatly limit their travel range and accessibility. To address these issues, this paper proposes a self-propelled pipeline robot design based on force analysis and dynamic simulation, with a specific focus on solving core challenges including vertical climbing failure and poor passability in T-branch pipes. Adopting a wheeled configuration and modular design, the robot prioritizes the core demand of body motion control. Specifically, 3D modeling of the robot was first completed using SolidWorks. Subsequently, the model was imported into ADAMS for dynamic simulation, which provided a basis for optimizing the drive module and motion control strategy.To verify the robot's dynamic performance, an experimental platform with acrylic pipes was constructed. Through adjusting its body posture to surmount obstacles and select directions, the robot has demonstrated its ability to stably traverse various complex pipeline scenarios. Notably, this work offers a technical feasibility reference for the application of pipeline robots in the inspection of medium and low-pressure urban gas pipelines.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-19T03:49:09+00:00",
      "updated": "2025-12-19T03:49:09+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17212v1",
      "file": "papers/2512.17212v1.pdf"
    },
    {
      "arxiv_id": "2512.17183v1",
      "title": "Semantic Co-Speech Gesture Synthesis and Real-Time Control for Humanoid Robots",
      "authors": [
        {
          "name": "Gang Zhang"
        }
      ],
      "abstract": "We present an innovative end-to-end framework for synthesizing semantically meaningful co-speech gestures and deploying them in real-time on a humanoid robot. This system addresses the challenge of creating natural, expressive non-verbal communication for robots by integrating advanced gesture generation techniques with robust physical control. Our core innovation lies in the meticulous integration of a semantics-aware gesture synthesis module, which derives expressive reference motions from speech input by leveraging a generative retrieval mechanism based on large language models (LLMs) and an autoregressive Motion-GPT model. This is coupled with a high-fidelity imitation learning control policy, the MotionTracker, which enables the Unitree G1 humanoid robot to execute these complex motions dynamically and maintain balance. To ensure feasibility, we employ a robust General Motion Retargeting (GMR) method to bridge the embodiment gap between human motion data and the robot platform. Through comprehensive evaluation, we demonstrate that our combined system produces semantically appropriate and rhythmically coherent gestures that are accurately tracked and executed by the physical robot. To our knowledge, this work represents a significant step toward general real-world use by providing a complete pipeline for automatic, semantic-aware, co-speech gesture generation and synchronized real-time physical deployment on a humanoid robot.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-19T02:55:10+00:00",
      "updated": "2025-12-19T02:55:10+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17183v1",
      "file": "papers/2512.17183v1.pdf"
    },
    {
      "arxiv_id": "2512.17180v2",
      "title": "Conservative Bias in Multi-Teacher Learning: Why Agents Prefer Low-Reward Advisors",
      "authors": [
        {
          "name": "Maher Mesto"
        },
        {
          "name": "Francisco Cruz"
        }
      ],
      "abstract": "Interactive reinforcement learning (IRL) has shown promise in enabling autonomous agents and robots to learn complex behaviours from human teachers, yet the dynamics of teacher selection remain poorly understood. This paper reveals an unexpected phenomenon in IRL: when given a choice between teachers with different reward structures, learning agents overwhelmingly prefer conservative, low-reward teachers (93.16% selection rate) over those offering 20x higher rewards. Through 1,250 experimental runs in navigation tasks with multiple expert teachers, we discovered: (1) Conservative bias dominates teacher selection: agents systematically choose the lowest-reward teacher, prioritising consistency over optimality; (2) Critical performance thresholds exist at teacher availability rho >= 0.6 and accuracy omega >= 0.6, below which the framework fails catastrophically; (3) The framework achieves 159% improvement over baseline Q-learning under concept drift. These findings challenge fundamental assumptions about optimal teaching in RL and suggest potential implications for human-robot collaboration, where human preferences for safety and consistency may align with the observed agent selection behaviour, potentially informing training paradigms for safety-critical robotic applications.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2025-12-19T02:38:04+00:00",
      "updated": "2025-12-23T06:26:45+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17180v2",
      "file": "papers/2512.17180v2.pdf"
    },
    {
      "arxiv_id": "2512.17136v2",
      "title": "Towards Senior-Robot Interaction: Reactive Robot Dog Gestures",
      "authors": [
        {
          "name": "Chunyang Meng"
        },
        {
          "name": "Eduardo B. Sandoval"
        },
        {
          "name": "Ricardo Sosa"
        },
        {
          "name": "Francisco Cruz"
        }
      ],
      "abstract": "As the global population ages, many seniors face the problem of loneliness. Companion robots offer a potential solution. However, current companion robots often lack advanced functionality, while task-oriented robots are not designed for social interaction, limiting their suitability and acceptance by seniors. Our work introduces a senior-oriented system for quadruped robots that allows for more intuitive user input and provides more socially expressive output. For user input, we implemented a MediaPipe-based module for hand gesture and head movement recognition, enabling control without a remote. For output, we designed and trained robotic dog gestures using curriculum-based reinforcement learning in Isaac Gym, progressing from simple standing to three-legged balancing and leg extensions, and more. The final tests achieved over 95\\% success on average in simulation, and we validated a key social gesture (the paw-lift) on a Unitree robot. Real-world tests demonstrated the feasibility and social expressiveness of this framework, while also revealing sim-to-real challenges in joint compliance, load distribution, and balance control. These contributions advance the development of practical quadruped robots as social companions for the senior and outline pathways for sim-to-real adaptation and inform future user studies.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-19T00:09:18+00:00",
      "updated": "2025-12-22T15:50:31+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17136v2",
      "file": "papers/2512.17136v2.pdf"
    },
    {
      "arxiv_id": "2512.17091v1",
      "title": "Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making",
      "authors": [
        {
          "name": "Toshiaki Hori"
        },
        {
          "name": "Jonathan DeCastro"
        },
        {
          "name": "Deepak Gopinath"
        },
        {
          "name": "Avinash Balachandran"
        },
        {
          "name": "Guy Rosman"
        }
      ],
      "abstract": "We propose a new approach for solving planning problems with a hierarchical structure, fusing reinforcement learning and MPC planning. Our formulation tightly and elegantly couples the two planning paradigms. It leverages reinforcement learning actions to inform the MPPI sampler, and adaptively aggregates MPPI samples to inform the value estimation. The resulting adaptive process leverages further MPPI exploration where value estimates are uncertain, and improves training robustness and the overall resulting policies. This results in a robust planning approach that can handle complex planning problems and easily adapts to different applications, as demonstrated over several domains, including race driving, modified Acrobot, and Lunar Lander with added obstacles. Our results in these domains show better data efficiency and overall performance in terms of both rewards and task success, with up to a 72% increase in success rate compared to existing approaches, as well as accelerated convergence (x2.1) compared to non-adaptive sampling.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "published": "2025-12-18T21:44:00+00:00",
      "updated": "2025-12-18T21:44:00+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17091v1",
      "file": "papers/2512.17091v1.pdf"
    },
    {
      "arxiv_id": "2512.17062v1",
      "title": "Lang2Manip: A Tool for LLM-Based Symbolic-to-Geometric Planning for Manipulation",
      "authors": [
        {
          "name": "Muhayy Ud Din"
        },
        {
          "name": "Jan Rosell"
        },
        {
          "name": "Waseem Akram"
        },
        {
          "name": "Irfan Hussain"
        }
      ],
      "abstract": "Simulation is essential for developing robotic manipulation systems, particularly for task and motion planning (TAMP), where symbolic reasoning interfaces with geometric, kinematic, and physics-based execution. Recent advances in Large Language Models (LLMs) enable robots to generate symbolic plans from natural language, yet executing these plans in simulation often requires robot-specific engineering or planner-dependent integration. In this work, we present a unified pipeline that connects an LLM-based symbolic planner with the Kautham motion planning framework to achieve generalizable, robot-agnostic symbolic-to-geometric manipulation. Kautham provides ROS-compatible support for a wide range of industrial manipulators and offers geometric, kinodynamic, physics-driven, and constraint-based motion planning under a single interface. Our system converts language instructions into symbolic actions and computes and executes collision-free trajectories using any of Kautham's planners without additional coding. The result is a flexible and scalable tool for language-driven TAMP that is generalized across robots, planning modalities, and manipulation tasks.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-18T20:58:02+00:00",
      "updated": "2025-12-18T20:58:02+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17062v1",
      "file": "papers/2512.17062v1.pdf"
    },
    {
      "arxiv_id": "2512.17001v1",
      "title": "Mr.MSTE: Multi-robot Multi-Source Term Estimation with Wind-Aware Coverage Control",
      "authors": [
        {
          "name": "Rohit V. Nanavati"
        },
        {
          "name": "Tim J. Glover"
        },
        {
          "name": "Matthew J. Coombes"
        },
        {
          "name": "Cunjia Liu"
        }
      ],
      "abstract": "This paper presents a Multi-Robot Multi-Source Term Estimation (MRMSTE) framework that enables teams of mobile robots to collaboratively sample gas concentrations and infer the parameters of an unknown number of airborne releases. The framework is built on a hybrid Bayesian inference scheme that represents the joint multi-source probability density and incorporates physics-informed state transitions, including source birth, removal, and merging induced by atmospheric dispersion. A superposition-based measurement model is naturally accommodated, allowing sparse concentration measurements to be exploited efficiently. To guide robot deployment, we introduce a wind-aware coverage control (WCC) strategy that integrates the evolving multi-source belief with local wind information to prioritize regions of high detection likelihood. Unlike conventional coverage control or information-theoretic planners, WCC explicitly accounts for anisotropic plume transport when modelling sensor performance, leading to more effective sensor placement for multi-source estimation. Monte Carlo studies demonstrate faster convergence and improved separation of individual source beliefs compared to traditional coverage-based strategies and small-scale static sensor networks. Real-world experiments with CO2 releases using TurtleBot platforms further validate the proposed approach, demonstrating its practicality for scalable multi-robot gas-sensing applications.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-18T19:02:07+00:00",
      "updated": "2025-12-18T19:02:07+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17001v1",
      "file": "papers/2512.17001v1.pdf"
    },
    {
      "arxiv_id": "2512.16911v1",
      "title": "Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning",
      "authors": [
        {
          "name": "Andrew Wagenmaker"
        },
        {
          "name": "Perry Dong"
        },
        {
          "name": "Raymond Tsao"
        },
        {
          "name": "Chelsea Finn"
        },
        {
          "name": "Sergey Levine"
        }
      ],
      "abstract": "Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "published": "2025-12-18T18:59:17+00:00",
      "updated": "2025-12-18T18:59:17+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16911v1",
      "file": "papers/2512.16911v1.pdf"
    },
    {
      "arxiv_id": "2512.16909v1",
      "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
      "authors": [
        {
          "name": "Yuanchen Ju"
        },
        {
          "name": "Yongyuan Liang"
        },
        {
          "name": "Yen-Jen Wang"
        },
        {
          "name": "Nandiraju Gireesh"
        },
        {
          "name": "Yuanliang Ju"
        },
        {
          "name": "Seungjae Lee"
        },
        {
          "name": "Qiao Gu"
        },
        {
          "name": "Elvis Hsieh"
        },
        {
          "name": "Furong Huang"
        },
        {
          "name": "Koushil Sreenath"
        }
      ],
      "abstract": "Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-12-18T18:59:03+00:00",
      "updated": "2025-12-18T18:59:03+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16909v1",
      "file": "papers/2512.16909v1.pdf"
    },
    {
      "arxiv_id": "2512.16881v1",
      "title": "PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies",
      "authors": [
        {
          "name": "Arhan Jain"
        },
        {
          "name": "Mingtong Zhang"
        },
        {
          "name": "Kanav Arora"
        },
        {
          "name": "William Chen"
        },
        {
          "name": "Marcel Torne"
        },
        {
          "name": "Muhammad Zubair Irshad"
        },
        {
          "name": "Sergey Zakharov"
        },
        {
          "name": "Yue Wang"
        },
        {
          "name": "Sergey Levine"
        },
        {
          "name": "Chelsea Finn"
        },
        {
          "name": "Wei-Chiu Ma"
        },
        {
          "name": "Dhruv Shah"
        },
        {
          "name": "Abhishek Gupta"
        },
        {
          "name": "Karl Pertsch"
        }
      ],
      "abstract": "A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "published": "2025-12-18T18:49:41+00:00",
      "updated": "2025-12-18T18:49:41+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16881v1",
      "file": "papers/2512.16881v1.pdf"
    },
    {
      "arxiv_id": "2512.16861v1",
      "title": "ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning",
      "authors": [
        {
          "name": "Zihan Zhou"
        },
        {
          "name": "Animesh Garg"
        },
        {
          "name": "Ajay Mandlekar"
        },
        {
          "name": "Caelan Garrett"
        }
      ],
      "abstract": "Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-18T18:32:39+00:00",
      "updated": "2025-12-18T18:32:39+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16861v1",
      "file": "papers/2512.16861v1.pdf"
    },
    {
      "arxiv_id": "2512.16811v1",
      "title": "GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation",
      "authors": [
        {
          "name": "Jingjing Qian"
        },
        {
          "name": "Boyao Han"
        },
        {
          "name": "Chen Shi"
        },
        {
          "name": "Lei Xiao"
        },
        {
          "name": "Long Yang"
        },
        {
          "name": "Shaoshuai Shi"
        },
        {
          "name": "Li Jiang"
        }
      ],
      "abstract": "Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-12-18T17:51:42+00:00",
      "updated": "2025-12-18T17:51:42+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16811v1",
      "file": "papers/2512.16811v1.pdf"
    },
    {
      "arxiv_id": "2512.16793v1",
      "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence",
      "authors": [
        {
          "name": "Xiaopeng Lin"
        },
        {
          "name": "Shijie Lian"
        },
        {
          "name": "Bin Yu"
        },
        {
          "name": "Ruoqi Yang"
        },
        {
          "name": "Changti Wu"
        },
        {
          "name": "Yuzhuo Miao"
        },
        {
          "name": "Yurun Jin"
        },
        {
          "name": "Yukun Shi"
        },
        {
          "name": "Cong Huang"
        },
        {
          "name": "Bojun Cheng"
        },
        {
          "name": "Kai Chen"
        }
      ],
      "abstract": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-18T17:27:03+00:00",
      "updated": "2025-12-18T17:27:03+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16793v1",
      "file": "papers/2512.16793v1.pdf"
    },
    {
      "arxiv_id": "2512.16724v1",
      "title": "VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation",
      "authors": [
        {
          "name": "Yixiang Chen"
        },
        {
          "name": "Yan Huang"
        },
        {
          "name": "Keji He"
        },
        {
          "name": "Peiyan Li"
        },
        {
          "name": "Liang Wang"
        }
      ],
      "abstract": "When performing 3D manipulation tasks, robots have to execute action planning based on perceptions from multiple fixed cameras. The multi-camera setup introduces substantial redundancy and irrelevant information, which increases computational costs and forces the model to spend extra training time extracting crucial task-relevant details. To filter out redundant information and accurately extract task-relevant features, we propose the VERM (Virtual Eye for Robotic Manipulation) method, leveraging the knowledge in foundation models to imagine a virtual task-adaptive view from the constructed 3D point cloud, which efficiently captures necessary information and mitigates occlusion. To facilitate 3D action planning and fine-grained manipulation, we further design a depth-aware module and a dynamic coarse-to-fine procedure. Extensive experimental results on both simulation benchmark RLBench and real-world evaluations demonstrate the effectiveness of our method, surpassing previous state-of-the-art methods while achieving 1.89x speedup in training time and 1.54x speedup in inference speed. More results can be found on our project website at https://verm-ral.github.io .",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published": "2025-12-18T16:26:17+00:00",
      "updated": "2025-12-18T16:26:17+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16724v1",
      "file": "papers/2512.16724v1.pdf"
    },
    {
      "arxiv_id": "2512.16705v1",
      "title": "Olaf: Bringing an Animated Character to Life in the Physical World",
      "authors": [
        {
          "name": "David Müller"
        },
        {
          "name": "Espen Knoop"
        },
        {
          "name": "Dario Mylonopoulos"
        },
        {
          "name": "Agon Serifi"
        },
        {
          "name": "Michael A. Hopkins"
        },
        {
          "name": "Ruben Grandia"
        },
        {
          "name": "Moritz Bächer"
        }
      ],
      "abstract": "Animated characters often move in non-physical ways and have proportions that are far from a typical walking robot. This provides an ideal platform for innovation in both mechanical design and stylized motion control. In this paper, we bring Olaf to life in the physical world, relying on reinforcement learning guided by animation references for control. To create the illusion of Olaf's feet moving along his body, we hide two asymmetric legs under a soft foam skirt. To fit actuators inside the character, we use spherical and planar linkages in the arms, mouth, and eyes. Because the walk cycle results in harsh contact sounds, we introduce additional rewards that noticeably reduce impact noise. The large head, driven by small actuators in the character's slim neck, creates a risk of overheating, amplified by the costume. To keep actuators from overheating, we feed temperature values as additional inputs to policies, introducing new rewards to keep them within bounds. We validate the efficacy of our modeling in simulation and on hardware, demonstrating an unmatched level of believability for a costumed robotic character.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "published": "2025-12-18T16:10:18+00:00",
      "updated": "2025-12-18T16:10:18+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16705v1",
      "file": "papers/2512.16705v1.pdf"
    },
    {
      "arxiv_id": "2512.16555v1",
      "title": "A Formal Modular Synthesis Approach for the Coordination of 3-D Robotic Construction with Multi-robots",
      "authors": [
        {
          "name": "Marcelo Rosa"
        },
        {
          "name": "José E. R. Cury"
        },
        {
          "name": "Fabio L. Baldissera"
        }
      ],
      "abstract": "In this paper, we deal with the problem of coordinating multiple robots to build 3-D structures. This problem consists of a set of mobile robots that interact with each other in order to autonomously build a predefined 3-D structure. Our approach is based on Supervisory Control Theory, and it allows us to synthesize from models that represent a single robot and the target structure a correct-by-construction reactive controller, called supervisor. When this supervisor is replicated for the other robots, then the target structure can be completed by all robots",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.FL",
        "cs.MA",
        "eess.SY"
      ],
      "published": "2025-12-18T13:58:17+00:00",
      "updated": "2025-12-18T13:58:17+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16555v1",
      "file": "papers/2512.16555v1.pdf"
    },
    {
      "arxiv_id": "2512.16461v1",
      "title": "SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning",
      "authors": [
        {
          "name": "Tin Stribor Sohn"
        },
        {
          "name": "Maximilian Dillitzer"
        },
        {
          "name": "Jason J. Corso"
        },
        {
          "name": "Eric Sax"
        }
      ],
      "abstract": "Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction. While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics. Conversely, geometric perception captures structure and motion but remains semantically sparse. We propose SNOW (Scene Understanding with Open-World Knowledge), a training-free and backbone-agnostic framework for unified 4D scene understanding that integrates VLM-derived semantics with point cloud geometry and temporal consistency. SNOW processes synchronized RGB images and 3D point clouds, using HDBSCAN clustering to generate object-level proposals that guide SAM2-based segmentation. Each segmented region is encoded through our proposed Spatio-Temporal Tokenized Patch Encoding (STEP), producing multimodal tokens that capture localized semantic, geometric, and temporal attributes. These tokens are incrementally integrated into a 4D Scene Graph (4DSG), which serves as 4D prior for downstream reasoning. A lightweight SLAM backend anchors all STEP tokens spatially in the environment, providing the global reference alignment, and ensuring unambiguous spatial grounding across time. The resulting 4DSG forms a queryable, unified world model through which VLMs can directly interpret spatial scene structure and temporal dynamics. Experiments on a diverse set of benchmarks demonstrate that SNOW enables precise 4D scene understanding and spatially grounded inference, thereby setting new state-of-the-art performance in several settings, highlighting the importance of structured 4D priors for embodied reasoning and autonomous robotics.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-12-18T12:27:06+00:00",
      "updated": "2025-12-18T12:27:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16461v1",
      "file": "papers/2512.16461v1.pdf"
    },
    {
      "arxiv_id": "2512.16449v1",
      "title": "Single-View Shape Completion for Robotic Grasping in Clutter",
      "authors": [
        {
          "name": "Abhishek Kashyap"
        },
        {
          "name": "Yuxuan Yang"
        },
        {
          "name": "Henrik Andreasson"
        },
        {
          "name": "Todor Stoyanov"
        }
      ],
      "abstract": "In vision-based robot manipulation, a single camera view can only capture one side of objects of interest, with additional occlusions in cluttered scenes further restricting visibility. As a result, the observed geometry is incomplete, and grasp estimation algorithms perform suboptimally. To address this limitation, we leverage diffusion models to perform category-level 3D shape completion from partial depth observations obtained from a single view, reconstructing complete object geometries to provide richer context for grasp planning. Our method focuses on common household items with diverse geometries, generating full 3D shapes that serve as input to downstream grasp inference networks. Unlike prior work, which primarily considers isolated objects or minimal clutter, we evaluate shape completion and grasping in realistic clutter scenarios with household objects. In preliminary evaluations on a cluttered scene, our approach consistently results in better grasp success rates than a naive baseline without shape completion by 23% and over a recent state of the art shape completion approach by 19%. Our code is available at https://amm.aass.oru.se/shape-completion-grasping/.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-18T12:11:05+00:00",
      "updated": "2025-12-18T12:11:05+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16449v1",
      "file": "papers/2512.16449v1.pdf"
    },
    {
      "arxiv_id": "2512.16446v1",
      "title": "E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion",
      "authors": [
        {
          "name": "Enis Yalcin"
        },
        {
          "name": "Joshua O'Hara"
        },
        {
          "name": "Maria Stamatopoulou"
        },
        {
          "name": "Chengxu Zhou"
        },
        {
          "name": "Dimitrios Kanoulas"
        }
      ],
      "abstract": "Vision-language models (VLMs) show promise in automating reward design in humanoid locomotion, which could eliminate the need for tedious manual engineering. However, current VLM-based methods are essentially \"blind\", as they lack the environmental perception required to navigate complex terrain. We present E-SDS (Environment-aware See it, Do it, Sorted), a framework that closes this perception gap. E-SDS integrates VLMs with real-time terrain sensor analysis to automatically generate reward functions that facilitate training of robust perceptive locomotion policies, grounded by example videos. Evaluated on a Unitree G1 humanoid across four distinct terrains (simple, gaps, obstacles, stairs), E-SDS uniquely enabled successful stair descent, while policies trained with manually-designed rewards or a non-perceptive automated baseline were unable to complete the task. In all terrains, E-SDS also reduced velocity tracking error by 51.9-82.6%. Our framework reduces the human effort of reward design from days to less than two hours while simultaneously producing more robust and capable locomotion policies.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2025-12-18T12:08:24+00:00",
      "updated": "2025-12-18T12:08:24+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16446v1",
      "file": "papers/2512.16446v1.pdf"
    },
    {
      "arxiv_id": "2512.16367v1",
      "title": "A2VISR: An Active and Adaptive Ground-Aerial Localization System Using Visual Inertial and Single-Range Fusion",
      "authors": [
        {
          "name": "Sijia Chen"
        },
        {
          "name": "Wei Dong"
        }
      ],
      "abstract": "It's a practical approach using the ground-aerial collaborative system to enhance the localization robustness of flying robots in cluttered environments, especially when visual sensors degrade. Conventional approaches estimate the flying robot's position using fixed cameras observing pre-attached markers, which could be constrained by limited distance and susceptible to capture failure. To address this issue, we improve the ground-aerial localization framework in a more comprehensive manner, which integrates active vision, single-ranging, inertial odometry, and optical flow. First, the designed active vision subsystem mounted on the ground vehicle can be dynamically rotated to detect and track infrared markers on the aerial robot, improving the field of view and the target recognition with a single camera. Meanwhile, the incorporation of single-ranging extends the feasible distance and enhances re-capture capability under visual degradation. During estimation, a dimension-reduced estimator fuses multi-source measurements based on polynomial approximation with an extended sliding window, balancing computational efficiency and redundancy. Considering different sensor fidelities, an adaptive sliding confidence evaluation algorithm is implemented to assess measurement quality and dynamically adjust the weighting parameters based on moving variance. Finally, extensive experiments under conditions such as smoke interference, illumination variation, obstacle occlusion, prolonged visual loss, and extended operating range demonstrate that the proposed approach achieves robust online localization, with an average root mean square error of approximately 0.09 m, while maintaining resilience to capture loss and sensor failures.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-18T10:07:06+00:00",
      "updated": "2025-12-18T10:07:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16367v1",
      "file": "papers/2512.16367v1.pdf"
    },
    {
      "arxiv_id": "2512.17958v1",
      "title": "Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization",
      "authors": [
        {
          "name": "Farida Mohsen"
        },
        {
          "name": "Ali Safa"
        }
      ],
      "abstract": "Service robots in public spaces require real-time understanding of human behavioral intentions for natural interaction. We present a practical multimodal framework for frame-accurate human-robot interaction intent detection that fuses camera-invariant 2D skeletal pose and facial emotion features extracted from monocular RGB video. Unlike prior methods requiring RGB-D sensors or GPU acceleration, our approach resource-constrained embedded hardware (Raspberry Pi 5, CPU-only). To address the severe class imbalance in natural human-robot interaction datasets, we introduce a novel approach to synthesize temporally coherent pose-emotion-label sequences for data re-balancing called MINT-RVAE (Multimodal Recurrent Variational Autoencoder for Intent Sequence Generation). Comprehensive offline evaluations under cross-subject and cross-scene protocols demonstrate strong generalization performance, achieving frame- and sequence-level AUROC of 0.95. Crucially, we validate real-world generalization through cross-camera evaluation on the MIRA robot head, which employs a different onboard RGB sensor and operates in uncontrolled environments not represented in the training data. Despite this domain shift, the deployed system achieves 91% accuracy and 100% recall across 32 live interaction trials. The close correspondence between offline and deployed performance confirms the cross-sensor and cross-environment robustness of the proposed multimodal approach, highlighting its suitability for ubiquitous multimedia-enabled social robots.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2025-12-18T08:44:22+00:00",
      "updated": "2025-12-18T08:44:22+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17958v1",
      "file": "papers/2512.17958v1.pdf"
    },
    {
      "arxiv_id": "2512.16302v1",
      "title": "ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation",
      "authors": [
        {
          "name": "Zixuan Chen"
        },
        {
          "name": "Chongkai Gao"
        },
        {
          "name": "Lin Shao"
        },
        {
          "name": "Jieqi Shi"
        },
        {
          "name": "Jing Huo"
        },
        {
          "name": "Yang Gao"
        }
      ],
      "abstract": "One-shot imitation learning (OSIL) offers a promising way to teach robots new skills without large-scale data collection. However, current OSIL methods are primarily limited to short-horizon tasks, thus limiting their applicability to complex, long-horizon manipulations. To address this limitation, we propose ManiLong-Shot, a novel framework that enables effective OSIL for long-horizon prehensile manipulation tasks. ManiLong-Shot structures long-horizon tasks around physical interaction events, reframing the problem as sequencing interaction-aware primitives instead of directly imitating continuous trajectories. This primitive decomposition can be driven by high-level reasoning from a vision-language model (VLM) or by rule-based heuristics derived from robot state changes. For each primitive, ManiLong-Shot predicts invariant regions critical to the interaction, establishes correspondences between the demonstration and the current observation, and computes the target end-effector pose, enabling effective task execution. Extensive simulation experiments show that ManiLong-Shot, trained on only 10 short-horizon tasks, generalizes to 20 unseen long-horizon tasks across three difficulty levels via one-shot imitation, achieving a 22.8% relative improvement over the SOTA. Additionally, real-robot experiments validate ManiLong-Shot's ability to robustly execute three long-horizon manipulation tasks via OSIL, confirming its practical applicability.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-18T08:39:34+00:00",
      "updated": "2025-12-18T08:39:34+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16302v1",
      "file": "papers/2512.16302v1.pdf"
    },
    {
      "arxiv_id": "2512.16069v1",
      "title": "A Task-Driven, Planner-in-the-Loop Computational Design Framework for Modular Manipulators",
      "authors": [
        {
          "name": "Maolin Lei"
        },
        {
          "name": "Edoardo Romiti"
        },
        {
          "name": "Arturo Laurenzi"
        },
        {
          "name": "Rui Dai"
        },
        {
          "name": "Matteo Dalle Vedove"
        },
        {
          "name": "Jiatao Ding"
        },
        {
          "name": "Daniele Fontanelli"
        },
        {
          "name": "Nikos Tsagarakis"
        }
      ],
      "abstract": "Modular manipulators composed of pre-manufactured and interchangeable modules offer high adaptability across diverse tasks. However, their deployment requires generating feasible motions while jointly optimizing morphology and mounted pose under kinematic, dynamic, and physical constraints. Moreover, traditional single-branch designs often extend reach by increasing link length, which can easily violate torque limits at the base joint. To address these challenges, we propose a unified task-driven computational framework that integrates trajectory planning across varying morphologies with the co-optimization of morphology and mounted pose. Within this framework, a hierarchical model predictive control (HMPC) strategy is developed to enable motion planning for both redundant and non-redundant manipulators. For design optimization, the CMA-ES is employed to efficiently explore a hybrid search space consisting of discrete morphology configurations and continuous mounted poses. Meanwhile, a virtual module abstraction is introduced to enable bi-branch morphologies, allowing an auxiliary branch to offload torque from the primary branch and extend the achievable workspace without increasing the capacity of individual joint modules. Extensive simulations and hardware experiments on polishing, drilling, and pick-and-place tasks demonstrate the effectiveness of the proposed framework. The results show that: 1) the framework can generate multiple feasible designs that satisfy kinematic and dynamic constraints while avoiding environmental collisions for given tasks; 2) flexible design objectives, such as maximizing manipulability, minimizing joint effort, or reducing the number of modules, can be achieved by customizing the cost functions; and 3) a bi-branch morphology capable of operating in a large workspace can be realized without requiring more powerful basic modules.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-18T01:27:34+00:00",
      "updated": "2025-12-18T01:27:34+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16069v1",
      "file": "papers/2512.16069v1.pdf"
    },
    {
      "arxiv_id": "2512.16027v1",
      "title": "SWIFT-Nav: Stability-Aware Waypoint-Level TD3 with Fuzzy Arbitration for UAV Navigation in Cluttered Environments",
      "authors": [
        {
          "name": "Shuaidong Ji"
        },
        {
          "name": "Mahdi Bamdad"
        },
        {
          "name": "Francisco Cruz"
        }
      ],
      "abstract": "Efficient and reliable UAV navigation in cluttered and dynamic environments remains challenging. We propose SWIFT-Nav: Stability-aware Waypoint-level Integration of Fuzzy arbitration and TD3 for Navigation, a TD3-based navigation framework that achieves fast, stable convergence to obstacle-aware paths. The system couples a sensor-driven perception front end with a TD3 waypoint policy: the perception module converts LiDAR ranges into a confidence-weighted safety map and goal cues, while the TD3 policy is trained with Prioritised Experience Replay to focus on high-error transitions and a decaying epsilon-greedy exploration schedule that gradually shifts from exploration to exploitation. A lightweight fuzzy-logic layer computes a safety score from radial measurements and near obstacles, gates mode switching and clamps unsafe actions; in parallel, task-aligned reward shaping combining goal progress, clearance, and switch-economy terms provides dense, well-scaled feedback that accelerates learning. Implemented in Webots with proximity-based collision checking, our approach consistently outperforms baselines in trajectory smoothness and generalization to unseen layouts, while preserving real-time responsiveness. These results show that combining TD3 with replay prioritisation, calibrated exploration, and fuzzy-safety rules yields a robust and deployable solution for UAV navigation in cluttered scenes.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-17T23:19:06+00:00",
      "updated": "2025-12-17T23:19:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16027v1",
      "file": "papers/2512.16027v1.pdf"
    },
    {
      "arxiv_id": "2512.16024v2",
      "title": "Maintaining the Level of a Payload carried by Multi-Robot System on Irregular Surface",
      "authors": [
        {
          "name": "Rishabh Dev Yadav"
        },
        {
          "name": "Shrey Agrawal"
        },
        {
          "name": "Kamalakar Karlapalem"
        }
      ],
      "abstract": "In this paper, we introduce a multi robot payload transport system to carry payloads through an environment of unknown and uneven inclinations while maintaining the desired orientation of the payload. For this task, we used custom built robots with a linear actuator (pistons) mounted on top of each robot. The system continuously monitors the payload's orientation and computes the required piston height of each robot to maintain the desired orientation of the payload. In this work, we propose an open loop controller coupled with a closed loop PID controller to achieve the goal. As our modelling makes no assumptions on the type of terrain, the system can work on any unknown and uneven terrains and inclinations. We showcase the efficacy of our proposed controller by testing it on various simulated environments with varied and complex terrains.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-17T23:16:51+00:00",
      "updated": "2025-12-19T17:23:33+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16024v2",
      "file": "papers/2512.16024v2.pdf"
    },
    {
      "arxiv_id": "2512.16019v1",
      "title": "Few-Shot Inference of Human Perceptions of Robot Performance in Social Navigation Scenarios",
      "authors": [
        {
          "name": "Qiping Zhang"
        },
        {
          "name": "Nathan Tsoi"
        },
        {
          "name": "Mofeed Nagib"
        },
        {
          "name": "Hao-Tien Lewis Chiang"
        },
        {
          "name": "Marynel Vázquez"
        }
      ],
      "abstract": "Understanding how humans evaluate robot behavior during human-robot interactions is crucial for developing socially aware robots that behave according to human expectations. While the traditional approach to capturing these evaluations is to conduct a user study, recent work has proposed utilizing machine learning instead. However, existing data-driven methods require large amounts of labeled data, which limits their use in practice. To address this gap, we propose leveraging the few-shot learning capabilities of Large Language Models (LLMs) to improve how well a robot can predict a user's perception of its performance, and study this idea experimentally in social navigation tasks. To this end, we extend the SEAN TOGETHER dataset with additional real-world human-robot navigation episodes and participant feedback. Using this augmented dataset, we evaluate the ability of several LLMs to predict human perceptions of robot performance from a small number of in-context examples, based on observed spatio-temporal cues of the robot and surrounding human motion. Our results demonstrate that LLMs can match or exceed the performance of traditional supervised learning models while requiring an order of magnitude fewer labeled instances. We further show that prediction performance can improve with more in-context examples, confirming the scalability of our approach. Additionally, we investigate what kind of sensor-based information an LLM relies on to make these inferences by conducting an ablation study on the input features considered for performance prediction. Finally, we explore the novel application of personalized examples for in-context learning, i.e., drawn from the same user being evaluated, finding that they further enhance prediction accuracy. This work paves the path to improving robot behavior in a scalable manner through user-centered feedback.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2025-12-17T23:06:36+00:00",
      "updated": "2025-12-17T23:06:36+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16019v1",
      "file": "papers/2512.16019v1.pdf"
    },
    {
      "arxiv_id": "2512.16011v1",
      "title": "dLITE: Differentiable Lighting-Informed Trajectory Evaluation for On-Orbit Inspection",
      "authors": [
        {
          "name": "Jack Naylor"
        },
        {
          "name": "Raghav Mishra"
        },
        {
          "name": "Nicholas H. Barbara"
        },
        {
          "name": "Donald G. Dansereau"
        }
      ],
      "abstract": "Visual inspection of space-borne assets is of increasing interest to spacecraft operators looking to plan maintenance, characterise damage, and extend the life of high-value satellites in orbit. The environment of Low Earth Orbit (LEO) presents unique challenges when planning inspection operations that maximise visibility, information, and data quality. Specular reflection of sunlight from spacecraft bodies, self-shadowing, and dynamic lighting in LEO significantly impact the quality of data captured throughout an orbit. This is exacerbated by the relative motion between spacecraft, which introduces variable imaging distances and attitudes during inspection. Planning inspection trajectories with the aide of simulation is a common approach. However, the ability to design and optimise an inspection trajectory specifically to improve the resulting image quality in proximity operations remains largely unexplored. In this work, we present $\\partial$LITE, an end-to-end differentiable simulation pipeline for on-orbit inspection operations. We leverage state-of-the-art differentiable rendering tools and a custom orbit propagator to enable end-to-end optimisation of orbital parameters based on visual sensor data. $\\partial$LITE enables us to automatically design non-obvious trajectories, vastly improving the quality and usefulness of attained data. To our knowledge, our differentiable inspection-planning pipeline is the first of its kind and provides new insights into modern computational approaches to spacecraft mission planning. Project page: https://appearance-aware.github.io/dlite/",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-17T22:40:05+00:00",
      "updated": "2025-12-17T22:40:05+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16011v1",
      "file": "papers/2512.16011v1.pdf"
    },
    {
      "arxiv_id": "2512.15994v1",
      "title": "SORS: A Modular, High-Fidelity Simulator for Soft Robots",
      "authors": [
        {
          "name": "Manuel Mekkattu"
        },
        {
          "name": "Mike Y. Michelis"
        },
        {
          "name": "Robert K. Katzschmann"
        }
      ],
      "abstract": "The deployment of complex soft robots in multiphysics environments requires advanced simulation frameworks that not only capture interactions between different types of material, but also translate accurately to real-world performance. Soft robots pose unique modeling challenges due to their large nonlinear deformations, material incompressibility, and contact interactions, which complicate both numerical stability and physical accuracy. Despite recent progress, robotic simulators often struggle with modeling such phenomena in a scalable and application-relevant manner. We present SORS (Soft Over Rigid Simulator), a versatile, high-fidelity simulator designed to handle these complexities for soft robot applications. Our energy-based framework, built on the finite element method, allows modular extensions, enabling the inclusion of custom-designed material and actuation models. To ensure physically consistent contact handling, we integrate a constrained nonlinear optimization based on sequential quadratic programming, allowing for stable and accurate modeling of contact phenomena. We validate our simulator through a diverse set of real-world experiments, which include cantilever deflection, pressure-actuation of a soft robotic arm, and contact interactions from the PokeFlex dataset. In addition, we showcase the potential of our framework for control optimization of a soft robotic leg. These tests confirm that our simulator can capture both fundamental material behavior and complex actuation dynamics with high physical fidelity. By bridging the sim-to-real gap in these challenging domains, our approach provides a validated tool for prototyping next-generation soft robots, filling the gap of extensibility, fidelity, and usability in the soft robotic ecosystem.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-17T21:58:46+00:00",
      "updated": "2025-12-17T21:58:46+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15994v1",
      "file": "papers/2512.15994v1.pdf"
    },
    {
      "arxiv_id": "2512.15940v1",
      "title": "R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space",
      "authors": [
        {
          "name": "Tin Stribor Sohn"
        },
        {
          "name": "Maximilian Dillitzer"
        },
        {
          "name": "Jason J. Corso"
        },
        {
          "name": "Eric Sax"
        }
      ],
      "abstract": "Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-12-17T20:08:32+00:00",
      "updated": "2025-12-17T20:08:32+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15940v1",
      "file": "papers/2512.15940v1.pdf"
    },
    {
      "arxiv_id": "2512.15692v2",
      "title": "mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs",
      "authors": [
        {
          "name": "Jonas Pai"
        },
        {
          "name": "Liam Achenbach"
        },
        {
          "name": "Victoriano Montesinos"
        },
        {
          "name": "Benedek Forrai"
        },
        {
          "name": "Oier Mees"
        },
        {
          "name": "Elvis Nava"
        }
      ],
      "abstract": "Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce mimic-video, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-12-17T18:47:31+00:00",
      "updated": "2025-12-19T18:30:30+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15692v2",
      "file": "papers/2512.15692v2.pdf"
    },
    {
      "arxiv_id": "2512.15840v1",
      "title": "Large Video Planner Enables Generalizable Robot Control",
      "authors": [
        {
          "name": "Boyuan Chen"
        },
        {
          "name": "Tianyuan Zhang"
        },
        {
          "name": "Haoran Geng"
        },
        {
          "name": "Kiwhan Song"
        },
        {
          "name": "Caiyi Zhang"
        },
        {
          "name": "Peihao Li"
        },
        {
          "name": "William T. Freeman"
        },
        {
          "name": "Jitendra Malik"
        },
        {
          "name": "Pieter Abbeel"
        },
        {
          "name": "Russ Tedrake"
        },
        {
          "name": "Vincent Sitzmann"
        },
        {
          "name": "Yilun Du"
        }
      ],
      "abstract": "General-purpose robots require decision-making models that generalize across diverse tasks and environments. Recent works build robot foundation models by extending multimodal large language models (MLLMs) with action outputs, creating vision-language-action (VLA) systems. These efforts are motivated by the intuition that MLLMs' large-scale language and image pretraining can be effectively transferred to the action output modality. In this work, we explore an alternative paradigm of using large-scale video pretraining as a primary modality for building robot foundation models. Unlike static images and language, videos capture spatio-temporal sequences of states and actions in the physical world that are naturally aligned with robotic behavior. We curate an internet-scale video dataset of human activities and task demonstrations, and train, for the first time at a foundation-model scale, an open video model for generative robotics planning. The model produces zero-shot video plans for novel scenes and tasks, which we post-process to extract executable robot actions. We evaluate task-level generalization through third-party selected tasks in the wild and real-robot experiments, demonstrating successful physical execution. Together, these results show robust instruction following, strong generalization, and real-world feasibility. We release both the model and dataset to support open, reproducible video-based robot learning. Our website is available at https://www.boyuan.space/large-video-planner/.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published": "2025-12-17T18:35:54+00:00",
      "updated": "2025-12-17T18:35:54+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15840v1",
      "file": "papers/2512.15840v1.pdf"
    },
    {
      "arxiv_id": "2512.15597v1",
      "title": "An Open Toolkit for Underwater Field Robotics",
      "authors": [
        {
          "name": "Giacomo Picardi"
        },
        {
          "name": "Saverio Iacoponi"
        },
        {
          "name": "Matias Carandell"
        },
        {
          "name": "Jorge Aguirregomezcorta"
        },
        {
          "name": "Mrudul Chellapurath"
        },
        {
          "name": "Joaquin del Rio"
        },
        {
          "name": "Marcello Calisti"
        },
        {
          "name": "Iacopo Aguzzi"
        }
      ],
      "abstract": "Underwater robotics is becoming increasingly important for marine science, environmental monitoring, and subsea industrial operations, yet the development of underwater manipulation and actuation systems remains restricted by high costs, proprietary designs, and limited access to modular, research-oriented hardware. While open-source initiatives have democratized vehicle construction and control software, a substantial gap persists for joint-actuated systems-particularly those requiring waterproof, feedback-enabled actuation suitable for manipulators, grippers, and bioinspired devices. As a result, many research groups face lengthy development cycles, limited reproducibility, and difficulty transitioning laboratory prototypes to field-ready platforms.\n  To address this gap, we introduce an open, cost-effective hardware and software toolkit for underwater manipulation research. The toolkit includes a depth-rated Underwater Robotic Joint (URJ) with early leakage detection, compact control and power management electronics, and a ROS2-based software stack for sensing and multi-mode actuation. All CAD models, fabrication files, PCB sources, firmware, and ROS2 packages are openly released, enabling local manufacturing, modification, and community-driven improvement.\n  The toolkit has undergone extensive laboratory testing and multiple field deployments, demonstrating reliable operation up to 40 m depth across diverse applications, including a 3-DoF underwater manipulator, a tendon-driven soft gripper, and an underactuated sediment sampler. These results validate the robustness, versatility, and reusability of the toolkit for real marine environments.\n  By providing a fully open, field-tested platform, this work aims to lower the barrier to entry for underwater manipulation research, improve reproducibility, and accelerate innovation in underwater field robotics.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-17T17:06:35+00:00",
      "updated": "2025-12-17T17:06:35+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15597v1",
      "file": "papers/2512.15597v1.pdf"
    },
    {
      "arxiv_id": "2512.15557v1",
      "title": "OMCL: Open-vocabulary Monte Carlo Localization",
      "authors": [
        {
          "name": "Evgenii Kruzhkov"
        },
        {
          "name": "Raphael Memmesheimer"
        },
        {
          "name": "Sven Behnke"
        }
      ],
      "abstract": "Robust robot localization is an important prerequisite for navigation planning. If the environment map was created from different sensors, robot measurements must be robustly associated with map features. In this work, we extend Monte Carlo Localization using vision-language features. These open-vocabulary features enable to robustly compute the likelihood of visual observations, given a camera pose and a 3D map created from posed RGB-D images or aligned point clouds. The abstract vision-language features enable to associate observations and map elements from different modalities. Global localization can be initialized by natural language descriptions of the objects present in the vicinity of locations. We evaluate our approach using Matterport3D and Replica for indoor scenes and demonstrate generalization on SemanticKITTI for outdoor scenes.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-17T16:08:53+00:00",
      "updated": "2025-12-17T16:08:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15557v1",
      "file": "papers/2512.15557v1.pdf"
    },
    {
      "arxiv_id": "2512.15448v1",
      "title": "Load-Based Variable Transmission Mechanism for Robotic Applications",
      "authors": [
        {
          "name": "Sinan Emre"
        },
        {
          "name": "Victor Barasuol"
        },
        {
          "name": "Matteo Villa"
        },
        {
          "name": "Claudio Semini"
        }
      ],
      "abstract": "This paper presents a Load-Based Variable Transmission (LBVT) mechanism designed to enhance robotic actuation by dynamically adjusting the transmission ratio in response to external torque demands. Unlike existing variable transmission systems that require additional actuators for active control, the proposed LBVT mechanism leverages a pre-tensioned spring and a four-bar linkage to passively modify the transmission ratio, thereby reducing the complexity of robot joint actuation systems. The effectiveness of the LBVT mechanism is evaluated through simulation-based analyses. The results confirm that the system achieves up to a 40 percent increase in transmission ratio upon reaching a predefined torque threshold, effectively amplifying joint torque when required without additional actuation. Furthermore, the simulations demonstrate a torque amplification effect triggered when the applied force exceeds 18 N, highlighting the system ability to autonomously respond to varying load conditions. This research contributes to the development of lightweight, efficient, and adaptive transmission systems for robotic applications, particularly in legged robots where dynamic torque adaptation is critical.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-17T13:45:22+00:00",
      "updated": "2025-12-17T13:45:22+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15448v1",
      "file": "papers/2512.15448v1.pdf"
    },
    {
      "arxiv_id": "2512.15411v2",
      "title": "MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training",
      "authors": [
        {
          "name": "Zhenhan Yin"
        },
        {
          "name": "Xuanhan Wang"
        },
        {
          "name": "Jiahao Jiang"
        },
        {
          "name": "Kaiyuan Deng"
        },
        {
          "name": "Pengqi Chen"
        },
        {
          "name": "Shuangle Li"
        },
        {
          "name": "Chong Liu"
        },
        {
          "name": "Xing Xu"
        },
        {
          "name": "Jingkuan Song"
        },
        {
          "name": "Lianli Gao"
        },
        {
          "name": "Heng Tao Shen"
        }
      ],
      "abstract": "While leveraging abundant human videos and simulated robot data poses a scalable solution to the scarcity of real-world robot data, the generalization capability of existing vision-language-action models (VLAs) remains limited by mismatches in camera views, visual appearance, and embodiment morphologies. To overcome this limitation, we propose MiVLA, a generalizable VLA empowered by human-robot mutual imitation pre-training, which leverages inherent behavioral similarity between human hands and robotic arms to build a foundation of strong behavioral priors for both human actions and robotic control. Specifically, our method utilizes kinematic rules with left/right hand coordinate systems for bidirectional alignment between human and robot action spaces. Given human or simulated robot demonstrations, MiVLA is trained to forecast behavior trajectories for one embodiment, and imitate behaviors for another one unseen in the demonstration. Based on this mutual imitation, it integrates the behavioral fidelity of real-world human data with the manipulative diversity of simulated robot data into a unified model, thereby enhancing the generalization capability for downstream tasks. Extensive experiments conducted on both simulation and real-world platforms with three robots (ARX, PiPer and LocoMan), demonstrate that MiVLA achieves strong improved generalization capability, outperforming state-of-the-art VLAs (e.g., $\\boldsymbolπ_{0}$, $\\boldsymbolπ_{0.5}$ and H-RDT) by 25% in simulation, and 14% in real-world robot control tasks.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published": "2025-12-17T12:59:41+00:00",
      "updated": "2025-12-19T09:10:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15411v2",
      "file": "papers/2512.15411v2.pdf"
    },
    {
      "arxiv_id": "2512.15379v1",
      "title": "Remotely Detectable Robot Policy Watermarking",
      "authors": [
        {
          "name": "Michael Amir"
        },
        {
          "name": "Manon Flageat"
        },
        {
          "name": "Amanda Prorok"
        }
      ],
      "abstract": "The success of machine learning for real-world robotic systems has created a new form of intellectual property: the trained policy. This raises a critical need for novel methods that verify ownership and detect unauthorized, possibly unsafe misuse. While watermarking is established in other domains, physical policies present a unique challenge: remote detection. Existing methods assume access to the robot's internal state, but auditors are often limited to external observations (e.g., video footage). This ``Physical Observation Gap'' means the watermark must be detected from signals that are noisy, asynchronous, and filtered by unknown system dynamics. We formalize this challenge using the concept of a \\textit{glimpse sequence}, and introduce Colored Noise Coherency (CoNoCo), the first watermarking strategy designed for remote detection. CoNoCo embeds a spectral signal into the robot's motions by leveraging the policy's inherent stochasticity. To show it does not degrade performance, we prove CoNoCo preserves the marginal action distribution. Our experiments demonstrate strong, robust detection across various remote modalities, including motion capture and side-way/top-down video footage, in both simulated and real-world robot experiments. This work provides a necessary step toward protecting intellectual property in robotics, offering the first method for validating the provenance of physical policies non-invasively, using purely remote observations.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CR",
        "cs.LG",
        "eess.SY"
      ],
      "published": "2025-12-17T12:28:03+00:00",
      "updated": "2025-12-17T12:28:03+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15379v1",
      "file": "papers/2512.15379v1.pdf"
    },
    {
      "arxiv_id": "2512.15309v1",
      "title": "GuangMing-Explorer: A Four-Legged Robot Platform for Autonomous Exploration in General Environments",
      "authors": [
        {
          "name": "Kai Zhang"
        },
        {
          "name": "Shoubin Chen"
        },
        {
          "name": "Dong Li"
        },
        {
          "name": "Baiyang Zhang"
        },
        {
          "name": "Tao Huang"
        },
        {
          "name": "Zehao Wu"
        },
        {
          "name": "Jiasheng Chen"
        },
        {
          "name": "Bo Zhang"
        }
      ],
      "abstract": "Autonomous exploration is a fundamental capability that tightly integrates perception, planning, control, and motion execution. It plays a critical role in a wide range of applications, including indoor target search, mapping of extreme environments, resource exploration, etc. Despite significant progress in individual components, a holistic and practical description of a completely autonomous exploration system, encompassing both hardware and software, remains scarce. In this paper, we present GuangMing-Explorer, a fully integrated autonomous exploration platform designed for robust operation across diverse environments. We provide a comprehensive overview of the system architecture, including hardware design, software stack, algorithm deployment, and experimental configuration. Extensive real-world experiments demonstrate the platform's effectiveness and efficiency in executing autonomous exploration tasks, highlighting its potential for practical deployment in complex and unstructured environments.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-17T10:53:32+00:00",
      "updated": "2025-12-17T10:53:32+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15309v1",
      "file": "papers/2512.15309v1.pdf"
    },
    {
      "arxiv_id": "2512.15282v1",
      "title": "A Network-Based Framework for Modeling and Analyzing Human-Robot Coordination Strategies",
      "authors": [
        {
          "name": "Martijn IJtsma"
        },
        {
          "name": "Salvatore Hargis"
        }
      ],
      "abstract": "Studies of human-robot interaction in dynamic and unstructured environments show that as more advanced robotic capabilities are deployed, the need for cooperative competencies to support collaboration with human problem-holders increases. Designing human-robot systems to meet these demands requires an explicit understanding of the work functions and constraints that shape the feasibility of alternative joint work strategies. Yet existing human-robot interaction frameworks either emphasize computational support for real-time execution or rely on static representations for design, offering limited support for reasoning about coordination dynamics during early-stage conceptual design. To address this gap, this article presents a novel computational framework for analyzing joint work strategies in human-robot systems by integrating techniques from functional modeling with graph-theoretic representations. The framework characterizes collective work in terms of the relationships among system functions and the physical and informational structure of the work environment, while explicitly capturing how coordination demands evolve over time. Its use during conceptual design is demonstrated through a case study in disaster robotics, which shows how the framework can be used to support early trade-space exploration of human-robot coordination strategies and to identify cooperative competencies that support flexible management of coordination overhead. These results show how the framework makes coordination demands and their temporal evolution explicit, supporting design-time reasoning about cooperative competency requirements and work demands prior to implementation.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "published": "2025-12-17T10:37:34+00:00",
      "updated": "2025-12-17T10:37:34+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15282v1",
      "file": "papers/2512.15282v1.pdf"
    },
    {
      "arxiv_id": "2512.15258v2",
      "title": "VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments",
      "authors": [
        {
          "name": "Yuze Wu"
        },
        {
          "name": "Mo Zhu"
        },
        {
          "name": "Xingxing Li"
        },
        {
          "name": "Yuheng Du"
        },
        {
          "name": "Yuxin Fan"
        },
        {
          "name": "Wenjun Li"
        },
        {
          "name": "Zhichao Han"
        },
        {
          "name": "Xin Zhou"
        },
        {
          "name": "Fei Gao"
        }
      ],
      "abstract": "This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2025-12-17T10:02:55+00:00",
      "updated": "2025-12-19T11:22:57+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15258v2",
      "file": "papers/2512.15258v2.pdf"
    },
    {
      "arxiv_id": "2512.15215v1",
      "title": "Infrastructure-based Autonomous Mobile Robots for Internal Logistics -- Challenges and Future Perspectives",
      "authors": [
        {
          "name": "Erik Brorsson"
        },
        {
          "name": "Kristian Ceder"
        },
        {
          "name": "Ze Zhang"
        },
        {
          "name": "Sabino Francesco Roselli"
        },
        {
          "name": "Endre Erős"
        },
        {
          "name": "Martin Dahl"
        },
        {
          "name": "Beatrice Alenljung"
        },
        {
          "name": "Jessica Lindblom"
        },
        {
          "name": "Thanh Bui"
        },
        {
          "name": "Emmanuel Dean"
        },
        {
          "name": "Lennart Svensson"
        },
        {
          "name": "Kristofer Bengtsson"
        },
        {
          "name": "Per-Lage Götvall"
        },
        {
          "name": "Knut Åkesson"
        }
      ],
      "abstract": "The adoption of Autonomous Mobile Robots (AMRs) for internal logistics is accelerating, with most solutions emphasizing decentralized, onboard intelligence. While AMRs in indoor environments like factories can be supported by infrastructure, involving external sensors and computational resources, such systems remain underexplored in the literature. This paper presents a comprehensive overview of infrastructure-based AMR systems, outlining key opportunities and challenges. To support this, we introduce a reference architecture combining infrastructure-based sensing, on-premise cloud computing, and onboard autonomy. Based on the architecture, we review core technologies for localization, perception, and planning. We demonstrate the approach in a real-world deployment in a heavy-vehicle manufacturing environment and summarize findings from a user experience (UX) evaluation. Our aim is to provide a holistic foundation for future development of scalable, robust, and human-compatible AMR systems in complex industrial environments.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-17T09:10:43+00:00",
      "updated": "2025-12-17T09:10:43+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15215v1",
      "file": "papers/2512.15215v1.pdf"
    },
    {
      "arxiv_id": "2512.15111v1",
      "title": "BEV-Patch-PF: Particle Filtering with BEV-Aerial Feature Matching for Off-Road Geo-Localization",
      "authors": [
        {
          "name": "Dongmyeong Lee"
        },
        {
          "name": "Jesse Quattrociocchi"
        },
        {
          "name": "Christian Ellis"
        },
        {
          "name": "Rwik Rana"
        },
        {
          "name": "Amanda Adkins"
        },
        {
          "name": "Adam Uccello"
        },
        {
          "name": "Garrett Warnell"
        },
        {
          "name": "Joydeep Biswas"
        }
      ],
      "abstract": "We propose BEV-Patch-PF, a GPS-free sequential geo-localization system that integrates a particle filter with learned bird's-eye-view (BEV) and aerial feature maps. From onboard RGB and depth images, we construct a BEV feature map. For each 3-DoF particle pose hypothesis, we crop the corresponding patch from an aerial feature map computed from a local aerial image queried around the approximate location. BEV-Patch-PF computes a per-particle log-likelihood by matching the BEV feature to the aerial patch feature. On two real-world off-road datasets, our method achieves 7.5x lower absolute trajectory error (ATE) on seen routes and 7.0x lower ATE on unseen routes than a retrieval-based baseline, while maintaining accuracy under dense canopy and shadow. The system runs in real time at 10 Hz on an NVIDIA Tesla T4, enabling practical robot deployment.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published": "2025-12-17T06:03:36+00:00",
      "updated": "2025-12-17T06:03:36+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15111v1",
      "file": "papers/2512.15111v1.pdf"
    },
    {
      "arxiv_id": "2512.15047v1",
      "title": "HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles",
      "authors": [
        {
          "name": "Yunheng Wang"
        },
        {
          "name": "Yixiao Feng"
        },
        {
          "name": "Yuetong Fang"
        },
        {
          "name": "Shuning Zhang"
        },
        {
          "name": "Tan Jing"
        },
        {
          "name": "Jian Li"
        },
        {
          "name": "Xiangrui Jiang"
        },
        {
          "name": "Renjing Xu"
        }
      ],
      "abstract": "3D Scene Graphs (3DSGs) constitute a powerful representation of the physical world, distinguished by their abilities to explicitly model the complex spatial, semantic, and functional relationships between entities, rendering a foundational understanding that enables agents to interact intelligently with their environment and execute versatile behaviors. Embodied navigation, as a crucial component of such capabilities, leverages the compact and expressive nature of 3DSGs to enable long-horizon reasoning and planning in complex, large-scale environments. However, prior works rely on a static-world assumption, defining traversable space solely based on static spatial layouts and thereby treating interactable obstacles as non-traversable. This fundamental limitation severely undermines their effectiveness in real-world scenarios, leading to limited reachability, low efficiency, and inferior extensibility. To address these issues, we propose HERO, a novel framework for constructing Hierarchical Traversable 3DSGs, that redefines traversability by modeling operable obstacles as pathways, capturing their physical interactivity, functional semantics, and the scene's relational hierarchy. The results show that, relative to its baseline, HERO reduces PL by 35.1% in partially obstructed environments and increases SR by 79.4% in fully obstructed ones, demonstrating substantially higher efficiency and reachability.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "published": "2025-12-17T03:22:27+00:00",
      "updated": "2025-12-17T03:22:27+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15047v1",
      "file": "papers/2512.15047v1.pdf"
    },
    {
      "arxiv_id": "2512.15020v1",
      "title": "ISS Policy : Scalable Diffusion Policy with Implicit Scene Supervision",
      "authors": [
        {
          "name": "Wenlong Xia"
        },
        {
          "name": "Jinhao Zhang"
        },
        {
          "name": "Ce Zhang"
        },
        {
          "name": "Yaojia Wang"
        },
        {
          "name": "Youmin Gong"
        },
        {
          "name": "Jie Mei"
        }
      ],
      "abstract": "Vision-based imitation learning has enabled impressive robotic manipulation skills, but its reliance on object appearance while ignoring the underlying 3D scene structure leads to low training efficiency and poor generalization. To address these challenges, we introduce \\emph{Implicit Scene Supervision (ISS) Policy}, a 3D visuomotor DiT-based diffusion policy that predicts sequences of continuous actions from point cloud observations. We extend DiT with a novel implicit scene supervision module that encourages the model to produce outputs consistent with the scene's geometric evolution, thereby improving the performance and robustness of the policy. Notably, ISS Policy achieves state-of-the-art performance on both single-arm manipulation tasks (MetaWorld) and dexterous hand manipulation (Adroit). In real-world experiments, it also demonstrates strong generalization and robustness. Additional ablation studies show that our method scales effectively with both data and parameters. Code and videos will be released.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-17T02:20:21+00:00",
      "updated": "2025-12-17T02:20:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15020v1",
      "file": "papers/2512.15020v1.pdf"
    },
    {
      "arxiv_id": "2512.14952v1",
      "title": "Breathe with Me: Synchronizing Biosignals for User Embodiment in Robots",
      "authors": [
        {
          "name": "Iddo Yehoshua Wald"
        },
        {
          "name": "Amber Maimon"
        },
        {
          "name": "Shiyao Zhang"
        },
        {
          "name": "Dennis Küster"
        },
        {
          "name": "Robert Porzel"
        },
        {
          "name": "Tanja Schultz"
        },
        {
          "name": "Rainer Malaka"
        }
      ],
      "abstract": "Embodiment of users within robotic systems has been explored in human-robot interaction, most often in telepresence and teleoperation. In these applications, synchronized visuomotor feedback can evoke a sense of body ownership and agency, contributing to the experience of embodiment. We extend this work by employing embreathment, the representation of the user's own breath in real time, as a means for enhancing user embodiment experience in robots. In a within-subjects experiment, participants controlled a robotic arm, while its movements were either synchronized or non-synchronized with their own breath. Synchrony was shown to significantly increase body ownership, and was preferred by most participants. We propose the representation of physiological signals as a novel interoceptive pathway for human-robot interaction, and discuss implications for telepresence, prosthetics, collaboration with robots, and shared autonomy.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "published": "2025-12-16T22:38:57+00:00",
      "updated": "2025-12-16T22:38:57+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14952v1",
      "file": "papers/2512.14952v1.pdf"
    },
    {
      "arxiv_id": "2512.14696v2",
      "title": "CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives",
      "authors": [
        {
          "name": "Zihan Wang"
        },
        {
          "name": "Jiashun Wang"
        },
        {
          "name": "Jeff Tan"
        },
        {
          "name": "Yiwen Zhao"
        },
        {
          "name": "Jessica Hodgins"
        },
        {
          "name": "Shubham Tulsiani"
        },
        {
          "name": "Deva Ramanan"
        }
      ],
      "abstract": "We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\\% to 6.9\\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.GR",
        "cs.RO"
      ],
      "published": "2025-12-16T18:59:50+00:00",
      "updated": "2025-12-21T20:38:23+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14696v2",
      "file": "papers/2512.14696v2.pdf"
    },
    {
      "arxiv_id": "2512.14689v1",
      "title": "CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation",
      "authors": [
        {
          "name": "Sirui Chen"
        },
        {
          "name": "Zi-ang Cao"
        },
        {
          "name": "Zhengyi Luo"
        },
        {
          "name": "Fernando Castañeda"
        },
        {
          "name": "Chenran Li"
        },
        {
          "name": "Tingwu Wang"
        },
        {
          "name": "Ye Yuan"
        },
        {
          "name": "Linxi \"Jim\" Fan"
        },
        {
          "name": "C. Karen Liu"
        },
        {
          "name": "Yuke Zhu"
        }
      ],
      "abstract": "Recent progress in humanoid robots has unlocked agile locomotion skills, including backflipping, running, and crawling. Yet it remains challenging for a humanoid robot to perform forceful manipulation tasks such as moving objects, wiping, and pushing a cart. We propose adaptive Compliance Humanoid control through hIsight Perturbation (CHIP), a plug-and-play module that enables controllable end-effector stiffness while preserving agile tracking of dynamic reference motions. CHIP is easy to implement and requires neither data augmentation nor additional reward tuning. We show that a generalist motion-tracking controller trained with CHIP can perform a diverse set of forceful manipulation tasks that require different end-effector compliance, such as multi-robot collaboration, wiping, box delivery, and door opening.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "published": "2025-12-16T18:56:04+00:00",
      "updated": "2025-12-16T18:56:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14689v1",
      "file": "papers/2512.14689v1.pdf"
    },
    {
      "arxiv_id": "2512.14666v1",
      "title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models",
      "authors": [
        {
          "name": "Zechen Bai"
        },
        {
          "name": "Chen Gao"
        },
        {
          "name": "Mike Zheng Shou"
        }
      ],
      "abstract": "Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\\% on long-horizon tasks, +22.0\\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\\% success on unseen tasks without task-specific demonstrations training (vs. 0\\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published": "2025-12-16T18:26:38+00:00",
      "updated": "2025-12-16T18:26:38+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14666v1",
      "file": "papers/2512.14666v1.pdf"
    },
    {
      "arxiv_id": "2512.14450v1",
      "title": "Nonlinear System Identification Nano-drone Benchmark",
      "authors": [
        {
          "name": "Riccardo Busetto"
        },
        {
          "name": "Elia Cereda"
        },
        {
          "name": "Marco Forgione"
        },
        {
          "name": "Gabriele Maroni"
        },
        {
          "name": "Dario Piga"
        },
        {
          "name": "Daniele Palossi"
        }
      ],
      "abstract": "We introduce a benchmark for system identification based on 75k real-world samples from the Crazyflie 2.1 Brushless nano-quadrotor, a sub-50g aerial vehicle widely adopted in robotics research. The platform presents a challenging testbed due to its multi-input, multi-output nature, open-loop instability, and nonlinear dynamics under agile maneuvers. The dataset comprises four aggressive trajectories with synchronized 4-dimensional motor inputs and 13-dimensional output measurements. To enable fair comparison of identification methods, the benchmark includes a suite of multi-horizon prediction metrics for evaluating both one-step and multi-step error propagation. In addition to the data, we provide a detailed description of the platform and experimental setup, as well as baseline models highlighting the challenge of accurate prediction under real-world noise and actuation nonlinearities. All data, scripts, and reference implementations are released as open-source at https://github.com/idsia-robotics/nanodrone-sysid-benchmark to facilitate transparent comparison of algorithms and support research on agile, miniaturized aerial robotics.",
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY",
        "cs.RO"
      ],
      "published": "2025-12-16T14:37:33+00:00",
      "updated": "2025-12-16T14:37:33+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14450v1",
      "file": "papers/2512.14450v1.pdf"
    },
    {
      "arxiv_id": "2512.14434v1",
      "title": "Geometric Parameter Optimization of a Novel 3-(PP(2-(UPS))) Redundant Parallel Mechanism based on Workspace Determination",
      "authors": [
        {
          "name": "Quan Yuan"
        },
        {
          "name": "Daqian Cao"
        },
        {
          "name": "Weibang Bai"
        }
      ],
      "abstract": "Redundant parallel robots are normally employed in scenarios requiring good precision, high load capability, and large workspace compared to traditional parallel mechanisms. However, the elementary robotic configuration and geometric parameter optimization are still quite challenging. This paper proposes a novel 3-(PP(2-(UPS))) redundant parallel mechanism, with good generalizability first, and further investigates the kinematic optimization issue by analyzing and investigating how its key geometric parameters influence the volume, shape, boundary completeness, and orientation capabilities of its workspace. The torsional capability index TI_1 and tilting capability index TI_2 are defined to evaluate the orientation performance of the mechanism. Numerical simulation studies are completed to indicate the analysis, providing reasonable but essential references for the parameter optimization of 3-(PP(2-(UPS))) and other similar redundant parallel mechanisms.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-16T14:22:13+00:00",
      "updated": "2025-12-16T14:22:13+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14434v1",
      "file": "papers/2512.14434v1.pdf"
    },
    {
      "arxiv_id": "2512.14411v1",
      "title": "Synthetic Data Pipelines for Adaptive, Mission-Ready Militarized Humanoids",
      "authors": [
        {
          "name": "Mohammed Ayman Habib"
        },
        {
          "name": "Aldo Petruzzelli"
        }
      ],
      "abstract": "Omnia presents a synthetic data driven pipeline to accelerate the training, validation, and deployment readiness of militarized humanoids. The approach converts first-person spatial observations captured from point-of-view recordings, smart glasses, augmented reality headsets, and spatial browsing workflows into scalable, mission-specific synthetic datasets for humanoid autonomy. By generating large volumes of high-fidelity simulated scenarios and pairing them with automated labeling and model training, the pipeline enables rapid iteration on perception, navigation, and decision-making capabilities without the cost, risk, or time constraints of extensive field trials. The resulting datasets can be tuned quickly for new operational environments and threat conditions, supporting both baseline humanoid performance and advanced subsystems such as multimodal sensing, counter-detection survivability, and CBRNE-relevant reconnaissance behaviors. This work targets faster development cycles and improved robustness in complex, contested settings by exposing humanoid systems to broad scenario diversity early in the development process.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-16T13:54:34+00:00",
      "updated": "2025-12-16T13:54:34+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14411v1",
      "file": "papers/2512.14411v1.pdf"
    },
    {
      "arxiv_id": "2512.14350v2",
      "title": "Fine-Tuning of Neural Network Approximate MPC without Retraining via Bayesian Optimization",
      "authors": [
        {
          "name": "Henrik Hose"
        },
        {
          "name": "Paul Brunzema"
        },
        {
          "name": "Alexander von Rohr"
        },
        {
          "name": "Alexander Gräfe"
        },
        {
          "name": "Angela P. Schoellig"
        },
        {
          "name": "Sebastian Trimpe"
        }
      ],
      "abstract": "Approximate model-predictive control (AMPC) aims to imitate an MPC's behavior with a neural network, removing the need to solve an expensive optimization problem at runtime. However, during deployment, the parameters of the underlying MPC must usually be fine-tuned. This often renders AMPC impractical as it requires repeatedly generating a new dataset and retraining the neural network. Recent work addresses this problem by adapting AMPC without retraining using approximated sensitivities of the MPC's optimization problem. Currently, this adaption must be done by hand, which is labor-intensive and can be unintuitive for high-dimensional systems. To solve this issue, we propose using Bayesian optimization to tune the parameters of AMPC policies based on experimental data. By combining model-based control with direct and local learning, our approach achieves superior performance to nominal AMPC on hardware, with minimal experimentation. This allows automatic and data-efficient adaptation of AMPC to new system instances and fine-tuning to cost functions that are difficult to directly implement in MPC. We demonstrate the proposed method in hardware experiments for the swing-up maneuver on an inverted cartpole and yaw control of an under-actuated balancing unicycle robot, a challenging control problem.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "published": "2025-12-16T12:24:08+00:00",
      "updated": "2025-12-21T14:30:50+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14350v2",
      "file": "papers/2512.14350v2.pdf"
    },
    {
      "arxiv_id": "2512.14349v1",
      "title": "A Geometric Task-Space Port-Hamiltonian Formulation for Redundant Manipulators",
      "authors": [
        {
          "name": "Federico Califano"
        },
        {
          "name": "Camilla Rota"
        },
        {
          "name": "Riccardo Zanella"
        },
        {
          "name": "Antonio Franchi"
        }
      ],
      "abstract": "We present a novel geometric port-Hamiltonian formulation of redundant manipulators performing a differential kinematic task $η=J(q)\\dot{q}$, where $q$ is a point on the configuration manifold, $η$ is a velocity-like task space variable, and $J(q)$ is a linear map representing the task, for example the classical analytic or geometric manipulator Jacobian matrix. The proposed model emerges from a change of coordinates from canonical Hamiltonian dynamics, and splits the standard Hamiltonian momentum variable into a task-space momentum variable and a null-space momentum variable. Properties of this model and relation to Lagrangian formulations present in the literature are highlighted. Finally, we apply the proposed model in an \\textit{Interconnection and Damping Assignment Passivity-Based Control} (IDA-PBC) design to stabilize and shape the impedance of a 7-DOF Emika Panda robot in simulation.",
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY",
        "cs.RO"
      ],
      "published": "2025-12-16T12:24:07+00:00",
      "updated": "2025-12-16T12:24:07+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14349v1",
      "file": "papers/2512.14349v1.pdf"
    },
    {
      "arxiv_id": "2512.14340v1",
      "title": "Field evaluation and optimization of a lightweight lidar-based UAV navigation system for dense boreal forest environments",
      "authors": [
        {
          "name": "Aleksi Karhunen"
        },
        {
          "name": "Teemu Hakala"
        },
        {
          "name": "Väinö Karjalainen"
        },
        {
          "name": "Eija Honkavaara"
        }
      ],
      "abstract": "The interest in the usage of uncrewed aerial vehicles (UAVs) for forest applications has increased in recent years. While above-canopy flight has reached a high level of autonomy, navigating under-canopy remains a significant challenge. The use of autonomous UAVs could reduce the burden of data collection, which has motivated the development of numerous solutions for under-canopy autonomous flight. However, the experiments conducted in the literature and their reporting lack rigor. Very rarely, the density and the difficulty of the test forests are reported, or multiple flights are flown, and the success rate of those flights is reported. The aim of this study was to implement an autonomously flying quadrotor based on a lightweight lidar using openly available algorithms and test its behavior in real forest environments. A set of rigorous experiments was conducted with a quadrotor prototype utilizing the IPC path planner and LTA-OM SLAM algorithm. Based on the results of the first 33 flights, the original system was further enhanced. With the optimized system, 60 flights were performed, resulting in a total of 93 test flights. The optimized system performed significantly better in terms of reliability and flight mission completion times, achieving success rates of 12/15 in a medium-density forest and 15/15 in a dense forest, at a target flight velocity of 1 m/s. At a target flight velocity of 2 m/s, it had a success rate of 12/15 and 5/15, respectively. Furthermore, a standardized testing setup and evaluation criteria were proposed, enabling consistent performance comparisons of autonomous under-canopy UAV systems, enhancing reproducibility, guiding system improvements, and accelerating progress in forest robotics.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-16T12:08:12+00:00",
      "updated": "2025-12-16T12:08:12+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14340v1",
      "file": "papers/2512.14340v1.pdf"
    },
    {
      "arxiv_id": "2512.14331v1",
      "title": "ARCADE: Adaptive Robot Control with Online Changepoint-Aware Bayesian Dynamics Learning",
      "authors": [
        {
          "name": "Rishabh Dev Yadav"
        },
        {
          "name": "Avirup Das"
        },
        {
          "name": "Hongyu Song"
        },
        {
          "name": "Samuel Kaski"
        },
        {
          "name": "Wei Pan"
        }
      ],
      "abstract": "Real-world robots must operate under evolving dynamics caused by changing operating conditions, external disturbances, and unmodeled effects. These may appear as gradual drifts, transient fluctuations, or abrupt shifts, demanding real-time adaptation that is robust to short-term variation yet responsive to lasting change. We propose a framework for modeling the nonlinear dynamics of robotic systems that can be updated in real time from streaming data. The method decouples representation learning from online adaptation, using latent representations learned offline to support online closed-form Bayesian updates. To handle evolving conditions, we introduce a changepoint-aware mechanism with a latent variable inferred from data likelihoods that indicates continuity or shift. When continuity is likely, evidence accumulates to refine predictions; when a shift is detected, past information is tempered to enable rapid re-learning. This maintains calibrated uncertainty and supports probabilistic reasoning about transient, gradual, or structural change. We prove that the adaptive regret of the framework grows only logarithmically in time and linearly with the number of shifts, competitive with an oracle that knows timings of shift. We validate on cartpole simulations and real quadrotor flights with swinging payloads and mid-flight drops, showing improved predictive accuracy, faster recovery, and more accurate closed-loop tracking than relevant baselines.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-16T11:57:42+00:00",
      "updated": "2025-12-16T11:57:42+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14331v1",
      "file": "papers/2512.14331v1.pdf"
    },
    {
      "arxiv_id": "2512.14270v2",
      "title": "CaFe-TeleVision: A Coarse-to-Fine Teleoperation System with Immersive Situated Visualization for Enhanced Ergonomics",
      "authors": [
        {
          "name": "Zixin Tang"
        },
        {
          "name": "Yiming Chen"
        },
        {
          "name": "Quentin Rouxel"
        },
        {
          "name": "Dianxi Li"
        },
        {
          "name": "Shuang Wu"
        },
        {
          "name": "Fei Chen"
        }
      ],
      "abstract": "Teleoperation presents a promising paradigm for remote control and robot proprioceptive data collection. Despite recent progress, current teleoperation systems still suffer from limitations in efficiency and ergonomics, particularly in challenging scenarios. In this paper, we propose CaFe-TeleVision, a coarse-to-fine teleoperation system with immersive situated visualization for enhanced ergonomics. At its core, a coarse-to-fine control mechanism is proposed in the retargeting module to bridge workspace disparities, jointly optimizing efficiency and physical ergonomics. To stream immersive feedback with adequate visual cues for human vision systems, an on-demand situated visualization technique is integrated in the perception module, which reduces the cognitive load for multi-view processing. The system is built on a humanoid collaborative robot and validated with six challenging bimanual manipulation tasks. User study among 24 participants confirms that CaFe-TeleVision enhances ergonomics with statistical significance, indicating a lower task load and a higher user acceptance during teleoperation. Quantitative results also validate the superior performance of our system across six tasks, surpassing comparative methods by up to 28.89% in success rate and accelerating by 26.81% in completion time. Project webpage: https://clover-cuhk.github.io/cafe_television/",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-16T10:25:58+00:00",
      "updated": "2025-12-17T04:07:51+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14270v2",
      "file": "papers/2512.14270v2.pdf"
    },
    {
      "arxiv_id": "2512.14222v2",
      "title": "History-Enhanced Two-Stage Transformer for Aerial Vision-and-Language Navigation",
      "authors": [
        {
          "name": "Xichen Ding"
        },
        {
          "name": "Jianzhe Gao"
        },
        {
          "name": "Cong Pan"
        },
        {
          "name": "Wenguan Wang"
        },
        {
          "name": "Jie Qin"
        }
      ],
      "abstract": "Aerial Vision-and-Language Navigation (AVLN) requires Unmanned Aerial Vehicle (UAV) agents to localize targets in large-scale urban environments based on linguistic instructions. While successful navigation demands both global environmental reasoning and local scene comprehension, existing UAV agents typically adopt mono-granularity frameworks that struggle to balance these two aspects. To address this limitation, this work proposes a History-Enhanced Two-Stage Transformer (HETT) framework, which integrates the two aspects through a coarse-to-fine navigation pipeline. Specifically, HETT first predicts coarse-grained target positions by fusing spatial landmarks and historical context, then refines actions via fine-grained visual analysis. In addition, a historical grid map is designed to dynamically aggregate visual features into a structured spatial memory, enhancing comprehensive scene awareness. Additionally, the CityNav dataset annotations are manually refined to enhance data quality. Experiments on the refined CityNav dataset show that HETT delivers significant performance gains, while extensive ablation studies further verify the effectiveness of each component.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-12-16T09:16:07+00:00",
      "updated": "2025-12-17T02:51:52+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14222v2",
      "file": "papers/2512.14222v2.pdf"
    },
    {
      "arxiv_id": "2512.14217v1",
      "title": "DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos",
      "authors": [
        {
          "name": "Yang Bai"
        },
        {
          "name": "Liudi Yang"
        },
        {
          "name": "George Eskandar"
        },
        {
          "name": "Fengyi Shen"
        },
        {
          "name": "Mohammad Altillawi"
        },
        {
          "name": "Ziyuan Liu"
        },
        {
          "name": "Gitta Kutyniok"
        }
      ],
      "abstract": "Video diffusion models provide powerful real-world simulators for embodied AI but remain limited in controllability for robotic manipulation. Recent works on trajectory-conditioned video generation address this gap but often rely on 2D trajectories or single modality conditioning, which restricts their ability to produce controllable and consistent robotic demonstrations. We present DRAW2ACT, a depth-aware trajectory-conditioned video generation framework that extracts multiple orthogonal representations from the input trajectory, capturing depth, semantics, shape and motion, and injects them into the diffusion model. Moreover, we propose to jointly generate spatially aligned RGB and depth videos, leveraging cross-modality attention mechanisms and depth supervision to enhance the spatio-temporal consistency. Finally, we introduce a multimodal policy model conditioned on the generated RGB and depth sequences to regress the robot's joint angles. Experiments on Bridge V2, Berkeley Autolab, and simulation benchmarks show that DRAW2ACT achieves superior visual fidelity and consistency while yielding higher manipulation success rates compared to existing baselines.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-12-16T09:11:36+00:00",
      "updated": "2025-12-16T09:11:36+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14217v1",
      "file": "papers/2512.14217v1.pdf"
    },
    {
      "arxiv_id": "2512.14206v1",
      "title": "Trajectory Tracking for Multi-Manipulator Systems in Constrained Environments",
      "authors": [
        {
          "name": "Mayank Sewlia"
        },
        {
          "name": "Christos K. Verginis"
        },
        {
          "name": "Dimos V. Dimarogonas"
        }
      ],
      "abstract": "We consider the problem of cooperative manipulation by a mobile multi-manipulator system operating in obstacle-cluttered and highly constrained environments under spatio-temporal task specifications. The task requires transporting a grasped object while respecting both continuous robot dynamics and discrete geometric constraints arising from obstacles and narrow passages. To address this hybrid structure, we propose a multi-rate planning and control framework that combines offline generation of an STL-satisfying object trajectory and collision-free base footprints with online constrained inverse kinematics and continuous-time feedback control. The resulting closed-loop system enables coordinated reconfiguration of multiple manipulators while tracking the desired object motion. The approach is evaluated in high-fidelity physics simulations using three Franka Emika Panda mobile manipulators rigidly grasping an object.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "published": "2025-12-16T09:01:49+00:00",
      "updated": "2025-12-16T09:01:49+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14206v1",
      "file": "papers/2512.14206v1.pdf"
    },
    {
      "arxiv_id": "2512.14111v1",
      "title": "Interactive Motion Planning for Human-Robot Collaboration Based on Human-Centric Configuration Space Ergonomic Field",
      "authors": [
        {
          "name": "Chenzui Li"
        },
        {
          "name": "Yiming Chen"
        },
        {
          "name": "Xi Wu"
        },
        {
          "name": "Tao Teng"
        },
        {
          "name": "Sylvain Calinon"
        },
        {
          "name": "Darwin Caldwell"
        },
        {
          "name": "Fei Chen"
        }
      ],
      "abstract": "Industrial human-robot collaboration requires motion planning that is collision-free, responsive, and ergonomically safe to reduce fatigue and musculoskeletal risk. We propose the Configuration Space Ergonomic Field (CSEF), a continuous and differentiable field over the human joint space that quantifies ergonomic quality and provides gradients for real-time ergonomics-aware planning. An efficient algorithm constructs CSEF from established metrics with joint-wise weighting and task conditioning, and we integrate it into a gradient-based planner compatible with impedance-controlled robots. In a 2-DoF benchmark, CSEF-based planning achieves higher success rates, lower ergonomic cost, and faster computation than a task-space ergonomic planner. Hardware experiments with a dual-arm robot in unimanual guidance, collaborative drilling, and bimanual cocarrying show faster ergonomic cost reduction, closer tracking to optimized joint targets, and lower muscle activation than a point-to-point baseline. CSEF-based planning method reduces average ergonomic scores by up to 10.31% for collaborative drilling tasks and 5.60% for bimanual co-carrying tasks while decreasing activation in key muscle groups, indicating practical benefits for real-world deployment.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-16T05:46:38+00:00",
      "updated": "2025-12-16T05:46:38+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14111v1",
      "file": "papers/2512.14111v1.pdf"
    },
    {
      "arxiv_id": "2512.14057v2",
      "title": "Context Representation via Action-Free Transformer encoder-decoder for Meta Reinforcement Learning",
      "authors": [
        {
          "name": "Amir M. Soufi Enayati"
        },
        {
          "name": "Homayoun Honari"
        },
        {
          "name": "Homayoun Najjaran"
        }
      ],
      "abstract": "Reinforcement learning (RL) enables robots to operate in uncertain environments, but standard approaches often struggle with poor generalization to unseen tasks. Context-adaptive meta reinforcement learning addresses these limitations by conditioning on the task representation, yet they mostly rely on complete action information in the experience making task inference tightly coupled to a specific policy. This paper introduces Context Representation via Action Free Transformer encoder decoder (CRAFT), a belief model that infers task representations solely from sequences of states and rewards. By removing the dependence on actions, CRAFT decouples task inference from policy optimization, supports modular training, and leverages amortized variational inference for scalable belief updates. Built on a transformer encoder decoder with rotary positional embeddings, the model captures long range temporal dependencies and robustly encodes both parametric and non-parametric task variations. Experiments on the MetaWorld ML-10 robotic manipulation benchmark show that CRAFT achieves faster adaptation, improved generalization, and more effective exploration compared to context adaptive meta--RL baselines. These findings highlight the potential of action-free inference as a foundation for scalable RL in robotic control.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-16T03:50:29+00:00",
      "updated": "2025-12-17T07:40:56+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14057v2",
      "file": "papers/2512.14057v2.pdf"
    },
    {
      "arxiv_id": "2512.14046v1",
      "title": "E-Navi: Environmental Adaptive Navigation for UAVs on Resource Constrained Platforms",
      "authors": [
        {
          "name": "Boyang Li"
        },
        {
          "name": "Zhongpeng Jin"
        },
        {
          "name": "Shuai Zhao"
        },
        {
          "name": "Jiahui Liao"
        },
        {
          "name": "Tian Liu"
        },
        {
          "name": "Han Liu"
        },
        {
          "name": "Yuanhai Zhang"
        },
        {
          "name": "Kai Huang"
        }
      ],
      "abstract": "The ability to adapt to changing environments is crucial for the autonomous navigation systems of Unmanned Aerial Vehicles (UAVs). However, existing navigation systems adopt fixed execution configurations without considering environmental dynamics based on available computing resources, e.g., with a high execution frequency and task workload. This static approach causes rigid flight strategies and excessive computations, ultimately degrading flight performance or even leading to failures in UAVs. Despite the necessity for an adaptive system, dynamically adjusting workloads remains challenging, due to difficulties in quantifying environmental complexity and modeling the relationship between environment and system configuration. Aiming at adapting to dynamic environments, this paper proposes E-Navi, an environmental-adaptive navigation system for UAVs that dynamically adjusts task executions on the CPUs in response to environmental changes based on available computational resources. Specifically, the perception-planning pipeline of UAVs navigation system is redesigned through dynamic adaptation of mapping resolution and execution frequency, driven by the quantitative environmental complexity evaluations. In addition, E-Navi supports flexible deployment across hardware platforms with varying levels of computing capability. Extensive Hardware-In-the-Loop and real-world experiments demonstrate that the proposed system significantly outperforms the baseline method across various hardware platforms, achieving up to 53.9% navigation task workload reduction, up to 63.8% flight time savings, and delivering more stable velocity control.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-16T03:28:28+00:00",
      "updated": "2025-12-16T03:28:28+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14046v1",
      "file": "papers/2512.14046v1.pdf"
    },
    {
      "arxiv_id": "2512.14031v1",
      "title": "Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model",
      "authors": [
        {
          "name": "Zhaofeng Hu"
        },
        {
          "name": "Hongrui Yu"
        },
        {
          "name": "Vaidhyanathan Chandramouli"
        },
        {
          "name": "Ci-Jyun Liang"
        }
      ],
      "abstract": "This study evaluates two leading approaches for teaching construction robots new skills to understand their applicability for construction automation: a Vision-Language-Action (VLA) model and Reinforcement Learning (RL) methods. The goal is to understand both task performance and the practical effort needed to deploy each approach on real jobs. The authors developed two teleoperation interfaces to control the robots and collect the demonstrations needed, both of which proved effective for training robots for long-horizon and dexterous tasks. In addition, the authors conduct a three-stage evaluation. First, the authors compare a Multi-Layer Perceptron (MLP) policy with a Deep Q-network (DQN) imitation model to identify the stronger RL baseline, focusing on model performance, generalization, and a pick-up experiment. Second, three different VLA models are trained in two different scenarios and compared with each other. Third, the authors benchmark the selected RL baseline against the VLA model using computational and sample-efficiency measures and then a robot experiment on a multi-stage panel installation task that includes transport and installation. The VLA model demonstrates strong generalization and few-shot capability, achieving 60% and 100% success in the pickup phase. In comparison, DQN can be made robust but needs additional noise during tuning, which increases the workload. Overall, the findings indicate that VLA offers practical advantages for changing tasks by reducing programming effort and enabling useful performance with minimal data, while DQN provides a viable baseline when sufficient tuning effort is acceptable.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2025-12-16T02:56:13+00:00",
      "updated": "2025-12-16T02:56:13+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14031v1",
      "file": "papers/2512.14031v1.pdf"
    },
    {
      "arxiv_id": "2512.13981v1",
      "title": "Impact of Robot Facial-Audio Expressions on Human Robot Trust Dynamics and Trust Repair",
      "authors": [
        {
          "name": "Hossein Naderi"
        },
        {
          "name": "Alireza Shojaei"
        },
        {
          "name": "Philip Agee"
        },
        {
          "name": "Kereshmeh Afsari"
        },
        {
          "name": "Abiola Akanmu"
        }
      ],
      "abstract": "Despite recent advances in robotics and human-robot collaboration in the AEC industry, trust has mostly been treated as a static factor, with little guidance on how it changes across events during collaboration. This paper investigates how a robot's task performance and its expressive responses after outcomes shape the dynamics of human trust over time. To this end, we designed a controlled within-subjects study with two construction-inspired tasks, Material Delivery (physical assistance) and Information Gathering (perceptual assistance), and measured trust repeatedly (four times per task) using the 14-item Trust Perception Scale for HRI plus a redelegation choice. The robot produced two multimodal expressions, a \"glad\" display with a brief confirmation after success, and a \"sad\" display with an apology and a request for a second chance after failure. The study was conducted in a lab environment with 30 participants and a quadruped platform, and we evaluated trust dynamics and repair across both tasks. Results show that robot success reliably increases trust, failure causes sharp drops, and apology-based expressions partially restores trust (44% recovery in Material Delivery; 38% in Information Gathering). Item-level analysis indicates that recovered trust was driven mostly by interaction and communication factors, with competence recovering partially and autonomy aspects changing least. Additionally, age group and prior attitudes moderated trust dynamics with younger participants showed larger but shorter-lived changes, mid-20s participants exhibited the most durable repair, and older participants showed most conservative dynamics. This work provides a foundation for future efforts that adapt repair strategies to task demands and user profiles to support safe, productive adoption of robots on construction sites.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-16T00:40:32+00:00",
      "updated": "2025-12-16T00:40:32+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13981v1",
      "file": "papers/2512.13981v1.pdf"
    },
    {
      "arxiv_id": "2512.13974v1",
      "title": "Autonomous Construction-Site Safety Inspection Using Mobile Robots: A Multilayer VLM-LLM Pipeline",
      "authors": [
        {
          "name": "Hossein Naderi"
        },
        {
          "name": "Alireza Shojaei"
        },
        {
          "name": "Philip Agee"
        },
        {
          "name": "Kereshmeh Afsari"
        },
        {
          "name": "Abiola Akanmu"
        }
      ],
      "abstract": "Construction safety inspection remains mostly manual, and automated approaches still rely on task-specific datasets that are hard to maintain in fast-changing construction environments due to frequent retraining. Meanwhile, field inspection with robots still depends on human teleoperation and manual reporting, which are labor-intensive. This paper aims to connect what a robot sees during autonomous navigation to the safety rules that are common in construction sites, automatically generating a safety inspection report. To this end, we proposed a multi-layer framework with two main modules: robotics and AI. On the robotics side, SLAM and autonomous navigation provide repeatable coverage and targeted revisits via waypoints. On AI side, a Vision Language Model (VLM)-based layer produces scene descriptions; a retrieval component powered grounds those descriptions in OSHA and site policies; Another VLM-based layer assesses the safety situation based on rules; and finally Large Language Model (LLM) layer generates safety reports based on previous outputs. The framework is validated with a proof-of-concept implementation and evaluated in a lab environment that simulates common hazards across three scenarios. Results show high recall with competitive precision compared to state-of-the-art closed-source models. This paper contributes a transparent, generalizable pipeline that moves beyond black-box models by exposing intermediate artifacts from each layer and keeping the human in the loop. This work provides a foundation for future extensions to additional tasks and settings within and beyond construction context.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-16T00:25:31+00:00",
      "updated": "2025-12-16T00:25:31+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13974v1",
      "file": "papers/2512.13974v1.pdf"
    },
    {
      "arxiv_id": "2512.13903v1",
      "title": "PrediFlow: A Flow-Based Prediction-Refinement Framework for Real-Time Human Motion Prediction in Human-Robot Collaboration",
      "authors": [
        {
          "name": "Sibo Tian"
        },
        {
          "name": "Minghui Zheng"
        },
        {
          "name": "Xiao Liang"
        }
      ],
      "abstract": "Stochastic human motion prediction is critical for safe and effective human-robot collaboration (HRC) in industrial remanufacturing, as it captures human motion uncertainties and multi-modal behaviors that deterministic methods cannot handle. While earlier works emphasize highly diverse predictions, they often generate unrealistic human motions. More recent methods focus on accuracy and real-time performance, yet there remains potential to improve prediction quality further without exceeding time budgets. Additionally, current research on stochastic human motion prediction in HRC typically considers human motion in isolation, neglecting the influence of robot motion on human behavior. To address these research gaps and enable real-time, realistic, and interaction-aware human motion prediction, we propose a novel prediction-refinement framework that integrates both human and robot observed motion to refine the initial predictions produced by a pretrained state-of-the-art predictor. The refinement module employs a Flow Matching structure to account for uncertainty. Experimental studies on the HRC desktop disassembly dataset demonstrate that our method significantly improves prediction accuracy while preserving the uncertainties and multi-modalities of human motion. Moreover, the total inference time of the proposed framework remains within the time budget, highlighting the effectiveness and practicality of our approach.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-15T21:20:11+00:00",
      "updated": "2025-12-15T21:20:11+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13903v1",
      "file": "papers/2512.13903v1.pdf"
    },
    {
      "arxiv_id": "2512.13670v1",
      "title": "NL2SpaTiaL: Generating Geometric Spatio-Temporal Logic Specifications from Natural Language for Manipulation Tasks",
      "authors": [
        {
          "name": "Licheng Luo"
        },
        {
          "name": "Yu Xia"
        },
        {
          "name": "Kaier Liang"
        },
        {
          "name": "Mingyu Cai"
        }
      ],
      "abstract": "Spatio-Temporal Logic (SpaTiaL) offers a principled formalism for expressing geometric spatial requirements-an essential component of robotic manipulation, where object locations, neighborhood relations, pose constraints, and interactions directly determine task success. Yet prior works have largely relied on standard temporal logic (TL), which models only robot trajectories and overlooks object-level interactions. Existing datasets built from randomly generated TL formulas paired with natural-language descriptions therefore cover temporal operators but fail to represent the layered spatial relations that manipulation tasks depend on. To address this gap, we introduce a dataset generation framework that synthesizes SpaTiaL specifications and converts them into natural-language descriptions through a deterministic, semantics-preserving back-translation procedure. This pipeline produces the NL2SpaTiaL dataset, aligning natural language with multi-level spatial relations and temporal objectives to reflect the compositional structure of manipulation tasks. Building on this foundation, we propose a translation-verification framework equipped with a language-based semantic checker that ensures the generated SpaTiaL formulas faithfully encode the semantics specified by the input description. Experiments across a suite of manipulation tasks show that SpaTiaL-based representations yield more interpretable, verifiable, and compositional grounding for instruction following. Project website: https://sites.google.com/view/nl2spatial",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-15T18:56:34+00:00",
      "updated": "2025-12-15T18:56:34+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13670v1",
      "file": "papers/2512.13670v1.pdf"
    },
    {
      "arxiv_id": "2512.13660v1",
      "title": "RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics",
      "authors": [
        {
          "name": "Enshen Zhou"
        },
        {
          "name": "Cheng Chi"
        },
        {
          "name": "Yibo Li"
        },
        {
          "name": "Jingkun An"
        },
        {
          "name": "Jiayuan Zhang"
        },
        {
          "name": "Shanyu Rong"
        },
        {
          "name": "Yi Han"
        },
        {
          "name": "Yuheng Ji"
        },
        {
          "name": "Mengzhen Liu"
        },
        {
          "name": "Pengwei Wang"
        },
        {
          "name": "Zhongyuan Wang"
        },
        {
          "name": "Lu Sheng"
        },
        {
          "name": "Shanghang Zhang"
        }
      ],
      "abstract": "Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published": "2025-12-15T18:52:43+00:00",
      "updated": "2025-12-15T18:52:43+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13660v1",
      "file": "papers/2512.13660v1.pdf"
    },
    {
      "arxiv_id": "2512.13644v1",
      "title": "World Models Can Leverage Human Videos for Dexterous Manipulation",
      "authors": [
        {
          "name": "Raktim Gautam Goswami"
        },
        {
          "name": "Amir Bar"
        },
        {
          "name": "David Fan"
        },
        {
          "name": "Tsung-Yen Yang"
        },
        {
          "name": "Gaoyue Zhou"
        },
        {
          "name": "Prashanth Krishnamurthy"
        },
        {
          "name": "Michael Rabbat"
        },
        {
          "name": "Farshad Khorrami"
        },
        {
          "name": "Yann LeCun"
        }
      ],
      "abstract": "Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-12-15T18:37:12+00:00",
      "updated": "2025-12-15T18:37:12+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13644v1",
      "file": "papers/2512.13644v1.pdf"
    },
    {
      "arxiv_id": "2512.13561v1",
      "title": "Near-Field Perception for Safety Enhancement of Autonomous Mobile Robots in Manufacturing Environments",
      "authors": [
        {
          "name": "Li-Wei Shih"
        },
        {
          "name": "Ruo-Syuan Mei"
        },
        {
          "name": "Jesse Heidrich"
        },
        {
          "name": "Hui-Ping Wang"
        },
        {
          "name": "Joel Hooton"
        },
        {
          "name": "Joshua Solomon"
        },
        {
          "name": "Jorge Arinez"
        },
        {
          "name": "Guangze Li"
        },
        {
          "name": "Chenhui Shao"
        }
      ],
      "abstract": "Near-field perception is essential for the safe operation of autonomous mobile robots (AMRs) in manufacturing environments. Conventional ranging sensors such as light detection and ranging (LiDAR) and ultrasonic devices provide broad situational awareness but often fail to detect small objects near the robot base. To address this limitation, this paper presents a three-tier near-field perception framework. The first approach employs light-discontinuity detection, which projects a laser stripe across the near-field zone and identifies interruptions in the stripe to perform fast, binary cutoff sensing for obstacle presence. The second approach utilizes light-displacement measurement to estimate object height by analyzing the geometric displacement of a projected stripe in the camera image, which provides quantitative obstacle height information with minimal computational overhead. The third approach employs a computer vision-based object detection model on embedded AI hardware to classify objects, enabling semantic perception and context-aware safety decisions. All methods are implemented on a Raspberry Pi 5 system, achieving real-time performance at 25 or 50 frames per second. Experimental evaluation and comparative analysis demonstrate that the proposed hierarchy balances precision, computation, and cost, thereby providing a scalable perception solution for enabling safe operations of AMRs in manufacturing environments.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-15T17:18:04+00:00",
      "updated": "2025-12-15T17:18:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13561v1",
      "file": "papers/2512.13561v1.pdf"
    },
    {
      "arxiv_id": "2512.13514v1",
      "title": "Reinforcement Learning based 6-DoF Maneuvers for Microgravity Intravehicular Docking: A Simulation Study with Int-Ball2 in ISS-JEM",
      "authors": [
        {
          "name": "Aman Arora"
        },
        {
          "name": "Matteo El-Hariry"
        },
        {
          "name": "Miguel Olivares-Mendez"
        }
      ],
      "abstract": "Autonomous free-flyers play a critical role in intravehicular tasks aboard the International Space Station (ISS), where their precise docking under sensing noise, small actuation mismatches, and environmental variability remains a nontrivial challenge. This work presents a reinforcement learning (RL) framework for six-degree-of-freedom (6-DoF) docking of JAXA's Int-Ball2 robot inside a high-fidelity Isaac Sim model of the Japanese Experiment Module (JEM). Using Proximal Policy Optimization (PPO), we train and evaluate controllers under domain-randomized dynamics and bounded observation noise, while explicitly modeling propeller drag-torque effects and polarity structure. This enables a controlled study of how Int-Ball2's propulsion physics influence RL-based docking performance in constrained microgravity interiors. The learned policy achieves stable and reliable docking across varied conditions and lays the groundwork for future extensions pertaining to Int-Ball2 in collision-aware navigation, safe RL, propulsion-accurate sim-to-real transfer, and vision-based end-to-end docking.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-15T16:42:48+00:00",
      "updated": "2025-12-15T16:42:48+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13514v1",
      "file": "papers/2512.13514v1.pdf"
    },
    {
      "arxiv_id": "2512.13477v1",
      "title": "Evaluating the Navigation Capabilities of a Modified COAST Guidewire Robot in an Anatomical Phantom Model",
      "authors": [
        {
          "name": "Timothy A. Brumfiel"
        },
        {
          "name": "Revanth Konda"
        },
        {
          "name": "Drew Elliott"
        },
        {
          "name": "Jaydev P. Desai"
        }
      ],
      "abstract": "To address the issues that arise due to the manual navigation of guidewires in endovascular interventions, research in medical robotics has taken a strong interest in developing robotically steerable guidewires, which offer the possibility of enhanced maneuverability and navigation, as the tip of the guidewire can be actively steered. The COaxially Aligned STeerable (COAST) guidewire robot has the ability to generate a wide variety of motions including bending motion with different bending lengths, follow-the-leader motion, and feedforward motion. In our past studies, we have explored different designs of the COAST guidewire robot and developed modeling, control, and sensing strategies for the COAST guidewire robot. In this study, the performance of a modified COAST guidewire robot is evaluated by conducting navigation experiments in an anatomical phantom model with pulsatile flow. The modified COAST guidewire robot is a simplified version of the COAST guidewire robot and consists of two tubes as opposed to three tubes. Through this study, we demonstrate the effectiveness of the modified COAST guidewire robot in navigating the tortuous phantom vasculature.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "published": "2025-12-15T16:14:22+00:00",
      "updated": "2025-12-15T16:14:22+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13477v1",
      "file": "papers/2512.13477v1.pdf"
    },
    {
      "arxiv_id": "2512.13380v1",
      "title": "Universal Dexterous Functional Grasping via Demonstration-Editing Reinforcement Learning",
      "authors": [
        {
          "name": "Chuan Mao"
        },
        {
          "name": "Haoqi Yuan"
        },
        {
          "name": "Ziye Huang"
        },
        {
          "name": "Chaoyi Xu"
        },
        {
          "name": "Kai Ma"
        },
        {
          "name": "Zongqing Lu"
        }
      ],
      "abstract": "Reinforcement learning (RL) has achieved great success in dexterous grasping, significantly improving grasp performance and generalization from simulation to the real world. However, fine-grained functional grasping, which is essential for downstream manipulation tasks, remains underexplored and faces several challenges: the complexity of specifying goals and reward functions for functional grasps across diverse objects, the difficulty of multi-task RL exploration, and the challenge of sim-to-real transfer. In this work, we propose DemoFunGrasp for universal dexterous functional grasping. We factorize functional grasping conditions into two complementary components - grasping style and affordance - and integrate them into an RL framework that can learn to grasp any object with any functional grasping condition. To address the multi-task optimization challenge, we leverage a single grasping demonstration and reformulate the RL problem as one-step demonstration editing, substantially enhancing sample efficiency and performance. Experimental results in both simulation and the real world show that DemoFunGrasp generalizes to unseen combinations of objects, affordances, and grasping styles, outperforming baselines in both success rate and functional grasping accuracy. In addition to strong sim-to-real capability, by incorporating a vision-language model (VLM) for planning, our system achieves autonomous instruction-following grasp execution.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-15T14:32:03+00:00",
      "updated": "2025-12-15T14:32:03+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13380v1",
      "file": "papers/2512.13380v1.pdf"
    },
    {
      "arxiv_id": "2512.14757v1",
      "title": "SocialNav-MoE: A Mixture-of-Experts Vision Language Model for Socially Compliant Navigation with Reinforcement Fine-Tuning",
      "authors": [
        {
          "name": "Tomohito Kawabata"
        },
        {
          "name": "Xinyu Zhang"
        },
        {
          "name": "Ling Xiao"
        }
      ],
      "abstract": "For robots navigating in human-populated environments, safety and social compliance are equally critical, yet prior work has mostly emphasized safety. Socially compliant navigation that accounts for human comfort, social norms, and contextual appropriateness remains underexplored. Vision language models (VLMs) show promise for this task; however, large-scale models incur substantial computational overhead, leading to higher inference latency and energy consumption, which makes them unsuitable for real-time deployment on resource-constrained robotic platforms. To address this issue, we investigate the effectiveness of small VLM and propose SocialNav-MoE, an efficient Mixture-of-Experts vision language model for socially compliant navigation with reinforcement fine-tuning (RFT). We further introduce a semantic similarity reward (SSR) to effectively leverage RFT for enhancing the decision-making capabilities. Additionally, we study the effectiveness of different small language model types (Phi, Qwen, and StableLM), routing strategies, and vision encoders (CLIP vs. SigLIP, frozen vs. fine-tuned). Experiments on the SNEI dataset demonstrate that SocialNav-MoE achieves an excellent balance between navigation accuracy and efficiency. The proposed SSR function is more effective than hard-level and character-level rewards. Source code will be released upon acceptance.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-12-15T14:21:15+00:00",
      "updated": "2025-12-15T14:21:15+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14757v1",
      "file": "papers/2512.14757v1.pdf"
    },
    {
      "arxiv_id": "2512.13304v1",
      "title": "Humanoid Robot Running Through Random Stepping Stones and Jumping Over Obstacles: Step Adaptation Using Spring-Mass Trajectories",
      "authors": [
        {
          "name": "Sait Sovukluk"
        },
        {
          "name": "Johannes Englsberger"
        },
        {
          "name": "Christian Ott"
        }
      ],
      "abstract": "This study proposes a step adaptation framework for running through spring-mass trajectories and deadbeat control gain libraries. It includes four main parts: (1) Automatic spring-mass trajectory library generation; (2) Deadbeat control gain library generation through an actively controlled template model that resembles the whole-body dynamics well; (3) Trajectory selection policy development for step adaptation; (4) Mapping spring-mass trajectories to a humanoid model through a whole-body control (WBC) framework also accounting for closed-kinematic chain systems, self collisions, and reactive limb swinging. We show the inclusiveness and the robustness of the proposed framework through various challenging and agile behaviors such as running through randomly generated stepping stones, jumping over random obstacles, performing slalom motions, changing the running direction suddenly with a random leg, and rejecting significant disturbances and uncertainties through the MuJoCo physics simulator. We also perform additional simulations under a comprehensive set of uncertainties and noise to better justify the proposed method's robustness to real-world challenges, including signal noise, imprecision, modeling errors, and delays. All the aforementioned behaviors are performed with a single library and the same set of WBC control parameters without additional tuning. The spring-mass and the deadbeat control gain library are automatically computed in 4.5 seconds in total for 315 different trajectories.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-15T13:22:16+00:00",
      "updated": "2025-12-15T13:22:16+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13304v1",
      "file": "papers/2512.13304v1.pdf"
    },
    {
      "arxiv_id": "2512.13293v2",
      "title": "Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration",
      "authors": [
        {
          "name": "Hao Fu"
        },
        {
          "name": "Wei Liu"
        },
        {
          "name": "Shuai Zhou"
        }
      ],
      "abstract": "This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2025-12-15T13:03:08+00:00",
      "updated": "2025-12-16T03:34:39+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13293v2",
      "file": "papers/2512.13293v2.pdf"
    },
    {
      "arxiv_id": "2512.13271v1",
      "title": "Lightweight Dynamic Modeling of Cable-Driven Continuum Robots Based on Actuation-Space Energy Formulation",
      "authors": [
        {
          "name": "Fangju Yang"
        },
        {
          "name": "Hang Yang"
        },
        {
          "name": "Ibrahim Alsarraj"
        },
        {
          "name": "Yuhao Wang"
        },
        {
          "name": "Ke Wu"
        }
      ],
      "abstract": "Cable-driven continuum robots (CDCRs) require accurate, real-time dynamic models for high-speed dynamics prediction or model-based control, making such capability an urgent need. In this paper, we propose the Lightweight Actuation-Space Energy Modeling (LASEM) framework for CDCRs, which formulates actuation potential energy directly in actuation space to enable lightweight yet accurate dynamic modeling. Through a unified variational derivation, the governing dynamics reduce to a single partial differential equation (PDE), requiring only the Euler moment balance while implicitly incorporating the Newton force balance. By also avoiding explicit computation of cable-backbone contact forces, the formulation simplifies the model structure and improves computational efficiency while preserving geometric accuracy and physical consistency. Importantly, the proposed framework for dynamic modeling natively supports both force-input and displacement-input actuation modes, a capability seldom achieved in existing dynamic formulations. Leveraging this lightweight structure, a Galerkin space-time modal discretization with analytical time-domain derivatives of the reduced state further enables an average 62.3% computational speedup over state-of-the-art real-time dynamic modeling approaches.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-15T12:30:24+00:00",
      "updated": "2025-12-15T12:30:24+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13271v1",
      "file": "papers/2512.13271v1.pdf"
    },
    {
      "arxiv_id": "2512.13215v1",
      "title": "Multi-directional Safe Rectangle Corridor-Based MPC for Nonholonomic Robots Navigation in Cluttered Environment",
      "authors": [
        {
          "name": "Yinsong Qu"
        },
        {
          "name": "Yunxiang Li"
        },
        {
          "name": "Shanlin Zhong"
        }
      ],
      "abstract": "Autonomous Mobile Robots (AMRs) have become indispensable in industrial applications due to their operational flexibility and efficiency. Navigation serves as a crucial technical foundation for accomplishing complex tasks. However, navigating AMRs in dense, cluttered, and semi-structured environments remains challenging, primarily due to nonholonomic vehicle dynamics, interactions with mixed static/dynamic obstacles, and the non-convex constrained nature of such operational spaces. To solve these problems, this paper proposes an Improved Sequential Model Predictive Control (ISMPC) navigation framework that systematically reformulates navigation tasks as sequential switched optimal control problems. The framework addresses the aforementioned challenges through two key innovations: 1) Implementation of a Multi-Directional Safety Rectangular Corridor (MDSRC) algorithm, which encodes the free space through rectangular convex regions to avoid collision with static obstacles, eliminating redundant computational burdens and accelerating solver convergence; 2) A sequential MPC navigation framework that integrates corridor constraints with barrier function constraints is proposed to achieve static and dynamic obstacle avoidance. The ISMPC navigation framework enables direct velocity generation for AMRs, simplifying traditional navigation algorithm architectures. Comparative experiments demonstrate the framework's superiority in free-space utilization ( an increase of 41.05$\\%$ in the average corridor area) while maintaining real-time computational performance (average corridors generation latency of 3 ms).",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-15T11:28:04+00:00",
      "updated": "2025-12-15T11:28:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13215v1",
      "file": "papers/2512.13215v1.pdf"
    },
    {
      "arxiv_id": "2512.13198v1",
      "title": "ALBATROSS: A robotised system for high-throughput electrolyte screening via automated electrolyte formulation, coin-cell fabrication, and electrochemical evaluation",
      "authors": [
        {
          "name": "Hyun-Gi Lee"
        },
        {
          "name": "Jaekyeong Han"
        },
        {
          "name": "Minjun Kwon"
        },
        {
          "name": "Hyeonuk Kwon"
        },
        {
          "name": "Jooha Park"
        },
        {
          "name": "Hoe Jin Ha"
        },
        {
          "name": "Dong-Hwa Seo"
        }
      ],
      "abstract": "As battery technologies advance toward higher stability and energy density, the need for extensive cell-level testing across various component configurations becomes critical. To evaluate performance and understand the operating principles of batteries in laboratory scale, fabrication and evaluation of coin cells are essential processes. However, the conventional coin-cell assembly and testing processes require significant time and labor from researchers, posing challenges to high-throughput screening research. In this study, we introduce an Automated Li-ion BAttery Testing RObot SyStem (ALBATROSS), an automated system capable of electrolyte formulation, coin-cell assembly, and electrochemical evaluation. The system, integrated within a argon-filled glovebox, enables fully automated assembly and testing of up to 48 cells without researcher intervention. By incorporating custom-designed robot gripper and 3D-printed structures optimized for precise cell handling, ALBATROSS achieved high assembly reliability, yielding a relative standard deviation (RSD) of less than 1.2% in discharge capacity and a standard deviation of less than 3 Ω in EIS measurements for NCM811||Li half cells. Owing to its high reliability and automation capability, ALBATROSS allows for the acquisition of high-quality coin-cell datasets, which are expected to accelerate the development of next-generation electrolytes.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-15T11:13:30+00:00",
      "updated": "2025-12-15T11:13:30+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13198v1",
      "file": "papers/2512.13198v1.pdf"
    },
    {
      "arxiv_id": "2512.13183v1",
      "title": "Efficient Generation of Smooth Paths with Curvature Guarantees by Mollification",
      "authors": [
        {
          "name": "Alfredo González-Calvin"
        },
        {
          "name": "Juan F. Jiménez"
        },
        {
          "name": "Héctor García de Marina"
        }
      ],
      "abstract": "Most path following and trajectory tracking algorithms in mobile robotics require the desired path or trajectory to be defined by at least twice continuously differentiable functions to guarantee key properties such as global convergence, especially for nonholonomic robots like unicycles with speed constraints. Consequently, these algorithms typically exclude continuous but non-differentiable paths, such as piecewise functions. Despite this exclusion, such paths provide convenient high-level inputs for describing robot missions or behavior. While techniques such as spline interpolation or optimization-based methods are commonly used to smooth non-differentiable paths or create feasible ones from sequences of waypoints, they either can produce unnecessarily complex trajectories or are computationally expensive. In this work, we present a method to regularize non-differentiable functions and generate feasible paths through mollification. Specifically, we approximate an arbitrary path with a differentiable function that can converge to it with arbitrary precision. Additionally, we provide a systematic method for bounding the curvature of generated paths, which we demonstrate by applying it to paths resulting from linking a sequence of waypoints with segments. The proposed approach is computationally efficient, enabling real-time implementation on microcontrollers and compatibility with standard trajectory tracking and path following algorithms.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "published": "2025-12-15T10:48:42+00:00",
      "updated": "2025-12-15T10:48:42+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13183v1",
      "file": "papers/2512.13183v1.pdf"
    },
    {
      "arxiv_id": "2512.13170v1",
      "title": "Iterative Tuning of Nonlinear Model Predictive Control for Robotic Manufacturing Tasks",
      "authors": [
        {
          "name": "Deepak Ingole"
        },
        {
          "name": "Valentin Bhend"
        },
        {
          "name": "Shiva Ganesh Murali"
        },
        {
          "name": "Oliver Dobrich"
        },
        {
          "name": "Alisa Rupenayan"
        }
      ],
      "abstract": "Manufacturing processes are often perturbed by drifts in the environment and wear in the system, requiring control re-tuning even in the presence of repetitive operations. This paper presents an iterative learning framework for automatic tuning of Nonlinear Model Predictive Control (NMPC) weighting matrices based on task-level performance feedback. Inspired by norm-optimal Iterative Learning Control (ILC), the proposed method adaptively adjusts NMPC weights Q and R across task repetitions to minimize key performance indicators (KPIs) related to tracking accuracy, control effort, and saturation. Unlike gradient-based approaches that require differentiating through the NMPC solver, we construct an empirical sensitivity matrix, enabling structured weight updates without analytic derivatives. The framework is validated through simulation on a UR10e robot performing carbon fiber winding on a tetrahedral core. Results demonstrate that the proposed approach converges to near-optimal tracking performance (RMSE within 0.3% of offline Bayesian Optimization (BO)) in just 4 online repetitions, compared to 100 offline evaluations required by BO algorithm. The method offers a practical solution for adaptive NMPC tuning in repetitive robotic tasks, combining the precision of carefully optimized controllers with the flexibility of online adaptation.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG",
        "eess.SY",
        "math.OC"
      ],
      "published": "2025-12-15T10:30:40+00:00",
      "updated": "2025-12-15T10:30:40+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13170v1",
      "file": "papers/2512.13170v1.pdf"
    },
    {
      "arxiv_id": "2512.13153v1",
      "title": "START: Traversing Sparse Footholds with Terrain Reconstruction",
      "authors": [
        {
          "name": "Ruiqi Yu"
        },
        {
          "name": "Qianshi Wang"
        },
        {
          "name": "Hongyi Li"
        },
        {
          "name": "Zheng Jun"
        },
        {
          "name": "Zhicheng Wang"
        },
        {
          "name": "Jun Wu"
        },
        {
          "name": "Qiuguo Zhu"
        }
      ],
      "abstract": "Traversing terrains with sparse footholds like legged animals presents a promising yet challenging task for quadruped robots, as it requires precise environmental perception and agile control to secure safe foot placement while maintaining dynamic stability. Model-based hierarchical controllers excel in laboratory settings, but suffer from limited generalization and overly conservative behaviors. End-to-end learning-based approaches unlock greater flexibility and adaptability, but existing state-of-the-art methods either rely on heightmaps that introduce noise and complex, costly pipelines, or implicitly infer terrain features from egocentric depth images, often missing accurate critical geometric cues and leading to inefficient learning and rigid gaits. To overcome these limitations, we propose START, a single-stage learning framework that enables agile, stable locomotion on highly sparse and randomized footholds. START leverages only low-cost onboard vision and proprioception to accurately reconstruct local terrain heightmap, providing an explicit intermediate representation to convey essential features relevant to sparse foothold regions. This supports comprehensive environmental understanding and precise terrain assessment, reducing exploration cost and accelerating skill acquisition. Experimental results demonstrate that START achieves zero-shot transfer across diverse real-world scenarios, showcasing superior adaptability, precise foothold placement, and robust locomotion.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-15T10:02:41+00:00",
      "updated": "2025-12-15T10:02:41+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13153v1",
      "file": "papers/2512.13153v1.pdf"
    },
    {
      "arxiv_id": "2512.13100v1",
      "title": "OXE-AugE: A Large-Scale Robot Augmentation of OXE for Scaling Cross-Embodiment Policy Learning",
      "authors": [
        {
          "name": "Guanhua Ji"
        },
        {
          "name": "Harsha Polavaram"
        },
        {
          "name": "Lawrence Yunliang Chen"
        },
        {
          "name": "Sandeep Bajamahal"
        },
        {
          "name": "Zehan Ma"
        },
        {
          "name": "Simeon Adebola"
        },
        {
          "name": "Chenfeng Xu"
        },
        {
          "name": "Ken Goldberg"
        }
      ],
      "abstract": "Large and diverse datasets are needed for training generalist robot policies that have potential to control a variety of robot embodiments -- robot arm and gripper combinations -- across diverse tasks and environments. As re-collecting demonstrations and retraining for each new hardware platform are prohibitively costly, we show that existing robot data can be augmented for transfer and generalization. The Open X-Embodiment (OXE) dataset, which aggregates demonstrations from over 60 robot datasets, has been widely used as the foundation for training generalist policies. However, it is highly imbalanced: the top four robot types account for over 85\\% of its real data, which risks overfitting to robot-scene combinations. We present AugE-Toolkit, a scalable robot augmentation pipeline, and OXE-AugE, a high-quality open-source dataset that augments OXE with 9 different robot embodiments. OXE-AugE provides over 4.4 million trajectories, more than triple the size of the original OXE. We conduct a systematic study of how scaling robot augmentation impacts cross-embodiment learning. Results suggest that augmenting datasets with diverse arms and grippers improves policy performance not only on the augmented robots, but also on unseen robots and even the original robots under distribution shifts. In physical experiments, we demonstrate that state-of-the-art generalist policies such as OpenVLA and $π_0$ benefit from fine-tuning on OXE-AugE, improving success rates by 24-45% on previously unseen robot-gripper combinations across four real-world manipulation tasks. Project website: https://OXE-AugE.github.io/.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2025-12-15T08:57:15+00:00",
      "updated": "2025-12-15T08:57:15+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13100v1",
      "file": "papers/2512.13100v1.pdf"
    },
    {
      "arxiv_id": "2512.13093v1",
      "title": "PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations",
      "authors": [
        {
          "name": "Mingqi Yuan"
        },
        {
          "name": "Tao Yu"
        },
        {
          "name": "Haolin Song"
        },
        {
          "name": "Bo Li"
        },
        {
          "name": "Xin Jin"
        },
        {
          "name": "Hua Chen"
        },
        {
          "name": "Wenjun Zeng"
        }
      ],
      "abstract": "Achieving efficient and robust whole-body control (WBC) is essential for enabling humanoid robots to perform complex tasks in dynamic environments. Despite the success of reinforcement learning (RL) in this domain, its sample inefficiency remains a significant challenge due to the intricate dynamics and partial observability of humanoid robots. To address this limitation, we propose PvP, a Proprioceptive-Privileged contrastive learning framework that leverages the intrinsic complementarity between proprioceptive and privileged states. PvP learns compact and task-relevant latent representations without requiring hand-crafted data augmentations, enabling faster and more stable policy learning. To support systematic evaluation, we develop SRL4Humanoid, the first unified and modular framework that provides high-quality implementations of representative state representation learning (SRL) methods for humanoid robot learning. Extensive experiments on the LimX Oli robot across velocity tracking and motion imitation tasks demonstrate that PvP significantly improves sample efficiency and final performance compared to baseline SRL methods. Our study further provides practical insights into integrating SRL with RL for humanoid WBC, offering valuable guidance for data-efficient humanoid robot learning.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "published": "2025-12-15T08:50:20+00:00",
      "updated": "2025-12-15T08:50:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13093v1",
      "file": "papers/2512.13093v1.pdf"
    },
    {
      "arxiv_id": "2512.13090v1",
      "title": "Multi-Robot Motion Planning from Vision and Language using Heat-Inspired Diffusion",
      "authors": [
        {
          "name": "Jebeom Chae"
        },
        {
          "name": "Junwoo Chang"
        },
        {
          "name": "Seungho Yeom"
        },
        {
          "name": "Yujin Kim"
        },
        {
          "name": "Jongeun Choi"
        }
      ],
      "abstract": "Diffusion models have recently emerged as powerful tools for robot motion planning by capturing the multi-modal distribution of feasible trajectories. However, their extension to multi-robot settings with flexible, language-conditioned task specifications remains limited. Furthermore, current diffusion-based approaches incur high computational cost during inference and struggle with generalization because they require explicit construction of environment representations and lack mechanisms for reasoning about geometric reachability. To address these limitations, we present Language-Conditioned Heat-Inspired Diffusion (LCHD), an end-to-end vision-based framework that generates language-conditioned, collision-free trajectories. LCHD integrates CLIP-based semantic priors with a collision-avoiding diffusion kernel serving as a physical inductive bias that enables the planner to interpret language commands strictly within the reachable workspace. This naturally handles out-of-distribution scenarios -- in terms of reachability -- by guiding robots toward accessible alternatives that match the semantic intent, while eliminating the need for explicit obstacle information at inference time. Extensive evaluations on diverse real-world-inspired maps, along with real-robot experiments, show that LCHD consistently outperforms prior diffusion-based planners in success rate, while reducing planning latency.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-15T08:43:13+00:00",
      "updated": "2025-12-15T08:43:13+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13090v1",
      "file": "papers/2512.13090v1.pdf"
    },
    {
      "arxiv_id": "2512.13080v1",
      "title": "Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos",
      "authors": [
        {
          "name": "Yicheng Feng"
        },
        {
          "name": "Wanpeng Zhang"
        },
        {
          "name": "Ye Wang"
        },
        {
          "name": "Hao Luo"
        },
        {
          "name": "Haoqi Yuan"
        },
        {
          "name": "Sipeng Zheng"
        },
        {
          "name": "Zongqing Lu"
        }
      ],
      "abstract": "Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-15T08:31:47+00:00",
      "updated": "2025-12-15T08:31:47+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13080v1",
      "file": "papers/2512.13080v1.pdf"
    },
    {
      "arxiv_id": "2512.13030v1",
      "title": "Motus: A Unified Latent Action World Model",
      "authors": [
        {
          "name": "Hongzhe Bi"
        },
        {
          "name": "Hengkai Tan"
        },
        {
          "name": "Shenghao Xie"
        },
        {
          "name": "Zeyuan Wang"
        },
        {
          "name": "Shuhe Huang"
        },
        {
          "name": "Haitian Liu"
        },
        {
          "name": "Ruowen Zhao"
        },
        {
          "name": "Yao Feng"
        },
        {
          "name": "Chendong Xiang"
        },
        {
          "name": "Yinze Rong"
        },
        {
          "name": "Hongyan Zhao"
        },
        {
          "name": "Hanyu Liu"
        },
        {
          "name": "Zhizhong Su"
        },
        {
          "name": "Lei Ma"
        },
        {
          "name": "Hang Su"
        },
        {
          "name": "Jun Zhu"
        }
      ],
      "abstract": "While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level \"delta action\" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "published": "2025-12-15T06:58:40+00:00",
      "updated": "2025-12-15T06:58:40+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13030v1",
      "file": "papers/2512.13030v1.pdf"
    },
    {
      "arxiv_id": "2512.13009v1",
      "title": "K-VARK: Kernelized Variance-Aware Residual Kalman Filter for Sensorless Force Estimation in Collaborative Robots",
      "authors": [
        {
          "name": "Oğuzhan Akbıyık"
        },
        {
          "name": "Naseem Alhousani"
        },
        {
          "name": "Fares J. Abu-Dakka"
        }
      ],
      "abstract": "Reliable estimation of contact forces is crucial for ensuring safe and precise interaction of robots with unstructured environments. However, accurate sensorless force estimation remains challenging due to inherent modeling errors and complex residual dynamics and friction. To address this challenge, in this paper, we propose K-VARK (Kernelized Variance-Aware Residual Kalman filter), a novel approach that integrates a kernelized, probabilistic model of joint residual torques into an adaptive Kalman filter framework. Through Kernelized Movement Primitives trained on optimized excitation trajectories, K-VARK captures both the predictive mean and input-dependent heteroscedastic variance of residual torques, reflecting data variability and distance-to-training effects. These statistics inform a variance-aware virtual measurement update by augmenting the measurement noise covariance, while the process noise covariance adapts online via variational Bayesian optimization to handle dynamic disturbances. Experimental validation on a 6-DoF collaborative manipulator demonstrates that K-VARK achieves over 20% reduction in RMSE compared to state-of-the-art sensorless force estimation methods, yielding robust and accurate external force/torque estimation suitable for advanced tasks such as polishing and assembly.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-15T06:11:14+00:00",
      "updated": "2025-12-15T06:11:14+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13009v1",
      "file": "papers/2512.13009v1.pdf"
    },
    {
      "arxiv_id": "2512.12993v1",
      "title": "Learning Terrain Aware Bipedal Locomotion via Reduced Dimensional Perceptual Representations",
      "authors": [
        {
          "name": "Guillermo A. Castillo"
        },
        {
          "name": "Himanshu Lodha"
        },
        {
          "name": "Ayonga Hereid"
        }
      ],
      "abstract": "This work introduces a hierarchical strategy for terrain-aware bipedal locomotion that integrates reduced-dimensional perceptual representations to enhance reinforcement learning (RL)-based high-level (HL) policies for real-time gait generation. Unlike end-to-end approaches, our framework leverages latent terrain encodings via a Convolutional Variational Autoencoder (CNN-VAE) alongside reduced-order robot dynamics, optimizing the locomotion decision process with a compact state. We systematically analyze the impact of latent space dimensionality on learning efficiency and policy robustness. Additionally, we extend our method to be history-aware, incorporating sequences of recent terrain observations into the latent representation to improve robustness. To address real-world feasibility, we introduce a distillation method to learn the latent representation directly from depth camera images and provide preliminary hardware validation by comparing simulated and real sensor data. We further validate our framework using the high-fidelity Agility Robotics (AR) simulator, incorporating realistic sensor noise, state estimation, and actuator dynamics. The results confirm the robustness and adaptability of our method, underscoring its potential for hardware deployment.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-15T05:38:08+00:00",
      "updated": "2025-12-15T05:38:08+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12993v1",
      "file": "papers/2512.12993v1.pdf"
    },
    {
      "arxiv_id": "2512.12945v1",
      "title": "SLIM-VDB: A Real-Time 3D Probabilistic Semantic Mapping Framework",
      "authors": [
        {
          "name": "Anja Sheppard"
        },
        {
          "name": "Parker Ewen"
        },
        {
          "name": "Joey Wilson"
        },
        {
          "name": "Advaith V. Sethuraman"
        },
        {
          "name": "Benard Adewole"
        },
        {
          "name": "Anran Li"
        },
        {
          "name": "Yuzhen Chen"
        },
        {
          "name": "Ram Vasudevan"
        },
        {
          "name": "Katherine A. Skinner"
        }
      ],
      "abstract": "This paper introduces SLIM-VDB, a new lightweight semantic mapping system with probabilistic semantic fusion for closed-set or open-set dictionaries. Advances in data structures from the computer graphics community, such as OpenVDB, have demonstrated significantly improved computational and memory efficiency in volumetric scene representation. Although OpenVDB has been used for geometric mapping in robotics applications, semantic mapping for scene understanding with OpenVDB remains unexplored. In addition, existing semantic mapping systems lack support for integrating both fixed-category and open-language label predictions within a single framework. In this paper, we propose a novel 3D semantic mapping system that leverages the OpenVDB data structure and integrates a unified Bayesian update framework for both closed- and open-set semantic fusion. Our proposed framework, SLIM-VDB, achieves significant reduction in both memory and integration times compared to current state-of-the-art semantic mapping approaches, while maintaining comparable mapping accuracy. An open-source C++ codebase with a Python interface is available at https://github.com/umfieldrobotics/slim-vdb.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published": "2025-12-15T03:16:04+00:00",
      "updated": "2025-12-15T03:16:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12945v1",
      "file": "papers/2512.12945v1.pdf"
    },
    {
      "arxiv_id": "2512.12842v1",
      "title": "SAGA: Open-World Mobile Manipulation via Structured Affordance Grounding",
      "authors": [
        {
          "name": "Kuan Fang"
        },
        {
          "name": "Yuxin Chen"
        },
        {
          "name": "Xinghao Zhu"
        },
        {
          "name": "Farzad Niroui"
        },
        {
          "name": "Lingfeng Sun"
        },
        {
          "name": "Jiuguang Wang"
        }
      ],
      "abstract": "We present SAGA, a versatile and adaptive framework for visuomotor control that can generalize across various environments, task objectives, and user specifications. To efficiently learn such capability, our key idea is to disentangle high-level semantic intent from low-level visuomotor control by explicitly grounding task objectives in the observed environment. Using an affordance-based task representation, we express diverse and complex behaviors in a unified, structured form. By leveraging multimodal foundation models, SAGA grounds the proposed task representation to the robot's visual observation as 3D affordance heatmaps, highlighting task-relevant entities while abstracting away spurious appearance variations that would hinder generalization. These grounded affordances enable us to effectively train a conditional policy on multi-task demonstration data for whole-body control. In a unified framework, SAGA can solve tasks specified in different forms, including language instructions, selected points, and example demonstrations, enabling both zero-shot execution and few-shot adaptation. We instantiate SAGA on a quadrupedal manipulator and conduct extensive experiments across eleven real-world tasks. SAGA consistently outperforms end-to-end and modular baselines by substantial margins. Together, these results demonstrate that structured affordance grounding offers a scalable and effective pathway toward generalist mobile manipulation.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-14T21:13:56+00:00",
      "updated": "2025-12-14T21:13:56+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12842v1",
      "file": "papers/2512.12842v1.pdf"
    },
    {
      "arxiv_id": "2512.12793v2",
      "title": "VLG-Loc: Vision-Language Global Localization from Labeled Footprint Maps",
      "authors": [
        {
          "name": "Mizuho Aoki"
        },
        {
          "name": "Kohei Honda"
        },
        {
          "name": "Yasuhiro Yoshimura"
        },
        {
          "name": "Takeshi Ishita"
        },
        {
          "name": "Ryo Yonetani"
        }
      ],
      "abstract": "This paper presents Vision-Language Global Localization (VLG-Loc), a novel global localization method that uses human-readable labeled footprint maps containing only names and areas of distinctive visual landmarks in an environment. While humans naturally localize themselves using such maps, translating this capability to robotic systems remains highly challenging due to the difficulty of establishing correspondences between observed landmarks and those in the map without geometric and appearance details. To address this challenge, VLG-Loc leverages a vision-language model (VLM) to search the robot's multi-directional image observations for the landmarks noted in the map. The method then identifies robot poses within a Monte Carlo localization framework, where the found landmarks are used to evaluate the likelihood of each pose hypothesis. Experimental validation in simulated and real-world retail environments demonstrates superior robustness compared to existing scan-based methods, particularly under environmental changes. Further improvements are achieved through the probabilistic fusion of visual and scan-based localization.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-14T18:22:00+00:00",
      "updated": "2025-12-18T10:25:28+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12793v2",
      "file": "papers/2512.12793v2.pdf"
    },
    {
      "arxiv_id": "2512.12722v1",
      "title": "Making Robots Play by the Rules: The ROS 2 CLIPS-Executive",
      "authors": [
        {
          "name": "Tarik Viehmann"
        },
        {
          "name": "Daniel Swoboda"
        },
        {
          "name": "Samridhi Kalra"
        },
        {
          "name": "Himanshu Grover"
        },
        {
          "name": "Gerhard Lakemeyer"
        }
      ],
      "abstract": "CLIPS is a rule-based programming language for building knowledge-driven applications, well suited for the complex task of coordinating autonomous robots. Inspired by the CLIPS-Executive originally developed for the lesser known Fawkes robotics framework, we present an Integration of CLIPS into the ROS ecosystem. Additionally, we show the flexibility of CLIPS by describing a PDDL-based planning framework integration.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-14T14:53:28+00:00",
      "updated": "2025-12-14T14:53:28+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12722v1",
      "file": "papers/2512.12722v1.pdf"
    },
    {
      "arxiv_id": "2512.12717v1",
      "title": "HMPCC: Human-Aware Model Predictive Coverage Control",
      "authors": [
        {
          "name": "Mattia Catellani"
        },
        {
          "name": "Marta Gabbi"
        },
        {
          "name": "Lorenzo Sabattini"
        }
      ],
      "abstract": "We address the problem of coordinating a team of robots to cover an unknown environment while ensuring safe operation and avoiding collisions with non-cooperative agents. Traditional coverage strategies often rely on simplified assumptions, such as known or convex environments and static density functions, and struggle to adapt to real-world scenarios, especially when humans are involved. In this work, we propose a human-aware coverage framework based on Model Predictive Control (MPC), namely HMPCC, where human motion predictions are integrated into the planning process. By anticipating human trajectories within the MPC horizon, robots can proactively coordinate their actions %avoid redundant exploration, and adapt to dynamic conditions. The environment is modeled as a Gaussian Mixture Model (GMM), representing regions of interest. Team members operate in a fully decentralized manner, without relying on explicit communication, an essential feature in hostile or communication-limited scenarios. Our results show that human trajectory forecasting enables more efficient and adaptive coverage, improving coordination between human and robotic agents.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-14T14:41:36+00:00",
      "updated": "2025-12-14T14:41:36+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12717v1",
      "file": "papers/2512.12717v1.pdf"
    },
    {
      "arxiv_id": "2512.12649v1",
      "title": "Bayesian Optimization Parameter Tuning Framework for a Lyapunov Based Path Following Controller",
      "authors": [
        {
          "name": "Zhewen Zheng"
        },
        {
          "name": "Wenjing Cao"
        },
        {
          "name": "Hongkang Yu"
        },
        {
          "name": "Mo Chen"
        },
        {
          "name": "Takashi Suzuki"
        }
      ],
      "abstract": "Parameter tuning in real-world experiments is constrained by the limited evaluation budget available on hardware. The path-following controller studied in this paper reflects a typical situation in nonlinear geometric controller, where multiple gains influence the dynamics through coupled nonlinear terms. Such interdependence makes manual tuning inefficient and unlikely to yield satisfactory performance within a practical number of trials. To address this challenge, we propose a Bayesian optimization (BO) framework that treats the closed-loop system as a black box and selects controller gains using a Gaussian-process surrogate. BO offers model-free exploration, quantified uncertainty, and data-efficient search, making it well suited for tuning tasks where each evaluation is costly. The framework is implemented on Honda's AI-Formula three-wheeled robot and assessed through repeated full-lap experiments on a fixed test track. The results show that BO improves controller performance within 32 trials, including 15 warm-start initial evaluations, indicating that it can efficiently locate high-performing regions of the parameter space under real-world conditions. These findings demonstrate that BO provides a practical, reliable, and data-efficient tuning approach for nonlinear path-following controllers on real robotic platforms.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "published": "2025-12-14T11:35:53+00:00",
      "updated": "2025-12-14T11:35:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12649v1",
      "file": "papers/2512.12649v1.pdf"
    },
    {
      "arxiv_id": "2512.12622v1",
      "title": "D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation",
      "authors": [
        {
          "name": "Zihan Wang"
        },
        {
          "name": "Seungjun Lee"
        },
        {
          "name": "Guangzhao Dai"
        },
        {
          "name": "Gim Hee Lee"
        }
      ],
      "abstract": "Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-12-14T09:53:15+00:00",
      "updated": "2025-12-14T09:53:15+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12622v1",
      "file": "papers/2512.12622v1.pdf"
    },
    {
      "arxiv_id": "2512.12468v1",
      "title": "Autonomously Unweaving Multiple Cables Using Visual Feedback",
      "authors": [
        {
          "name": "Tina Tian"
        },
        {
          "name": "Xinyu Wang"
        },
        {
          "name": "Andrew L. Orekhov"
        },
        {
          "name": "Fujun Ruan"
        },
        {
          "name": "Lu Li"
        },
        {
          "name": "Oliver Kroemer"
        },
        {
          "name": "Howie Choset"
        }
      ],
      "abstract": "Many cable management tasks involve separating out the different cables and removing tangles. Automating this task is challenging because cables are deformable and can have combinations of knots and multiple interwoven segments. Prior works have focused on untying knots in one cable, which is one subtask of cable management. However, in this paper, we focus on a different subtask called multi-cable unweaving, which refers to removing the intersections among multiple interwoven cables to separate them and facilitate further manipulation. We propose a method that utilizes visual feedback to unweave a bundle of loosely entangled cables. We formulate cable unweaving as a pick-and-place problem, where the grasp position is selected from discrete nodes in a graph-based cable state representation. Our cable state representation encodes both topological and geometric information about the cables from the visual image. To predict future cable states and identify valid actions, we present a novel state transition model that takes into account the straightening and bending of cables during manipulation. Using this state transition model, we select between two high-level action primitives and calculate predicted immediate costs to optimize the lower-level actions. We experimentally demonstrate that iterating the above perception-planning-action process enables unweaving electric cables and shoelaces with an 84% success rate on average.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-13T21:42:45+00:00",
      "updated": "2025-12-13T21:42:45+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12468v1",
      "file": "papers/2512.12468v1.pdf"
    },
    {
      "arxiv_id": "2512.12437v1",
      "title": "Sim2Real Reinforcement Learning for Soccer skills",
      "authors": [
        {
          "name": "Jonathan Spraggett"
        }
      ],
      "abstract": "This thesis work presents a more efficient and effective approach to training control-related tasks for humanoid robots using Reinforcement Learning (RL). The traditional RL methods are limited in adapting to real-world environments, complexity, and natural motions, but the proposed approach overcomes these limitations by using curriculum training and Adversarial Motion Priors (AMP) technique. The results show that the developed RL policies for kicking, walking, and jumping are more dynamic, and adaptive, and outperformed previous methods. However, the transfer of the learned policy from simulation to the real world was unsuccessful, highlighting the limitations of current RL methods in fully adapting to real-world scenarios.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "published": "2025-12-13T19:29:35+00:00",
      "updated": "2025-12-13T19:29:35+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12437v1",
      "file": "papers/2512.12437v1.pdf"
    },
    {
      "arxiv_id": "2512.12427v1",
      "title": "Unifying Quadrotor Motion Planning and Control by Chaining Different Fidelity Models",
      "authors": [
        {
          "name": "Rudolf Reiter"
        },
        {
          "name": "Chao Qin"
        },
        {
          "name": "Leonard Bauersfeld"
        },
        {
          "name": "Davide Scaramuzza"
        }
      ],
      "abstract": "Many aerial tasks involving quadrotors demand both instant reactivity and long-horizon planning. High-fidelity models enable accurate control but are too slow for long horizons; low-fidelity planners scale but degrade closed-loop performance. We present Unique, a unified MPC that cascades models of different fidelity within a single optimization: a short-horizon, high-fidelity model for accurate control, and a long-horizon, low-fidelity model for planning. We align costs across horizons, derive feasibility-preserving thrust and body-rate constraints for the point-mass model, and introduce transition constraints that match the different states, thrust-induced acceleration, and jerk-body-rate relations. To prevent local minima emerging from nonsmooth clutter, we propose a 3D progressive smoothing schedule that morphs norm-based obstacles along the horizon. In addition, we deploy parallel randomly initialized MPC solvers to discover lower-cost local minima on the long, low-fidelity horizon. In simulation and real flights, under equal computational budgets, Unique improves closed-loop position or velocity tracking by up to 75% compared with standard MPC and hierarchical planner-tracker baselines. Ablations and Pareto analyses confirm robust gains across horizon variations, constraint approximations, and smoothing schedules.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "published": "2025-12-13T18:53:34+00:00",
      "updated": "2025-12-13T18:53:34+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12427v1",
      "file": "papers/2512.12427v1.pdf"
    },
    {
      "arxiv_id": "2512.15776v1",
      "title": "Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents via Active Querying",
      "authors": [
        {
          "name": "Shaun Baek"
        },
        {
          "name": "Sam Liu"
        },
        {
          "name": "Joseph Ukpong"
        }
      ],
      "abstract": "Large Language Models (LLMs) act as powerful reasoning engines but struggle with \"symbol grounding\" in embodied environments, particularly when information is asymmetrically distributed. We investigate the Privileged Information Bias (or \"Curse of Knowledge\"), where a knowledgeable \"Leader\" agent fails to guide a sensor-limited \"Follower\" due to a lack of Theory of Mind. To quantify this phenomenon, we propose a novel Asymmetric Assistive Reasoning framework within AI2-THOR. Our experiments reveal a significant \"Success Gap\": while the Leader successfully perceives the target in 35.0% of episodes, the collaborative team succeeds only 17.0% of the time, implying that nearly 50% of feasible plans fail solely due to communicative grounding errors. We demonstrate that a \"Pull-based\" protocol (active querying) is significantly more robust than standard \"Push-based\" instruction, with successful episodes featuring 2x the frequency of clarification requests. This research isolates the mechanism of active uncertainty reduction as a prerequisite for safe human-AI and robot-robot collaboration.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.RO"
      ],
      "published": "2025-12-13T17:17:51+00:00",
      "updated": "2025-12-13T17:17:51+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15776v1",
      "file": "papers/2512.15776v1.pdf"
    },
    {
      "arxiv_id": "2512.12377v1",
      "title": "INDOOR-LiDAR: Bridging Simulation and Reality for Robot-Centric 360 degree Indoor LiDAR Perception -- A Robot-Centric Hybrid Dataset",
      "authors": [
        {
          "name": "Haichuan Li"
        },
        {
          "name": "Changda Tian"
        },
        {
          "name": "Panos Trahanias"
        },
        {
          "name": "Tomi Westerlund"
        }
      ],
      "abstract": "We present INDOOR-LIDAR, a comprehensive hybrid dataset of indoor 3D LiDAR point clouds designed to advance research in robot perception. Existing indoor LiDAR datasets often suffer from limited scale, inconsistent annotation formats, and human-induced variability during data collection. INDOOR-LIDAR addresses these limitations by integrating simulated environments with real-world scans acquired using autonomous ground robots, providing consistent coverage and realistic sensor behavior under controlled variations. Each sample consists of dense point cloud data enriched with intensity measurements and KITTI-style annotations. The annotation schema encompasses common indoor object categories within various scenes. The simulated subset enables flexible configuration of layouts, point densities, and occlusions, while the real-world subset captures authentic sensor noise, clutter, and domain-specific artifacts characteristic of real indoor settings. INDOOR-LIDAR supports a wide range of applications including 3D object detection, bird's-eye-view (BEV) perception, SLAM, semantic scene understanding, and domain adaptation between simulated and real indoor domains. By bridging the gap between synthetic and real-world data, INDOOR-LIDAR establishes a scalable, realistic, and reproducible benchmark for advancing robotic perception in complex indoor environments.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-13T16:08:10+00:00",
      "updated": "2025-12-13T16:08:10+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12377v1",
      "file": "papers/2512.12377v1.pdf"
    },
    {
      "arxiv_id": "2512.12320v1",
      "title": "Programmable Deformation Design of Porous Soft Actuator through Volumetric-Pattern-Induced Anisotropy",
      "authors": [
        {
          "name": "Canqi Meng"
        },
        {
          "name": "Weibang Bai"
        }
      ],
      "abstract": "Conventional soft pneumatic actuators, typically based on hollow elastomeric chambers, often suffer from small structural support and require costly geometry-specific redesigns for multimodal functionality. Porous materials such as foam, filled into chambers, can provide structural stability for the actuators. However, methods to achieve programmable deformation by tailoring the porous body itself remain underexplored. In this paper, a novel design method is presented to realize soft porous actuators with programmable deformation by incising specific patterns into the porous foam body. This approach introduces localized structural anisotropy of the foam guiding the material's deformation under a global vacuum input. Furthermore, three fundamental patterns on a cylindrical foam substrate are discussed: transverse for bending, longitudinal for tilting, and diagonal for twisting. A computational model is built with Finite Element Analysis (FEA), to investigate the mechanism of the incision-patterning method. Experiments demonstrate that with a potential optimal design of the pattern array number N, actuators can achieve bending up to $80^{\\circ}$ (N=2), tilting of $18^{\\circ}$ (N=1), and twisting of $115^{\\circ}$ (N=8). The versatility of our approach is demonstrated via pattern transferability, scalability, and mold-less rapid prototyping of complex designs. As a comprehensive application, we translate the human hand crease map into a functional incision pattern, creating a bio-inspired soft robot hand capable of human-like adaptive grasping. Our work provides a new, efficient, and scalable paradigm for the design of multi-functional soft porous robots.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-13T13:17:17+00:00",
      "updated": "2025-12-13T13:17:17+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12320v1",
      "file": "papers/2512.12320v1.pdf"
    },
    {
      "arxiv_id": "2512.12243v1",
      "title": "CAR-CHASE: Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement",
      "authors": [
        {
          "name": "HT To"
        },
        {
          "name": "S Nguyen"
        },
        {
          "name": "NH Pham"
        }
      ],
      "abstract": "Multi-Agent Path Finding (MAPF) for car-like robots, addressed by algorithms such as Conflict-Based Search with Continuous Time (CL-CBS), faces significant computational challenges due to expensive kinematic heuristic calculations. Traditional heuristic caching assumes that the heuristic function depends only on the state, which is incorrect in CBS where constraints from conflict resolution make the search space context-dependent. We propose \\textbf{CAR-CHASE} (Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement), a novel approach that combines \\textbf{conflict-aware heuristic caching} -- which caches heuristic values based on both state and relevant constraint context -- with an \\textbf{adaptive hybrid heuristic} that intelligently switches between fast approximate and exact computations. Our key innovations are (1) a compact \\emph{conflict fingerprint} that efficiently encodes which constraints affect a state's heuristic, (2) a relevance filter using spatial, temporal, and geometric criteria, and (3) an adaptive switching strategy with theoretical quality bounds. Experimental evaluation on 480 benchmark instances with varying agent counts (10 to 30) and obstacle densities (0\\% and 50\\%) demonstrates a geometric mean speedup of 2.46$\\times$ over the baseline CL-CBS implementation while maintaining solution optimality. The optimizations improve success rate from 77.9\\% to 84.8\\% (+6.9 percentage points), reduce total runtime by 70.1\\%, and enable solving 33 additional instances that previously timed out. Performance gains scale with problem complexity, reaching up to 4.06$\\times$ speedup for challenging 30-agent obstacle scenarios. Our techniques are general and applicable to other CBS variants.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-13T08:42:18+00:00",
      "updated": "2025-12-13T08:42:18+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12243v1",
      "file": "papers/2512.12243v1.pdf"
    },
    {
      "arxiv_id": "2512.12230v1",
      "title": "Learning to Get Up Across Morphologies: Zero-Shot Recovery with a Unified Humanoid Policy",
      "authors": [
        {
          "name": "Jonathan Spraggett"
        }
      ],
      "abstract": "Fall recovery is a critical skill for humanoid robots in dynamic environments such as RoboCup, where prolonged downtime often decides the match. Recent techniques using deep reinforcement learning (DRL) have produced robust get-up behaviors, yet existing methods require training of separate policies for each robot morphology. This paper presents a single DRL policy capable of recovering from falls across seven humanoid robots with diverse heights (0.48 - 0.81 m), weights (2.8 - 7.9 kg), and dynamics. Trained with CrossQ, the unified policy transfers zero-shot up to 86 +/- 7% (95% CI [81, 89]) on unseen morphologies, eliminating the need for robot-specific training. Comprehensive leave-one-out experiments, morph scaling analysis, and diversity ablations show that targeted morphological coverage improves zero-shot generalization. In some cases, the shared policy even surpasses the specialist baselines. These findings illustrate the practicality of morphology-agnostic control for fall recovery, laying the foundation for generalist humanoid control. The software is open-source and available at: https://github.com/utra-robosoccer/unified-humanoid-getup",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "published": "2025-12-13T07:59:52+00:00",
      "updated": "2025-12-13T07:59:52+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12230v1",
      "file": "papers/2512.12230v1.pdf"
    },
    {
      "arxiv_id": "2512.12228v1",
      "title": "Semantic Zone based 3D Map Management for Mobile Robot",
      "authors": [
        {
          "name": "Huichang Yun"
        },
        {
          "name": "Seungho Yoo"
        }
      ],
      "abstract": "Mobile robots in large-scale indoor environments, such as hospitals and logistics centers, require accurate 3D spatial representations. However, 3D maps consume substantial memory, making it difficult to maintain complete map data within limited computational resources. Existing SLAM frameworks typically rely on geometric distance or temporal metrics for memory management, often resulting in inefficient data retrieval in spatially compartmentalized environments. To address this, we propose a semantic zone-based 3D map management method that shifts the paradigm from geometry-centric to semantics-centric control. Our approach partitions the environment into meaningful spatial units (e.g., lobbies, hallways) and designates these zones as the primary unit for memory management. By dynamically loading only task-relevant zones into Working Memory (WM) and offloading inactive zones to Long-Term Memory (LTM), the system strictly enforces user-defined memory thresholds. Implemented within the RTAB-Map framework, our method demonstrates substantial reductions in unnecessary signature load/unload cycles and cumulative memory utilization compared to standard approaches. The results confirm that semantic zone-based management ensures stable, predictable memory usage while preserving map availability for navigation. Code is available at: https://github.com/huichangs/rtabmap/tree/segment",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-13T07:55:40+00:00",
      "updated": "2025-12-13T07:55:40+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12228v1",
      "file": "papers/2512.12228v1.pdf"
    },
    {
      "arxiv_id": "2512.12208v1",
      "title": "A Hybrid Deep Learning Framework for Emotion Recognition in Children with Autism During NAO Robot-Mediated Interaction",
      "authors": [
        {
          "name": "Indranil Bhattacharjee"
        },
        {
          "name": "Vartika Narayani Srinet"
        },
        {
          "name": "Anirudha Bhattacharjee"
        },
        {
          "name": "Braj Bhushan"
        },
        {
          "name": "Bishakh Bhattacharya"
        }
      ],
      "abstract": "Understanding emotional responses in children with Autism Spectrum Disorder (ASD) during social interaction remains a critical challenge in both developmental psychology and human-robot interaction. This study presents a novel deep learning pipeline for emotion recognition in autistic children in response to a name-calling event by a humanoid robot (NAO), under controlled experimental settings. The dataset comprises of around 50,000 facial frames extracted from video recordings of 15 children with ASD. A hybrid model combining a fine-tuned ResNet-50-based Convolutional Neural Network (CNN) and a three-layer Graph Convolutional Network (GCN) trained on both visual and geometric features extracted from MediaPipe FaceMesh landmarks. Emotions were probabilistically labeled using a weighted ensemble of two models: DeepFace's and FER, each contributing to soft-label generation across seven emotion classes. Final classification leveraged a fused embedding optimized via Kullback-Leibler divergence. The proposed method demonstrates robust performance in modeling subtle affective responses and offers significant promise for affective profiling of ASD children in clinical and therapeutic human-robot interaction contexts, as the pipeline effectively captures micro emotional cues in neurodivergent children, addressing a major gap in autism-specific HRI research. This work represents the first such large-scale, real-world dataset and pipeline from India on autism-focused emotion analysis using social robotics, contributing an essential foundation for future personalized assistive technologies.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-12-13T06:40:01+00:00",
      "updated": "2025-12-13T06:40:01+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12208v1",
      "file": "papers/2512.12208v1.pdf"
    },
    {
      "arxiv_id": "2512.12203v1",
      "title": "Navigation Around Unknown Space Objects Using Visible-Thermal Image Fusion",
      "authors": [
        {
          "name": "Eric J. Elias"
        },
        {
          "name": "Michael Esswein"
        },
        {
          "name": "Jonathan P. How"
        },
        {
          "name": "David W. Miller"
        }
      ],
      "abstract": "As the popularity of on-orbit operations grows, so does the need for precise navigation around unknown resident space objects (RSOs) such as other spacecraft, orbital debris, and asteroids. The use of Simultaneous Localization and Mapping (SLAM) algorithms is often studied as a method to map out the surface of an RSO and find the inspector's relative pose using a lidar or conventional camera. However, conventional cameras struggle during eclipse or shadowed periods, and lidar, though robust to lighting conditions, tends to be heavier, bulkier, and more power-intensive. Thermal-infrared cameras can track the target RSO throughout difficult illumination conditions without these limitations. While useful, thermal-infrared imagery lacks the resolution and feature-richness of visible cameras. In this work, images of a target satellite in low Earth orbit are photo-realistically simulated in both visible and thermal-infrared bands. Pixel-level fusion methods are used to create visible/thermal-infrared composites that leverage the best aspects of each camera. Navigation errors from a monocular SLAM algorithm are compared between visible, thermal-infrared, and fused imagery in various lighting and trajectories. Fused imagery yields substantially improved navigation performance over visible-only and thermal-only methods.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published": "2025-12-13T06:24:26+00:00",
      "updated": "2025-12-13T06:24:26+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12203v1",
      "file": "papers/2512.12203v1.pdf"
    },
    {
      "arxiv_id": "2512.12194v1",
      "title": "B-ActiveSEAL: Scalable Uncertainty-Aware Active Exploration with Tightly Coupled Localization-Mapping",
      "authors": [
        {
          "name": "Min-Won Seo"
        },
        {
          "name": "Aamodh Suresh"
        },
        {
          "name": "Carlos Nieto-Granda"
        },
        {
          "name": "Solmaz S. Kia"
        }
      ],
      "abstract": "Active robot exploration requires decision-making processes that integrate localization and mapping under tightly coupled uncertainty. However, managing these interdependent uncertainties over long-term operations in large-scale environments rapidly becomes computationally intractable. To address this challenge, we propose B-ActiveSEAL, a scalable information-theoretic active exploration framework that explicitly accounts for coupled uncertainties-from perception through mapping-into the decision-making process. Our framework (i) adaptively balances map uncertainty (exploration) and localization uncertainty (exploitation), (ii) accommodates a broad class of generalized entropy measures, enabling flexible and uncertainty-aware active exploration, and (iii) establishes Behavioral entropy (BE) as an effective information measure for active exploration by enabling intuitive and adaptive decision-making under coupled uncertainties. We establish a theoretical foundation for propagating coupled uncertainties and integrating them into general entropy formulations, enabling uncertainty-aware active exploration under tightly coupled localization-mapping. The effectiveness of the proposed approach is validated through rigorous theoretical analysis and extensive experiments on open-source maps and ROS-Unity simulations across diverse and complex environments. The results demonstrate that B-ActiveSEAL achieves a well-balanced exploration-exploitation trade-off and produces diverse, adaptive exploration behaviors across environments, highlighting clear advantages over representative baselines.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "published": "2025-12-13T05:48:12+00:00",
      "updated": "2025-12-13T05:48:12+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12194v1",
      "file": "papers/2512.12194v1.pdf"
    },
    {
      "arxiv_id": "2512.12046v1",
      "title": "Goal Reaching with Eikonal-Constrained Hierarchical Quasimetric Reinforcement Learning",
      "authors": [
        {
          "name": "Vittorio Giammarino"
        },
        {
          "name": "Ahmed H. Qureshi"
        }
      ],
      "abstract": "Goal-Conditioned Reinforcement Learning (GCRL) mitigates the difficulty of reward design by framing tasks as goal reaching rather than maximizing hand-crafted reward signals. In this setting, the optimal goal-conditioned value function naturally forms a quasimetric, motivating Quasimetric RL (QRL), which constrains value learning to quasimetric mappings and enforces local consistency through discrete, trajectory-based constraints. We propose Eikonal-Constrained Quasimetric RL (Eik-QRL), a continuous-time reformulation of QRL based on the Eikonal Partial Differential Equation (PDE). This PDE-based structure makes Eik-QRL trajectory-free, requiring only sampled states and goals, while improving out-of-distribution generalization. We provide theoretical guarantees for Eik-QRL and identify limitations that arise under complex dynamics. To address these challenges, we introduce Eik-Hierarchical QRL (Eik-HiQRL), which integrates Eik-QRL into a hierarchical decomposition. Empirically, Eik-HiQRL achieves state-of-the-art performance in offline goal-conditioned navigation and yields consistent gains over QRL in manipulation tasks, matching temporal-difference methods.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.RO",
        "eess.SY",
        "stat.ML"
      ],
      "published": "2025-12-12T21:37:11+00:00",
      "updated": "2025-12-12T21:37:11+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12046v1",
      "file": "papers/2512.12046v1.pdf"
    }
  ]
}