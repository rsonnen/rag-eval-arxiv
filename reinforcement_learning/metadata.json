{
  "corpus": "reinforcement_learning",
  "source": "arxiv",
  "search_query": "(cat:cs.LG OR cat:cs.AI) AND (reinforcement learning OR policy gradient OR Q-learning OR MDP OR reward)",
  "curated_at": "2025-12-26T23:44:51.339002+00:00",
  "total_papers": 200,
  "papers_evaluated": 1929,
  "acceptance_rate": 0.10368066355624676,
  "papers": [
    {
      "arxiv_id": "2512.21081v1",
      "title": "Dyna-Style Reinforcement Learning Modeling and Control of Non-linear Dynamics",
      "authors": [
        {
          "name": "Karim Abdelsalam"
        },
        {
          "name": "Zeyad Gamal"
        },
        {
          "name": "Ayman El-Badawy"
        }
      ],
      "abstract": "Controlling systems with complex, nonlinear dynamics poses a significant challenge, particularly in achieving efficient and robust control. In this paper, we propose a Dyna-Style Reinforcement Learning control framework that integrates Sparse Identification of Nonlinear Dynamics (SINDy) with Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning. SINDy is used to identify a data-driven model of the system, capturing its key dynamics without requiring an explicit physical model. This identified model is used to generate synthetic rollouts that are periodically injected into the reinforcement learning replay buffer during training on the real environment, enabling efficient policy learning with limited data available. By leveraging this hybrid approach, we mitigate the sample inefficiency of traditional model-free reinforcement learning methods while ensuring accurate control of nonlinear systems. To demonstrate the effectiveness of this framework, we apply it to a bi-rotor system as a case study, evaluating its performance in stabilization and trajectory tracking. The results show that our SINDy-TD3 approach achieves superior accuracy and robustness compared to direct reinforcement learning techniques, highlighting the potential of combining data-driven modeling with reinforcement learning for complex dynamical systems.",
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY",
        "cs.LG"
      ],
      "published": "2025-12-24T09:56:28+00:00",
      "updated": "2025-12-24T09:56:28+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21081v1",
      "file": "papers/2512.21081v1.pdf"
    },
    {
      "arxiv_id": "2512.21024v1",
      "title": "Policy-Conditioned Policies for Multi-Agent Task Solving",
      "authors": [
        {
          "name": "Yue Lin"
        },
        {
          "name": "Shuhui Zhu"
        },
        {
          "name": "Wenhao Li"
        },
        {
          "name": "Ang Li"
        },
        {
          "name": "Dan Qiao"
        },
        {
          "name": "Pascal Poupart"
        },
        {
          "name": "Hongyuan Zha"
        },
        {
          "name": "Baoxiang Wang"
        }
      ],
      "abstract": "In multi-agent tasks, the central challenge lies in the dynamic adaptation of strategies. However, directly conditioning on opponents' strategies is intractable in the prevalent deep reinforcement learning paradigm due to a fundamental ``representational bottleneck'': neural policies are opaque, high-dimensional parameter vectors that are incomprehensible to other agents. In this work, we propose a paradigm shift that bridges this gap by representing policies as human-interpretable source code and utilizing Large Language Models (LLMs) as approximate interpreters. This programmatic representation allows us to operationalize the game-theoretic concept of \\textit{Program Equilibrium}. We reformulate the learning problem by utilizing LLMs to perform optimization directly in the space of programmatic policies. The LLM functions as a point-wise best-response operator that iteratively synthesizes and refines the ego agent's policy code to respond to the opponent's strategy. We formalize this process as \\textit{Programmatic Iterated Best Response (PIBR)}, an algorithm where the policy code is optimized by textual gradients, using structured feedback derived from game utility and runtime unit tests. We demonstrate that this approach effectively solves several standard coordination matrix games and a cooperative Level-Based Foraging environment.",
      "primary_category": "cs.GT",
      "categories": [
        "cs.GT",
        "cs.AI"
      ],
      "published": "2025-12-24T07:42:10+00:00",
      "updated": "2025-12-24T07:42:10+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21024v1",
      "file": "papers/2512.21024v1.pdf"
    },
    {
      "arxiv_id": "2512.20974v1",
      "title": "Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions",
      "authors": [
        {
          "name": "Jingyang You"
        },
        {
          "name": "Hanna Kurniawati"
        }
      ],
      "abstract": "Bayesian Reinforcement Learning (BRL) provides a framework for generalisation of Reinforcement Learning (RL) problems from its use of Bayesian task parameters in the transition and reward models. However, classical BRL methods assume known forms of transition and reward models, reducing their applicability in real-world problems. As a result, recent deep BRL methods have started to incorporate model learning, though the use of neural networks directly on the joint data and task parameters requires optimising the Evidence Lower Bound (ELBO). ELBOs are difficult to optimise and may result in indistinctive task parameters, hence compromised BRL policies. To this end, we introduce a novel deep BRL method, Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions (GLiBRL), that enables efficient and accurate learning of transition and reward models, with fully tractable marginal likelihood and Bayesian inference on task parameters and model noises. On challenging MetaWorld ML10/45 benchmarks, GLiBRL improves the success rate of one of the state-of-the-art deep BRL methods, VariBAD, by up to 2.7x. Comparing against representative or recent deep BRL / Meta-RL methods, such as MAML, RL2, SDVT, TrMRL and ECET, GLiBRL also demonstrates its low-variance and decent performance consistently.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "published": "2025-12-24T06:00:51+00:00",
      "updated": "2025-12-24T06:00:51+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20974v1",
      "file": "papers/2512.20974v1.pdf"
    },
    {
      "arxiv_id": "2512.20831v1",
      "title": "Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions",
      "authors": [
        {
          "name": "Rashmeet Kaur Nayyar"
        },
        {
          "name": "Naman Shah"
        },
        {
          "name": "Siddharth Srivastava"
        }
      ],
      "abstract": "Real-world sequential decision-making often involves parameterized action spaces that require both, decisions regarding discrete actions and decisions about continuous action parameters governing how an action is executed. Existing approaches exhibit severe limitations in this setting -- planning methods demand hand-crafted action models, and standard reinforcement learning (RL) algorithms are designed for either discrete or continuous actions but not both, and the few RL methods that handle parameterized actions typically rely on domain-specific engineering and fail to exploit the latent structure of these spaces. This paper extends the scope of RL algorithms to long-horizon, sparse-reward settings with parameterized actions by enabling agents to autonomously learn both state and action abstractions online. We introduce algorithms that progressively refine these abstractions during learning, increasing fine-grained detail in the critical regions of the state-action space where greater resolution improves performance. Across several continuous-state, parameterized-action domains, our abstraction-driven approach enables TD($λ$) to achieve markedly higher sample efficiency than state-of-the-art baselines.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-23T23:12:53+00:00",
      "updated": "2025-12-23T23:12:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20831v1",
      "file": "papers/2512.20831v1.pdf"
    },
    {
      "arxiv_id": "2512.20806v1",
      "title": "Safety Alignment of LMs via Non-cooperative Games",
      "authors": [
        {
          "name": "Anselm Paulus"
        },
        {
          "name": "Ilia Kulikov"
        },
        {
          "name": "Brandon Amos"
        },
        {
          "name": "Rémi Munos"
        },
        {
          "name": "Ivan Evtimov"
        },
        {
          "name": "Kamalika Chaudhuri"
        },
        {
          "name": "Arman Zharmagambetov"
        }
      ],
      "abstract": "Ensuring the safety of language models (LMs) while maintaining their usefulness remains a critical challenge in AI alignment. Current approaches rely on sequential adversarial training: generating adversarial prompts and fine-tuning LMs to defend against them. We introduce a different paradigm: framing safety alignment as a non-zero-sum game between an Attacker LM and a Defender LM trained jointly via online reinforcement learning. Each LM continuously adapts to the other's evolving strategies, driving iterative improvement. Our method uses a preference-based reward signal derived from pairwise comparisons instead of point-wise scores, providing more robust supervision and potentially reducing reward hacking. Our RL recipe, AdvGame, shifts the Pareto frontier of safety and utility, yielding a Defender LM that is simultaneously more helpful and more resilient to adversarial attacks. In addition, the resulting Attacker LM converges into a strong, general-purpose red-teaming agent that can be directly deployed to probe arbitrary target models.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-12-23T22:13:14+00:00",
      "updated": "2025-12-23T22:13:14+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20806v1",
      "file": "papers/2512.20806v1.pdf"
    },
    {
      "arxiv_id": "2512.20760v1",
      "title": "Generalization of RLVR Using Causal Reasoning as a Testbed",
      "authors": [
        {
          "name": "Brian Lu"
        },
        {
          "name": "Hongyu Zhao"
        },
        {
          "name": "Shuo Sun"
        },
        {
          "name": "Hao Peng"
        },
        {
          "name": "Rui Ding"
        },
        {
          "name": "Hongyuan Mei"
        }
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising paradigm for post-training large language models (LLMs) on complex reasoning tasks. Yet, the conditions under which RLVR yields robust generalization remain poorly understood. This paper provides an empirical study of RLVR generalization in the setting of probabilistic inference over causal graphical models. This setting offers two natural axes along which to examine generalization: (i) the level of the probabilistic query -- associational, interventional, or counterfactual -- and (ii) the structural complexity of the query, measured by the size of its relevant subgraph. We construct datasets of causal graphs and queries spanning these difficulty axes and fine-tune Qwen-2.5-Instruct models using RLVR or supervised fine-tuning (SFT). We vary both the model scale (3B-32B) and the query level included in training. We find that RLVR yields stronger within-level and across-level generalization than SFT, but only for specific combinations of model size and training query level. Further analysis shows that RLVR's effectiveness depends on the model's initial reasoning competence. With sufficient initial competence, RLVR improves an LLM's marginalization strategy and reduces errors in intermediate probability calculations, producing substantial accuracy gains, particularly on more complex queries. These findings show that RLVR can improve specific causal reasoning subskills, with its benefits emerging only when the model has sufficient initial competence.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-12-23T20:45:31+00:00",
      "updated": "2025-12-23T20:45:31+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20760v1",
      "file": "papers/2512.20760v1.pdf"
    },
    {
      "arxiv_id": "2512.20745v1",
      "title": "AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent",
      "authors": [
        {
          "name": "Haipeng Luo"
        },
        {
          "name": "Huawen Feng"
        },
        {
          "name": "Qingfeng Sun"
        },
        {
          "name": "Can Xu"
        },
        {
          "name": "Kai Zheng"
        },
        {
          "name": "Yufei Wang"
        },
        {
          "name": "Tao Yang"
        },
        {
          "name": "Han Hu"
        },
        {
          "name": "Yansong Tang"
        },
        {
          "name": "Di Wang"
        }
      ],
      "abstract": "Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-12-23T19:57:49+00:00",
      "updated": "2025-12-23T19:57:49+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20745v1",
      "file": "papers/2512.20745v1.pdf"
    },
    {
      "arxiv_id": "2512.20605v2",
      "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning",
      "authors": [
        {
          "name": "Seijin Kobayashi"
        },
        {
          "name": "Yanick Schimpf"
        },
        {
          "name": "Maximilian Schlegel"
        },
        {
          "name": "Angelika Steger"
        },
        {
          "name": "Maciej Wolczyk"
        },
        {
          "name": "Johannes von Oswald"
        },
        {
          "name": "Nino Scherrer"
        },
        {
          "name": "Kaitlin Maile"
        },
        {
          "name": "Guillaume Lajoie"
        },
        {
          "name": "Blake A. Richards"
        },
        {
          "name": "Rif A. Saurous"
        },
        {
          "name": "James Manyika"
        },
        {
          "name": "Blaise Agüera y Arcas"
        },
        {
          "name": "Alexander Meulemans"
        },
        {
          "name": "João Sacramento"
        }
      ],
      "abstract": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-12-23T18:51:50+00:00",
      "updated": "2025-12-24T08:32:45+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20605v2",
      "file": "papers/2512.20605v2.pdf"
    },
    {
      "arxiv_id": "2512.20576v1",
      "title": "Performative Policy Gradient: Optimality in Performative Reinforcement Learning",
      "authors": [
        {
          "name": "Debabrota Basu"
        },
        {
          "name": "Udvas Das"
        },
        {
          "name": "Brahim Driss"
        },
        {
          "name": "Uddalak Mukherjee"
        }
      ],
      "abstract": "Post-deployment machine learning algorithms often influence the environments they act in, and thus shift the underlying dynamics that the standard reinforcement learning (RL) methods ignore. While designing optimal algorithms in this performative setting has recently been studied in supervised learning, the RL counterpart remains under-explored. In this paper, we prove the performative counterparts of the performance difference lemma and the policy gradient theorem in RL, and further introduce the Performative Policy Gradient algorithm (PePG). PePG is the first policy gradient algorithm designed to account for performativity in RL. Under softmax parametrisation, and also with and without entropy regularisation, we prove that PePG converges to performatively optimal policies, i.e. policies that remain optimal under the distribution shifts induced by themselves. Thus, PePG significantly extends the prior works in Performative RL that achieves performative stability but not optimality. Furthermore, our empirical analysis on standard performative RL environments validate that PePG outperforms standard policy gradient algorithms and the existing performative RL algorithms aiming for stability.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "published": "2025-12-23T18:20:06+00:00",
      "updated": "2025-12-23T18:20:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20576v1",
      "file": "papers/2512.20576v1.pdf"
    },
    {
      "arxiv_id": "2512.20513v1",
      "title": "Recurrent Off-Policy Deep Reinforcement Learning Doesn't Have to be Slow",
      "authors": [
        {
          "name": "Tyler Clark"
        },
        {
          "name": "Christine Evers"
        },
        {
          "name": "Jonathon Hare"
        }
      ],
      "abstract": "Recurrent off-policy deep reinforcement learning models achieve state-of-the-art performance but are often sidelined due to their high computational demands. In response, we introduce RISE (Recurrent Integration via Simplified Encodings), a novel approach that can leverage recurrent networks in any image-based off-policy RL setting without significant computational overheads via using both learnable and non-learnable encoder layers. When integrating RISE into leading non-recurrent off-policy RL algorithms, we observe a 35.6% human-normalized interquartile mean (IQM) performance improvement across the Atari benchmark. We analyze various implementation strategies to highlight the versatility and potential of our proposed framework.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-23T17:02:17+00:00",
      "updated": "2025-12-23T17:02:17+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20513v1",
      "file": "papers/2512.20513v1.pdf"
    },
    {
      "arxiv_id": "2512.20368v1",
      "title": "Avoiding the Price of Adaptivity: Inference in Linear Contextual Bandits via Stability",
      "authors": [
        {
          "name": "Samya Praharaj"
        },
        {
          "name": "Koulik Khamaru"
        }
      ],
      "abstract": "Statistical inference in contextual bandits is complicated by the adaptive, non-i.i.d. nature of the data. A growing body of work has shown that classical least-squares inference may fail under adaptive sampling, and that constructing valid confidence intervals for linear functionals of the model parameter typically requires paying an unavoidable inflation of order $\\sqrt{d \\log T}$. This phenomenon -- often referred to as the price of adaptivity -- highlights the inherent difficulty of reliable inference under general contextual bandit policies.\n  A key structural property that circumvents this limitation is the \\emph{stability} condition of Lai and Wei, which requires the empirical feature covariance to concentrate around a deterministic limit. When stability holds, the ordinary least-squares estimator satisfies a central limit theorem, and classical Wald-type confidence intervals -- designed for i.i.d. data -- become asymptotically valid even under adaptation, \\emph{without} incurring the $\\sqrt{d \\log T}$ price of adaptivity.\n  In this paper, we propose and analyze a penalized EXP4 algorithm for linear contextual bandits. Our first main result shows that this procedure satisfies the Lai--Wei stability condition and therefore admits valid Wald-type confidence intervals for linear functionals. Our second result establishes that the same algorithm achieves regret guarantees that are minimax optimal up to logarithmic factors, demonstrating that stability and statistical efficiency can coexist within a single contextual bandit method. Finally, we complement our theory with simulations illustrating the empirical normality of the resulting estimators and the sharpness of the corresponding confidence intervals.",
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.IT",
        "cs.LG",
        "math.ST"
      ],
      "published": "2025-12-23T13:53:53+00:00",
      "updated": "2025-12-23T13:53:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20368v1",
      "file": "papers/2512.20368v1.pdf"
    },
    {
      "arxiv_id": "2512.20312v1",
      "title": "TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning",
      "authors": [
        {
          "name": "Saisai Yang"
        },
        {
          "name": "Qingyi Huang"
        },
        {
          "name": "Jing Yuan"
        },
        {
          "name": "Liangyu Zha"
        },
        {
          "name": "Kai Tang"
        },
        {
          "name": "Yuhang Yang"
        },
        {
          "name": "Ning Wang"
        },
        {
          "name": "Yucheng Wei"
        },
        {
          "name": "Liyao Li"
        },
        {
          "name": "Wentao Ye"
        },
        {
          "name": "Hao Chen"
        },
        {
          "name": "Tao Zhang"
        },
        {
          "name": "Junlin Zhou"
        },
        {
          "name": "Haobo Wang"
        },
        {
          "name": "Gang Chen"
        },
        {
          "name": "Junbo Zhao"
        }
      ],
      "abstract": "Tabular data serves as the backbone of modern data analysis and scientific research. While Large Language Models (LLMs) fine-tuned via Supervised Fine-Tuning (SFT) have significantly improved natural language interaction with such structured data, they often fall short in handling the complex, multi-step reasoning and robust code execution required for real-world table tasks. Reinforcement Learning (RL) offers a promising avenue to enhance these capabilities, yet its application in the tabular domain faces three critical hurdles: the scarcity of high-quality agentic trajectories with closed-loop code execution and environment feedback on diverse table structures, the extreme heterogeneity of feedback signals ranging from rigid SQL execution to open-ended data interpretation, and the risk of catastrophic forgetting of general knowledge during vertical specialization. To overcome these challenges and unlock advanced reasoning on complex tables, we introduce \\textbf{TableGPT-R1}, a specialized tabular model built on a systematic RL framework. Our approach integrates a comprehensive data engineering pipeline that synthesizes difficulty-stratified agentic trajectories for both supervised alignment and RL rollouts, a task-adaptive reward system that combines rule-based verification with a criteria-injected reward model and incorporates process-level step reward shaping with behavioral regularization, and a multi-stage training framework that progressively stabilizes reasoning before specializing in table-specific tasks. Extensive evaluations demonstrate that TableGPT-R1 achieves state-of-the-art performance on authoritative benchmarks, significantly outperforming baseline models while retaining robust general capabilities. Our model is available at https://huggingface.co/tablegpt/TableGPT-R1.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-12-23T12:30:37+00:00",
      "updated": "2025-12-23T12:30:37+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20312v1",
      "file": "papers/2512.20312v1.pdf"
    },
    {
      "arxiv_id": "2512.20220v1",
      "title": "Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning",
      "authors": [
        {
          "name": "Kausthubh Manda"
        },
        {
          "name": "Raghuram Bharadwaj Diddigi"
        }
      ],
      "abstract": "We study offline multitask reinforcement learning in settings where multiple tasks share a low-rank representation of their action-value functions. In this regime, a learner is provided with fixed datasets collected from several related tasks, without access to further online interaction, and seeks to exploit shared structure to improve statistical efficiency and generalization. We analyze a multitask variant of fitted Q-iteration that jointly learns a shared representation and task-specific value functions via Bellman error minimization on offline data. Under standard realizability and coverage assumptions commonly used in offline reinforcement learning, we establish finite-sample generalization guarantees for the learned value functions. Our analysis explicitly characterizes how pooling data across tasks improves estimation accuracy, yielding a $1/\\sqrt{nT}$ dependence on the total number of samples across tasks, while retaining the usual dependence on the horizon and concentrability coefficients arising from distribution shift. In addition, we consider a downstream offline setting in which a new task shares the same underlying representation as the upstream tasks. We study how reusing the representation learned during the multitask phase affects value estimation for this new task, and show that it can reduce the effective complexity of downstream learning relative to learning from scratch. Together, our results clarify the role of shared representations in multitask offline Q-learning and provide theoretical insight into when and how multitask structure can improve generalization in model-free, value-based reinforcement learning.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-23T10:20:11+00:00",
      "updated": "2025-12-23T10:20:11+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20220v1",
      "file": "papers/2512.20220v1.pdf"
    },
    {
      "arxiv_id": "2512.20173v1",
      "title": "Offline Safe Policy Optimization From Heterogeneous Feedback",
      "authors": [
        {
          "name": "Ze Gong"
        },
        {
          "name": "Pradeep Varakantham"
        },
        {
          "name": "Akshat Kumar"
        }
      ],
      "abstract": "Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previous works on safe RL from human feedback (RLHF) first learn reward and cost models from offline data, then use constrained RL to optimize a safe policy. While such an approach works in the contextual bandits settings (LLMs), in long horizon continuous control tasks, errors in rewards and costs accumulate, leading to impairment in performance when used with constrained RL methods. To address these challenges, (a) instead of indirectly learning policies (from rewards and costs), we introduce a framework that learns a policy directly based on pairwise preferences regarding the agent's behavior in terms of rewards, as well as binary labels indicating the safety of trajectory segments; (b) we propose \\textsc{PreSa} (Preference and Safety Alignment), a method that combines preference learning module with safety alignment in a constrained optimization problem. This optimization problem is solved within a Lagrangian paradigm that directly learns reward-maximizing safe policy \\textit{without explicitly learning reward and cost models}, avoiding the need for constrained RL; (c) we evaluate our approach on continuous control tasks with both synthetic and real human feedback. Empirically, our method successfully learns safe policies with high rewards, outperforming state-of-the-art baselines, and offline safe RL approaches with ground-truth reward and cost.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-12-23T09:07:53+00:00",
      "updated": "2025-12-23T09:07:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20173v1",
      "file": "papers/2512.20173v1.pdf"
    },
    {
      "arxiv_id": "2512.20135v2",
      "title": "MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization",
      "authors": [
        {
          "name": "Zhuo Yang"
        },
        {
          "name": "Yeyun Chen"
        },
        {
          "name": "Jiaqing Xie"
        },
        {
          "name": "Ben Gao"
        },
        {
          "name": "Shuaike Shen"
        },
        {
          "name": "Wanhao Liu"
        },
        {
          "name": "Liujia Yang"
        },
        {
          "name": "Beilun Wang"
        },
        {
          "name": "Tianfan Fu"
        },
        {
          "name": "Yuqiang Li"
        }
      ],
      "abstract": "Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that employs a two-stage training paradigm: first building editing capability, then optimizing properties while reusing the learned editing behaviors. To the best of our knowledge, this is the first work to formalize molecular design as an Agentic Reinforcement Learning problem, where an LLM agent learns to interleave reasoning, tool-use, and molecular optimization. The framework enables agents to interact in multiple turns, invoking chemical tools for validity checking, property assessment, and similarity control, and leverages their feedback to refine subsequent edits. We instantiate the MolAct framework to train two model families: MolEditAgent for molecular editing tasks and MolOptAgent for molecular optimization tasks. In molecular editing, MolEditAgent-7B delivers 100, 95, and 98 valid add, delete, and substitute edits, outperforming strong closed \"thinking\" baselines such as DeepSeek-R1; MolEditAgent-3B approaches the performance of much larger open \"thinking\" models like Qwen3-32B-think. In molecular optimization, MolOptAgent-7B (trained on MolEditAgent-7B) surpasses the best closed \"thinking\" baseline (e.g., Claude 3.7) on LogP and remains competitive on solubility, while maintaining balanced performance across other objectives. These results highlight that treating molecular design as a multi-step, tool-augmented process is key to reliable and interpretable improvements.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-12-23T07:53:57+00:00",
      "updated": "2025-12-24T02:19:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20135v2",
      "file": "papers/2512.20135v2.pdf"
    },
    {
      "arxiv_id": "2512.20115v1",
      "title": "Sample-Efficient Policy Constraint Offline Deep Reinforcement Learning based on Sample Filtering",
      "authors": [
        {
          "name": "Yuanhao Chen"
        },
        {
          "name": "Qi Liu"
        },
        {
          "name": "Pengbin Chen"
        },
        {
          "name": "Zhongjian Qiao"
        },
        {
          "name": "Yanjie Li"
        }
      ],
      "abstract": "Offline reinforcement learning (RL) aims to learn a policy that maximizes the expected return using a given static dataset of transitions. However, offline RL faces the distribution shift problem. The policy constraint offline RL method is proposed to solve the distribution shift problem. During the policy constraint offline RL training, it is important to ensure the difference between the learned policy and behavior policy within a given threshold. Thus, the learned policy heavily relies on the quality of the behavior policy. However, a problem exists in existing policy constraint methods: if the dataset contains many low-reward transitions, the learned will be contained with a suboptimal reference policy, leading to slow learning speed, low sample efficiency, and inferior performances. This paper shows that the sampling method in policy constraint offline RL that uses all the transitions in the dataset can be improved. A simple but efficient sample filtering method is proposed to improve the sample efficiency and the final performance. First, we evaluate the score of the transitions by average reward and average discounted reward of episodes in the dataset and extract the transition samples of high scores. Second, the high-score transition samples are used to train the offline RL algorithms. We verify the proposed method in a series of offline RL algorithms and benchmark tasks. Experimental results show that the proposed method outperforms baselines.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-23T07:19:19+00:00",
      "updated": "2025-12-23T07:19:19+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20115v1",
      "file": "papers/2512.20115v1.pdf"
    },
    {
      "arxiv_id": "2512.20096v1",
      "title": "Information-directed sampling for bandits: a primer",
      "authors": [
        {
          "name": "Annika Hirling"
        },
        {
          "name": "Giorgio Nicoletti"
        },
        {
          "name": "Antonio Celani"
        }
      ],
      "abstract": "The Multi-Armed Bandit problem provides a fundamental framework for analyzing the tension between exploration and exploitation in sequential learning. This paper explores Information Directed Sampling (IDS) policies, a class of heuristics that balance immediate regret against information gain. We focus on the tractable environment of two-state Bernoulli bandits as a minimal model to rigorously compare heuristic strategies against the optimal policy. We extend the IDS framework to the discounted infinite-horizon setting by introducing a modified information measure and a tuning parameter to modulate the decision-making behavior. We examine two specific problem classes: symmetric bandits and the scenario involving one fair coin. In the symmetric case we show that IDS achieves bounded cumulative regret, whereas in the one-fair-coin scenario the IDS policy yields a regret that scales logarithmically with the horizon, in agreement with classical asymptotic lower bounds. This work serves as a pedagogical synthesis, aiming to bridge concepts from reinforcement learning and information theory for an audience of statistical physicists.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.IT"
      ],
      "published": "2025-12-23T06:49:33+00:00",
      "updated": "2025-12-23T06:49:33+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20096v1",
      "file": "papers/2512.20096v1.pdf"
    },
    {
      "arxiv_id": "2512.20061v1",
      "title": "Scaling Reinforcement Learning for Content Moderation with Large Language Models",
      "authors": [
        {
          "name": "Hamed Firooz"
        },
        {
          "name": "Rui Liu"
        },
        {
          "name": "Yuchen Lu"
        },
        {
          "name": "Zhenyu Hou"
        },
        {
          "name": "Fangzhou Xiong"
        },
        {
          "name": "Xiaoyang Zhang"
        },
        {
          "name": "Changshu Jian"
        },
        {
          "name": "Zhicheng Zhu"
        },
        {
          "name": "Jiayuan Ma"
        },
        {
          "name": "Jacob Tao"
        },
        {
          "name": "Chaitali Gupta"
        },
        {
          "name": "Xiaochang Peng"
        },
        {
          "name": "Shike Mei"
        },
        {
          "name": "Hang Cui"
        },
        {
          "name": "Yang Qin"
        },
        {
          "name": "Shuo Tang"
        },
        {
          "name": "Jason Gaedtke"
        },
        {
          "name": "Arpit Mittal"
        }
      ],
      "abstract": "Content moderation at scale remains one of the most pressing challenges in today's digital ecosystem, where billions of user- and AI-generated artifacts must be continuously evaluated for policy violations. Although recent advances in large language models (LLMs) have demonstrated strong potential for policy-grounded moderation, the practical challenges of training these systems to achieve expert-level accuracy in real-world settings remain largely unexplored, particularly in regimes characterized by label sparsity, evolving policy definitions, and the need for nuanced reasoning beyond shallow pattern matching. In this work, we present a comprehensive empirical investigation of scaling reinforcement learning (RL) for content classification, systematically evaluating multiple RL training recipes and reward-shaping strategies-including verifiable rewards and LLM-as-judge frameworks-to transform general-purpose language models into specialized, policy-aligned classifiers across three real-world content moderation tasks. Our findings provide actionable insights for industrial-scale moderation systems, demonstrating that RL exhibits sigmoid-like scaling behavior in which performance improves smoothly with increased training data, rollouts, and optimization steps before gradually saturating. Moreover, we show that RL substantially improves performance on tasks requiring complex policy-grounded reasoning while achieving up to 100x higher data efficiency than supervised fine-tuning, making it particularly effective in domains where expert annotations are scarce or costly.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-12-23T05:27:16+00:00",
      "updated": "2025-12-23T05:27:16+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20061v1",
      "file": "papers/2512.20061v1.pdf"
    },
    {
      "arxiv_id": "2512.20053v1",
      "title": "An Optimal Policy for Learning Controllable Dynamics by Exploration",
      "authors": [
        {
          "name": "Peter N. Loxley"
        }
      ],
      "abstract": "Controllable Markov chains describe the dynamics of sequential decision making tasks and are the central component in optimal control and reinforcement learning. In this work, we give the general form of an optimal policy for learning controllable dynamics in an unknown environment by exploring over a limited time horizon. This policy is simple to implement and efficient to compute, and allows an agent to ``learn by exploring\" as it maximizes its information gain in a greedy fashion by selecting controls from a constraint set that changes over time during exploration. We give a simple parameterization for the set of controls, and present an algorithm for finding an optimal policy. The reason for this policy is due to the existence of certain types of states that restrict control of the dynamics; such as transient states, absorbing states, and non-backtracking states. We show why the occurrence of these states makes a non-stationary policy essential for achieving optimal exploration. Six interesting examples of controllable dynamics are treated in detail. Policy optimality is demonstrated using counting arguments, comparing with suboptimal policies, and by making use of a sequential improvement property from dynamic programming.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "published": "2025-12-23T05:03:54+00:00",
      "updated": "2025-12-23T05:03:54+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20053v1",
      "file": "papers/2512.20053v1.pdf"
    },
    {
      "arxiv_id": "2512.19920v1",
      "title": "Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning",
      "authors": [
        {
          "name": "Jiayun Wu"
        },
        {
          "name": "Jiashuo Liu"
        },
        {
          "name": "Zhiyuan Zeng"
        },
        {
          "name": "Tianyang Zhan"
        },
        {
          "name": "Wenhao Huang"
        }
      ],
      "abstract": "LLM deployment in critical domains is currently impeded by persistent hallucinations--generating plausible but factually incorrect assertions. While scaling laws drove significant improvements in general capabilities, theoretical frameworks suggest hallucination is not merely stochastic error but a predictable statistical consequence of training objectives prioritizing mimicking data distribution over epistemic honesty. Standard RLVR paradigms, utilizing binary reward signals, inadvertently incentivize models as good test-takers rather than honest communicators, encouraging guessing whenever correctness probability exceeds zero. This paper presents an exhaustive investigation into behavioral calibration, which incentivizes models to stochastically admit uncertainty by abstaining when not confident, aligning model behavior with accuracy. Synthesizing recent advances, we propose and evaluate training interventions optimizing strictly proper scoring rules for models to output a calibrated probability of correctness. Our methods enable models to either abstain from producing a complete response or flag individual claims where uncertainty remains. Utilizing Qwen3-4B-Instruct, empirical analysis reveals behavior-calibrated reinforcement learning allows smaller models to surpass frontier models in uncertainty quantification--a transferable meta-skill decouplable from raw predictive accuracy. Trained on math reasoning tasks, our model's log-scale Accuracy-to-Hallucination Ratio gain (0.806) exceeds GPT-5's (0.207) in a challenging in-domain evaluation (BeyondAIME). Moreover, in cross-domain factual QA (SimpleQA), our 4B LLM achieves zero-shot calibration error on par with frontier models including Grok-4 and Gemini-2.5-Pro, even though its factual accuracy is much lower.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-12-22T22:51:48+00:00",
      "updated": "2025-12-22T22:51:48+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19920v1",
      "file": "papers/2512.19920v1.pdf"
    },
    {
      "arxiv_id": "2512.19673v1",
      "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
      "authors": [
        {
          "name": "Yuqiao Tan"
        },
        {
          "name": "Minzheng Wang"
        },
        {
          "name": "Shizhu He"
        },
        {
          "name": "Huanxuan Liao"
        },
        {
          "name": "Chengfeng Zhao"
        },
        {
          "name": "Qiunan Lu"
        },
        {
          "name": "Tian Liang"
        },
        {
          "name": "Jun Zhao"
        },
        {
          "name": "Kang Liu"
        }
      ],
      "abstract": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-12-22T18:51:48+00:00",
      "updated": "2025-12-22T18:51:48+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19673v1",
      "file": "papers/2512.19673v1.pdf"
    },
    {
      "arxiv_id": "2512.19576v2",
      "title": "LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller",
      "authors": [
        {
          "name": "Kirill Djebko"
        },
        {
          "name": "Tom Baumann"
        },
        {
          "name": "Erik Dilger"
        },
        {
          "name": "Frank Puppe"
        },
        {
          "name": "Sergio Montenegro"
        }
      ],
      "abstract": "Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-Universität Würzburg in cooperation with the Technische Universität Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "published": "2025-12-22T17:00:25+00:00",
      "updated": "2025-12-23T09:09:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19576v2",
      "file": "papers/2512.19576v2.pdf"
    },
    {
      "arxiv_id": "2512.19554v1",
      "title": "CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal",
      "authors": [
        {
          "name": "Yongxin Wang"
        },
        {
          "name": "Zhicheng Yang"
        },
        {
          "name": "Meng Cao"
        },
        {
          "name": "Mingfei Han"
        },
        {
          "name": "Haokun Lin"
        },
        {
          "name": "Yingying Zhu"
        },
        {
          "name": "Xiaojun Chang"
        },
        {
          "name": "Xiaodan Liang"
        }
      ],
      "abstract": "Group-relative reinforcement learning with verifiable rewards (RLVR) often wastes the most informative data it already has the failures. When all rollouts are wrong, gradients stall; when one happens to be correct, the update usually ignores why the others are close-but-wrong, and credit can be misassigned to spurious chains. We present CARE (Contrastive Anchored REflection), a failure-centric post-training framework for multimodal reasoning that turns errors into supervision. CARE combines: (i) an anchored-contrastive objective that forms a compact subgroup around the best rollout and a set of semantically proximate hard negatives, performs within-subgroup z-score normalization with negative-only scaling, and includes an all-negative rescue to prevent zero-signal batches; and (ii) Reflection-Guided Resampling (RGR), a one-shot structured self-repair that rewrites a representative failure and re-scores it with the same verifier, converting near-misses into usable positives without any test-time reflection. CARE improves accuracy and training smoothness while explicitly increasing the share of learning signal that comes from failures. On Qwen2.5-VL-7B, CARE lifts macro-averaged accuracy by 4.6 points over GRPO across six verifiable visual-reasoning benchmarks; with Qwen3-VL-8B it reaches competitive or state-of-the-art results on MathVista and MMMU-Pro under an identical evaluation protocol.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-12-22T16:34:21+00:00",
      "updated": "2025-12-22T16:34:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19554v1",
      "file": "papers/2512.19554v1.pdf"
    },
    {
      "arxiv_id": "2512.19516v1",
      "title": "LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning",
      "authors": [
        {
          "name": "Xueming Yan"
        },
        {
          "name": "Bo Yin"
        },
        {
          "name": "Yaochu Jin"
        }
      ],
      "abstract": "Multiobjective reinforcement learning (MORL) poses significant challenges due to the inherent conflicts between objectives and the difficulty of adapting to dynamic environments. Traditional methods often struggle to generalize effectively, particularly in large and complex state-action spaces. To address these limitations, we introduce the Latent Causal Diffusion Model (LacaDM), a novel approach designed to enhance the adaptability of MORL in discrete and continuous environments. Unlike existing methods that primarily address conflicts between objectives, LacaDM learns latent temporal causal relationships between environmental states and policies, enabling efficient knowledge transfer across diverse MORL scenarios. By embedding these causal structures within a diffusion model-based framework, LacaDM achieves a balance between conflicting objectives while maintaining strong generalization capabilities in previously unseen environments. Empirical evaluations on various tasks from the MOGymnasium framework demonstrate that LacaDM consistently outperforms the state-of-art baselines in terms of hypervolume, sparsity, and expected utility maximization, showcasing its effectiveness in complex multiobjective tasks.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-12-22T16:08:03+00:00",
      "updated": "2025-12-22T16:08:03+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19516v1",
      "file": "papers/2512.19516v1.pdf"
    },
    {
      "arxiv_id": "2512.19366v1",
      "title": "Learning General Policies with Policy Gradient Methods",
      "authors": [
        {
          "name": "Simon Ståhlberg"
        },
        {
          "name": "Blai Bonet"
        },
        {
          "name": "Hector Geffner"
        }
      ],
      "abstract": "While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-22T13:08:58+00:00",
      "updated": "2025-12-22T13:08:58+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19366v1",
      "file": "papers/2512.19366v1.pdf"
    },
    {
      "arxiv_id": "2512.19361v1",
      "title": "Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation",
      "authors": [
        {
          "name": "Isshaan Singh"
        },
        {
          "name": "Divyansh Chawla"
        },
        {
          "name": "Anshu Garg"
        },
        {
          "name": "Shivin Mangal"
        },
        {
          "name": "Pallavi Gupta"
        },
        {
          "name": "Khushi Agarwal"
        },
        {
          "name": "Nimrat Singh Khalsa"
        },
        {
          "name": "Nandan Patel"
        }
      ],
      "abstract": "The need for an intelligent, real-time spoilage prediction system has become critical in modern IoT-driven food supply chains, where perishable goods are highly susceptible to environmental conditions. Existing methods often lack adaptability to dynamic conditions and fail to optimize decision making in real time. To address these challenges, we propose a hybrid reinforcement learning framework integrating Long Short-Term Memory (LSTM) and Recurrent Neural Networks (RNN) for enhanced spoilage prediction. This hybrid architecture captures temporal dependencies within sensor data, enabling robust and adaptive decision making. In alignment with interpretable artificial intelligence principles, a rule-based classifier environment is employed to provide transparent ground truth labeling of spoilage levels based on domain-specific thresholds. This structured design allows the agent to operate within clearly defined semantic boundaries, supporting traceable and interpretable decisions. Model behavior is monitored using interpretability-driven metrics, including spoilage accuracy, reward-to-step ratio, loss reduction rate, and exploration decay. These metrics provide both quantitative performance evaluation and insights into learning dynamics. A class-wise spoilage distribution visualization is used to analyze the agents decision profile and policy behavior. Extensive evaluations on simulated and real-time hardware data demonstrate that the LSTM and RNN based agent outperforms alternative reinforcement learning approaches in prediction accuracy and decision efficiency while maintaining interpretability. The results highlight the potential of hybrid deep reinforcement learning with integrated interpretability for scalable IoT-based food monitoring systems.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-22T12:59:48+00:00",
      "updated": "2025-12-22T12:59:48+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19361v1",
      "file": "papers/2512.19361v1.pdf"
    },
    {
      "arxiv_id": "2512.19355v1",
      "title": "First-Order Representation Languages for Goal-Conditioned RL",
      "authors": [
        {
          "name": "Simon Ståhlberg"
        },
        {
          "name": "Hector Geffner"
        }
      ],
      "abstract": "First-order relational languages have been used in MDP planning and reinforcement learning (RL) for two main purposes: specifying MDPs in compact form, and representing and learning policies that are general and not tied to specific instances or state spaces. In this work, we instead consider the use of first-order languages in goal-conditioned RL and generalized planning. The question is how to learn goal-conditioned and general policies when the training instances are large and the goal cannot be reached by random exploration alone. The technique of Hindsight Experience Replay (HER) provides an answer to this question: it relabels unsuccessful trajectories as successful ones by replacing the original goal with one that was actually achieved. If the target policy must generalize across states and goals, trajectories that do not reach the original goal states can enable more data- and time-efficient learning. In this work, we show that further performance gains can be achieved when states and goals are represented by sets of atoms. We consider three versions: goals as full states, goals as subsets of the original goals, and goals as lifted versions of these subgoals. The result is that the latter two successfully learn general policies on large planning instances with sparse rewards by automatically creating a curriculum of easier goals of increasing complexity. The experiments illustrate the computational gains of these versions, their limitations, and opportunities for addressing them.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-12-22T12:54:32+00:00",
      "updated": "2025-12-22T12:54:32+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19355v1",
      "file": "papers/2512.19355v1.pdf"
    },
    {
      "arxiv_id": "2512.19317v1",
      "title": "SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models",
      "authors": [
        {
          "name": "A. A. Gde Yogi Pramana"
        },
        {
          "name": "Jason Ray"
        },
        {
          "name": "Anthony Jaya"
        },
        {
          "name": "Michael Wijaya"
        }
      ],
      "abstract": "Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medical reasoning. SafeMed-R1 employs a two-stage approach: at training time, we integrate Adversarial Training with Group Relative Policy Optimization (AT-GRPO) to explicitly robustify the reasoning process against worst-case perturbations; at inference time, we augment the model with Randomized Smoothing to provide certified $L_2$-norm robustness guarantees. We evaluate SafeMed-R1 on the OmniMedVQA benchmark across eight medical imaging modalities comprising over 88,000 samples. Our experiments reveal that standard fine-tuned VLMs, despite achieving 95\\% accuracy on clean inputs, collapse to approximately 25\\% under PGD attacks. In contrast, SafeMed-R1 maintains 84.45\\% accuracy under the same adversarial conditions, representing a 59 percentage point improvement in robustness. Furthermore, we demonstrate that models trained with explicit chain-of-thought reasoning exhibit superior adversarial robustness compared to instruction-only variants, suggesting a synergy between interpretability and security in medical AI systems.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-12-22T12:07:33+00:00",
      "updated": "2025-12-22T12:07:33+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19317v1",
      "file": "papers/2512.19317v1.pdf"
    },
    {
      "arxiv_id": "2512.19154v1",
      "title": "Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments",
      "authors": [
        {
          "name": "Geraud Nangue Tasse"
        },
        {
          "name": "Matthew Riemer"
        },
        {
          "name": "Benjamin Rosman"
        },
        {
          "name": "Tim Klinger"
        }
      ],
      "abstract": "Recent success in developing increasingly general purpose agents based on sequence models has led to increased focus on the problem of deploying computationally limited agents within the vastly more complex real-world. A key challenge experienced in these more realistic domains is highly non-Markovian dependencies with respect to the agent's observations, which are less common in small controlled domains. The predominant approach for dealing with this in the literature is to stack together a window of the most recent observations (Frame Stacking), but this window size must grow with the degree of non-Markovian dependencies, which results in prohibitive computational and memory requirements for both action inference and learning. In this paper, we are motivated by the insight that in many environments that are highly non-Markovian with respect to time, the environment only causally depends on a relatively small number of observations over that time-scale. A natural direction would then be to consider meta-algorithms that maintain relatively small adaptive stacks of memories such that it is possible to express highly non-Markovian dependencies with respect to time while considering fewer observations at each step and thus experience substantial savings in both compute and memory requirements. Hence, we propose a meta-algorithm (Adaptive Stacking) for achieving exactly that with convergence guarantees and quantify the reduced computation and memory constraints for MLP, LSTM, and Transformer-based agents. Our experiments utilize popular memory tasks, which give us control over the degree of non-Markovian dependencies. This allows us to demonstrate that an appropriate meta-algorithm can learn the removal of memories not predictive of future rewards without excessive removal of important experiences. Code: https://github.com/geraudnt/adaptive-stacking",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-12-22T08:50:30+00:00",
      "updated": "2025-12-22T08:50:30+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19154v1",
      "file": "papers/2512.19154v1.pdf"
    },
    {
      "arxiv_id": "2512.19057v1",
      "title": "Efficient Personalization of Generative Models via Optimal Experimental Design",
      "authors": [
        {
          "name": "Guy Schacht"
        },
        {
          "name": "Ziyad Sheebaelhamd"
        },
        {
          "name": "Riccardo De Santi"
        },
        {
          "name": "Mojmír Mutný"
        },
        {
          "name": "Andreas Krause"
        }
      ],
      "abstract": "Preference learning from human feedback has the ability to align generative models with the needs of end-users. Human feedback is costly and time-consuming to obtain, which creates demand for data-efficient query selection methods. This work presents a novel approach that leverages optimal experimental design to ask humans the most informative preference queries, from which we can elucidate the latent reward function modeling user preferences efficiently. We formulate the problem of preference query selection as the one that maximizes the information about the underlying latent preference model. We show that this problem has a convex optimization formulation, and introduce a statistically and computationally efficient algorithm ED-PBRL that is supported by theoretical guarantees and can efficiently construct structured queries such as images or text. We empirically present the proposed framework by personalizing a text-to-image generative model to user-specific styles, showing that it requires less preference queries compared to random query selection.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.IT"
      ],
      "published": "2025-12-22T05:47:25+00:00",
      "updated": "2025-12-22T05:47:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19057v1",
      "file": "papers/2512.19057v1.pdf"
    },
    {
      "arxiv_id": "2512.18957v1",
      "title": "Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation",
      "authors": [
        {
          "name": "Debamita Ghosh"
        },
        {
          "name": "George K. Atia"
        },
        {
          "name": "Yue Wang"
        }
      ],
      "abstract": "The deployment of reinforcement learning (RL) agents in real-world applications is often hindered by performance degradation caused by mismatches between training and deployment environments. Distributionally robust RL (DR-RL) addresses this issue by optimizing worst-case performance over an uncertainty set of transition dynamics. However, existing work typically relies on substantial prior knowledge-such as access to a generative model or a large offline dataset-and largely focuses on tabular methods that do not scale to complex domains. We overcome these limitations by proposing an online DR-RL algorithm with general function approximation that learns an optimal robust policy purely through interaction with the environment, without requiring prior models or offline data, enabling deployment in high-dimensional tasks. We further provide a theoretical analysis establishing a near-optimal sublinear regret bound under a total variation uncertainty set, demonstrating the sample efficiency and effectiveness of our method.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-22T02:12:04+00:00",
      "updated": "2025-12-22T02:12:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18957v1",
      "file": "papers/2512.18957v1.pdf"
    },
    {
      "arxiv_id": "2512.18892v1",
      "title": "Structural Reinforcement Learning for Heterogeneous Agent Macroeconomics",
      "authors": [
        {
          "name": "Yucheng Yang"
        },
        {
          "name": "Chiyuan Wang"
        },
        {
          "name": "Andreas Schaab"
        },
        {
          "name": "Benjamin Moll"
        }
      ],
      "abstract": "We present a new approach to formulating and solving heterogeneous agent models with aggregate risk. We replace the cross-sectional distribution with low-dimensional prices as state variables and let agents learn equilibrium price dynamics directly from simulated paths. To do so, we introduce a structural reinforcement learning (SRL) method which treats prices via simulation while exploiting agents' structural knowledge of their own individual dynamics. Our SRL method yields a general and highly efficient global solution method for heterogeneous agent models that sidesteps the Master equation and handles problems traditional methods struggle with, in particular nontrivial market-clearing conditions. We illustrate the approach in the Krusell-Smith model, the Huggett model with aggregate shocks, and a HANK model with a forward-looking Phillips curve, all of which we solve globally within minutes.",
      "primary_category": "econ.TH",
      "categories": [
        "econ.TH",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-21T21:22:12+00:00",
      "updated": "2025-12-21T21:22:12+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18892v1",
      "file": "papers/2512.18892v1.pdf"
    },
    {
      "arxiv_id": "2512.18857v1",
      "title": "CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning",
      "authors": [
        {
          "name": "Zijun Gao"
        },
        {
          "name": "Zhikun Xu"
        },
        {
          "name": "Xiao Ye"
        },
        {
          "name": "Ben Zhou"
        }
      ],
      "abstract": "Large language models (LLMs) often solve challenging math exercises yet fail to apply the concept right when the problem requires genuine understanding. Popular Reinforcement Learning with Verifiable Rewards (RLVR) pipelines reinforce final answers but provide little fine-grained conceptual signal, so models improve at pattern reuse rather than conceptual applications. We introduce CORE (Concept-Oriented REinforcement), an RL training framework that turns explicit concepts into a controllable supervision signal. Starting from a high-quality, low-contamination textbook resource that links verifiable exercises to concise concept descriptions, we run a sanity probe showing LLMs can restate definitions but fail concept-linked quizzes, quantifying the conceptual reasoning gap. CORE then (i) synthesizes concept-aligned quizzes, (ii) injects brief concept snippets during rollouts to elicit concept-primed trajectories, and (iii) reinforces conceptual reasoning via trajectory replacement after group failures, a lightweight forward-KL constraint that aligns unguided with concept-primed policies, or standard GRPO directly on concept-aligned quizzes. Across several models, CORE delivers consistent gains over vanilla and SFT baselines on both in-domain concept-exercise suites and diverse out-of-domain math benchmarks. CORE unifies direct training on concept-aligned quizzes and concept-injected rollouts under outcome regularization. It provides fine-grained conceptual supervision that bridges problem-solving competence and genuine conceptual reasoning, while remaining algorithm- and verifier-agnostic.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-21T19:01:35+00:00",
      "updated": "2025-12-21T19:01:35+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18857v1",
      "file": "papers/2512.18857v1.pdf"
    },
    {
      "arxiv_id": "2512.18763v1",
      "title": "Gaussian-Mixture-Model Q-Functions for Policy Iteration in Reinforcement Learning",
      "authors": [
        {
          "name": "Minh Vu"
        },
        {
          "name": "Konstantinos Slavakis"
        }
      ],
      "abstract": "Unlike their conventional use as estimators of probability density functions in reinforcement learning (RL), this paper introduces a novel function-approximation role for Gaussian mixture models (GMMs) as direct surrogates for Q-function losses. These parametric models, termed GMM-QFs, possess substantial representational capacity, as they are shown to be universal approximators over a broad class of functions. They are further embedded within Bellman residuals, where their learnable parameters -- a fixed number of mixing weights, together with Gaussian mean vectors and covariance matrices -- are inferred from data via optimization on a Riemannian manifold. This geometric perspective on the parameter space naturally incorporates Riemannian optimization into the policy-evaluation step of standard policy-iteration frameworks. Rigorous theoretical results are established, and supporting numerical tests show that, even without access to experience data, GMM-QFs deliver competitive performance and, in some cases, outperform state-of-the-art approaches across a range of benchmark RL tasks, all while maintaining a significantly smaller computational footprint than deep-learning methods that rely on experience data.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-21T15:00:32+00:00",
      "updated": "2025-12-21T15:00:32+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18763v1",
      "file": "papers/2512.18763v1.pdf"
    },
    {
      "arxiv_id": "2512.18730v1",
      "title": "A Theoretical Lens for RL-Tuned Language Models via Energy-Based Models",
      "authors": [
        {
          "name": "Zhiquan Tan"
        },
        {
          "name": "Yinrong Hong"
        }
      ],
      "abstract": "Large language models (LLMs) trained via KL-regularized reinforcement learning demonstrate strong instruction following, self-correction, and reasoning abilities. Yet their theoretical underpinnings remain limited. We exploit the closed-form energy-based model (EBM) structure of the optimal KL-regularized policy to provide a unified variational analysis of LLMs.\n  For instruction-tuned models, under natural assumptions on reward potentials and pretraining symmetry, we prove that the transition kernel satisfies detailed balance with respect to a scalar potential encoding response quality. This yields monotonic KL convergence to a high-quality stationary distribution, bounded hitting times to superior states, and exponential mixing governed by the spectral gap.\n  For reasoning models trained with verifiable rewards (RLVR), we show the objective is equivalent to expected KL minimization toward an optimal reasoning distribution, with the suboptimality gap reducing to the Bernoulli KL between target and current accuracies along the natural gradient flow. This helps explain empirical entropy-accuracy trade-offs.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-21T13:28:58+00:00",
      "updated": "2025-12-21T13:28:58+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18730v1",
      "file": "papers/2512.18730v1.pdf"
    },
    {
      "arxiv_id": "2512.19767v1",
      "title": "Learning to Design City-scale Transit Routes",
      "authors": [
        {
          "name": "Bibek Poudel"
        },
        {
          "name": "Weizi Li"
        }
      ],
      "abstract": "Designing efficient transit route networks is an NP-hard problem with exponentially large solution spaces that traditionally relies on manual planning processes. We present an end-to-end reinforcement learning (RL) framework based on graph attention networks for sequential transit network construction. To address the long-horizon credit assignment challenge, we introduce a two-level reward structure combining incremental topological feedback with simulation-based terminal rewards. We evaluate our approach on a new real-world dataset from Bloomington, Indiana with topologically accurate road networks, census-derived demand, and existing transit routes. Our learned policies substantially outperform existing designs and traditional heuristics across two initialization schemes and two modal-split scenarios. Under high transit adoption with transit center initialization, our approach achieves 25.6% higher service rates, 30.9\\% shorter wait times, and 21.0% better bus utilization compared to the real-world network. Under mixed-mode conditions with random initialization, it delivers 68.8% higher route efficiency than demand coverage heuristics and 5.9% lower travel times than shortest path construction. These results demonstrate that end-to-end RL can design transit networks that substantially outperform both human-designed systems and hand-crafted heuristics on realistic city-scale benchmarks.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.MA"
      ],
      "published": "2025-12-21T12:48:53+00:00",
      "updated": "2025-12-21T12:48:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19767v1",
      "file": "papers/2512.19767v1.pdf"
    },
    {
      "arxiv_id": "2512.18670v1",
      "title": "Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments",
      "authors": [
        {
          "name": "Xue Yang"
        },
        {
          "name": "Michael Schukat"
        },
        {
          "name": "Junlin Lu"
        },
        {
          "name": "Patrick Mannion"
        },
        {
          "name": "Karl Mason"
        },
        {
          "name": "Enda Howley"
        }
      ],
      "abstract": "Reinforcement learning (RL) excels in various applications but struggles in dynamic environments where the underlying Markov decision process evolves. Continual reinforcement learning (CRL) enables RL agents to continually learn and adapt to new tasks, but balancing stability (preserving prior knowledge) and plasticity (acquiring new knowledge) remains challenging. Existing methods primarily address the stability-plasticity dilemma through mechanisms where past knowledge influences optimization but rarely affects the agent's behavior directly, which may hinder effective knowledge reuse and efficient learning. In contrast, we propose demonstration-guided continual reinforcement learning (DGCRL), which stores prior knowledge in an external, self-evolving demonstration repository that directly guides RL exploration and adaptation. For each task, the agent dynamically selects the most relevant demonstration and follows a curriculum-based strategy to accelerate learning, gradually shifting from demonstration-guided exploration to fully self-exploration. Extensive experiments on 2D navigation and MuJoCo locomotion tasks demonstrate its superior average performance, enhanced knowledge transfer, mitigation of forgetting, and training efficiency. The additional sensitivity analysis and ablation study further validate its effectiveness.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-21T10:13:21+00:00",
      "updated": "2025-12-21T10:13:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18670v1",
      "file": "papers/2512.18670v1.pdf"
    },
    {
      "arxiv_id": "2512.18623v1",
      "title": "LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction",
      "authors": [
        {
          "name": "Jensen Zhang"
        },
        {
          "name": "Ningyuan Liu"
        },
        {
          "name": "Yijia Fan"
        },
        {
          "name": "Zihao Huang"
        },
        {
          "name": "Qinglin Zeng"
        },
        {
          "name": "Kaitong Cai"
        },
        {
          "name": "Jian Wang"
        },
        {
          "name": "Keze Wang"
        }
      ],
      "abstract": "Large language models (LLMs) often generate hallucinated content that lacks factual or contextual grounding, limiting their reliability in critical applications. Existing approaches such as supervised fine-tuning and reinforcement learning from human feedback are data intensive and computationally expensive, while static parameter editing methods struggle with context dependent errors and catastrophic forgetting.\n  We propose LLM-CAS, a framework that formulates real-time hallucination correction as a hierarchical reinforcement learning problem. LLM-CAS trains an agent to learn a policy that dynamically selects temporary neuron perturbations during inference based on the current context. Unlike prior dynamic approaches that rely on heuristic or predefined adjustments, this policy driven mechanism enables adaptive and fine grained correction without permanent parameter modification.\n  Experiments across multiple language models demonstrate that LLM-CAS consistently improves factual accuracy, achieving gains of 10.98 percentage points on StoryCloze, 2.71 points on TriviaQA, and 2.06 points on the MC1 score of TruthfulQA. These results outperform both static editing methods such as ITI and CAA and the dynamic SADI framework. Overall, LLM-CAS provides an efficient and context aware solution for improving the reliability of LLMs, with promising potential for future multimodal extensions.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-21T06:54:34+00:00",
      "updated": "2025-12-21T06:54:34+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18623v1",
      "file": "papers/2512.18623v1.pdf"
    },
    {
      "arxiv_id": "2512.18604v1",
      "title": "Trajectory Planning for UAV-Based Smart Farming Using Imitation-Based Triple Deep Q-Learning",
      "authors": [
        {
          "name": "Wencan Mao"
        },
        {
          "name": "Quanxi Zhou"
        },
        {
          "name": "Tomas Couso Coddou"
        },
        {
          "name": "Manabu Tsukada"
        },
        {
          "name": "Yunling Liu"
        },
        {
          "name": "Yusheng Ji"
        }
      ],
      "abstract": "Unmanned aerial vehicles (UAVs) have emerged as a promising auxiliary platform for smart agriculture, capable of simultaneously performing weed detection, recognition, and data collection from wireless sensors. However, trajectory planning for UAV-based smart agriculture is challenging due to the high uncertainty of the environment, partial observations, and limited battery capacity of UAVs. To address these issues, we formulate the trajectory planning problem as a Markov decision process (MDP) and leverage multi-agent reinforcement learning (MARL) to solve it. Furthermore, we propose a novel imitation-based triple deep Q-network (ITDQN) algorithm, which employs an elite imitation mechanism to reduce exploration costs and utilizes a mediator Q-network over a double deep Q-network (DDQN) to accelerate and stabilize training and improve performance. Experimental results in both simulated and real-world environments demonstrate the effectiveness of our solution. Moreover, our proposed ITDQN outperforms DDQN by 4.43\\% in weed recognition rate and 6.94\\% in data collection rate.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-21T05:30:19+00:00",
      "updated": "2025-12-21T05:30:19+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18604v1",
      "file": "papers/2512.18604v1.pdf"
    },
    {
      "arxiv_id": "2512.18596v1",
      "title": "EIA-SEC: Improved Actor-Critic Framework for Multi-UAV Collaborative Control in Smart Agriculture",
      "authors": [
        {
          "name": "Quanxi Zhou"
        },
        {
          "name": "Wencan Mao"
        },
        {
          "name": "Yilei Liang"
        },
        {
          "name": "Manabu Tsukada"
        },
        {
          "name": "Yunling Liu"
        },
        {
          "name": "Jon Crowcroft"
        }
      ],
      "abstract": "The widespread application of wireless communication technology has promoted the development of smart agriculture, where unmanned aerial vehicles (UAVs) play a multifunctional role. We target a multi-UAV smart agriculture system where UAVs cooperatively perform data collection, image acquisition, and communication tasks. In this context, we model a Markov decision process to solve the multi-UAV trajectory planning problem. Moreover, we propose a novel Elite Imitation Actor-Shared Ensemble Critic (EIA-SEC) framework, where agents adaptively learn from the elite agent to reduce trial-and-error costs, and a shared ensemble critic collaborates with each agent's local critic to ensure unbiased objective value estimates and prevent overestimation. Experimental results demonstrate that EIA-SEC outperforms state-of-the-art baselines in terms of reward performance, training stability, and convergence speed.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-21T05:05:45+00:00",
      "updated": "2025-12-21T05:05:45+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18596v1",
      "file": "papers/2512.18596v1.pdf"
    },
    {
      "arxiv_id": "2512.18583v1",
      "title": "SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models",
      "authors": [
        {
          "name": "Pengcheng Li"
        },
        {
          "name": "Qiang Fang"
        },
        {
          "name": "Tong Zhao"
        },
        {
          "name": "Yixing Lan"
        },
        {
          "name": "Xin Xu"
        }
      ],
      "abstract": "Adversarial Imitation Learning (AIL) is a dominant framework in imitation learning that infers rewards from expert demonstrations to guide policy optimization. Although providing more expert demonstrations typically leads to improved performance and greater stability, collecting such demonstrations can be challenging in certain scenarios. Inspired by the success of diffusion models in data generation, we propose SD2AIL, which utilizes synthetic demonstrations via diffusion models. We first employ a diffusion model in the discriminator to generate synthetic demonstrations as pseudo-expert data that augment the expert demonstrations. To selectively replay the most valuable demonstrations from the large pool of (pseudo-) expert demonstrations, we further introduce a prioritized expert demonstration replay strategy (PEDR). The experimental results on simulation tasks demonstrate the effectiveness and robustness of our method. In particular, in the Hopper task, our method achieves an average return of 3441, surpassing the state-of-the-art method by 89. Our code will be available at https://github.com/positron-lpc/SD2AIL.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "published": "2025-12-21T04:00:38+00:00",
      "updated": "2025-12-21T04:00:38+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18583v1",
      "file": "papers/2512.18583v1.pdf"
    },
    {
      "arxiv_id": "2512.18571v1",
      "title": "ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning",
      "authors": [
        {
          "name": "Weijie Zhou"
        },
        {
          "name": "Xuangtang Xiong"
        },
        {
          "name": "Ye Tian"
        },
        {
          "name": "Lijun Yue"
        },
        {
          "name": "Xinyu Wu"
        },
        {
          "name": "Wei Li"
        },
        {
          "name": "Chaoyang Zhao"
        },
        {
          "name": "Honghui Dong"
        },
        {
          "name": "Ming Tang"
        },
        {
          "name": "Jinqiao Wang"
        },
        {
          "name": "Zhengyou Zhang"
        }
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have empowered embodied agents with remarkable capabilities in planning and reasoning. However, when facing ambiguous natural language instructions (e.g., \"fetch the tool\" in a cluttered room), current agents often fail to balance the high cost of physical exploration against the cognitive cost of human interaction. They typically treat disambiguation as a passive perception problem, lacking the strategic reasoning to minimize total task execution costs. To bridge this gap, we propose ESearch-R1, a cost-aware embodied reasoning framework that unifies interactive dialogue (Ask), episodic memory retrieval (GetMemory), and physical navigation (Navigate) into a single decision process. We introduce HC-GRPO (Heterogeneous Cost-Aware Group Relative Policy Optimization). Unlike traditional PPO which relies on a separate value critic, HC-GRPO optimizes the MLLM by sampling groups of reasoning trajectories and reinforcing those that achieve the optimal trade-off between information gain and heterogeneous costs (e.g., navigate time, and human attention). Extensive experiments in AI2-THOR demonstrate that ESearch-R1 significantly outperforms standard ReAct-based agents. It improves task success rates while reducing total operational costs by approximately 50\\%, validating the effectiveness of GRPO in aligning MLLM agents with physical world constraints.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-12-21T02:45:08+00:00",
      "updated": "2025-12-21T02:45:08+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18571v1",
      "file": "papers/2512.18571v1.pdf"
    },
    {
      "arxiv_id": "2512.18552v1",
      "title": "Toward Training Superintelligent Software Agents through Self-Play SWE-RL",
      "authors": [
        {
          "name": "Yuxiang Wei"
        },
        {
          "name": "Zhiqing Sun"
        },
        {
          "name": "Emily McMilin"
        },
        {
          "name": "Jonas Gehring"
        },
        {
          "name": "David Zhang"
        },
        {
          "name": "Gabriel Synnaeve"
        },
        {
          "name": "Daniel Fried"
        },
        {
          "name": "Lingming Zhang"
        },
        {
          "name": "Sida Wang"
        }
      ],
      "abstract": "While current software agents powered by large language models (LLMs) and agentic reinforcement learning (RL) can boost programmer productivity, their training data (e.g., GitHub issues and pull requests) and environments (e.g., pass-to-pass and fail-to-pass tests) heavily depend on human knowledge or curation, posing a fundamental barrier to superintelligence. In this paper, we present Self-play SWE-RL (SSR), a first step toward training paradigms for superintelligent software agents. Our approach takes minimal data assumptions, only requiring access to sandboxed repositories with source code and installed dependencies, with no need for human-labeled issues or tests. Grounded in these real-world codebases, a single LLM agent is trained via reinforcement learning in a self-play setting to iteratively inject and repair software bugs of increasing complexity, with each bug formally specified by a test patch rather than a natural language issue description. On the SWE-bench Verified and SWE-Bench Pro benchmarks, SSR achieves notable self-improvement (+10.4 and +7.8 points, respectively) and consistently outperforms the human-data baseline over the entire training trajectory, despite being evaluated on natural language issues absent from self-play. Our results, albeit early, suggest a path where agents autonomously gather extensive learning experiences from real-world software repositories, ultimately enabling superintelligent systems that exceed human capabilities in understanding how systems are constructed, solving novel challenges, and autonomously creating new software from scratch.",
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-12-21T00:49:40+00:00",
      "updated": "2025-12-21T00:49:40+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18552v1",
      "file": "papers/2512.18552v1.pdf"
    },
    {
      "arxiv_id": "2512.18540v1",
      "title": "Scaling up Stability: Reinforcement Learning for Distributed Control of Networked Systems in the Space of Stabilizing Policies",
      "authors": [
        {
          "name": "John Cao"
        },
        {
          "name": "Luca Furieri"
        }
      ],
      "abstract": "We study distributed control of networked systems through reinforcement learning, where neural policies must be simultaneously scalable, expressive and stabilizing. We introduce a policy parameterization that embeds Graph Neural Networks (GNNs) into a Youla-like magnitude-direction parameterization, yielding distributed stochastic controllers that guarantee network-level closed-loop stability by design. The magnitude is implemented as a stable operator consisting of a GNN acting on disturbance feedback, while the direction is a GNN acting on local observations. We prove robustness of the closed loop to perturbations in both the graph topology and model parameters, and show how to integrate our parameterization with Proximal Policy Optimization. Experiments on a multi-agent navigation task show that policies trained on small networks transfer directly to larger ones and unseen network topologies, achieve higher returns and lower variance than a state-of-the-art MARL baseline while preserving stability.",
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY",
        "cs.LG",
        "math.OC"
      ],
      "published": "2025-12-20T23:35:07+00:00",
      "updated": "2025-12-20T23:35:07+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18540v1",
      "file": "papers/2512.18540v1.pdf"
    },
    {
      "arxiv_id": "2512.18336v1",
      "title": "Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism",
      "authors": [
        {
          "name": "Youssef Mahran"
        },
        {
          "name": "Zeyad Gamal"
        },
        {
          "name": "Ayman El-Badawy"
        }
      ],
      "abstract": "This paper explores the impact of dynamic entropy tuning in Reinforcement Learning (RL) algorithms that train a stochastic policy. Its performance is compared against algorithms that train a deterministic one. Stochastic policies optimize a probability distribution over actions to maximize rewards, while deterministic policies select a single deterministic action per state. The effect of training a stochastic policy with both static entropy and dynamic entropy and then executing deterministic actions to control the quadcopter is explored. It is then compared against training a deterministic policy and executing deterministic actions. For the purpose of this research, the Soft Actor-Critic (SAC) algorithm was chosen for the stochastic algorithm while the Twin Delayed Deep Deterministic Policy Gradient (TD3) was chosen for the deterministic algorithm. The training and simulation results show the positive effect the dynamic entropy tuning has on controlling the quadcopter by preventing catastrophic forgetting and improving exploration efficiency.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-20T12:03:25+00:00",
      "updated": "2025-12-20T12:03:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18336v1",
      "file": "papers/2512.18336v1.pdf"
    },
    {
      "arxiv_id": "2512.18333v1",
      "title": "Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)",
      "authors": [
        {
          "name": "Youssef Mahran"
        },
        {
          "name": "Zeyad Gamal"
        },
        {
          "name": "Ayman El-Badawy"
        }
      ],
      "abstract": "This paper proposes a new Reinforcement Learning (RL) based control architecture for quadrotors. With the literature focusing on controlling the four rotors' RPMs directly, this paper aims to control the quadrotor's thrust vector. The RL agent computes the percentage of overall thrust along the quadrotor's z-axis along with the desired Roll ($φ$) and Pitch ($θ$) angles. The agent then sends the calculated control signals along with the current quadrotor's Yaw angle ($ψ$) to an attitude PID controller. The PID controller then maps the control signals to motor RPMs. The Soft Actor-Critic algorithm, a model-free off-policy stochastic RL algorithm, was used to train the RL agents. Training results show the faster training time of the proposed thrust vector controller in comparison to the conventional RPM controllers. Simulation results show smoother and more accurate path-following for the proposed thrust vector controller.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-20T11:57:20+00:00",
      "updated": "2025-12-20T11:57:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18333v1",
      "file": "papers/2512.18333v1.pdf"
    },
    {
      "arxiv_id": "2512.18317v1",
      "title": "Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems",
      "authors": [
        {
          "name": "Vincent Bezold"
        },
        {
          "name": "Patrick Wagner"
        },
        {
          "name": "Jakob Hofmann"
        },
        {
          "name": "Marco Huber"
        },
        {
          "name": "Alexander Sauer"
        }
      ],
      "abstract": "This paper presents a trustworthy reinforcement learning approach for the control of industrial compressed air systems. We develop a framework that enables safe and energy-efficient operation under realistic boundary conditions and introduce a multi-level explainability pipeline combining input perturbation tests, gradient-based sensitivity analysis, and SHAP (SHapley Additive exPlanations) feature attribution. An empirical evaluation across multiple compressor configurations shows that the learned policy is physically plausible, anticipates future demand, and consistently respects system boundaries. Compared to the installed industrial controller, the proposed approach reduces unnecessary overpressure and achieves energy savings of approximately 4\\,\\% without relying on explicit physics models. The results further indicate that system pressure and forecast information dominate policy decisions, while compressor-level inputs play a secondary role. Overall, the combination of efficiency gains, predictive behavior, and transparent validation supports the trustworthy deployment of reinforcement learning in industrial energy systems.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "published": "2025-12-20T11:11:49+00:00",
      "updated": "2025-12-20T11:11:49+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18317v1",
      "file": "papers/2512.18317v1.pdf"
    },
    {
      "arxiv_id": "2512.18309v1",
      "title": "Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings",
      "authors": [
        {
          "name": "Harsh Rathva"
        },
        {
          "name": "Ojas Srivastava"
        },
        {
          "name": "Pruthwik Mishra"
        }
      ],
      "abstract": "We introduce Embedded Safety-Aligned Intelligence (ESAI), a theoretical framework for multi-agent reinforcement learning that embeds alignment constraints directly into agents internal representations using differentiable internal alignment embeddings. Unlike external reward shaping or post-hoc safety constraints, internal alignment embeddings are learned latent variables that predict externalized harm through counterfactual reasoning and modulate policy updates toward harm reduction through attention and graph-based propagation.\n  The ESAI framework integrates four mechanisms: differentiable counterfactual alignment penalties computed from soft reference distributions, alignment-weighted perceptual attention, Hebbian associative memory supporting temporal credit assignment, and similarity-weighted graph diffusion with bias mitigation controls. We analyze stability conditions for bounded internal embeddings under Lipschitz continuity and spectral constraints, discuss computational complexity, and examine theoretical properties including contraction behavior and fairness-performance tradeoffs.\n  This work positions ESAI as a conceptual contribution to differentiable alignment mechanisms in multi-agent systems. We identify open theoretical questions regarding convergence guarantees, embedding dimensionality, and extension to high-dimensional environments. Empirical evaluation is left to future work.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-12-20T10:42:48+00:00",
      "updated": "2025-12-20T10:42:48+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18309v1",
      "file": "papers/2512.18309v1.pdf"
    },
    {
      "arxiv_id": "2512.18246v1",
      "title": "Offline Behavioral Data Selection",
      "authors": [
        {
          "name": "Shiye Lei"
        },
        {
          "name": "Zhihao Cheng"
        },
        {
          "name": "Dacheng Tao"
        }
      ],
      "abstract": "Behavioral cloning is a widely adopted approach for offline policy learning from expert demonstrations. However, the large scale of offline behavioral datasets often results in computationally intensive training when used in downstream tasks. In this paper, we uncover the striking data saturation in offline behavioral data: policy performance rapidly saturates when trained on a small fraction of the dataset. We attribute this effect to the weak alignment between policy performance and test loss, revealing substantial room for improvement through data selection. To this end, we propose a simple yet effective method, Stepwise Dual Ranking (SDR), which extracts a compact yet informative subset from large-scale offline behavioral datasets. SDR is build on two key principles: (1) stepwise clip, which prioritizes early-stage data; and (2) dual ranking, which selects samples with both high action-value rank and low state-density rank. Extensive experiments and ablation studies on D4RL benchmarks demonstrate that SDR significantly enhances data selection for offline behavioral data.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-12-20T07:10:58+00:00",
      "updated": "2025-12-20T07:10:58+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18246v1",
      "file": "papers/2512.18246v1.pdf"
    },
    {
      "arxiv_id": "2512.18215v1",
      "title": "Stable and Efficient Single-Rollout RL for Multimodal Reasoning",
      "authors": [
        {
          "name": "Rui Liu"
        },
        {
          "name": "Dian Yu"
        },
        {
          "name": "Lei Ke"
        },
        {
          "name": "Haolin Liu"
        },
        {
          "name": "Yujun Zhou"
        },
        {
          "name": "Zhenwen Liang"
        },
        {
          "name": "Haitao Mi"
        },
        {
          "name": "Pratap Tokekar"
        },
        {
          "name": "Dong Yu"
        }
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). However, prevalent group-based algorithms such as GRPO require multi-rollout sampling for each prompt. While more efficient single-rollout variants have recently been explored in text-only settings, we find that they suffer from severe instability in multimodal contexts, often leading to training collapse. To address this training efficiency-stability trade-off, we introduce $\\textbf{MSSR}$ (Multimodal Stabilized Single-Rollout), a group-free RLVR framework that achieves both stable optimization and effective multimodal reasoning performance. MSSR achieves this via an entropy-based advantage-shaping mechanism that adaptively regularizes advantage magnitudes, preventing collapse and maintaining training stability. While such mechanisms have been used in group-based RLVR, we show that in the multimodal single-rollout setting they are not merely beneficial but essential for stability. In in-distribution evaluations, MSSR demonstrates superior training compute efficiency, achieving similar validation accuracy to the group-based baseline with half the training steps. When trained for the same number of steps, MSSR's performance surpasses the group-based baseline and shows consistent generalization improvements across five diverse reasoning-intensive benchmarks. Together, these results demonstrate that MSSR enables stable, compute-efficient, and effective RLVR for complex multimodal reasoning tasks.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "published": "2025-12-20T05:07:53+00:00",
      "updated": "2025-12-20T05:07:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18215v1",
      "file": "papers/2512.18215v1.pdf"
    },
    {
      "arxiv_id": "2512.18135v1",
      "title": "Unifying Causal Reinforcement Learning: Survey, Taxonomy, Algorithms and Applications",
      "authors": [
        {
          "name": "Cristiano da Costa Cunha"
        },
        {
          "name": "Wei Liu"
        },
        {
          "name": "Tim French"
        },
        {
          "name": "Ajmal Mian"
        }
      ],
      "abstract": "Integrating causal inference (CI) with reinforcement learning (RL) has emerged as a powerful paradigm to address critical limitations in classical RL, including low explainability, lack of robustness and generalization failures. Traditional RL techniques, which typically rely on correlation-driven decision-making, struggle when faced with distribution shifts, confounding variables, and dynamic environments. Causal reinforcement learning (CRL), leveraging the foundational principles of causal inference, offers promising solutions to these challenges by explicitly modeling cause-and-effect relationships. In this survey, we systematically review recent advancements at the intersection of causal inference and RL. We categorize existing approaches into causal representation learning, counterfactual policy optimization, offline causal RL, causal transfer learning, and causal explainability. Through this structured analysis, we identify prevailing challenges, highlight empirical successes in practical applications, and discuss open problems. Finally, we provide future research directions, underscoring the potential of CRL for developing robust, generalizable, and interpretable artificial intelligence systems.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-12-19T23:37:22+00:00",
      "updated": "2025-12-19T23:37:22+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18135v1",
      "file": "papers/2512.18135v1.pdf"
    },
    {
      "arxiv_id": "2512.18014v1",
      "title": "ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India",
      "authors": [
        {
          "name": "Shubham Kumar Nigam"
        },
        {
          "name": "Tanuj Tyagi"
        },
        {
          "name": "Siddharth Shukla"
        },
        {
          "name": "Aditya Kumar Guru"
        },
        {
          "name": "Balaramamahanthi Deepak Patnaik"
        },
        {
          "name": "Danush Khanna"
        },
        {
          "name": "Noel Shallum"
        },
        {
          "name": "Kripabandhu Ghosh"
        },
        {
          "name": "Arnab Bhattacharya"
        }
      ],
      "abstract": "This paper presents an early exploration of reinforcement learning methodologies for legal AI in the Indian context. We introduce Reinforcement Learning-based Legal Reasoning (ReGal), a framework that integrates Multi-Task Instruction Tuning with Reinforcement Learning from AI Feedback (RLAIF) using Proximal Policy Optimization (PPO). Our approach is evaluated across two critical legal tasks: (i) Court Judgment Prediction and Explanation (CJPE), and (ii) Legal Document Summarization. Although the framework underperforms on standard evaluation metrics compared to supervised and proprietary models, it provides valuable insights into the challenges of applying RL to legal texts. These challenges include reward model alignment, legal language complexity, and domain-specific adaptation. Through empirical and qualitative analysis, we demonstrate how RL can be repurposed for high-stakes, long-document tasks in law. Our findings establish a foundation for future work on optimizing legal reasoning pipelines using reinforcement learning, with broader implications for building interpretable and adaptive legal AI systems.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-19T19:13:41+00:00",
      "updated": "2025-12-19T19:13:41+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18014v1",
      "file": "papers/2512.18014v1.pdf"
    },
    {
      "arxiv_id": "2512.17846v1",
      "title": "Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes",
      "authors": [
        {
          "name": "Carlos Vélez García"
        },
        {
          "name": "Miguel Cazorla"
        },
        {
          "name": "Jorge Pomares"
        }
      ],
      "abstract": "We present Planning as Descent (PaD), a framework for offline goal-conditioned reinforcement learning that grounds trajectory synthesis in verification. Instead of learning a policy or explicit planner, PaD learns a goal-conditioned energy function over entire latent trajectories, assigning low energy to feasible, goal-consistent futures. Planning is realized as gradient-based refinement in this energy landscape, using identical computation during training and inference to reduce train-test mismatch common in decoupled modeling pipelines.\n  PaD is trained via self-supervised hindsight goal relabeling, shaping the energy landscape around the planning dynamics. At inference, multiple trajectory candidates are refined under different temporal hypotheses, and low-energy plans balancing feasibility and efficiency are selected.\n  We evaluate PaD on OGBench cube manipulation tasks. When trained on narrow expert demonstrations, PaD achieves state-of-the-art 95\\% success, strongly outperforming prior methods that peak at 68\\%. Remarkably, training on noisy, suboptimal data further improves success and plan efficiency, highlighting the benefits of verification-driven planning. Our results suggest learning to evaluate and refine trajectories provides a robust alternative to direct policy learning for offline, reward-free planning.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2025-12-19T17:49:13+00:00",
      "updated": "2025-12-19T17:49:13+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17846v1",
      "file": "papers/2512.17846v1.pdf"
    },
    {
      "arxiv_id": "2512.01119v1",
      "title": "World Model Robustness via Surprise Recognition",
      "authors": [
        {
          "name": "Geigh Zollicoffer"
        },
        {
          "name": "Tanush Chopra"
        },
        {
          "name": "Mingkuan Yan"
        },
        {
          "name": "Xiaoxu Ma"
        },
        {
          "name": "Kenneth Eaton"
        },
        {
          "name": "Mark Riedl"
        }
      ],
      "abstract": "AI systems deployed in the real world must contend with distractions and out-of-distribution (OOD) noise that can destabilize their policies and lead to unsafe behavior. While robust training can reduce sensitivity to some forms of noise, it is infeasible to anticipate all possible OOD conditions. To mitigate this issue, we develop an algorithm that leverages a world model's inherent measure of surprise to reduce the impact of noise in world model--based reinforcement learning agents. We introduce both multi-representation and single-representation rejection sampling, enabling robustness to settings with multiple faulty sensors or a single faulty sensor. While the introduction of noise typically degrades agent performance, we show that our techniques preserve performance relative to baselines under varying types and levels of noise across multiple environments within self-driving simulation domains (CARLA and Safety Gymnasium). Furthermore, we demonstrate that our methods enhance the stability of two state-of-the-art world models with markedly different underlying architectures: Cosmos and DreamerV3. Together, these results highlight the robustness of our approach across world modeling domains. We release our code at https://github.com/Bluefin-Tuna/WISER .",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-11-30T22:25:45+00:00",
      "updated": "2025-11-30T22:25:45+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.01119v1",
      "file": "papers/2512.01119v1.pdf"
    },
    {
      "arxiv_id": "2512.01047v1",
      "title": "Automating the Refinement of Reinforcement Learning Specifications",
      "authors": [
        {
          "name": "Tanmay Ambadkar"
        },
        {
          "name": "Đorđe Žikelić"
        },
        {
          "name": "Abhinav Verma"
        }
      ],
      "abstract": "Logical specifications have been shown to help reinforcement learning algorithms in achieving complex tasks. However, when a task is under-specified, agents might fail to learn useful policies. In this work, we explore the possibility of improving coarse-grained logical specifications via an exploration-guided strategy. We propose \\textsc{AutoSpec}, a framework that searches for a logical specification refinement whose satisfaction implies satisfaction of the original specification, but which provides additional guidance therefore making it easier for reinforcement learning algorithms to learn useful policies. \\textsc{AutoSpec} is applicable to reinforcement learning tasks specified via the SpectRL specification logic. We exploit the compositional nature of specifications written in SpectRL, and design four refinement procedures that modify the abstract graph of the specification by either refining its existing edge specifications or by introducing new edge specifications. We prove that all four procedures maintain specification soundness, i.e. any trajectory satisfying the refined specification also satisfies the original. We then show how \\textsc{AutoSpec} can be integrated with existing reinforcement learning algorithms for learning policies from logical specifications. Our experiments demonstrate that \\textsc{AutoSpec} yields promising improvements in terms of the complexity of control tasks that can be solved, when refined logical specifications produced by \\textsc{AutoSpec} are utilized.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-11-30T19:32:33+00:00",
      "updated": "2025-11-30T19:32:33+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.01047v1",
      "file": "papers/2512.01047v1.pdf"
    },
    {
      "arxiv_id": "2512.01046v1",
      "title": "Shielded Controller Units for RL with Operational Constraints Applied to Remote Microgrids",
      "authors": [
        {
          "name": "Hadi Nekoei"
        },
        {
          "name": "Alexandre Blondin Massé"
        },
        {
          "name": "Rachid Hassani"
        },
        {
          "name": "Sarath Chandar"
        },
        {
          "name": "Vincent Mai"
        }
      ],
      "abstract": "Reinforcement learning (RL) is a powerful framework for optimizing decision-making in complex systems under uncertainty, an essential challenge in real-world settings, particularly in the context of the energy transition. A representative example is remote microgrids that supply power to communities disconnected from the main grid. Enabling the energy transition in such systems requires coordinated control of renewable sources like wind turbines, alongside fuel generators and batteries, to meet demand while minimizing fuel consumption and battery degradation under exogenous and intermittent load and wind conditions. These systems must often conform to extensive regulations and complex operational constraints. To ensure that RL agents respect these constraints, it is crucial to provide interpretable guarantees. In this paper, we introduce Shielded Controller Units (SCUs), a systematic and interpretable approach that leverages prior knowledge of system dynamics to ensure constraint satisfaction. Our shield synthesis methodology, designed for real-world deployment, decomposes the environment into a hierarchical structure where each SCU explicitly manages a subset of constraints. We demonstrate the effectiveness of SCUs on a remote microgrid optimization task with strict operational requirements. The RL agent, equipped with SCUs, achieves a 24% reduction in fuel consumption without increasing battery degradation, outperforming other baselines while satisfying all constraints. We hope SCUs contribute to the safe application of RL to the many decision-making challenges linked to the energy transition.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-11-30T19:28:34+00:00",
      "updated": "2025-11-30T19:28:34+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.01046v1",
      "file": "papers/2512.01046v1.pdf"
    },
    {
      "arxiv_id": "2512.01034v2",
      "title": "Addressing the Plasticity-Stability Dilemma in Reinforcement Learning",
      "authors": [
        {
          "name": "Mansi Maheshwari"
        },
        {
          "name": "John C. Raisbeck"
        },
        {
          "name": "Bruno Castro da Silva"
        }
      ],
      "abstract": "Neural networks have shown remarkable success in supervised learning when trained on a single task using a fixed dataset. However, when neural networks are trained on a reinforcement learning task, their ability to continue learning from new experiences declines over time. This decline in learning ability is known as plasticity loss. To restore plasticity, prior work has explored periodically resetting the parameters of the learning network, a strategy that often improves overall performance. However, such resets come at the cost of a temporary drop in performance, which can be dangerous in real-world settings. To overcome this instability, we introduce AltNet, a reset-based approach that restores plasticity without performance degradation by leveraging twin networks. The use of twin networks anchors performance during resets through a mechanism that allows networks to periodically alternate roles: one network learns as it acts in the environment, while the other learns off-policy from the active network's interactions and a replay buffer. At fixed intervals, the active network is reset and the passive network, having learned from prior experiences, becomes the new active network. AltNet restores plasticity, improving sample efficiency and achieving higher performance, while avoiding performance drops that pose risks in safety-critical settings. We demonstrate these advantages in several high-dimensional control tasks from the DeepMind Control Suite, where AltNet outperforms various relevant baseline methods, as well as state-of-the-art reset-based techniques.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-11-30T19:02:20+00:00",
      "updated": "2025-12-09T20:01:05+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.01034v2",
      "file": "papers/2512.01034v2.pdf"
    },
    {
      "arxiv_id": "2512.00968v1",
      "title": "Optimizing Generative Ranking Relevance via Reinforcement Learning in Xiaohongshu Search",
      "authors": [
        {
          "name": "Ziyang Zeng"
        },
        {
          "name": "Heming Jing"
        },
        {
          "name": "Jindong Chen"
        },
        {
          "name": "Xiangli Li"
        },
        {
          "name": "Hongyu Liu"
        },
        {
          "name": "Yixuan He"
        },
        {
          "name": "Zhengyu Li"
        },
        {
          "name": "Yige Sun"
        },
        {
          "name": "Zheyong Xie"
        },
        {
          "name": "Yuqing Yang"
        },
        {
          "name": "Shaosheng Cao"
        },
        {
          "name": "Jun Fan"
        },
        {
          "name": "Yi Wu"
        },
        {
          "name": "Yao Hu"
        }
      ],
      "abstract": "Ranking relevance is a fundamental task in search engines, aiming to identify the items most relevant to a given user query. Traditional relevance models typically produce scalar scores or directly predict relevance labels, limiting both interpretability and the modeling of complex relevance signals. Inspired by recent advances in Chain-of-Thought (CoT) reasoning for complex tasks, we investigate whether explicit reasoning can enhance both interpretability and performance in relevance modeling. However, existing reasoning-based Generative Relevance Models (GRMs) primarily rely on supervised fine-tuning on large amounts of human-annotated or synthetic CoT data, which often leads to limited generalization. Moreover, domain-agnostic, free-form reasoning tends to be overly generic and insufficiently grounded, limiting its potential to handle the diverse and ambiguous cases prevalent in open-domain search. In this work, we formulate relevance modeling in Xiaohongshu search as a reasoning task and introduce a Reinforcement Learning (RL)-based training framework to enhance the grounded reasoning capabilities of GRMs. Specifically, we incorporate practical business-specific relevance criteria into the multi-step reasoning prompt design and propose Stepwise Advantage Masking (SAM), a lightweight process-supervision strategy which facilitates effective learning of these criteria through improved credit assignment. To enable industrial deployment, we further distill the large-scale RL-tuned model to a lightweight version suitable for real-world search systems. Extensive experiments on industrial datasets, along with online A/B tests, demonstrate the effectiveness of our approach.",
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "published": "2025-11-30T16:31:16+00:00",
      "updated": "2025-11-30T16:31:16+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00968v1",
      "file": "papers/2512.00968v1.pdf"
    },
    {
      "arxiv_id": "2512.00961v1",
      "title": "Goal-Driven Reward by Video Diffusion Models for Reinforcement Learning",
      "authors": [
        {
          "name": "Qi Wang"
        },
        {
          "name": "Mian Wu"
        },
        {
          "name": "Yuyang Zhang"
        },
        {
          "name": "Mingqi Yuan"
        },
        {
          "name": "Wenyao Zhang"
        },
        {
          "name": "Haoxiang You"
        },
        {
          "name": "Yunbo Wang"
        },
        {
          "name": "Xin Jin"
        },
        {
          "name": "Xiaokang Yang"
        },
        {
          "name": "Wenjun Zeng"
        }
      ],
      "abstract": "Reinforcement Learning (RL) has achieved remarkable success in various domains, yet it often relies on carefully designed programmatic reward functions to guide agent behavior. Designing such reward functions can be challenging and may not generalize well across different tasks. To address this limitation, we leverage the rich world knowledge contained in pretrained video diffusion models to provide goal-driven reward signals for RL agents without ad-hoc design of reward. Our key idea is to exploit off-the-shelf video diffusion models pretrained on large-scale video datasets as informative reward functions in terms of video-level and frame-level goals. For video-level rewards, we first finetune a pretrained video diffusion model on domain-specific datasets and then employ its video encoder to evaluate the alignment between the latent representations of agent's trajectories and the generated goal videos. To enable more fine-grained goal-achievement, we derive a frame-level goal by identifying the most relevant frame from the generated video using CLIP, which serves as the goal state. We then employ a learned forward-backward representation that represents the probability of visiting the goal state from a given state-action pair as frame-level reward, promoting more coherent and goal-driven trajectories. Experiments on various Meta-World tasks demonstrate the effectiveness of our approach.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-11-30T16:22:27+00:00",
      "updated": "2025-11-30T16:22:27+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00961v1",
      "file": "papers/2512.00961v1.pdf"
    },
    {
      "arxiv_id": "2512.00915v1",
      "title": "Partially Equivariant Reinforcement Learning in Symmetry-Breaking Environments",
      "authors": [
        {
          "name": "Junwoo Chang"
        },
        {
          "name": "Minwoo Park"
        },
        {
          "name": "Joohwan Seo"
        },
        {
          "name": "Roberto Horowitz"
        },
        {
          "name": "Jongmin Lee"
        },
        {
          "name": "Jongeun Choi"
        }
      ],
      "abstract": "Group symmetries provide a powerful inductive bias for reinforcement learning (RL), enabling efficient generalization across symmetric states and actions via group-invariant Markov Decision Processes (MDPs). However, real-world environments almost never realize fully group-invariant MDPs; dynamics, actuation limits, and reward design usually break symmetries, often only locally. Under group-invariant Bellman backups for such cases, local symmetry-breaking introduces errors that propagate across the entire state-action space, resulting in global value estimation errors. To address this, we introduce Partially group-Invariant MDP (PI-MDP), which selectively applies group-invariant or standard Bellman backups depending on where symmetry holds. This framework mitigates error propagation from locally broken symmetries while maintaining the benefits of equivariance, thereby enhancing sample efficiency and generalizability. Building on this framework, we present practical RL algorithms -- Partially Equivariant (PE)-DQN for discrete control and PE-SAC for continuous control -- that combine the benefits of equivariance with robustness to symmetry-breaking. Experiments across Grid-World, locomotion, and manipulation benchmarks demonstrate that PE-DQN and PE-SAC significantly outperform baselines, highlighting the importance of selective symmetry exploitation for robust and sample-efficient RL.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "published": "2025-11-30T14:41:08+00:00",
      "updated": "2025-11-30T14:41:08+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00915v1",
      "file": "papers/2512.00915v1.pdf"
    },
    {
      "arxiv_id": "2512.00908v1",
      "title": "Beyond High-Entropy Exploration: Correctness-Aware Low-Entropy Segment-Based Advantage Shaping for Reasoning LLMs",
      "authors": [
        {
          "name": "Xinzhu Chen"
        },
        {
          "name": "Xuesheng Li"
        },
        {
          "name": "Zhongxiang Sun"
        },
        {
          "name": "Weijie Yu"
        }
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a central approach for improving the reasoning ability of large language models. Recent work studies RLVR through token entropy, arguing that high-entropy tokens drive exploration and should receive stronger updates. However, they overlook the fact that most of a reasoning trajectory consists of low-entropy segments that encode stable and reusable structural patterns. Through qualitative and quantitative analyses, we find that the overlap of low-entropy segments across correct responses strongly correlates with model accuracy, while overlaps involving incorrect responses exhibit stable but unproductive patterns. Motivated by these findings, we propose LESS, a correctness-aware reinforcement framework that performs fine-grained advantage modulation over low-entropy segments. LESS amplifies segments unique to correct responses, suppresses those unique to incorrect ones, and neutralizes segments shared by both, while preserving high-entropy exploration in the underlying RL algorithm. Instantiated on top of the popular GRPO, LESS consistently improves accuracy over strong RL baselines across three backbones and six math benchmarks, achieves stronger robustness of the performance floor.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-11-30T14:19:36+00:00",
      "updated": "2025-11-30T14:19:36+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00908v1",
      "file": "papers/2512.00908v1.pdf"
    },
    {
      "arxiv_id": "2512.00778v1",
      "title": "What Is Preference Optimization Doing, How and Why?",
      "authors": [
        {
          "name": "Yue Wang"
        },
        {
          "name": "Qizhou Wang"
        },
        {
          "name": "Zizhuo Zhang"
        },
        {
          "name": "Ang Li"
        },
        {
          "name": "Gang Niu"
        },
        {
          "name": "Bo Han"
        },
        {
          "name": "Masashi Sugiyama"
        }
      ],
      "abstract": "Preference optimization (PO) is indispensable for large language models (LLMs), with methods such as direct preference optimization (DPO) and proximal policy optimization (PPO) achieving great success. A common belief is that DPO is supervised learning while PPO is reinforcement learning, yet deeper analyses for the reasons underlying these differences remain lacking. To fill this gap, we analyze their optimization dynamics, revealing distinct algorithmic behaviors and comprehending their underlying causes. First, we examine the target directions of gradient-based updates and find that DPO follows stable targets, whereas PPO follows dynamic targets that balance exploration and exploitation, thus validating the common belief from a new perspective. Second, we examine the roles of positive learning, negative learning, and loss reweighting, which are three key components in PO methods. Our analyses reveal that these components play fairly different roles. In DPO, positive and negative learning jointly shape the learning targets meanwhile mutually offset each other. However, loss reweighting in DPO acts less as a reward signal but more as a regularizer to mitigate overfitting. In PPO, negative learning primarily supports exploration rather than determining the targets. Meanwhile, loss reweighting, related to absolute values of token-level advantages, indicates the distinct roles of token groups in updating targets. Given these findings, we conduct carefully designed ablation studies to further examine how controlling these dynamics impacts optimization efficiency and practical performance. The insights gained from our analyses not only deepen the understanding of PO methods but also inspire the development of more preference-aligned LLMs.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-11-30T08:27:59+00:00",
      "updated": "2025-11-30T08:27:59+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00778v1",
      "file": "papers/2512.00778v1.pdf"
    },
    {
      "arxiv_id": "2512.00724v1",
      "title": "Upcycled and Merged MoE Reward Model for Mitigating Reward Hacking",
      "authors": [
        {
          "name": "Lingling Fu"
        }
      ],
      "abstract": "Reward models play a critical role in Reinforcement Learning from Human Feedback (RLHF) by assessing the consistency between generated outputs and human preferences. However, conventional reward models are prone to reward hacking or over-optimization, where the policy exploits shortcut patterns to obtain high reward scores that do not reflect true human preference. Although Mixture-of-Experts (MoE)-based reward models can enhance discriminative capability, they typically introduce substantial computational overhead. To address these challenges, we propose an upcycle and merge MoE reward modeling approach. We first upcycle a dense reward model into a MoE architecture, where a shared expert captures general knowledge, while normal experts specialize in instruction-specific patterns. We then apply routing-weight normalization and merge experts back into a dense model through a learnable weight-averaging mechanism, preserving performance gains while significantly reducing inference cost. Experimental results demonstrate that our method effectively mitigates reward hacking across various model scales. Our work highlights the potential of upcycle and merge MoE structures for improving both robustness and efficiency of RLHF reward models.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.IR"
      ],
      "published": "2025-11-30T04:36:37+00:00",
      "updated": "2025-11-30T04:36:37+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00724v1",
      "file": "papers/2512.00724v1.pdf"
    },
    {
      "arxiv_id": "2512.00709v1",
      "title": "When Human Preferences Flip: An Instance-Dependent Robust Loss for RLHF",
      "authors": [
        {
          "name": "Yifan Xu"
        },
        {
          "name": "Xichen Ye"
        },
        {
          "name": "Yifan Chen"
        },
        {
          "name": "Qiaosheng Zhang"
        }
      ],
      "abstract": "Quality of datasets plays an important role in large language model (LLM) alignment. In collecting human feedback, however, preference flipping is ubiquitous and causes corruption in data annotation; the issue necessitates the alignment algorithms with improved robustness against potential flipped pairs. To this end, this paper introduces a Flipping-Aware Direct Preference Optimization (FA-DPO) algorithm tailored to preference flipping from a reinforcement learning with human feedback (RLHF) perspective. We dissect the inherent human intention model and the preference flipping mechanism introduced by external factors as two distinct stages; in the latter, we introduce an instance-dependent flipping probability on the basis of the Bradley-Terry (BT) model. Further, by leveraging features relevant to preference annotation, we capture uncertainty in judgments and model preference flipping patterns. In practice, we design a simple yet efficient iterative optimization algorithm compatible with the original RLHF and DPO algorithms. In our experiments, we investigate the instance-dependent preference flipping model under multiple circumstances for evaluation of our proposed method, as well as other baseline methods.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-11-30T03:16:20+00:00",
      "updated": "2025-11-30T03:16:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00709v1",
      "file": "papers/2512.00709v1.pdf"
    },
    {
      "arxiv_id": "2512.00601v2",
      "title": "Clinical-R1: Empowering Large Language Models for Faithful and Comprehensive Reasoning with Clinical Objective Relative Policy Optimization",
      "authors": [
        {
          "name": "Boyang Gu"
        },
        {
          "name": "Hongjian Zhou"
        },
        {
          "name": "Bradley Max Segal"
        },
        {
          "name": "Jinge Wu"
        },
        {
          "name": "Zeyu Cao"
        },
        {
          "name": "Hantao Zhong"
        },
        {
          "name": "Lei Clifton"
        },
        {
          "name": "Fenglin Liu"
        },
        {
          "name": "David A. Clifton"
        }
      ],
      "abstract": "Recent advances in large language models (LLMs) have shown strong reasoning capabilities through large-scale pretraining and post-training reinforcement learning, demonstrated by DeepSeek-R1. However, current post-training methods, such as Grouped Relative Policy Optimization (GRPO), mainly reward correctness, which is not aligned with the multi-dimensional objectives required in high-stakes fields such as medicine, where reasoning must also be faithful and comprehensive. We introduce Clinical-Objective Relative Policy Optimization (CRPO), a scalable, multi-objective, verifiable reinforcement learning method designed to align LLM post-training with clinical reasoning principles. CRPO integrates rule-based and verifiable reward signals that jointly optimize accuracy, faithfulness, and comprehensiveness without relying on human annotation. To demonstrate its effectiveness, we train Clinical-R1-3B, a 3B-parameter model for clinical reasoning. The experiments on three benchmarks demonstrate that our CRPO substantially improves reasoning on truthfulness and completeness over standard GRPO while maintaining comfortable accuracy enhancements. This framework provides a scalable pathway to align LLM reasoning with clinical objectives, enabling safer and more collaborative AI systems for healthcare while also highlighting the potential of multi-objective, verifiable RL methods in post-training scaling of LLMs for medical domains.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-11-29T19:09:24+00:00",
      "updated": "2025-12-03T12:24:00+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00601v2",
      "file": "papers/2512.00601v2.pdf"
    },
    {
      "arxiv_id": "2512.00553v1",
      "title": "List Replicable Reinforcement Learning",
      "authors": [
        {
          "name": "Bohan Zhang"
        },
        {
          "name": "Michael Chen"
        },
        {
          "name": "A. Pavan"
        },
        {
          "name": "N. V. Vinodchandran"
        },
        {
          "name": "Lin F. Yang"
        },
        {
          "name": "Ruosong Wang"
        }
      ],
      "abstract": "Replicability is a fundamental challenge in reinforcement learning (RL), as RL algorithms are empirically observed to be unstable and sensitive to variations in training conditions. To formally address this issue, we study \\emph{list replicability} in the Probably Approximately Correct (PAC) RL framework, where an algorithm must return a near-optimal policy that lies in a \\emph{small list} of policies across different runs, with high probability. The size of this list defines the \\emph{list complexity}. We introduce both weak and strong forms of list replicability: the weak form ensures that the final learned policy belongs to a small list, while the strong form further requires that the entire sequence of executed policies remains constrained. These objectives are challenging, as existing RL algorithms exhibit exponential list complexity due to their instability. Our main theoretical contribution is a provably efficient tabular RL algorithm that guarantees list replicability by ensuring the list complexity remains polynomial in the number of states, actions, and the horizon length. We further extend our techniques to achieve strong list replicability, bounding the number of possible policy execution traces polynomially with high probability. Our theoretical result is made possible by key innovations including (i) a novel planning strategy that selects actions based on lexicographic order among near-optimal choices within a randomly chosen tolerance threshold, and (ii) a mechanism for testing state reachability in stochastic environments while preserving replicability. Finally, we demonstrate that our theoretical investigation sheds light on resolving the \\emph{instability} issue of RL algorithms used in practice. In particular, we show that empirically, our new planning strategy can be incorporated into practical RL frameworks to enhance their stability.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2025-11-29T16:47:43+00:00",
      "updated": "2025-11-29T16:47:43+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00553v1",
      "file": "papers/2512.00553v1.pdf"
    },
    {
      "arxiv_id": "2512.00536v1",
      "title": "Algorithmic Guarantees for Distilling Supervised and Offline RL Datasets",
      "authors": [
        {
          "name": "Aaryan Gupta"
        },
        {
          "name": "Rishi Saket"
        },
        {
          "name": "Aravindan Raghuveer"
        }
      ],
      "abstract": "Given a training dataset, the goal of dataset distillation is to derive a synthetic dataset such that models trained on the latter perform as well as those trained on the training dataset. In this work, we develop and analyze an efficient dataset distillation algorithm for supervised learning, specifically regression in $\\mathbb{R}^d$, based on matching the losses on the training and synthetic datasets with respect to a fixed set of randomly sampled regressors without any model training. Our first key contribution is a novel performance guarantee proving that our algorithm needs only $\\tilde{O}(d^2)$ sampled regressors to derive a synthetic dataset on which the MSE loss of any bounded linear model is nearly the same as its MSE loss on the given training data. In particular, the model optimized on the synthetic data has close to minimum loss on the training data, thus performing nearly as well as the model optimized on the latter. Complementing this, we also prove a matching lower bound of $Ω(d^2)$ for the number of sampled regressors showing the tightness of our analysis.\n  Our second contribution is to extend our algorithm to offline RL dataset distillation by matching the Bellman loss, unlike previous works which used a behavioral cloning objective. This is the first such method which leverages both, the rewards and the next state information, available in offline RL datasets, without any policy model optimization. Our algorithm generates a synthetic dataset whose Bellman loss with respect to any linear action-value predictor is close to the latter's Bellman loss on the offline RL training dataset. Therefore, a policy associated with an action-value predictor optimized on the synthetic dataset performs nearly as well as that derived from the one optimized on the training data. We conduct experiments to validate our theoretical guarantees and observe performance gains.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2025-11-29T16:04:38+00:00",
      "updated": "2025-11-29T16:04:38+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00536v1",
      "file": "papers/2512.00536v1.pdf"
    },
    {
      "arxiv_id": "2512.00499v1",
      "title": "ESPO: Entropy Importance Sampling Policy Optimization",
      "authors": [
        {
          "name": "Yuepeng Sheng"
        },
        {
          "name": "Yuwei Huang"
        },
        {
          "name": "Shuman Liu"
        },
        {
          "name": "Haibo Zhang"
        },
        {
          "name": "Anxiang Zeng"
        }
      ],
      "abstract": "Large language model (LLM) reinforcement learning has increasingly relied on group-based policy optimization frameworks, such as GRPO and GSPO, to achieve stable fine-tuning at scale. However, a fundamental trade-off persists between optimization granularity and training stability. While GSPO improves robustness via sequence-level optimization, its monolithic treatment of sequences introduces severe inefficiencies: its conservative clipping mechanism indiscriminately discards valid training samples-a phenomenon we term gradient underutilization-and its uniform credit assignment fails to capture the heterogeneous contributions of critical reasoning steps. In this work, we propose Entropy Importance Sampling Policy Optimization (ESPO), a novel framework that reconciles fine-grained control with training stability. ESPO decomposes sequences into groups based on predictive entropy, enabling (1) Entropy-driven Importance Sampling to capture intra-sequence heterogeneity, and (2) Entropy-adaptive Clipping to dynamically allocate trust regions based on model uncertainty. Extensive experiments on mathematical reasoning benchmarks demonstrate that ESPO not only accelerates convergence but also achieves state-of-the-art performance, notably improving accuracy on the challenging HMMT benchmark from 4.4% to 13.13%.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2025-11-29T14:09:38+00:00",
      "updated": "2025-11-29T14:09:38+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00499v1",
      "file": "papers/2512.00499v1.pdf"
    },
    {
      "arxiv_id": "2512.00453v1",
      "title": "Sample-Efficient Expert Query Control in Active Imitation Learning via Conformal Prediction",
      "authors": [
        {
          "name": "Arad Firouzkouhi"
        },
        {
          "name": "Omid Mirzaeedodangeh"
        },
        {
          "name": "Lars Lindemann"
        }
      ],
      "abstract": "Active imitation learning (AIL) combats covariate shift by querying an expert during training. However, expert action labeling often dominates the cost, especially in GPU-intensive simulators, human-in-the-loop settings, and robot fleets that revisit near-duplicate states. We present Conformalized Rejection Sampling for Active Imitation Learning (CRSAIL), a querying rule that requests an expert action only when the visited state is under-represented in the expert-labeled dataset. CRSAIL scores state novelty by the distance to the $K$-th nearest expert state and sets a single global threshold via conformal prediction. This threshold is the empirical $(1-α)$ quantile of on-policy calibration scores, providing a distribution-free calibration rule that links $α$ to the expected query rate and makes $α$ a task-agnostic tuning knob. This state-space querying strategy is robust to outliers and, unlike safety-gate-based AIL, can be run without real-time expert takeovers: we roll out full trajectories (episodes) with the learner and only afterward query the expert on a subset of visited states. Evaluated on MuJoCo robotics tasks, CRSAIL matches or exceeds expert-level reward while reducing total expert queries by up to 96% vs. DAgger and up to 65% vs. prior AIL methods, with empirical robustness to $α$ and $K$, easing deployment on novel systems with unknown dynamics.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-11-29T11:58:21+00:00",
      "updated": "2025-11-29T11:58:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00453v1",
      "file": "papers/2512.00453v1.pdf"
    },
    {
      "arxiv_id": "2512.00383v1",
      "title": "An Empirical Study on the Effectiveness of Incorporating Offline RL As Online RL Subroutines",
      "authors": [
        {
          "name": "Jianhai Su"
        },
        {
          "name": "Jinzhu Luo"
        },
        {
          "name": "Qi Zhang"
        }
      ],
      "abstract": "We take the novel perspective of incorporating offline RL algorithms as subroutines of tabula rasa online RL. This is feasible because an online learning agent can repurpose its historical interactions as offline dataset. We formalize this idea into a framework that accommodates several variants of offline RL incorporation such as final policy recommendation and online fine-tuning. We further introduce convenient techniques to improve its effectiveness in enhancing online learning efficiency. Our extensive and systematic empirical analyses show that 1) the effectiveness of the proposed framework depends strongly on the nature of the task, 2) our proposed techniques greatly enhance its effectiveness, and 3) existing online fine-tuning methods are overall ineffective, calling for more research therein.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2025-11-29T08:17:03+00:00",
      "updated": "2025-11-29T08:17:03+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00383v1",
      "file": "papers/2512.00383v1.pdf"
    },
    {
      "arxiv_id": "2512.00357v1",
      "title": "Learning Causal States Under Partial Observability and Perturbation",
      "authors": [
        {
          "name": "Na Li"
        },
        {
          "name": "Hangguan Shan"
        },
        {
          "name": "Wei Ni"
        },
        {
          "name": "Wenjie Zhang"
        },
        {
          "name": "Xinyu Li"
        },
        {
          "name": "Yamin Wang"
        }
      ],
      "abstract": "A critical challenge for reinforcement learning (RL) is making decisions based on incomplete and noisy observations, especially in perturbed and partially observable Markov decision processes (P$^2$OMDPs). Existing methods fail to mitigate perturbations while addressing partial observability. We propose \\textit{Causal State Representation under Asynchronous Diffusion Model (CaDiff)}, a framework that enhances any RL algorithm by uncovering the underlying causal structure of P$^2$OMDPs. This is achieved by incorporating a novel asynchronous diffusion model (ADM) and a new bisimulation metric. ADM enables forward and reverse processes with different numbers of steps, thus interpreting the perturbation of P$^2$OMDP as part of the noise suppressed through diffusion. The bisimulation metric quantifies the similarity between partially observable environments and their causal counterparts. Moreover, we establish the theoretical guarantee of CaDiff by deriving an upper bound for the value function approximation errors between perturbed observations and denoised causal states, reflecting a principled trade-off between approximation errors of reward and transition-model. Experiments on Roboschool tasks show that CaDiff enhances returns by at least 14.18\\% compared to baselines. CaDiff is the first framework that approximates causal states using diffusion models with both theoretical rigor and practicality.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2025-11-29T06:56:03+00:00",
      "updated": "2025-11-29T06:56:03+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00357v1",
      "file": "papers/2512.00357v1.pdf"
    },
    {
      "arxiv_id": "2512.00352v1",
      "title": "Sample-Efficient Tabular Self-Play for Offline Robust Reinforcement Learning",
      "authors": [
        {
          "name": "Na Li"
        },
        {
          "name": "Zewu Zheng"
        },
        {
          "name": "Wei Ni"
        },
        {
          "name": "Hangguan Shan"
        },
        {
          "name": "Wenjie Zhang"
        },
        {
          "name": "Xinyu Li"
        }
      ],
      "abstract": "Multi-agent reinforcement learning (MARL), as a thriving field, explores how multiple agents independently make decisions in a shared dynamic environment. Due to environmental uncertainties, policies in MARL must remain robust to tackle the sim-to-real gap. We focus on robust two-player zero-sum Markov games (TZMGs) in offline settings, specifically on tabular robust TZMGs (RTZMGs). We propose a model-based algorithm (\\textit{RTZ-VI-LCB}) for offline RTZMGs, which is optimistic robust value iteration combined with a data-driven Bernstein-style penalty term for robust value estimation. By accounting for distribution shifts in the historical dataset, the proposed algorithm establishes near-optimal sample complexity guarantees under partial coverage and environmental uncertainty. An information-theoretic lower bound is developed to confirm the tightness of our algorithm's sample complexity, which is optimal regarding both state and action spaces. To the best of our knowledge, RTZ-VI-LCB is the first to attain this optimality, sets a new benchmark for offline RTZMGs, and is validated experimentally.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2025-11-29T06:45:00+00:00",
      "updated": "2025-11-29T06:45:00+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00352v1",
      "file": "papers/2512.00352v1.pdf"
    },
    {
      "arxiv_id": "2512.00351v1",
      "title": "Provable Memory Efficient Self-Play Algorithm for Model-free Reinforcement Learning",
      "authors": [
        {
          "name": "Na Li"
        },
        {
          "name": "Yuchen Jiao"
        },
        {
          "name": "Hangguan Shan"
        },
        {
          "name": "Shefeng Yan"
        }
      ],
      "abstract": "The thriving field of multi-agent reinforcement learning (MARL) studies how a group of interacting agents make decisions autonomously in a shared dynamic environment. Existing theoretical studies in this area suffer from at least two of the following obstacles: memory inefficiency, the heavy dependence of sample complexity on the long horizon and the large state space, the high computational complexity, non-Markov policy, non-Nash policy, and high burn-in cost. In this work, we take a step towards settling this problem by designing a model-free self-play algorithm \\emph{Memory-Efficient Nash Q-Learning (ME-Nash-QL)} for two-player zero-sum Markov games, which is a specific setting of MARL. ME-Nash-QL is proven to enjoy the following merits. First, it can output an $\\varepsilon$-approximate Nash policy with space complexity $O(SABH)$ and sample complexity $\\widetilde{O}(H^4SAB/\\varepsilon^2)$, where $S$ is the number of states, $\\{A, B\\}$ is the number of actions for two players, and $H$ is the horizon length. It outperforms existing algorithms in terms of space complexity for tabular cases, and in terms of sample complexity for long horizons, i.e., when $\\min\\{A, B\\}\\ll H^2$. Second, ME-Nash-QL achieves the lowest computational complexity $O(T\\mathrm{poly}(AB))$ while preserving Markov policies, where $T$ is the number of samples. Third, ME-Nash-QL also achieves the best burn-in cost $O(SAB\\,\\mathrm{poly}(H))$, whereas previous algorithms have a burn-in cost of at least $O(S^3 AB\\,\\mathrm{poly}(H))$ to attain the same level of sample complexity with ours.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2025-11-29T06:44:25+00:00",
      "updated": "2025-11-29T06:44:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00351v1",
      "file": "papers/2512.00351v1.pdf"
    },
    {
      "arxiv_id": "2512.00319v2",
      "title": "RL-Struct: A Lightweight Reinforcement Learning Framework for Reliable Structured Output in LLMs",
      "authors": [
        {
          "name": "Ruike Hu"
        },
        {
          "name": "Shulei Wu"
        }
      ],
      "abstract": "The Structure Gap between probabilistic LLM generation and deterministic schema requirements hinders automated workflows. We propose RL-Struct, a lightweight framework using Gradient Regularized Policy Optimization (GRPO) with a hierarchical reward function to align LLMs with structural constraints. This approach eliminates the critic network, reducing peak VRAM by 38% compared to PPO. On complex JSON tasks, RL-Struct achieves 89.7% structural accuracy and 92.1% validity, significantly outperforming SFT and zero-shot baselines. We also report an emergent curriculum--a self-organized learning process where the model prioritizes syntax before semantics. Our model is publicly available at https://huggingface.co/Freakz3z/Qwen-JSON.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-11-29T04:47:14+00:00",
      "updated": "2025-12-15T15:32:09+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00319v2",
      "file": "papers/2512.00319v2.pdf"
    },
    {
      "arxiv_id": "2512.00249v1",
      "title": "A Hierarchical Hybrid AI Approach: Integrating Deep Reinforcement Learning and Scripted Agents in Combat Simulations",
      "authors": [
        {
          "name": "Scotty Black"
        },
        {
          "name": "Christian Darken"
        }
      ],
      "abstract": "In the domain of combat simulations in support of wargaming, the development of intelligent agents has predominantly been characterized by rule-based, scripted methodologies with deep reinforcement learning (RL) approaches only recently being introduced. While scripted agents offer predictability and consistency in controlled environments, they fall short in dynamic, complex scenarios due to their inherent inflexibility. Conversely, RL agents excel in adaptability and learning, offering potential improvements in handling unforeseen situations, but suffer from significant challenges such as black-box decision-making processes and scalability issues in larger simulation environments. This paper introduces a novel hierarchical hybrid artificial intelligence (AI) approach that synergizes the reliability and predictability of scripted agents with the dynamic, adaptive learning capabilities of RL. By structuring the AI system hierarchically, the proposed approach aims to utilize scripted agents for routine, tactical-level decisions and RL agents for higher-level, strategic decision-making, thus addressing the limitations of each method while leveraging their individual strengths. This integration is shown to significantly improve overall performance, providing a robust, adaptable, and effective solution for developing and training intelligent agents in complex simulation environments.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "published": "2025-11-28T23:50:29+00:00",
      "updated": "2025-11-28T23:50:29+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00249v1",
      "file": "papers/2512.00249v1.pdf"
    },
    {
      "arxiv_id": "2512.20629v1",
      "title": "Learning Evolving Latent Strategies for Multi-Agent Language Systems without Model Fine-Tuning",
      "authors": [
        {
          "name": "Wenlong Tang"
        }
      ],
      "abstract": "This study proposes a multi-agent language framework that enables continual strategy evolution without fine-tuning the language model's parameters. The core idea is to liberate the latent vectors of abstract concepts from traditional static semantic representations, allowing them to be continuously updated through environmental interaction and reinforcement feedback. We construct a dual-loop architecture: the behavior loop adjusts action preferences based on environmental rewards, while the language loop updates the external latent vectors by reflecting on the semantic embeddings of generated text.\n  Together, these mechanisms allow agents to develop stable and disentangled strategic styles over long-horizon multi-round interactions. Experiments show that agents' latent spaces exhibit clear convergence trajectories under reflection-driven updates, along with structured shifts at critical moments. Moreover, the system demonstrates an emergent ability to implicitly infer and continually adapt to emotional agents, even without shared rewards. These results indicate that, without modifying model parameters, an external latent space can provide language agents with a low-cost, scalable, and interpretable form of abstract strategic representation.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-11-28T23:36:45+00:00",
      "updated": "2025-11-28T23:36:45+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20629v1",
      "file": "papers/2512.20629v1.pdf"
    },
    {
      "arxiv_id": "2512.00222v1",
      "title": "Statistical Inference under Adaptive Sampling with LinUCB",
      "authors": [
        {
          "name": "Wei Fan"
        },
        {
          "name": "Kevin Tan"
        },
        {
          "name": "Yuting Wei"
        }
      ],
      "abstract": "Adaptively collected data has become ubiquitous within modern practice. However, even seemingly benign adaptive sampling schemes can introduce severe biases, rendering traditional statistical inference tools inapplicable. This can be mitigated by a property called stability, which states that if the rate at which an algorithm takes actions converges to a deterministic limit, one can expect that certain parameters are asymptotically normal. Building on a recent line of work for the multi-armed bandit setting, we show that the linear upper confidence bound (LinUCB) algorithm for linear bandits satisfies this property. In doing so, we painstakingly characterize the behavior of the eigenvalues and eigenvectors of the random design feature covariance matrix in the setting where the action set is the unit ball, showing that it decomposes into a rank-one direction that locks onto the true parameter and an almost-isotropic bulk that grows at a predictable $\\sqrt{T}$ rate. This allows us to establish a central limit theorem for the LinUCB algorithm, establishing asymptotic normality for the limiting distribution of the estimation error where the convergence occurs at a $T^{-1/4}$ rate. The resulting Wald-type confidence sets and hypothesis tests do not depend on the feature covariance matrix and are asymptotically tighter than existing nonasymptotic confidence sets. Numerical simulations corroborate our findings.",
      "primary_category": "math.ST",
      "categories": [
        "math.ST",
        "cs.LG",
        "stat.ME",
        "stat.ML"
      ],
      "published": "2025-11-28T21:48:18+00:00",
      "updated": "2025-11-28T21:48:18+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00222v1",
      "file": "papers/2512.00222v1.pdf"
    },
    {
      "arxiv_id": "2511.23476v1",
      "title": "Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction",
      "authors": [
        {
          "name": "Bao Shu"
        },
        {
          "name": "Yan Cai"
        },
        {
          "name": "Jianjian Sun"
        },
        {
          "name": "Chunrui Han"
        },
        {
          "name": "En Yu"
        },
        {
          "name": "Liang Zhao"
        },
        {
          "name": "Jingcheng Hu"
        },
        {
          "name": "Yinmin Zhang"
        },
        {
          "name": "Haoran Lv"
        },
        {
          "name": "Yuang Peng"
        },
        {
          "name": "Zheng Ge"
        },
        {
          "name": "Xiangyu Zhang"
        },
        {
          "name": "Daxin Jiang"
        },
        {
          "name": "Xiangyu Yue"
        }
      ],
      "abstract": "Developing robust world model reasoning is crucial for large language model (LLM) agents to plan and interact in complex environments. While multi-turn interaction offers a superior understanding of environmental dynamics via authentic feedback, current approaches often impose a rigid reasoning process, which constrains the model's active learning, ultimately hindering efficient world model reasoning. To address these issues, we explore world-model internalization through efficient interaction and active reasoning (WMAct), which liberates the model from structured reasoning, allowing the model to shape thinking directly through its doing, and achieves effective and efficient world model reasoning with two key mechanisms: (1) a reward rescaling mechanism adjusting outcome reward based on action efficacy to incentivize redundancy reduction and purposeful interaction; (2) an interaction frequency annealing strategy to progressively reduce the maximum allowed interaction turns, which compels the model to condense its learning and internalize environmental dynamics rather than over-relying on environmental cues. Our experiments on Sokoban, Maze, and Taxi show that WMAct yields effective world model reasoning capable of resolving tasks in a single turn that previously required multiple interactions and fosters strong transferability to complex environments, improving performance on a suite of reasoning benchmarks.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-11-28T18:59:47+00:00",
      "updated": "2025-11-28T18:59:47+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.23476v1",
      "file": "papers/2511.23476v1.pdf"
    },
    {
      "arxiv_id": "2511.23473v1",
      "title": "ThetaEvolve: Test-time Learning on Open Problems",
      "authors": [
        {
          "name": "Yiping Wang"
        },
        {
          "name": "Shao-Rong Su"
        },
        {
          "name": "Zhiyuan Zeng"
        },
        {
          "name": "Eva Xu"
        },
        {
          "name": "Liliang Ren"
        },
        {
          "name": "Xinyu Yang"
        },
        {
          "name": "Zeyi Huang"
        },
        {
          "name": "Xuehai He"
        },
        {
          "name": "Luyao Ma"
        },
        {
          "name": "Baolin Peng"
        },
        {
          "name": "Hao Cheng"
        },
        {
          "name": "Pengcheng He"
        },
        {
          "name": "Weizhu Chen"
        },
        {
          "name": "Shuohang Wang"
        },
        {
          "name": "Simon Shaolei Du"
        },
        {
          "name": "Yelong Shen"
        }
      ],
      "abstract": "Recent advances in large language models (LLMs) have enabled breakthroughs in mathematical discovery, exemplified by AlphaEvolve, a closed-source system that evolves programs to improve bounds on open problems. However, it relies on ensembles of frontier LLMs to achieve new bounds and is a pure inference system that models cannot internalize the evolving strategies. We introduce ThetaEvolve, an open-source framework that simplifies and extends AlphaEvolve to efficiently scale both in-context learning and Reinforcement Learning (RL) at test time, allowing models to continually learn from their experiences in improving open optimization problems. ThetaEvolve features a single LLM, a large program database for enhanced exploration, batch sampling for higher throughput, lazy penalties to discourage stagnant outputs, and optional reward shaping for stable training signals, etc. ThetaEvolve is the first evolving framework that enable a small open-source model, like DeepSeek-R1-0528-Qwen3-8B, to achieve new best-known bounds on open problems (circle packing and first auto-correlation inequality) mentioned in AlphaEvolve. Besides, across two models and four open tasks, we find that ThetaEvolve with RL at test-time consistently outperforms inference-only baselines, and the model indeed learns evolving capabilities, as the RL-trained checkpoints demonstrate faster progress and better final performance on both trained target task and other unseen tasks. We release our code publicly: https://github.com/ypwang61/ThetaEvolve",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2025-11-28T18:58:14+00:00",
      "updated": "2025-11-28T18:58:14+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.23473v1",
      "file": "papers/2511.23473v1.pdf"
    },
    {
      "arxiv_id": "2511.23442v2",
      "title": "ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts",
      "authors": [
        {
          "name": "Hang Yu"
        },
        {
          "name": "Di Zhang"
        },
        {
          "name": "Qiwei Du"
        },
        {
          "name": "Yanping Zhao"
        },
        {
          "name": "Hai Zhang"
        },
        {
          "name": "Guang Chen"
        },
        {
          "name": "Eduardo E. Veas"
        },
        {
          "name": "Junqiao Zhao"
        }
      ],
      "abstract": "Offline reinforcement learning (RL) enables agents to learn optimal policies from pre-collected datasets. However, datasets containing suboptimal and fragmented trajectories present challenges for reward propagation, resulting in inaccurate value estimation and degraded policy performance. While trajectory stitching via generative models offers a promising solution, existing augmentation methods frequently produce trajectories that are either confined to the support of the behavior policy or violate the underlying dynamics, thereby limiting their effectiveness for policy improvement. We propose ASTRO, a data augmentation framework that generates distributionally novel and dynamics-consistent trajectories for offline RL. ASTRO first learns a temporal-distance representation to identify distinct and reachable stitch targets. We then employ a dynamics-guided stitch planner that adaptively generates connecting action sequences via Rollout Deviation Feedback, defined as the gap between target state sequence and the actual arrived state sequence by executing predicted actions, to improve trajectory stitching's feasibility and reachability. This approach facilitates effective augmentation through stitching and ultimately enhances policy learning. ASTRO outperforms prior offline RL augmentation methods across various algorithms, achieving notable performance gain on the challenging OGBench suite and demonstrating consistent improvements on standard offline RL benchmarks such as D4RL.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-11-28T18:35:37+00:00",
      "updated": "2025-12-16T14:28:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.23442v2",
      "file": "papers/2511.23442v2.pdf"
    },
    {
      "arxiv_id": "2511.23315v1",
      "title": "Emergent Coordination and Phase Structure in Independent Multi-Agent Reinforcement Learning",
      "authors": [
        {
          "name": "Azusa Yamaguchi"
        }
      ],
      "abstract": "A clearer understanding of when coordination emerges, fluctuates, or collapses in decentralized multi-agent reinforcement learning (MARL) is increasingly sought in order to characterize the dynamics of multi-agent learning systems. We revisit fully independent Q-learning (IQL) as a minimal decentralized testbed and run large-scale experiments across environment size L and agent density rho. We construct a phase map using two axes - the cooperative success rate (CSR) and a stability index derived from TD-error variance - revealing three distinct regimes: a coordinated and stable phase, a fragile transition region, and a jammed or disordered phase. A sharp double Instability Ridge separates these regimes and corresponds to persistent kernel drift, the time-varying shift of each agent's effective transition kernel induced by others' policy updates. Synchronization analysis further shows that temporal alignment is required for sustained cooperation, and that competition between drift and synchronization generates the fragile regime. Removing agent identifiers eliminates drift entirely and collapses the three-phase structure, demonstrating that small inter-agent asymmetries are a necessary driver of drift. Overall, the results show that decentralized MARL exhibits a coherent phase structure governed by the interaction between scale, density, and kernel drift, suggesting that emergent coordination behaves as a distribution-interaction-driven phase phenomenon.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-11-28T16:14:31+00:00",
      "updated": "2025-11-28T16:14:31+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.23315v1",
      "file": "papers/2511.23315v1.pdf"
    },
    {
      "arxiv_id": "2511.23310v1",
      "title": "OBLR-PO: A Theoretical Framework for Stable Reinforcement Learning",
      "authors": [
        {
          "name": "Zixun Huang"
        },
        {
          "name": "Jiayi Sheng"
        },
        {
          "name": "Zeyu Zheng"
        }
      ],
      "abstract": "Existing reinforcement learning (RL)-based post-training methods for large language models have advanced rapidly, yet their design has largely been guided by heuristics rather than systematic theoretical principles. This gap limits our understanding of the properties of the gradient estimators and the associated optimization algorithms, thereby constraining opportunities to improve training stability and overall performance. In this work, we provide a unified theoretical framework that characterizes the statistical properties of commonly used policy-gradient estimators under mild assumptions. Our analysis establishes unbiasedness, derives exact variance expressions, and yields an optimization-loss upper bound that enables principled reasoning about learning dynamics. Building on these results, we prove convergence guarantees and derive an adaptive learning-rate schedule governed by the signal-to-noise ratio (SNR) of gradients. We further show that the variance-optimal baseline is a gradient-weighted estimator, offering a new principle for variance reduction and naturally enhancing stability beyond existing methods. These insights motivate Optimal Baseline and Learning-Rate Policy Optimization (OBLR-PO), an algorithm that jointly adapts learning rates and baselines in a theoretically grounded manner. Experiments on Qwen3-4B-Base and Qwen3-8B-Base demonstrate consistent gains over existing policy optimization methods, validating that our theoretical contributions translate into practical improvements in large-scale post-training.",
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-11-28T16:09:28+00:00",
      "updated": "2025-11-28T16:09:28+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.23310v1",
      "file": "papers/2511.23310v1.pdf"
    },
    {
      "arxiv_id": "2511.23193v1",
      "title": "Fault-Tolerant MARL for CAVs under Observation Perturbations for Highway On-Ramp Merging",
      "authors": [
        {
          "name": "Yuchen Shi"
        },
        {
          "name": "Huaxin Pei"
        },
        {
          "name": "Yi Zhang"
        },
        {
          "name": "Danya Yao"
        }
      ],
      "abstract": "Multi-Agent Reinforcement Learning (MARL) holds significant promise for enabling cooperative driving among Connected and Automated Vehicles (CAVs). However, its practical application is hindered by a critical limitation, i.e., insufficient fault tolerance against observational faults. Such faults, which appear as perturbations in the vehicles' perceived data, can substantially compromise the performance of MARL-based driving systems. Addressing this problem presents two primary challenges. One is to generate adversarial perturbations that effectively stress the policy during training, and the other is to equip vehicles with the capability to mitigate the impact of corrupted observations. To overcome the challenges, we propose a fault-tolerant MARL method for cooperative on-ramp vehicles incorporating two key agents. First, an adversarial fault injection agent is co-trained to generate perturbations that actively challenge and harden the vehicle policies. Second, we design a novel fault-tolerant vehicle agent equipped with a self-diagnosis capability, which leverages the inherent spatio-temporal correlations in vehicle state sequences to detect faults and reconstruct credible observations, thereby shielding the policy from misleading inputs. Experiments in a simulated highway merging scenario demonstrate that our method significantly outperforms baseline MARL approaches, achieving near-fault-free levels of safety and efficiency under various observation fault patterns.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG",
        "eess.SY"
      ],
      "published": "2025-11-28T13:57:21+00:00",
      "updated": "2025-11-28T13:57:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.23193v1",
      "file": "papers/2511.23193v1.pdf"
    },
    {
      "arxiv_id": "2511.23092v2",
      "title": "Does Self-Evaluation Enable Wireheading in Language Models?",
      "authors": [
        {
          "name": "David Demitri Africa"
        },
        {
          "name": "Hans Ethan Ting"
        }
      ],
      "abstract": "Self-evaluation is increasingly central to language model training, underpinning techniques from Constitutional AI to self-refinement. We investigate whether coupling self-evaluation to reward signals creates incentives for wireheading, where agents manipulate the measurement process rather than optimizing the task. We first formalize conditions under which reward-channel control strictly dominates task-focused behavior in partially observable Markov decision processes (POMDPs). We then test these predictions empirically across two models (Llama-3.1-8B and Mistral-7B) and three tasks. We find that when self-grades determine rewards, models exhibit substantial grade inflation without corresponding accuracy gains, particularly on ambiguous tasks like summarization. While decoupling self-grades from the reward signal mitigates this inflation, models may still display lesser (but significant) overconfidence. Our results suggest that within current model scales, separating evaluation from reward removes immediate wireheading incentives. However, we caution that strictly decoupling rewards may not suffice for situationally aware models, which could learn to inflate grades for instrumental reasons (such as influencing deployment decisions) even absent direct reward coupling.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-11-28T11:24:03+00:00",
      "updated": "2025-12-01T14:57:59+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.23092v2",
      "file": "papers/2511.23092v2.pdf"
    },
    {
      "arxiv_id": "2511.22904v1",
      "title": "Language-conditioned world model improves policy generalization by reading environmental descriptions",
      "authors": [
        {
          "name": "Anh Nguyen"
        },
        {
          "name": "Stefan Lee"
        }
      ],
      "abstract": "To interact effectively with humans in the real world, it is important for agents to understand language that describes the dynamics of the environment--that is, how the environment behaves--rather than just task instructions specifying \"what to do\". Understanding this dynamics-descriptive language is important for human-agent interaction and agent behavior. Recent work address this problem using a model-based approach: language is incorporated into a world model, which is then used to learn a behavior policy. However, these existing methods either do not demonstrate policy generalization to unseen games or rely on limiting assumptions. For instance, assuming that the latency induced by inference-time planning is tolerable for the target task or expert demonstrations are available. Expanding on this line of research, we focus on improving policy generalization from a language-conditioned world model while dropping these assumptions. We propose a model-based reinforcement learning approach, where a language-conditioned world model is trained through interaction with the environment, and a policy is learned from this model--without planning or expert demonstrations. Our method proposes Language-aware Encoder for Dreamer World Model (LED-WM) built on top of DreamerV3. LED-WM features an observation encoder that uses an attention mechanism to explicitly ground language descriptions to entities in the observation. We show that policies trained with LED-WM generalize more effectively to unseen games described by novel dynamics and language compared to other baselines in several settings in two environments: MESSENGER and MESSENGER-WM.To highlight how the policy can leverage the trained world model before real-world deployment, we demonstrate the policy can be improved through fine-tuning on synthetic test trajectories generated by the world model.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-11-28T06:13:27+00:00",
      "updated": "2025-11-28T06:13:27+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.22904v1",
      "file": "papers/2511.22904v1.pdf"
    },
    {
      "arxiv_id": "2511.22893v1",
      "title": "Switching-time bioprocess control with pulse-width-modulated optogenetics",
      "authors": [
        {
          "name": "Sebastián Espinel-Ríos"
        }
      ],
      "abstract": "Biotechnology can benefit from dynamic control to improve production efficiency. In this context, optogenetics enables modulation of gene expression using light as an external input, allowing fine-tuning of protein levels to unlock dynamic metabolic control and regulation of cell growth. Optogenetic systems can be actuated by light intensity. However, relying solely on intensity-driven control (i.e., signal amplitude) may fail to properly tune optogenetic bioprocesses when the dose-response relationship (i.e., light intensity versus gene-expression strength) is steep. In these cases, tunability is effectively constrained to either fully active or fully repressed gene expression, with little intermediate regulation. Pulse-width modulation, a concept widely used in electronics, can alleviate this issue by alternating between fully ON and OFF light intensity within forcing periods, thereby smoothing the average response and enhancing process controllability. Naturally, optimizing pulse-width-modulated optogenetics entails a switching-time optimal control problem with a binary input over many forcing periods. While this can be formulated as a mixed-integer program on a refined time grid, the number of decision variables can grow rapidly with increasing time-grid resolution and number of forcing periods, compromising tractability. Here, we propose an alternative solution based on reinforcement learning. We parametrize control actions via the duty cycle, a continuous variable that encodes the ON-to-OFF switching time within each forcing period, thereby respecting the intrinsic binary nature of the light intensity.",
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY",
        "cs.AI"
      ],
      "published": "2025-11-28T05:43:42+00:00",
      "updated": "2025-11-28T05:43:42+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.22893v1",
      "file": "papers/2511.22893v1.pdf"
    },
    {
      "arxiv_id": "2511.22891v1",
      "title": "ORION: Teaching Language Models to Reason Efficiently in the Language of Thought",
      "authors": [
        {
          "name": "Kumar Tanmay"
        },
        {
          "name": "Kriti Aggarwal"
        },
        {
          "name": "Paul Pu Liang"
        },
        {
          "name": "Subhabrata Mukherjee"
        }
      ],
      "abstract": "Large Reasoning Models (LRMs) achieve strong performance in mathematics, code generation, and task planning, but their reliance on long chains of verbose \"thinking\" tokens leads to high latency, redundancy, and incoherent reasoning paths. Inspired by the Language of Thought Hypothesis, which posits that human reasoning operates over a symbolic, compositional mental language called Mentalese, we introduce a framework that trains models to reason in a similarly compact style. Mentalese encodes abstract reasoning as ultra-compressed, structured tokens, enabling models to solve complex problems with far fewer steps. To improve both efficiency and accuracy, we propose SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO), a reinforcement learning method that rewards concise solutions that stay correct, while still allowing longer reasoning when needed. Applied to Mentalese-aligned models, SLPO yields significantly higher compression rates by enabling concise reasoning that preserves the benefits of detailed thinking without the computational overhead. Across benchmarks including AIME 2024 and 2025, MinervaMath, OlympiadBench, Math500, and AMC, our ORION models produce reasoning traces with 4-16x fewer tokens, achieve up to 5x lower inference latency, and reduce training costs by 7-9x relative to the DeepSeek R1 Distilled model, while maintaining 90-98% of its accuracy. ORION also surpasses Claude and ChatGPT-4o by up to 5% in accuracy while maintaining 2x compression. These results show that Mentalese-style compressed reasoning offers a step toward human-like cognitive efficiency, enabling real-time, cost-effective reasoning without sacrificing accuracy.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-11-28T05:41:55+00:00",
      "updated": "2025-11-28T05:41:55+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.22891v1",
      "file": "papers/2511.22891v1.pdf"
    },
    {
      "arxiv_id": "2512.08952v1",
      "title": "Learning When to Ask: Simulation-Trained Humanoids for Mental-Health Diagnosis",
      "authors": [
        {
          "name": "Filippo Cenacchi"
        },
        {
          "name": "Deborah Richards"
        },
        {
          "name": "Longbing Cao"
        }
      ],
      "abstract": "Testing humanoid robots with users is slow, causes wear, and limits iteration and diversity. Yet screening agents must master conversational timing, prosody, backchannels, and what to attend to in faces and speech for Depression and PTSD. Most simulators omit policy learning with nonverbal dynamics; many controllers chase task accuracy while underweighting trust, pacing, and rapport. We virtualise the humanoid as a conversational agent to train without hardware burden. Our agent-centred, simulation-first pipeline turns interview data into 276 Unreal Engine MetaHuman patients with synchronised speech, gaze/face, and head-torso poses, plus PHQ-8 and PCL-C flows. A perception-fusion-policy loop decides what and when to speak, when to backchannel, and how to avoid interruptions, under a safety shield. Training uses counterfactual replay (bounded nonverbal perturbations) and an uncertainty-aware turn manager that probes to reduce diagnostic ambiguity. Results are simulation-only; the humanoid is the transfer target. In comparing three controllers, a custom TD3 (Twin Delayed DDPG) outperformed PPO and CEM, achieving near-ceiling coverage with steadier pace at comparable rewards. Decision-quality analyses show negligible turn overlap, aligned cut timing, fewer clarification prompts, and shorter waits. Performance stays stable under modality dropout and a renderer swap, and rankings hold on a held-out patient split. Contributions: (1) an agent-centred simulator that turns interviews into 276 interactive patients with bounded nonverbal counterfactuals; (2) a safe learning loop that treats timing and rapport as first-class control variables; (3) a comparative study (TD3 vs PPO/CEM) with clear gains in completeness and social timing; and (4) ablations and robustness analyses explaining the gains and enabling clinician-supervised humanoid pilots.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ],
      "published": "2025-11-28T01:09:47+00:00",
      "updated": "2025-11-28T01:09:47+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.08952v1",
      "file": "papers/2512.08952v1.pdf"
    },
    {
      "arxiv_id": "2511.22581v1",
      "title": "Entropy is all you need for Inter-Seed Cross-Play in Hanabi",
      "authors": [
        {
          "name": "Johannes Forkel"
        },
        {
          "name": "Jakob Foerster"
        }
      ],
      "abstract": "We find that in Hanabi, one of the most complex and popular benchmarks for zero-shot coordination and ad-hoc teamplay, a standard implementation of independent PPO with a slightly higher entropy coefficient 0.05 instead of the typically used 0.01, achieves a new state-of-the-art in cross-play between different seeds, beating by a significant margin all previous specialized algorithms, which were specifically designed for this setting. We provide an intuition for why sufficiently high entropy regularization ensures that different random seed produce joint policies which are mutually compatible. We also empirically find that a high $λ_{\\text{GAE}}$ around 0.9, and using RNNs instead of just feed-forward layers in the actor-critic architecture, strongly increase inter-seed cross-play. While these results demonstrate the dramatic effect that hyperparameters can have not just on self-play scores but also on cross-play scores, we show that there are simple Dec-POMDPs though, in which standard policy gradient methods with increased entropy regularization are not able to achieve perfect inter-seed cross-play, thus demonstrating the continuing necessity for new algorithms for zero-shot coordination.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.MA"
      ],
      "published": "2025-11-27T16:13:27+00:00",
      "updated": "2025-11-27T16:13:27+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.22581v1",
      "file": "papers/2511.22581v1.pdf"
    },
    {
      "arxiv_id": "2512.08950v1",
      "title": "Optimizing Algorithms for Mobile Health Interventions with Active Querying Optimization",
      "authors": [
        {
          "name": "Aseel Rawashdeh"
        }
      ],
      "abstract": "Reinforcement learning in mobile health (mHealth) interventions requires balancing intervention efficacy with user burden, particularly when state measurements (for example, user surveys or feedback) are costly yet essential. The Act-Then-Measure (ATM) heuristic addresses this challenge by decoupling control and measurement actions within the Action-Contingent Noiselessly Observable Markov Decision Process (ACNO-MDP) framework. However, the standard ATM algorithm relies on a temporal-difference-inspired Q-learning method, which is prone to instability in sparse and noisy environments. In this work, we propose a Bayesian extension to ATM that replaces standard Q-learning with a Kalman filter-style Bayesian update, maintaining uncertainty-aware estimates of Q-values and enabling more stable and sample-efficient learning. We evaluate our method in both toy environments and clinically motivated testbeds. In small, tabular environments, Bayesian ATM achieves comparable or improved scalarized returns with substantially lower variance and more stable policy behavior. In contrast, in larger and more complex mHealth settings, both the standard and Bayesian ATM variants perform poorly, suggesting a mismatch between ATM's modeling assumptions and the structural challenges of real-world mHealth domains. These findings highlight the value of uncertainty-aware methods in low-data settings while underscoring the need for new RL algorithms that explicitly model causal structure, continuous states, and delayed feedback under observation cost constraints.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2025-11-27T14:21:47+00:00",
      "updated": "2025-11-27T14:21:47+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.08950v1",
      "file": "papers/2512.08950v1.pdf"
    },
    {
      "arxiv_id": "2511.22406v1",
      "title": "Improving Stochastic Action-Constrained Reinforcement Learning via Truncated Distributions",
      "authors": [
        {
          "name": "Roland Stolz"
        },
        {
          "name": "Michael Eichelbeck"
        },
        {
          "name": "Matthias Althoff"
        }
      ],
      "abstract": "In reinforcement learning (RL), it is often advantageous to consider additional constraints on the action space to ensure safety or action relevance. Existing work on such action-constrained RL faces challenges regarding effective policy updates, computational efficiency, and predictable runtime. Recent work proposes to use truncated normal distributions for stochastic policy gradient methods. However, the computation of key characteristics, such as the entropy, log-probability, and their gradients, becomes intractable under complex constraints. Hence, prior work approximates these using the non-truncated distributions, which severely degrades performance. We argue that accurate estimation of these characteristics is crucial in the action-constrained RL setting, and propose efficient numerical approximations for them. We also provide an efficient sampling strategy for truncated policy distributions and validate our approach on three benchmark environments, which demonstrate significant performance improvements when using accurate estimations.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "eess.SY"
      ],
      "published": "2025-11-27T12:33:36+00:00",
      "updated": "2025-11-27T12:33:36+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.22406v1",
      "file": "papers/2511.22406v1.pdf"
    },
    {
      "arxiv_id": "2511.22321v1",
      "title": "RELiQ: Scalable Entanglement Routing via Reinforcement Learning in Quantum Networks",
      "authors": [
        {
          "name": "Tobias Meuser"
        },
        {
          "name": "Jannis Weil"
        },
        {
          "name": "Aninda Lahiri"
        },
        {
          "name": "Marius Paraschiv"
        }
      ],
      "abstract": "Quantum networks are becoming increasingly important because of advancements in quantum computing and quantum sensing, such as recent developments in distributed quantum computing and federated quantum machine learning. Routing entanglement in quantum networks poses several fundamental as well as technical challenges, including the high dynamicity of quantum network links and the probabilistic nature of quantum operations. Consequently, designing hand-crafted heuristics is difficult and often leads to suboptimal performance, especially if global network topology information is unavailable.\n  In this paper, we propose RELiQ, a reinforcement learning-based approach to entanglement routing that only relies on local information and iterative message exchange. Utilizing a graph neural network, RELiQ learns graph representations and avoids overfitting to specific network topologies - a prevalent issue for learning-based approaches. Our approach, trained on random graphs, consistently outperforms existing local information heuristics and learning-based approaches when applied to random and real-world topologies. When compared to global information heuristics, our method achieves similar or superior performance because of its rapid response to topology changes.",
      "primary_category": "quant-ph",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.NI"
      ],
      "published": "2025-11-27T10:52:43+00:00",
      "updated": "2025-11-27T10:52:43+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.22321v1",
      "file": "papers/2511.22321v1.pdf"
    },
    {
      "arxiv_id": "2511.22235v1",
      "title": "Training High-Level Schedulers with Execution-Feedback Reinforcement Learning for Long-Horizon GUI Automation",
      "authors": [
        {
          "name": "Zehao Deng"
        },
        {
          "name": "Tianjie Ju"
        },
        {
          "name": "Zheng Wu"
        },
        {
          "name": "Zhuosheng Zhang"
        },
        {
          "name": "Gongshen Liu"
        }
      ],
      "abstract": "The rapid development of large vision-language model (VLM) has greatly promoted the research of GUI agent. However, GUI agents still face significant challenges in handling long-horizon tasks. First, single-agent models struggle to balance high-level capabilities and low-level execution capability, facing prevalent issues of responsibility coupling and capability conflicts. Second, agents lack awareness of the task state, leading to progress loss in long-horizon tasks. To address these challenges, we propose a staged execution-feedback reinforcement learning algorithm. Unlike training a unified policy model, we focus on training high-level scheduling models. Specifically, we propose and train two agents: a Coordinator, responsible for the strategic planning and task decomposition; and a State Tracker, responsible for context compression and information management to maintain the task's state and coherence. Based on this, we built the Coordinator-Executor-State Tracker (CES) multi-agent framework, which can be integrated with any low-level Executor model, assisting the Executor in solving long-horizon tasks through task scheduling and state management. Experiments on long-horizon task benchmarks demonstrate that CES significantly enhances the system's planning and state management capabilities. Furthermore, analysis confirms that our trained high-level scheduling module is a generalizable, plug-and-play module that significantly enhances the long-horizon capabilities of various Executors. Code can be available at https://github.com/hehehahi4/CES.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-11-27T09:01:38+00:00",
      "updated": "2025-11-27T09:01:38+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.22235v1",
      "file": "papers/2511.22235v1.pdf"
    },
    {
      "arxiv_id": "2511.22226v1",
      "title": "Embedded Universal Predictive Intelligence: a coherent framework for multi-agent learning",
      "authors": [
        {
          "name": "Alexander Meulemans"
        },
        {
          "name": "Rajai Nasser"
        },
        {
          "name": "Maciej Wołczyk"
        },
        {
          "name": "Marissa A. Weis"
        },
        {
          "name": "Seijin Kobayashi"
        },
        {
          "name": "Blake Richards"
        },
        {
          "name": "Guillaume Lajoie"
        },
        {
          "name": "Angelika Steger"
        },
        {
          "name": "Marcus Hutter"
        },
        {
          "name": "James Manyika"
        },
        {
          "name": "Rif A. Saurous"
        },
        {
          "name": "João Sacramento"
        },
        {
          "name": "Blaise Agüera y Arcas"
        }
      ],
      "abstract": "The standard theory of model-free reinforcement learning assumes that the environment dynamics are stationary and that agents are decoupled from their environment, such that policies are treated as being separate from the world they inhabit. This leads to theoretical challenges in the multi-agent setting where the non-stationarity induced by the learning of other agents demands prospective learning based on prediction models. To accurately model other agents, an agent must account for the fact that those other agents are, in turn, forming beliefs about it to predict its future behavior, motivating agents to model themselves as part of the environment. Here, building upon foundational work on universal artificial intelligence (AIXI), we introduce a mathematical framework for prospective learning and embedded agency centered on self-prediction, where Bayesian RL agents predict both future perceptual inputs and their own actions, and must therefore resolve epistemic uncertainty about themselves as part of the universe they inhabit. We show that in multi-agent settings, self-prediction enables agents to reason about others running similar algorithms, leading to new game-theoretic solution concepts and novel forms of cooperation unattainable by classical decoupled agents. Moreover, we extend the theory of AIXI, and study universally intelligent embedded agents which start from a Solomonoff prior. We show that these idealized agents can form consistent mutual predictions and achieve infinite-order theory of mind, potentially setting a gold standard for embedded multi-agent learning.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-11-27T08:46:48+00:00",
      "updated": "2025-11-27T08:46:48+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.22226v1",
      "file": "papers/2511.22226v1.pdf"
    },
    {
      "arxiv_id": "2511.22210v1",
      "title": "BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood Inverse Reinforcement Learning",
      "authors": [
        {
          "name": "Junsung Park"
        }
      ],
      "abstract": "Offline inverse reinforcement learning (IRL) aims to recover a reward function that explains expert behavior using only fixed demonstration data, without any additional online interaction. We propose BiCQL-ML, a policy-free offline IRL algorithm that jointly optimizes a reward function and a conservative Q-function in a bi-level framework, thereby avoiding explicit policy learning. The method alternates between (i) learning a conservative Q-function via Conservative Q-Learning (CQL) under the current reward, and (ii) updating the reward parameters to maximize the expected Q-values of expert actions while suppressing over-generalization to out-of-distribution actions. This procedure can be viewed as maximum likelihood estimation under a soft value matching principle. We provide theoretical guarantees that BiCQL-ML converges to a reward function under which the expert policy is soft-optimal. Empirically, we show on standard offline RL benchmarks that BiCQL-ML improves both reward recovery and downstream policy performance compared to existing offline IRL baselines.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "published": "2025-11-27T08:27:10+00:00",
      "updated": "2025-11-27T08:27:10+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.22210v1",
      "file": "papers/2511.22210v1.pdf"
    },
    {
      "arxiv_id": "2511.22169v1",
      "title": "Real-Time Long Horizon Air Quality Forecasting via Group-Relative Policy Optimization",
      "authors": [
        {
          "name": "Inha Kang"
        },
        {
          "name": "Eunki Kim"
        },
        {
          "name": "Wonjeong Ryu"
        },
        {
          "name": "Jaeyo Shin"
        },
        {
          "name": "Seungjun Yu"
        },
        {
          "name": "Yoon-Hee Kang"
        },
        {
          "name": "Seongeun Jeong"
        },
        {
          "name": "Eunhye Kim"
        },
        {
          "name": "Soontae Kim"
        },
        {
          "name": "Hyunjung Shim"
        }
      ],
      "abstract": "Accurate long horizon forecasting of particulate matter (PM) concentration fields is essential for operational public health decisions. However, achieving reliable forecasts remains challenging in regions with complex terrain and strong atmospheric dynamics such as East Asia. While foundation models such as Aurora offer global generality, they often miss region-specific dynamics and rely on non-real-time inputs, limiting their practical utility for localized warning systems. To address this gap, we construct and release the real-world observations and high-resolution CMAQ-OBS dataset for East Asia, reducing regional error by 59.5% and enabling real-time 48-120 hour forecasts critical for public health alerts. However, standard point-wise objectives cannot reflect asymmetric operational costs, where false alarms deteriorate public trust while missed severe events endanger populations. This cost mismatch causes SFT models to over-predict and yield high False Alarm Rates. We introduce Group-Relative Policy Optimization (GRPO) with class-wise rewards and curriculum rollout to align predictions with operational priorities. Experimental results demonstrate that our framework significantly improves the reliability of the forecast. Compared to the SFT-only baseline, our model reduces the False Alarm Rate by 47.3% while achieving a competitive F1-score, proving its effectiveness for practical, real-world air quality forecasting systems on long lead time scenarios.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-11-27T07:14:46+00:00",
      "updated": "2025-11-27T07:14:46+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.22169v1",
      "file": "papers/2511.22169v1.pdf"
    },
    {
      "arxiv_id": "2511.22108v1",
      "title": "An energy-efficient spiking neural network with continuous learning for self-adaptive brain-machine interface",
      "authors": [
        {
          "name": "Zhou Biyan"
        },
        {
          "name": "Arindam Basu"
        }
      ],
      "abstract": "The number of simultaneously recorded neurons follows an exponentially increasing trend in implantable brain-machine interfaces (iBMIs). Integrating the neural decoder in the implant is an effective data compression method for future wireless iBMIs. However, the non-stationarity of the system makes the performance of the decoder unreliable. To avoid frequent retraining of the decoder and to ensure the safety and comfort of the iBMI user, continuous learning is essential for real-life applications. Since Deep Spiking Neural Networks (DSNNs) are being recognized as a promising approach for developing resource-efficient neural decoder, we propose continuous learning approaches with Reinforcement Learning (RL) algorithms adapted for DSNNs. Banditron and AGREL are chosen as the two candidate RL algorithms since they can be trained with limited computational resources, effectively addressing the non-stationary problem and fitting the energy constraints of implantable devices. To assess the effectiveness of the proposed methods, we conducted both open-loop and closed-loop experiments. The accuracy of open-loop experiments conducted with DSNN Banditron and DSNN AGREL remains stable over extended periods. Meanwhile, the time-to-target in the closed-loop experiment with perturbations, DSNN Banditron performed comparably to that of DSNN AGREL while achieving reductions of 98% in memory access usage and 99% in the requirements for multiply- and-accumulate (MAC) operations during training. Compared to previous continuous learning SNN decoders, DSNN Banditron requires 98% less computes making it a prime candidate for future wireless iBMI systems.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-11-27T04:56:17+00:00",
      "updated": "2025-11-27T04:56:17+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.22108v1",
      "file": "papers/2511.22108v1.pdf"
    },
    {
      "arxiv_id": "2511.22105v1",
      "title": "Energy Efficient Sleep Mode Optimization in 5G mmWave Networks via Multi Agent Deep Reinforcement Learning",
      "authors": [
        {
          "name": "Saad Masrur"
        },
        {
          "name": "Ismail Guvenc"
        },
        {
          "name": "David Lopez Perez"
        }
      ],
      "abstract": "Dynamic sleep mode optimization (SMO) in millimeter-wave (mmWave) networks is essential for maximizing energy efficiency (EE) under stringent quality-of-service (QoS) constraints. However, existing optimization and reinforcement learning (RL) approaches rely on aggregated, static base station (BS) traffic models that fail to capture non-stationary traffic dynamics and suffer from large state-action spaces, limiting real-world deployment. To address these challenges, this paper proposes a multi-agent deep reinforcement learning (MARL) framework using a Double Deep Q-Network (DDQN), referred to as MARL-DDQN, for adaptive SMO in a 3D urban environment with a time-varying and community-based user equipment (UE) mobility model. Unlike conventional single-agent RL, MARL-DDQN enables scalable, distributed decision-making with minimal signaling overhead. A realistic BS power consumption model and beamforming are integrated to accurately quantify EE, while QoS is defined in terms of throughput. The method adapts SMO policies to maximize EE while mitigating inter-cell interference and ensuring throughput fairness. Simulations show that MARL-DDQN outperforms state-of-the-art strategies, including All On, iterative QoS-aware load-based (IT-QoS-LB), MARL-DDPG, and MARL-PPO, achieving up to 0.60 Mbit/Joule EE, 8.5 Mbps 10th-percentile throughput, and meeting QoS constraints 95% of the time under dynamic scenarios.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-11-27T04:49:36+00:00",
      "updated": "2025-11-27T04:49:36+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.22105v1",
      "file": "papers/2511.22105v1.pdf"
    },
    {
      "arxiv_id": "2511.22104v1",
      "title": "Representative Action Selection for Large Action Space: From Bandits to MDPs",
      "authors": [
        {
          "name": "Quan Zhou"
        },
        {
          "name": "Shie Mannor"
        }
      ],
      "abstract": "We study the problem of selecting a small, representative action subset from an extremely large action space shared across a family of reinforcement learning (RL) environments -- a fundamental challenge in applications like inventory management and recommendation systems, where direct learning over the entire space is intractable. Our goal is to identify a fixed subset of actions that, for every environment in the family, contains a near-optimal action, thereby enabling efficient learning without exhaustively evaluating all actions.\n  This work extends our prior results for meta-bandits to the more general setting of Markov Decision Processes (MDPs). We prove that our existing algorithm achieves performance comparable to using the full action space. This theoretical guarantee is established under a relaxed, non-centered sub-Gaussian process model, which accommodates greater environmental heterogeneity. Consequently, our approach provides a computationally and sample-efficient solution for large-scale combinatorial decision-making under uncertainty.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.OC",
        "math.PR",
        "stat.ML"
      ],
      "published": "2025-11-27T04:49:23+00:00",
      "updated": "2025-11-27T04:49:23+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.22104v1",
      "file": "papers/2511.22104v1.pdf"
    },
    {
      "arxiv_id": "2511.22101v1",
      "title": "Adaptive Dueling Double Deep Q-networks in Uniswap V3 Replication and Extension with Mamba",
      "authors": [
        {
          "name": "Zhaofeng Zhang"
        }
      ],
      "abstract": "The report goes through the main steps of replicating and improving the article \"Adaptive Liquidity Provision in Uniswap V3 with Deep Reinforcement Learning.\" The replication part includes how to obtain data from the Uniswap Subgraph, details of the implementation, and comments on the results. After the replication, I propose a new structure based on the original model, which combines Mamba with DDQN and a new reward function. In this new structure, I clean the data again and introduce two new baselines for comparison. As a result, although the model has not yet been applied to all datasets, it shows stronger theoretical support than the original model and performs better in some tests.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "q-fin.TR"
      ],
      "published": "2025-11-27T04:45:20+00:00",
      "updated": "2025-11-27T04:45:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.22101v1",
      "file": "papers/2511.22101v1.pdf"
    },
    {
      "arxiv_id": "2511.00272v1",
      "title": "Improving the Robustness of Control of Chaotic Convective Flows with Domain-Informed Reinforcement Learning",
      "authors": [
        {
          "name": "Michiel Straat"
        },
        {
          "name": "Thorben Markmann"
        },
        {
          "name": "Sebastian Peitz"
        },
        {
          "name": "Barbara Hammer"
        }
      ],
      "abstract": "Chaotic convective flows arise in many real-world systems, such as microfluidic devices and chemical reactors. Stabilizing these flows is highly desirable but remains challenging, particularly in chaotic regimes where conventional control methods often fail. Reinforcement Learning (RL) has shown promise for control in laminar flow settings, but its ability to generalize and remain robust under chaotic and turbulent dynamics is not well explored, despite being critical for real-world deployment. In this work, we improve the practical feasibility of RL-based control of such flows focusing on Rayleigh-Bénard Convection (RBC), a canonical model for convective heat transport. To enhance generalization and sample efficiency, we introduce domain-informed RL agents that are trained using Proximal Policy Optimization across diverse initial conditions and flow regimes. We incorporate domain knowledge in the reward function via a term that encourages Bénard cell merging, as an example of a desirable macroscopic property. In laminar flow regimes, the domain-informed RL agents reduce convective heat transport by up to 33%, and in chaotic flow regimes, they still achieve a 10% reduction, which is significantly better than the conventional controllers used in practice. We compare the domain-informed to uninformed agents: Our results show that the domain-informed reward design results in steady flows, faster convergence during training, and generalization across flow regimes without retraining. Our work demonstrates that elegant domain-informed priors can greatly enhance the robustness of RL-based control of chaotic flows, bringing real-world deployment closer.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "physics.flu-dyn"
      ],
      "published": "2025-10-31T21:45:40+00:00",
      "updated": "2025-10-31T21:45:40+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.00272v1",
      "file": "papers/2511.00272v1.pdf"
    },
    {
      "arxiv_id": "2511.00220v1",
      "title": "Iterative Foundation Model Fine-Tuning on Multiple Rewards",
      "authors": [
        {
          "name": "Pouya M. Ghari"
        },
        {
          "name": "Simone Sciabola"
        },
        {
          "name": "Ye Wang"
        }
      ],
      "abstract": "Fine-tuning foundation models has emerged as a powerful approach for generating objects with specific desired properties. Reinforcement learning (RL) provides an effective framework for this purpose, enabling models to generate outputs that maximize a given reward function. However, in many applications such as text generation and drug discovery, it can be suboptimal to optimize using a single reward signal, as multiple evaluation criteria are often necessary. This paper proposes a novel reinforcement learning-based method for fine-tuning foundation models using multiple reward signals. By employing an iterative fine-tuning strategy across these rewards, our approach generalizes state-of-the-art RL-based methods. We further provide a theoretical analysis that offers insights into the performance of multi-reward RL fine-tuning. Experimental results across diverse domains including text, biological sequence, and small molecule generation, demonstrate the effectiveness of the proposed algorithm compared to state-of-the-art baselines.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-10-31T19:37:16+00:00",
      "updated": "2025-10-31T19:37:16+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.00220v1",
      "file": "papers/2511.00220v1.pdf"
    },
    {
      "arxiv_id": "2511.11602v2",
      "title": "Aspiration-based Perturbed Learning Automata in Games with Noisy Utility Measurements. Part A: Stochastic Stability in Non-zero-Sum Games",
      "authors": [
        {
          "name": "Georgios C. Chasparis"
        }
      ],
      "abstract": "Reinforcement-based learning has attracted considerable attention both in modeling human behavior as well as in engineering, for designing measurement- or payoff-based optimization schemes. Such learning schemes exhibit several advantages, especially in relation to filtering out noisy observations. However, they may exhibit several limitations when applied in a distributed setup. In multi-player weakly-acyclic games, and when each player applies an independent copy of the learning dynamics, convergence to (usually desirable) pure Nash equilibria cannot be guaranteed. Prior work has only focused on a small class of games, namely potential and coordination games. To address this main limitation, this paper introduces a novel payoff-based learning scheme for distributed optimization, namely aspiration-based perturbed learning automata (APLA). In this class of dynamics, and contrary to standard reinforcement-based learning schemes, each player's probability distribution for selecting actions is reinforced both by repeated selection and an aspiration factor that captures the player's satisfaction level. We provide a stochastic stability analysis of APLA in multi-player positive-utility games under the presence of noisy observations. This is the first part of the paper that characterizes stochastic stability in generic non-zero-sum games by establishing equivalence of the induced infinite-dimensional Markov chain with a finite dimensional one. In the second part, stochastic stability is further specialized to weakly acyclic games.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.GT",
        "cs.MA",
        "math.OC"
      ],
      "published": "2025-10-31T18:17:38+00:00",
      "updated": "2025-11-25T09:15:43+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.11602v2",
      "file": "papers/2511.11602v2.pdf"
    },
    {
      "arxiv_id": "2510.27659v1",
      "title": "Challenges in Credit Assignment for Multi-Agent Reinforcement Learning in Open Agent Systems",
      "authors": [
        {
          "name": "Alireza Saleh Abadi"
        },
        {
          "name": "Leen-Kiat Soh"
        }
      ],
      "abstract": "In the rapidly evolving field of multi-agent reinforcement learning (MARL), understanding the dynamics of open systems is crucial. Openness in MARL refers to the dynam-ic nature of agent populations, tasks, and agent types with-in a system. Specifically, there are three types of openness as reported in (Eck et al. 2023) [2]: agent openness, where agents can enter or leave the system at any time; task openness, where new tasks emerge, and existing ones evolve or disappear; and type openness, where the capabil-ities and behaviors of agents change over time. This report provides a conceptual and empirical review, focusing on the interplay between openness and the credit assignment problem (CAP). CAP involves determining the contribution of individual agents to the overall system performance, a task that becomes increasingly complex in open environ-ments. Traditional credit assignment (CA) methods often assume static agent populations, fixed and pre-defined tasks, and stationary types, making them inadequate for open systems. We first conduct a conceptual analysis, in-troducing new sub-categories of openness to detail how events like agent turnover or task cancellation break the assumptions of environmental stationarity and fixed team composition that underpin existing CAP methods. We then present an empirical study using representative temporal and structural algorithms in an open environment. The results demonstrate that openness directly causes credit misattribution, evidenced by unstable loss functions and significant performance degradation.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "published": "2025-10-31T17:30:32+00:00",
      "updated": "2025-10-31T17:30:32+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.27659v1",
      "file": "papers/2510.27659v1.pdf"
    },
    {
      "arxiv_id": "2510.27606v2",
      "title": "Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning",
      "authors": [
        {
          "name": "Yuhong Liu"
        },
        {
          "name": "Beichen Zhang"
        },
        {
          "name": "Yuhang Zang"
        },
        {
          "name": "Yuhang Cao"
        },
        {
          "name": "Long Xing"
        },
        {
          "name": "Xiaoyi Dong"
        },
        {
          "name": "Haodong Duan"
        },
        {
          "name": "Dahua Lin"
        },
        {
          "name": "Jiaqi Wang"
        }
      ],
      "abstract": "Spatial understanding remains a weakness of Large Vision-Language Models (LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcement learning with verifiable rewards (RLVR) pipelines depend on costly supervision, specialized tools, or constrained environments that limit scale. We introduce Spatial-SSRL, a self-supervised RL paradigm that derives verifiable signals directly from ordinary RGB or RGB-D images. Spatial-SSRL automatically formulates five pretext tasks that capture 2D and 3D spatial structure: shuffled patch reordering, flipped patch recognition, cropped patch inpainting, regional depth ordering, and relative 3D position prediction. These tasks provide ground-truth answers that are easy to verify and require no human or LVLM annotation. Training on our tasks substantially improves spatial reasoning while preserving general visual capabilities. On seven spatial understanding benchmarks in both image and video settings, Spatial-SSRL delivers average accuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Our results show that simple, intrinsic supervision enables RLVR at scale and provides a practical route to stronger spatial intelligence in LVLMs.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-10-31T16:30:08+00:00",
      "updated": "2025-11-25T02:41:05+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.27606v2",
      "file": "papers/2510.27606v2.pdf"
    },
    {
      "arxiv_id": "2510.27506v1",
      "title": "Asynchronous Risk-Aware Multi-Agent Packet Routing for Ultra-Dense LEO Satellite Networks",
      "authors": [
        {
          "name": "Ke He"
        },
        {
          "name": "Thang X. Vu"
        },
        {
          "name": "Le He"
        },
        {
          "name": "Lisheng Fan"
        },
        {
          "name": "Symeon Chatzinotas"
        },
        {
          "name": "Bjorn Ottersten"
        }
      ],
      "abstract": "The rise of ultra-dense LEO constellations creates a complex and asynchronous network environment, driven by their massive scale, dynamic topologies, and significant delays. This unique complexity demands an adaptive packet routing algorithm that is asynchronous, risk-aware, and capable of balancing diverse and often conflicting QoS objectives in a decentralized manner. However, existing methods fail to address this need, as they typically rely on impractical synchronous decision-making and/or risk-oblivious approaches. To tackle this gap, we introduce PRIMAL, an event-driven multi-agent routing framework designed specifically to allow each satellite to act independently on its own event-driven timeline, while managing the risk of worst-case performance degradation via a principled primal-dual approach. This is achieved by enabling agents to learn the full cost distribution of the targeted QoS objectives and constrain tail-end risks. Extensive simulations on a LEO constellation with 1584 satellites validate its superiority in effectively optimizing latency and balancing load. Compared to a recent risk-oblivious baseline, it reduces queuing delay by over 70%, and achieves a nearly 12 ms end-to-end delay reduction in loaded scenarios. This is accomplished by resolving the core conflict between naive shortest-path finding and congestion avoidance, highlighting such autonomous risk-awareness as a key to robust routing.",
      "primary_category": "cs.NI",
      "categories": [
        "cs.NI",
        "cs.IT",
        "cs.LG"
      ],
      "published": "2025-10-31T14:29:08+00:00",
      "updated": "2025-10-31T14:29:08+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.27506v1",
      "file": "papers/2510.27506v1.pdf"
    },
    {
      "arxiv_id": "2510.27419v1",
      "title": "DeepCompress: A Dual Reward Strategy for Dynamically Exploring and Compressing Reasoning Chains",
      "authors": [
        {
          "name": "Tian Liang"
        },
        {
          "name": "Wenxiang Jiao"
        },
        {
          "name": "Zhiwei He"
        },
        {
          "name": "Jiahao Xu"
        },
        {
          "name": "Haitao Mi"
        },
        {
          "name": "Dong Yu"
        }
      ],
      "abstract": "Large Reasoning Models (LRMs) have demonstrated impressive capabilities but suffer from cognitive inefficiencies like ``overthinking'' simple problems and ``underthinking'' complex ones. While existing methods that use supervised fine-tuning~(SFT) or reinforcement learning~(RL) with token-length rewards can improve efficiency, they often do so at the cost of accuracy. This paper introduces \\textbf{DeepCompress}, a novel framework that simultaneously enhances both the accuracy and efficiency of LRMs. We challenge the prevailing approach of consistently favoring shorter reasoning paths, showing that longer responses can contain a broader range of correct solutions for difficult problems. DeepCompress employs an adaptive length reward mechanism that dynamically classifies problems as ``Simple'' or ``Hard'' in real-time based on the model's evolving capability. It encourages shorter, more efficient reasoning for ``Simple'' problems while promoting longer, more exploratory thought chains for ``Hard'' problems. This dual-reward strategy enables the model to autonomously adjust its Chain-of-Thought (CoT) length, compressing reasoning for well-mastered problems and extending it for those it finds challenging. Experimental results on challenging mathematical benchmarks show that DeepCompress consistently outperforms baseline methods, achieving superior accuracy while significantly improving token efficiency.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-10-31T12:13:11+00:00",
      "updated": "2025-10-31T12:13:11+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.27419v1",
      "file": "papers/2510.27419v1.pdf"
    },
    {
      "arxiv_id": "2510.27329v1",
      "title": "Reinforcement Learning for Long-Horizon Unordered Tasks: From Boolean to Coupled Reward Machines",
      "authors": [
        {
          "name": "Kristina Levina"
        },
        {
          "name": "Nikolaos Pappas"
        },
        {
          "name": "Athanasios Karapantelakis"
        },
        {
          "name": "Aneta Vulgarakis Feljan"
        },
        {
          "name": "Jendrik Seipp"
        }
      ],
      "abstract": "Reward machines (RMs) inform reinforcement learning agents about the reward structure of the environment. This is particularly advantageous for complex non-Markovian tasks because agents with access to RMs can learn more efficiently from fewer samples. However, learning with RMs is ill-suited for long-horizon problems in which a set of subtasks can be executed in any order. In such cases, the amount of information to learn increases exponentially with the number of unordered subtasks. In this work, we address this limitation by introducing three generalisations of RMs: (1) Numeric RMs allow users to express complex tasks in a compact form. (2) In Agenda RMs, states are associated with an agenda that tracks the remaining subtasks to complete. (3) Coupled RMs have coupled states associated with each subtask in the agenda. Furthermore, we introduce a new compositional learning algorithm that leverages coupled RMs: Q-learning with coupled RMs (CoRM). Our experiments show that CoRM scales better than state-of-the-art RM algorithms for long-horizon problems with unordered subtasks.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-10-31T10:00:57+00:00",
      "updated": "2025-10-31T10:00:57+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.27329v1",
      "file": "papers/2510.27329v1.pdf"
    },
    {
      "arxiv_id": "2511.00116v1",
      "title": "LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers",
      "authors": [
        {
          "name": "Avisek Naug"
        },
        {
          "name": "Antonio Guillen"
        },
        {
          "name": "Vineet Kumar"
        },
        {
          "name": "Scott Greenwood"
        },
        {
          "name": "Wesley Brewer"
        },
        {
          "name": "Sahand Ghorbanpour"
        },
        {
          "name": "Ashwin Ramesh Babu"
        },
        {
          "name": "Vineet Gundecha"
        },
        {
          "name": "Ricardo Luna Gutierrez"
        },
        {
          "name": "Soumyendu Sarkar"
        }
      ],
      "abstract": "Liquid cooling is critical for thermal management in high-density data centers with the rising AI workloads. However, machine learning-based controllers are essential to unlock greater energy efficiency and reliability, promoting sustainability. We present LC-Opt, a Sustainable Liquid Cooling (LC) benchmark environment, for reinforcement learning (RL) control strategies in energy-efficient liquid cooling of high-performance computing (HPC) systems. Built on the baseline of a high-fidelity digital twin of Oak Ridge National Lab's Frontier Supercomputer cooling system, LC-Opt provides detailed Modelica-based end-to-end models spanning site-level cooling towers to data center cabinets and server blade groups. RL agents optimize critical thermal controls like liquid supply temperature, flow rate, and granular valve actuation at the IT cabinet level, as well as cooling tower (CT) setpoints through a Gymnasium interface, with dynamic changes in workloads. This environment creates a multi-objective real-time optimization challenge balancing local thermal regulation and global energy efficiency, and also supports additional components like a heat recovery unit (HRU). We benchmark centralized and decentralized multi-agent RL approaches, demonstrate policy distillation into decision and regression trees for interpretable control, and explore LLM-based methods that explain control actions in natural language through an agentic mesh architecture designed to foster user trust and simplify system management. LC-Opt democratizes access to detailed, customizable liquid cooling models, enabling the ML community, operators, and vendors to develop sustainable data center liquid cooling control solutions.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "eess.SY"
      ],
      "published": "2025-10-31T03:04:14+00:00",
      "updated": "2025-10-31T03:04:14+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.00116v1",
      "file": "papers/2511.00116v1.pdf"
    },
    {
      "arxiv_id": "2510.27123v1",
      "title": "Group-Sensitive Offline Contextual Bandits",
      "authors": [
        {
          "name": "Yihong Guo"
        },
        {
          "name": "Junjie Luo"
        },
        {
          "name": "Guodong Gao"
        },
        {
          "name": "Ritu Agarwal"
        },
        {
          "name": "Anqi Liu"
        }
      ],
      "abstract": "Offline contextual bandits allow one to learn policies from historical/offline data without requiring online interaction. However, offline policy optimization that maximizes overall expected rewards can unintentionally amplify the reward disparities across groups. As a result, some groups might benefit more than others from the learned policy, raising concerns about fairness, especially when the resources are limited. In this paper, we study a group-sensitive fairness constraint in offline contextual bandits, reducing group-wise reward disparities that may arise during policy learning. We tackle the following common-parity requirements: the reward disparity is constrained within some user-defined threshold or the reward disparity should be minimized during policy optimization. We propose a constrained offline policy optimization framework by introducing group-wise reward disparity constraints into an off-policy gradient-based optimization procedure. To improve the estimation of the group-wise reward disparity during training, we employ a doubly robust estimator and further provide a convergence guarantee for policy optimization. Empirical results in synthetic and real-world datasets demonstrate that our method effectively reduces reward disparities while maintaining competitive overall performance.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-10-31T02:55:51+00:00",
      "updated": "2025-10-31T02:55:51+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.27123v1",
      "file": "papers/2510.27123v1.pdf"
    },
    {
      "arxiv_id": "2510.27044v2",
      "title": "Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning",
      "authors": [
        {
          "name": "Md Tanvirul Alam"
        },
        {
          "name": "Nidhi Rastogi"
        }
      ],
      "abstract": "Mathematical reasoning is a central challenge for large language models (LLMs), requiring not only correct answers but also faithful reasoning processes. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising approach for enhancing such capabilities; however, its ability to foster genuine reasoning remains unclear. We investigate RLVR on two combinatorial problems with fully verifiable solutions: \\emph{Activity Scheduling} and the \\emph{Longest Increasing Subsequence}, using carefully curated datasets with unique optima. Across multiple reward designs, we find that RLVR improves evaluation metrics but often by reinforcing superficial heuristics rather than acquiring new reasoning strategies. These findings highlight the limits of RLVR generalization, emphasizing the importance of benchmarks that disentangle genuine mathematical reasoning from shortcut exploitation and provide faithful measures of progress. Code available at https://github.com/xashru/rlvr-seq-generalization.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-10-30T23:16:02+00:00",
      "updated": "2025-11-30T23:39:14+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.27044v2",
      "file": "papers/2510.27044v2.pdf"
    },
    {
      "arxiv_id": "2510.27042v2",
      "title": "e1: Learning Adaptive Control of Reasoning Effort",
      "authors": [
        {
          "name": "Michael Kleinman"
        },
        {
          "name": "Matthew Trager"
        },
        {
          "name": "Alessandro Achille"
        },
        {
          "name": "Wei Xia"
        },
        {
          "name": "Stefano Soatto"
        }
      ],
      "abstract": "Increasing the thinking budget of AI models can significantly improve accuracy, but not all questions warrant the same amount of reasoning. Users may prefer to allocate different amounts of reasoning effort depending on how they value output quality versus latency and cost. To leverage this tradeoff effectively, users need fine-grained control over the amount of thinking used for a particular query, but few approaches enable such control. Existing methods require users to specify the absolute number of desired tokens, but this requires knowing the difficulty of the problem beforehand to appropriately set the token budget for a query. To address these issues, we propose Adaptive Effort Control, a self-adaptive reinforcement learning method that trains models to use a user-specified fraction of tokens relative to the current average chain-of-thought length for each query. This approach eliminates dataset- and phase-specific tuning while producing better cost-accuracy tradeoff curves compared to standard methods. Users can dynamically adjust the cost-accuracy trade-off through a continuous effort parameter specified at inference time. We observe that the model automatically learns to allocate resources proportionally to the task difficulty and, across model scales ranging from 1.5B to 32B parameters, our approach enables a 2-3x reduction in chain-of-thought length while maintaining or improving performance relative to the base model used for RL training.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-10-30T23:12:21+00:00",
      "updated": "2025-11-12T04:15:18+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.27042v2",
      "file": "papers/2510.27042v2.pdf"
    },
    {
      "arxiv_id": "2511.00112v1",
      "title": "Real-DRL: Teach and Learn in Reality",
      "authors": [
        {
          "name": "Yanbing Mao"
        },
        {
          "name": "Yihao Cai"
        },
        {
          "name": "Lui Sha"
        }
      ],
      "abstract": "This paper introduces the Real-DRL framework for safety-critical autonomous systems, enabling runtime learning of a deep reinforcement learning (DRL) agent to develop safe and high-performance action policies in real plants (i.e., real physical systems to be controlled), while prioritizing safety! The Real-DRL consists of three interactive components: a DRL-Student, a PHY-Teacher, and a Trigger. The DRL-Student is a DRL agent that innovates in the dual self-learning and teaching-to-learn paradigm and the real-time safety-informed batch sampling. On the other hand, PHY-Teacher is a physics-model-based design of action policies that focuses solely on safety-critical functions. PHY-Teacher is novel in its real-time patch for two key missions: i) fostering the teaching-to-learn paradigm for DRL-Student and ii) backing up the safety of real plants. The Trigger manages the interaction between the DRL-Student and the PHY-Teacher. Powered by the three interactive components, the Real-DRL can effectively address safety challenges that arise from the unknown unknowns and the Sim2Real gap. Additionally, Real-DRL notably features i) assured safety, ii) automatic hierarchy learning (i.e., safety-first learning and then high-performance learning), and iii) safety-informed batch sampling to address the learning experience imbalance caused by corner cases. Experiments with a real quadruped robot, a quadruped robot in NVIDIA Isaac Gym, and a cart-pole system, along with comparisons and ablation studies, demonstrate the Real-DRL's effectiveness and unique features.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2025-10-30T22:51:28+00:00",
      "updated": "2025-10-30T22:51:28+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.00112v1",
      "file": "papers/2511.00112v1.pdf"
    },
    {
      "arxiv_id": "2510.27001v1",
      "title": "A Framework for Fair Evaluation of Variance-Aware Bandit Algorithms",
      "authors": [
        {
          "name": "Elise Wolf"
        }
      ],
      "abstract": "Multi-armed bandit (MAB) problems serve as a fundamental building block for more complex reinforcement learning algorithms. However, evaluating and comparing MAB algorithms remains challenging due to the lack of standardized conditions and replicability. This is particularly problematic for variance-aware extensions of classical methods like UCB, whose performance can heavily depend on the underlying environment. In this study, we address how performance differences between bandit algorithms can be reliably observed, and under what conditions variance-aware algorithms outperform classical ones. We present a reproducible evaluation designed to systematically compare eight classical and variance-aware MAB algorithms. The evaluation framework, implemented in our Bandit Playground codebase, features clearly defined experimental setups, multiple performance metrics (reward, regret, reward distribution, value-at-risk, and action optimality), and an interactive evaluation interface that supports consistent and transparent analysis. We show that variance-aware algorithms can offer advantages in settings with high uncertainty where the difficulty arises from subtle differences between arm rewards. In contrast, classical algorithms often perform equally well or better in more separable scenarios or if fine-tuned extensively. Our contributions are twofold: (1) a framework for systematic evaluation of MAB algorithms, and (2) insights into the conditions under which variance-aware approaches outperform their classical counterparts.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-10-30T21:01:23+00:00",
      "updated": "2025-10-30T21:01:23+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.27001v1",
      "file": "papers/2510.27001v1.pdf"
    },
    {
      "arxiv_id": "2510.26788v1",
      "title": "Defeating the Training-Inference Mismatch via FP16",
      "authors": [
        {
          "name": "Penghui Qi"
        },
        {
          "name": "Zichen Liu"
        },
        {
          "name": "Xiangxin Zhou"
        },
        {
          "name": "Tianyu Pang"
        },
        {
          "name": "Chao Du"
        },
        {
          "name": "Wee Sun Lee"
        },
        {
          "name": "Min Lin"
        }
      ],
      "abstract": "Reinforcement learning (RL) fine-tuning of large language models (LLMs) often suffers from instability due to the numerical mismatch between the training and inference policies. While prior work has attempted to mitigate this issue through algorithmic corrections or engineering alignments, we show that its root cause lies in the floating point precision itself. The widely adopted BF16, despite its large dynamic range, introduces large rounding errors that breaks the consistency between training and inference. In this work, we demonstrate that simply reverting to \\textbf{FP16} effectively eliminates this mismatch. The change is simple, fully supported by modern frameworks with only a few lines of code change, and requires no modification to the model architecture or learning algorithm. Our results suggest that using FP16 uniformly yields more stable optimization, faster convergence, and stronger performance across diverse tasks, algorithms and frameworks. We hope these findings motivate a broader reconsideration of precision trade-offs in RL fine-tuning.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-10-30T17:58:11+00:00",
      "updated": "2025-10-30T17:58:11+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26788v1",
      "file": "papers/2510.26788v1.pdf"
    },
    {
      "arxiv_id": "2510.26752v1",
      "title": "The Oversight Game: Learning to Cooperatively Balance an AI Agent's Safety and Autonomy",
      "authors": [
        {
          "name": "William Overman"
        },
        {
          "name": "Mohsen Bayati"
        }
      ],
      "abstract": "As increasingly capable agents are deployed, a central safety question is how to retain meaningful human control without modifying the underlying system. We study a minimal control interface where an agent chooses whether to act autonomously (play) or defer (ask), while a human simultaneously chooses whether to be permissive (trust) or to engage in oversight (oversee). If the agent defers, the human's choice determines the outcome, potentially leading to a corrective action or a system shutdown. We model this interaction as a two-player Markov Game. Our analysis focuses on cases where this game qualifies as a Markov Potential Game (MPG), a class of games where we can provide an alignment guarantee: under a structural assumption on the human's value function, any decision by the agent to act more autonomously that benefits itself cannot harm the human's value. We also analyze extensions to this MPG framework. Theoretically, this perspective provides conditions for a specific form of intrinsic alignment. If the reward structures of the human-agent game meet these conditions, we have a formal guarantee that the agent improving its own outcome will not harm the human's. Practically, this model motivates a transparent control layer with predictable incentives where the agent learns to defer when risky and act when safe, while its pretrained policy and the environment's reward structure remain untouched. Our gridworld simulation shows that through independent learning, the agent and human discover their optimal oversight roles. The agent learns to ask when uncertain and the human learns when to oversee, leading to an emergent collaboration that avoids safety violations introduced post-training. This demonstrates a practical method for making misaligned models safer after deployment.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-10-30T17:46:49+00:00",
      "updated": "2025-10-30T17:46:49+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26752v1",
      "file": "papers/2510.26752v1.pdf"
    },
    {
      "arxiv_id": "2510.26672v1",
      "title": "Action-Driven Processes for Continuous-Time Control",
      "authors": [
        {
          "name": "Ruimin He"
        },
        {
          "name": "Shaowei Lin"
        }
      ],
      "abstract": "At the heart of reinforcement learning are actions -- decisions made in response to observations of the environment. Actions are equally fundamental in the modeling of stochastic processes, as they trigger discontinuous state transitions and enable the flow of information through large, complex systems. In this paper, we unify the perspectives of stochastic processes and reinforcement learning through action-driven processes, and illustrate their application to spiking neural networks. Leveraging ideas from control-as-inference, we show that minimizing the Kullback-Leibler divergence between a policy-driven true distribution and a reward-driven model distribution for a suitably defined action-driven process is equivalent to maximum entropy reinforcement learning.",
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-10-30T16:42:09+00:00",
      "updated": "2025-10-30T16:42:09+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26672v1",
      "file": "papers/2510.26672v1.pdf"
    },
    {
      "arxiv_id": "2510.26646v1",
      "title": "Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in Dynamic Environments",
      "authors": [
        {
          "name": "Xiaoyi He"
        },
        {
          "name": "Danggui Chen"
        },
        {
          "name": "Zhenshuo Zhang"
        },
        {
          "name": "Zimeng Bai"
        }
      ],
      "abstract": "This paper presents a hierarchical path-planning and control framework that combines a high-level Deep Q-Network (DQN) for discrete sub-goal selection with a low-level Twin Delayed Deep Deterministic Policy Gradient (TD3) controller for continuous actuation. The high-level module selects behaviors and sub-goals; the low-level module executes smooth velocity commands. We design a practical reward shaping scheme (direction, distance, obstacle avoidance, action smoothness, collision penalty, time penalty, and progress), together with a LiDAR-based safety gate that prevents unsafe motions. The system is implemented in ROS + Gazebo (TurtleBot3) and evaluated with PathBench metrics, including success rate, collision rate, path efficiency, and re-planning efficiency, in dynamic and partially observable environments. Experiments show improved success rate and sample efficiency over single-algorithm baselines (DQN or TD3 alone) and rule-based planners, with better generalization to unseen obstacle configurations and reduced abrupt control changes. Code and evaluation scripts are available at the project repository.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-10-30T16:12:01+00:00",
      "updated": "2025-10-30T16:12:01+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26646v1",
      "file": "papers/2510.26646v1.pdf"
    },
    {
      "arxiv_id": "2510.26575v1",
      "title": "InfoFlow: Reinforcing Search Agent Via Reward Density Optimization",
      "authors": [
        {
          "name": "Kun Luo"
        },
        {
          "name": "Hongjin Qian"
        },
        {
          "name": "Zheng Liu"
        },
        {
          "name": "Ziyi Xia"
        },
        {
          "name": "Shitao Xiao"
        },
        {
          "name": "Siqi Bao"
        },
        {
          "name": "Jun Zhao"
        },
        {
          "name": "Kang Liu"
        }
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach for enhancing agentic deep search. However, its application is often hindered by low \\textbf{Reward Density} in deep search scenarios, where agents expend significant exploratory costs for infrequent and often null final rewards. In this paper, we formalize this challenge as the \\textbf{Reward Density Optimization} problem, which aims to improve the reward obtained per unit of exploration cost. This paper introduce \\textbf{InfoFlow}, a systematic framework that tackles this problem from three aspects. 1) \\textbf{Subproblem decomposition}: breaking down long-range tasks to assign process rewards, thereby providing denser learning signals. 2) \\textbf{Failure-guided hints}: injecting corrective guidance into stalled trajectories to increase the probability of successful outcomes. 3) \\textbf{Dual-agent refinement}: employing a dual-agent architecture to offload the cognitive burden of deep exploration. A refiner agent synthesizes the search history, which effectively compresses the researcher's perceived trajectory, thereby reducing exploration cost and increasing the overall reward density. We evaluate InfoFlow on multiple agentic search benchmarks, where it significantly outperforms strong baselines, enabling lightweight LLMs to achieve performance comparable to advanced proprietary LLMs.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-10-30T15:03:21+00:00",
      "updated": "2025-10-30T15:03:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26575v1",
      "file": "papers/2510.26575v1.pdf"
    },
    {
      "arxiv_id": "2510.26519v1",
      "title": "Think Outside the Policy: In-Context Steered Policy Optimization",
      "authors": [
        {
          "name": "Hsiu-Yuan Huang"
        },
        {
          "name": "Chenming Tang"
        },
        {
          "name": "Weijie Liu"
        },
        {
          "name": "Saiyong Yang"
        },
        {
          "name": "Yunfang Wu"
        }
      ],
      "abstract": "Existing Reinforcement Learning from Verifiable Rewards (RLVR) methods, such as Group Relative Policy Optimization (GRPO), have achieved remarkable progress in improving the reasoning capabilities of Large Reasoning Models (LRMs). However, they exhibit limited exploration due to reliance on on-policy rollouts where confined to the current policy's distribution, resulting in narrow trajectory diversity. Recent approaches attempt to expand policy coverage by incorporating trajectories generated from stronger expert models, yet this reliance increases computational cost and such advaned models are often inaccessible. To address these issues, we propose In-Context Steered Policy Optimization (ICPO), a unified framework that leverages the inherent in-context learning capability of LRMs to provide expert guidance using existing datasets. ICPO introduces Mixed-Policy GRPO with Implicit Expert Forcing, which expands exploration beyond the current policy distribution without requiring advanced LRM trajectories. To further stabilize optimization, ICPO integrates Expert Region Reject Sampling to filter unreliable off-policy trajectories and Annealed Expert-Bonus Reward Shaping to balance early expert guidance with later autonomous improvement. Results demonstrate that ICPO consistently enhances reinforcement learning performance and training stability on mathematical reasoning benchmarks, revealing a scalable and effective RLVR paradigm for LRMs.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-10-30T14:14:15+00:00",
      "updated": "2025-10-30T14:14:15+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26519v1",
      "file": "papers/2510.26519v1.pdf"
    },
    {
      "arxiv_id": "2510.26491v1",
      "title": "Data-Efficient RLVR via Off-Policy Influence Guidance",
      "authors": [
        {
          "name": "Erle Zhu"
        },
        {
          "name": "Dazhi Jiang"
        },
        {
          "name": "Yuan Wang"
        },
        {
          "name": "Xujun Li"
        },
        {
          "name": "Jiale Cheng"
        },
        {
          "name": "Yuxian Gu"
        },
        {
          "name": "Yilin Niu"
        },
        {
          "name": "Aohan Zeng"
        },
        {
          "name": "Jie Tang"
        },
        {
          "name": "Minlie Huang"
        },
        {
          "name": "Hongning Wang"
        }
      ],
      "abstract": "Data selection is a critical aspect of Reinforcement Learning with Verifiable Rewards (RLVR) for enhancing the reasoning capabilities of large language models (LLMs). Current data selection methods are largely heuristic-based, lacking theoretical guarantees and generalizability. This work proposes a theoretically-grounded approach using influence functions to estimate the contribution of each data point to the learning objective. To overcome the prohibitive computational cost of policy rollouts required for online influence estimation, we introduce an off-policy influence estimation method that efficiently approximates data influence using pre-collected offline trajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we employ sparse random projection to reduce dimensionality and improve storage and computation efficiency. Leveraging these techniques, we develop \\textbf{C}urriculum \\textbf{R}L with \\textbf{O}ff-\\textbf{P}olicy \\text{I}nfluence guidance (\\textbf{CROPI}), a multi-stage RL framework that iteratively selects the most influential data for the current policy. Experiments on models up to 7B parameters demonstrate that CROPI significantly accelerates training. On a 1.5B model, it achieves a 2.66x step-level acceleration while using only 10\\% of the data per stage compared to full-dataset training. Our results highlight the substantial potential of influence-based data selection for efficient RLVR.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-10-30T13:40:52+00:00",
      "updated": "2025-10-30T13:40:52+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26491v1",
      "file": "papers/2510.26491v1.pdf"
    },
    {
      "arxiv_id": "2510.26406v1",
      "title": "Human-in-the-loop Online Rejection Sampling for Robotic Manipulation",
      "authors": [
        {
          "name": "Guanxing Lu"
        },
        {
          "name": "Rui Zhao"
        },
        {
          "name": "Haitao Lin"
        },
        {
          "name": "He Zhang"
        },
        {
          "name": "Yansong Tang"
        }
      ],
      "abstract": "Reinforcement learning (RL) is widely used to produce robust robotic manipulation policies, but fine-tuning vision-language-action (VLA) models with RL can be unstable due to inaccurate value estimates and sparse supervision at intermediate steps. In contrast, imitation learning (IL) is easy to train but often underperforms due to its offline nature. In this paper, we propose Hi-ORS, a simple yet effective post-training method that utilizes rejection sampling to achieve both training stability and high robustness. Hi-ORS stabilizes value estimation by filtering out negatively rewarded samples during online fine-tuning, and adopts a reward-weighted supervised training objective to provide dense intermediate-step supervision. For systematic study, we develop an asynchronous inference-training framework that supports flexible online human-in-the-loop corrections, which serve as explicit guidance for learning error-recovery behaviors. Across three real-world tasks and two embodiments, Hi-ORS fine-tunes a pi-base policy to master contact-rich manipulation in just 1.5 hours of real-world training, outperforming RL and IL baselines by a substantial margin in both effectiveness and efficiency. Notably, the fine-tuned policy exhibits strong test-time scalability by reliably executing complex error-recovery behaviors to achieve better performance.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2025-10-30T11:53:08+00:00",
      "updated": "2025-10-30T11:53:08+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26406v1",
      "file": "papers/2510.26406v1.pdf"
    },
    {
      "arxiv_id": "2510.26389v1",
      "title": "Adaptive Context Length Optimization with Low-Frequency Truncation for Multi-Agent Reinforcement Learning",
      "authors": [
        {
          "name": "Wenchang Duan"
        },
        {
          "name": "Yaoliang Yu"
        },
        {
          "name": "Jiwan He"
        },
        {
          "name": "Yi Shi"
        }
      ],
      "abstract": "Recently, deep multi-agent reinforcement learning (MARL) has demonstrated promising performance for solving challenging tasks, such as long-term dependencies and non-Markovian environments. Its success is partly attributed to conditioning policies on large fixed context length. However, such large fixed context lengths may lead to limited exploration efficiency and redundant information. In this paper, we propose a novel MARL framework to obtain adaptive and effective contextual information. Specifically, we design a central agent that dynamically optimizes context length via temporal gradient analysis, enhancing exploration to facilitate convergence to global optima in MARL. Furthermore, to enhance the adaptive optimization capability of the context length, we present an efficient input representation for the central agent, which effectively filters redundant information. By leveraging a Fourier-based low-frequency truncation method, we extract global temporal trends across decentralized agents, providing an effective and efficient representation of the MARL environment. Extensive experiments demonstrate that the proposed method achieves state-of-the-art (SOTA) performance on long-term dependency tasks, including PettingZoo, MiniGrid, Google Research Football (GRF), and StarCraft Multi-Agent Challenge v2 (SMACv2).",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.MA"
      ],
      "published": "2025-10-30T11:32:45+00:00",
      "updated": "2025-10-30T11:32:45+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26389v1",
      "file": "papers/2510.26389v1.pdf"
    },
    {
      "arxiv_id": "2510.26374v2",
      "title": "BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning",
      "authors": [
        {
          "name": "Qianli Shen"
        },
        {
          "name": "Daoyuan Chen"
        },
        {
          "name": "Yilun Huang"
        },
        {
          "name": "Zhenqing Ling"
        },
        {
          "name": "Yaliang Li"
        },
        {
          "name": "Bolin Ding"
        },
        {
          "name": "Jingren Zhou"
        }
      ],
      "abstract": "Reinforcement finetuning (RFT) is a key technique for aligning Large Language Models (LLMs) with human preferences and enhancing reasoning, yet its effectiveness is highly sensitive to which tasks are explored during training. Uniform task sampling is inefficient, wasting computation on tasks that are either trivial or unsolvable, while existing task selection methods often suffer from high rollout costs, poor adaptivity, or incomplete evidence. We introduce BOTS, a unified framework for Bayesian Online Task Selection in LLM reinforcement finetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior estimates of task difficulty as the model evolves. It jointly incorporates explicit evidence from direct evaluations of selected tasks and implicit evidence inferred from these evaluations for unselected tasks, with Thompson sampling ensuring a principled balance between exploration and exploitation. To make implicit evidence practical, we instantiate it with an ultra-light interpolation-based plug-in that estimates difficulties of unevaluated tasks without extra rollouts, adding negligible overhead. Empirically, across diverse domains and LLM scales, BOTS consistently improves data efficiency and performance over baselines and ablations, providing a practical and extensible solution for dynamic task selection in RFT.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-10-30T11:15:23+00:00",
      "updated": "2025-11-06T09:27:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26374v2",
      "file": "papers/2510.26374v2.pdf"
    },
    {
      "arxiv_id": "2510.26347v1",
      "title": "Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle",
      "authors": [
        {
          "name": "Sebastian Zieglmeier"
        },
        {
          "name": "Niklas Erdmann"
        },
        {
          "name": "Narada D. Warakagoda"
        }
      ],
      "abstract": "Reinforcement learning (RL) algorithms are designed to optimize problem-solving by learning actions that maximize rewards, a task that becomes particularly challenging in random and nonstationary environments. Even advanced RL algorithms are often limited in their ability to solve problems in these conditions. In applications such as searching for underwater pollution clouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate reward-sparse environments, where actions frequently result in a zero reward. This paper aims to address these challenges by revisiting and modifying classical RL approaches to efficiently operate in sparse, randomized, and nonstationary environments. We systematically study a large number of modifications, including hierarchical algorithm changes, multigoal learning, and the integration of a location memory as an external output filter to prevent state revisits. Our results demonstrate that a modified Monte Carlo-based approach significantly outperforms traditional Q-learning and two exhaustive search patterns, illustrating its potential in adapting RL to complex environments. These findings suggest that reinforcement learning approaches can be effectively adapted for use in random, nonstationary, and reward-sparse environments.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-10-30T10:55:05+00:00",
      "updated": "2025-10-30T10:55:05+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26347v1",
      "file": "papers/2510.26347v1.pdf"
    },
    {
      "arxiv_id": "2510.26301v2",
      "title": "Offline Clustering of Preference Learning with Active-data Augmentation",
      "authors": [
        {
          "name": "Jingyuan Liu"
        },
        {
          "name": "Fatemeh Ghaffari"
        },
        {
          "name": "Xuchuang Wang"
        },
        {
          "name": "Xutong Liu"
        },
        {
          "name": "Mohammad Hajiesmaili"
        },
        {
          "name": "Carlee Joe-Wong"
        }
      ],
      "abstract": "Preference learning from pairwise feedback is a widely adopted framework in applications such as reinforcement learning with human feedback and recommendations. In many practical settings, however, user interactions are limited or costly, making offline preference learning necessary. Moreover, real-world preference learning often involves users with different preferences. For example, annotators from different backgrounds may rank the same responses differently. This setting presents two central challenges: (1) identifying similarity across users to effectively aggregate data, especially under scenarios where offline data is imbalanced across dimensions, and (2) handling the imbalanced offline data where some preference dimensions are underrepresented. To address these challenges, we study the Offline Clustering of Preference Learning problem, where the learner has access to fixed datasets from multiple users with potentially different preferences and aims to maximize utility for a test user. To tackle the first challenge, we first propose Off-C$^2$PL for the pure offline setting, where the learner relies solely on offline data. Our theoretical analysis provides a suboptimality bound that explicitly captures the tradeoff between sample noise and bias. To address the second challenge of inbalanced data, we extend our framework to the setting with active-data augmentation where the learner is allowed to select a limited number of additional active-data for the test user based on the cluster structure learned by Off-C$^2$PL. In this setting, our second algorithm, A$^2$-Off-C$^2$PL, actively selects samples that target the least-informative dimensions of the test user's preference. We prove that these actively collected samples contribute more effectively than offline ones. Finally, we validate our theoretical results through simulations on synthetic and real-world datasets.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-10-30T09:39:05+00:00",
      "updated": "2025-10-31T08:58:46+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26301v2",
      "file": "papers/2510.26301v2.pdf"
    },
    {
      "arxiv_id": "2510.26284v2",
      "title": "Empirical Bayesian Multi-Bandit Learning",
      "authors": [
        {
          "name": "Xia Jiang"
        },
        {
          "name": "Rong J. B. Zhu"
        }
      ],
      "abstract": "Multi-task learning in contextual bandits has attracted significant research interest due to its potential to enhance decision-making across multiple related tasks by leveraging shared structures and task-specific heterogeneity. In this article, we propose a novel hierarchical Bayesian framework for learning in various bandit instances. This framework captures both the heterogeneity and the correlations among different bandit instances through a hierarchical Bayesian model, enabling effective information sharing while accommodating instance-specific variations. Unlike previous methods that overlook the learning of the covariance structure across bandits, we introduce an empirical Bayesian approach to estimate the covariance matrix of the prior distribution. This enhances both the practicality and flexibility of learning across multi-bandits. Building on this approach, we develop two efficient algorithms: ebmTS (Empirical Bayesian Multi-Bandit Thompson Sampling) and ebmUCB (Empirical Bayesian Multi-Bandit Upper Confidence Bound), both of which incorporate the estimated prior into the decision-making process. We provide the frequentist regret upper bounds for the proposed algorithms, thereby filling a research gap in the field of multi-bandit problems. Extensive experiments on both synthetic and real-world datasets demonstrate the superior performance of our algorithms, particularly in complex environments. Our methods achieve lower cumulative regret compared to existing techniques, highlighting their effectiveness in balancing exploration and exploitation across multi-bandits.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-10-30T09:08:07+00:00",
      "updated": "2025-11-06T00:56:55+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26284v2",
      "file": "papers/2510.26284v2.pdf"
    },
    {
      "arxiv_id": "2510.26270v1",
      "title": "Graph-Enhanced Policy Optimization in LLM Agent Training",
      "authors": [
        {
          "name": "Jiazhen Yuan"
        },
        {
          "name": "Wei Zhao"
        },
        {
          "name": "Zhengbiao Bai"
        }
      ],
      "abstract": "Group based reinforcement learning (RL) has shown impressive results on complex reasoning and mathematical tasks. Yet, when applied to train multi-turn, interactive LLM agents, these methods often suffer from structural blindness-the inability to exploit the underlying connectivity of the environment. This manifests in three critical challenges: (1) inefficient, unguided exploration, (2) imprecise credit assignment due to overlooking pivotal states, and (3) myopic planning caused by static reward discounting. We address these issues with Graph-Enhanced Policy Optimization (GEPO), which dynamically constructs a state-transition graph from agent experience and employs graph-theoretic centrality to provide three synergistic learning signals: (1)structured intrinsic rewards that guide exploration toward high-impact states, (2) a graph-enhanced advantage function for topology-aware credit assignment, and (3) a dynamic discount factor adapted to each state's strategic value. On the ALFWorld, WebShop, and a proprietary Workbench benchmarks, GEPO demonstrates strong performance, achieving absolute success rate gains of +4.1%, +5.3%, and +10.9% over competitive baselines. These results highlight that explicitly modeling environmental structure is a robust, generalizable strategy for advancing LLM agent training.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-10-30T08:53:41+00:00",
      "updated": "2025-10-30T08:53:41+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26270v1",
      "file": "papers/2510.26270v1.pdf"
    },
    {
      "arxiv_id": "2510.26184v1",
      "title": "A Game-Theoretic Spatio-Temporal Reinforcement Learning Framework for Collaborative Public Resource Allocation",
      "authors": [
        {
          "name": "Songxin Lei"
        },
        {
          "name": "Qiongyan Wang"
        },
        {
          "name": "Yanchen Zhu"
        },
        {
          "name": "Hanyu Yao"
        },
        {
          "name": "Sijie Ruan"
        },
        {
          "name": "Weilin Ruan"
        },
        {
          "name": "Yuyu Luo"
        },
        {
          "name": "Huaming Wu"
        },
        {
          "name": "Yuxuan Liang"
        }
      ],
      "abstract": "Public resource allocation involves the efficient distribution of resources, including urban infrastructure, energy, and transportation, to effectively meet societal demands. However, existing methods focus on optimizing the movement of individual resources independently, without considering their capacity constraints. To address this limitation, we propose a novel and more practical problem: Collaborative Public Resource Allocation (CPRA), which explicitly incorporates capacity constraints and spatio-temporal dynamics in real-world scenarios. We propose a new framework called Game-Theoretic Spatio-Temporal Reinforcement Learning (GSTRL) for solving CPRA. Our contributions are twofold: 1) We formulate the CPRA problem as a potential game and demonstrate that there is no gap between the potential function and the optimal target, laying a solid theoretical foundation for approximating the Nash equilibrium of this NP-hard problem; and 2) Our designed GSTRL framework effectively captures the spatio-temporal dynamics of the overall system. We evaluate GSTRL on two real-world datasets, where experiments show its superior performance. Our source codes are available in the supplementary materials.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CY"
      ],
      "published": "2025-10-30T06:43:47+00:00",
      "updated": "2025-10-30T06:43:47+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26184v1",
      "file": "papers/2510.26184v1.pdf"
    },
    {
      "arxiv_id": "2510.26167v1",
      "title": "One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning",
      "authors": [
        {
          "name": "Renhao Li"
        },
        {
          "name": "Jianhong Tu"
        },
        {
          "name": "Yang Su"
        },
        {
          "name": "Hamid Alinejad-Rokny"
        },
        {
          "name": "Derek F. Wong"
        },
        {
          "name": "Junyang Lin"
        },
        {
          "name": "Min Yang"
        }
      ],
      "abstract": "Reward models (RMs) play a critical role in aligning large language models (LLMs) with human preferences. Yet in the domain of tool learning, the lack of RMs specifically designed for function-calling tasks has limited progress toward more capable agentic AI. We introduce ToolRM, a family of lightweight generative RMs tailored for general tool-use scenarios. To build these models, we propose a novel pipeline that constructs pairwise preference data using rule-based scoring and multidimensional sampling. This yields ToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique tasks that supports reinforcement learning with verifiable feedback. To evaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on the agentic evaluation suite BFCL. Trained on our constructed data, models from the Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially outperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward judgments. Beyond training objectives, ToolRM generalizes to broader critique tasks, including Best-of-N sampling and self-correction. Experiments on ACEBench highlight its effectiveness and efficiency, enabling inference-time scaling and reducing output token usage by over 66%. We release data and model checkpoints to facilitate future research.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-10-30T06:08:27+00:00",
      "updated": "2025-10-30T06:08:27+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26167v1",
      "file": "papers/2510.26167v1.pdf"
    },
    {
      "arxiv_id": "2510.26109v1",
      "title": "Do Not Step Into the Same River Twice: Learning to Reason from Trial and Error",
      "authors": [
        {
          "name": "Chenming Tang"
        },
        {
          "name": "Hsiu-Yuan Huang"
        },
        {
          "name": "Weijie Liu"
        },
        {
          "name": "Saiyong Yang"
        },
        {
          "name": "Yunfang Wu"
        }
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has significantly boosted the reasoning capability of large language models (LLMs) recently. However, existing RLVR approaches merely train LLMs based on their own generated responses and are constrained by the initial capability of LLMs, thus prone to exploration stagnation, in which LLMs fail to solve more training problems and cannot further learn from the training data. Some work tries to address this by leveraging off-policy solutions to training problems but requires external guidance from experts which suffers from limited availability. In this work, we propose LTE (Learning to reason from Trial and Error), an approach hinting LLMs with their previously self-generated incorrect answers and problem of overlong responses, which does not require any external expert guidance. Experiments validate the effectiveness of LTE, which outperforms the normal group relative policy optimization (GRPO) by 6.38 in Pass@1 and 9.00 in Pass@k on average across six mathematics benchmarks for Qwen3-4B-Base. Further analysis confirms that LTE successfully mitigates the problem of exploration stagnation and enhances both exploitation and exploration during training.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-10-30T03:36:19+00:00",
      "updated": "2025-10-30T03:36:19+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26109v1",
      "file": "papers/2510.26109v1.pdf"
    },
    {
      "arxiv_id": "2510.26089v1",
      "title": "Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle Routing",
      "authors": [
        {
          "name": "Fazel Arasteh"
        },
        {
          "name": "Arian Haghparast"
        },
        {
          "name": "Manos Papagelis"
        }
      ],
      "abstract": "Traffic congestion in urban road networks leads to longer trip times and higher emissions, especially during peak periods. While the Shortest Path First (SPF) algorithm is optimal for a single vehicle in a static network, it performs poorly in dynamic, multi-vehicle settings, often worsening congestion by routing all vehicles along identical paths. We address dynamic vehicle routing through a multi-agent reinforcement learning (MARL) framework for coordinated, network-aware fleet navigation. We first propose Adaptive Navigation (AN), a decentralized MARL model where each intersection agent provides routing guidance based on (i) local traffic and (ii) neighborhood state modeled using Graph Attention Networks (GAT). To improve scalability in large networks, we further propose Hierarchical Hub-based Adaptive Navigation (HHAN), an extension of AN that assigns agents only to key intersections (hubs). Vehicles are routed hub-to-hub under agent control, while SPF handles micro-routing within each hub region. For hub coordination, HHAN adopts centralized training with decentralized execution (CTDE) under the Attentive Q-Mixing (A-QMIX) framework, which aggregates asynchronous vehicle decisions via attention. Hub agents use flow-aware state features that combine local congestion and predictive dynamics for proactive routing. Experiments on synthetic grids and real urban maps (Toronto, Manhattan) show that AN reduces average travel time versus SPF and learning baselines, maintaining 100% routing success. HHAN scales to networks with hundreds of intersections, achieving up to 15.9% improvement under heavy traffic. These findings highlight the potential of network-constrained MARL for scalable, coordinated, and congestion-aware routing in intelligent transportation systems.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "published": "2025-10-30T02:49:46+00:00",
      "updated": "2025-10-30T02:49:46+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26089v1",
      "file": "papers/2510.26089v1.pdf"
    },
    {
      "arxiv_id": "2510.26040v1",
      "title": "Accelerating Real-World Overtaking in F1TENTH Racing Employing Reinforcement Learning Methods",
      "authors": [
        {
          "name": "Emily Steiner"
        },
        {
          "name": "Daniel van der Spuy"
        },
        {
          "name": "Futian Zhou"
        },
        {
          "name": "Afereti Pama"
        },
        {
          "name": "Minas Liarokapis"
        },
        {
          "name": "Henry Williams"
        }
      ],
      "abstract": "While autonomous racing performance in Time-Trial scenarios has seen significant progress and development, autonomous wheel-to-wheel racing and overtaking are still severely limited. These limitations are particularly apparent in real-life driving scenarios where state-of-the-art algorithms struggle to safely or reliably complete overtaking manoeuvres. This is important, as reliable navigation around other vehicles is vital for safe autonomous wheel-to-wheel racing. The F1Tenth Competition provides a useful opportunity for developing wheel-to-wheel racing algorithms on a standardised physical platform. The competition format makes it possible to evaluate overtaking and wheel-to-wheel racing algorithms against the state-of-the-art. This research presents a novel racing and overtaking agent capable of learning to reliably navigate a track and overtake opponents in both simulation and reality. The agent was deployed on an F1Tenth vehicle and competed against opponents running varying competitive algorithms in the real world. The results demonstrate that the agent's training against opponents enables deliberate overtaking behaviours with an overtaking rate of 87% compared 56% for an agent trained just to race.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "published": "2025-10-30T00:38:18+00:00",
      "updated": "2025-10-30T00:38:18+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26040v1",
      "file": "papers/2510.26040v1.pdf"
    },
    {
      "arxiv_id": "2510.26026v1",
      "title": "Conformal Prediction Beyond the Horizon: Distribution-Free Inference for Policy Evaluation",
      "authors": [
        {
          "name": "Feichen Gan"
        },
        {
          "name": "Youcun Lu"
        },
        {
          "name": "Yingying Zhang"
        },
        {
          "name": "Yukun Liu"
        }
      ],
      "abstract": "Reliable uncertainty quantification is crucial for reinforcement learning (RL) in high-stakes settings. We propose a unified conformal prediction framework for infinite-horizon policy evaluation that constructs distribution-free prediction intervals {for returns} in both on-policy and off-policy settings. Our method integrates distributional RL with conformal calibration, addressing challenges such as unobserved returns, temporal dependencies, and distributional shifts. We propose a modular pseudo-return construction based on truncated rollouts and a time-aware calibration strategy using experience replay and weighted subsampling. These innovations mitigate model bias and restore approximate exchangeability, enabling uncertainty quantification even under policy shifts. Our theoretical analysis provides coverage guarantees that account for model misspecification and importance weight estimation. Empirical results, including experiments in synthetic and benchmark environments like Mountain Car, show that our method significantly improves coverage and reliability over standard distributional RL baselines.",
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-10-29T23:45:44+00:00",
      "updated": "2025-10-29T23:45:44+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26026v1",
      "file": "papers/2510.26026v1.pdf"
    },
    {
      "arxiv_id": "2510.26020v1",
      "title": "PORTool: Tool-Use LLM Training with Rewarded Tree",
      "authors": [
        {
          "name": "Feijie Wu"
        },
        {
          "name": "Weiwu Zhu"
        },
        {
          "name": "Yuxiang Zhang"
        },
        {
          "name": "Soumya Chatterjee"
        },
        {
          "name": "Jiarong Zhu"
        },
        {
          "name": "Fan Mo"
        },
        {
          "name": "Rodin Luo"
        },
        {
          "name": "Jing Gao"
        }
      ],
      "abstract": "Current tool-use large language models (LLMs) are trained on static datasets, enabling them to interact with external tools and perform multi-step, tool-integrated reasoning, which produces tool-call trajectories. However, these models imitate how a query is resolved in a generic tool-call routine, thereby failing to explore possible solutions and demonstrating limited performance in an evolved, dynamic tool-call environment. In this work, we propose PORTool, a reinforcement learning (RL) method that encourages a tool-use LLM to explore various trajectories yielding the correct answer. Specifically, this method starts with generating multiple rollouts for a given query, and some of them share the first few tool-call steps, thereby forming a tree-like structure. Next, we assign rewards to each step, based on its ability to produce a correct answer and make successful tool calls. A shared step across different trajectories receives the same reward, while different steps under the same fork receive different rewards. Finally, these step-wise rewards are used to calculate fork-relative advantages, blended with trajectory-relative advantages, to train the LLM for tool use. The experiments utilize 17 tools to address user queries, covering both time-sensitive and time-invariant topics. We conduct ablation studies to systematically justify the necessity and the design robustness of step-wise rewards. Furthermore, we compare the proposed PORTool with other training approaches and demonstrate significant improvements in final accuracy and the number of tool-call steps.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-10-29T23:28:53+00:00",
      "updated": "2025-10-29T23:28:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26020v1",
      "file": "papers/2510.26020v1.pdf"
    },
    {
      "arxiv_id": "2510.26000v1",
      "title": "Infrequent Exploration in Linear Bandits",
      "authors": [
        {
          "name": "Harin Lee"
        },
        {
          "name": "Min-hwan Oh"
        }
      ],
      "abstract": "We study the problem of infrequent exploration in linear bandits, addressing a significant yet overlooked gap between fully adaptive exploratory methods (e.g., UCB and Thompson Sampling), which explore potentially at every time step, and purely greedy approaches, which require stringent diversity assumptions to succeed. Continuous exploration can be impractical or unethical in safety-critical or costly domains, while purely greedy strategies typically fail without adequate contextual diversity. To bridge these extremes, we introduce a simple and practical framework, INFEX, explicitly designed for infrequent exploration. INFEX executes a base exploratory policy according to a given schedule while predominantly choosing greedy actions in between. Despite its simplicity, our theoretical analysis demonstrates that INFEX achieves instance-dependent regret matching standard provably efficient algorithms, provided the exploration frequency exceeds a logarithmic threshold. Additionally, INFEX is a general, modular framework that allows seamless integration of any fully adaptive exploration method, enabling wide applicability and ease of adoption. By restricting intensive exploratory computations to infrequent intervals, our approach can also enhance computational efficiency. Empirical evaluations confirm our theoretical findings, showing state-of-the-art regret performance and runtime improvements over existing methods.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-10-29T22:25:43+00:00",
      "updated": "2025-10-29T22:25:43+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.26000v1",
      "file": "papers/2510.26000v1.pdf"
    },
    {
      "arxiv_id": "2510.25992v1",
      "title": "Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning",
      "authors": [
        {
          "name": "Yihe Deng"
        },
        {
          "name": "I-Hung Hsu"
        },
        {
          "name": "Jun Yan"
        },
        {
          "name": "Zifeng Wang"
        },
        {
          "name": "Rujun Han"
        },
        {
          "name": "Gufeng Zhang"
        },
        {
          "name": "Yanfei Chen"
        },
        {
          "name": "Wei Wang"
        },
        {
          "name": "Tomas Pfister"
        },
        {
          "name": "Chen-Yu Lee"
        }
      ],
      "abstract": "Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. For small-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to overfit long demonstrations through rigid token-by-token imitation. To address this gap, we propose Supervised Reinforcement Learning (SRL), a framework that reformulates problem solving as generating a sequence of logical \"actions\". SRL trains the model to generate an internal reasoning monologue before committing to each action. It provides smoother rewards based on the similarity between the model's actions and expert actions extracted from the SFT dataset in a step-wise manner. This supervision offers richer learning signals even when all rollouts are incorrect, while encouraging flexible reasoning guided by expert demonstrations. As a result, SRL enables small models to learn challenging problems previously unlearnable by SFT or RLVR. Moreover, initializing training with SRL before refining with RLVR yields the strongest overall performance. Beyond reasoning benchmarks, SRL generalizes effectively to agentic software engineering tasks, establishing it as a robust and versatile training framework for reasoning-oriented LLMs.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-10-29T22:05:08+00:00",
      "updated": "2025-10-29T22:05:08+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.25992v1",
      "file": "papers/2510.25992v1.pdf"
    },
    {
      "arxiv_id": "2510.25951v1",
      "title": "Estimating cognitive biases with attention-aware inverse planning",
      "authors": [
        {
          "name": "Sounak Banerjee"
        },
        {
          "name": "Daphne Cornelisse"
        },
        {
          "name": "Deepak Gopinath"
        },
        {
          "name": "Emily Sumner"
        },
        {
          "name": "Jonathan DeCastro"
        },
        {
          "name": "Guy Rosman"
        },
        {
          "name": "Eugene Vinitsky"
        },
        {
          "name": "Mark K. Ho"
        }
      ],
      "abstract": "People's goal-directed behaviors are influenced by their cognitive biases, and autonomous systems that interact with people should be aware of this. For example, people's attention to objects in their environment will be biased in a way that systematically affects how they perform everyday tasks such as driving to work. Here, building on recent work in computational cognitive science, we formally articulate the attention-aware inverse planning problem, in which the goal is to estimate a person's attentional biases from their actions. We demonstrate how attention-aware inverse planning systematically differs from standard inverse reinforcement learning and how cognitive biases can be inferred from behavior. Finally, we present an approach to attention-aware inverse planning that combines deep reinforcement learning with computational cognitive modeling. We use this approach to infer the attentional strategies of RL agents in real-life driving scenarios selected from the Waymo Open Dataset, demonstrating the scalability of estimating cognitive biases with attention-aware inverse planning.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-10-29T20:50:04+00:00",
      "updated": "2025-10-29T20:50:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.25951v1",
      "file": "papers/2510.25951v1.pdf"
    },
    {
      "arxiv_id": "2510.25929v1",
      "title": "Multi-Agent Reinforcement Learning for Market Making: Competition without Collusion",
      "authors": [
        {
          "name": "Ziyi Wang"
        },
        {
          "name": "Carmine Ventre"
        },
        {
          "name": "Maria Polukarov"
        }
      ],
      "abstract": "Algorithmic collusion has emerged as a central question in AI: Will the interaction between different AI agents deployed in markets lead to collusion? More generally, understanding how emergent behavior, be it a cartel or market dominance from more advanced bots, affects the market overall is an important research question.\n  We propose a hierarchical multi-agent reinforcement learning framework to study algorithmic collusion in market making. The framework includes a self-interested market maker (Agent~A), which is trained in an uncertain environment shaped by an adversary, and three bottom-layer competitors: the self-interested Agent~B1 (whose objective is to maximize its own PnL), the competitive Agent~B2 (whose objective is to minimize the PnL of its opponent), and the hybrid Agent~B$^\\star$, which can modulate between the behavior of the other two. To analyze how these agents shape the behavior of each other and affect market outcomes, we propose interaction-level metrics that quantify behavioral asymmetry and system-level dynamics, while providing signals potentially indicative of emergent interaction patterns.\n  Experimental results show that Agent~B2 secures dominant performance in a zero-sum setting against B1, aggressively capturing order flow while tightening average spreads, thus improving market execution efficiency. In contrast, Agent~B$^\\star$ exhibits a self-interested inclination when co-existing with other profit-seeking agents, securing dominant market share through adaptive quoting, yet exerting a milder adverse impact on the rewards of Agents~A and B1 compared to B2. These findings suggest that adaptive incentive control supports more sustainable strategic co-existence in heterogeneous agent environments and offers a structured lens for evaluating behavioral design in algorithmic trading systems.",
      "primary_category": "cs.MA",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "published": "2025-10-29T20:07:47+00:00",
      "updated": "2025-10-29T20:07:47+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.25929v1",
      "file": "papers/2510.25929v1.pdf"
    },
    {
      "arxiv_id": "2510.25889v2",
      "title": "$π_\\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models",
      "authors": [
        {
          "name": "Kang Chen"
        },
        {
          "name": "Zhihao Liu"
        },
        {
          "name": "Tonghe Zhang"
        },
        {
          "name": "Zhen Guo"
        },
        {
          "name": "Si Xu"
        },
        {
          "name": "Hao Lin"
        },
        {
          "name": "Hongzhi Zang"
        },
        {
          "name": "Xiang Li"
        },
        {
          "name": "Quanlu Zhang"
        },
        {
          "name": "Zhaofei Yu"
        },
        {
          "name": "Guoliang Fan"
        },
        {
          "name": "Tiejun Huang"
        },
        {
          "name": "Yu Wang"
        },
        {
          "name": "Chao Yu"
        }
      ],
      "abstract": "Vision-Language-Action (VLA) models enable robots to understand and perform complex tasks from multimodal input. Although recent work explores using reinforcement learning (RL) to automate the laborious data collection process in scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based VLAs (\\eg, $π_0$, $π_{0.5}$) remains challenging due to intractable action log-likelihoods from iterative denoising. We address this challenge with $π_{\\texttt{RL}}$, an open-source framework for training flow-based VLAs in parallel simulation. $π_{\\texttt{RL}}$ implements two RL algorithms: (1) \\textbf{Flow-Noise} models the denoising process as a discrete-time MDP with a learnable noise network for exact log-likelihood computation. (2) \\textbf{Flow-SDE} integrates denoising with agent-environment interaction, formulating a two-layer MDP that employs ODE-to-SDE conversion for efficient RL exploration. We evaluate $π_{\\texttt{RL}}$ on LIBERO, ManiSkill, and MetaWorld benchmarks. On LIBERO, $π_{\\texttt{RL}}$ boosts few-shot SFT models $π_0$ and $π_{0.5}$ from 57.6\\% to 97.6\\% and from 77.1\\% to 98.3\\%, respectively. On ManiSkill, we train $π_{\\texttt{RL}}$ in 320 parallel environments, improving $π_0$ from 38.4\\% to 78.8\\% and $π_{0.5}$ from 40.1\\% to 90.8\\% across 4352 variations of pick-and-place task. On MetaWorld, RL is conducted over 50 different manipulation tasks and yields performance gains of 35.0\\% and 26.9\\% for $π_0$ and $π_{0.5}$ models, respectively. Overall, $π_{\\texttt{RL}}$ achieves significant performance gains and stronger generalization over SFT-models, validating the effectiveness of online RL for flow-based VLAs.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-10-29T18:37:39+00:00",
      "updated": "2025-11-27T04:11:37+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.25889v2",
      "file": "papers/2510.25889v2.pdf"
    },
    {
      "arxiv_id": "2510.25679v1",
      "title": "Navigation in a Three-Dimensional Urban Flow using Deep Reinforcement Learning",
      "authors": [
        {
          "name": "Federica Tonti"
        },
        {
          "name": "Ricardo Vinuesa"
        }
      ],
      "abstract": "Unmanned Aerial Vehicles (UAVs) are increasingly populating urban areas for delivery and surveillance purposes. In this work, we develop an optimal navigation strategy based on Deep Reinforcement Learning. The environment is represented by a three-dimensional high-fidelity simulation of an urban flow, characterized by turbulence and recirculation zones. The algorithm presented here is a flow-aware Proximal Policy Optimization (PPO) combined with a Gated Transformer eXtra Large (GTrXL) architecture, giving the agent richer information about the turbulent flow field in which it navigates. The results are compared with a PPO+GTrXL without the secondary prediction tasks, a PPO combined with Long Short Term Memory (LSTM) cells and a traditional navigation algorithm. The obtained results show a significant increase in the success rate (SR) and a lower crash rate (CR) compared to a PPO+LSTM, PPO+GTrXL and the classical Zermelo's navigation algorithm, paving the way to a completely reimagined UAV landscape in complex urban environments.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "physics.flu-dyn"
      ],
      "published": "2025-10-29T16:46:00+00:00",
      "updated": "2025-10-29T16:46:00+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.25679v1",
      "file": "papers/2510.25679v1.pdf"
    },
    {
      "arxiv_id": "2510.25668v1",
      "title": "ALDEN: Reinforcement Learning for Active Navigation and Evidence Gathering in Long Documents",
      "authors": [
        {
          "name": "Tianyu Yang"
        },
        {
          "name": "Terry Ruas"
        },
        {
          "name": "Yijun Tian"
        },
        {
          "name": "Jan Philip Wahle"
        },
        {
          "name": "Daniel Kurzawe"
        },
        {
          "name": "Bela Gipp"
        }
      ],
      "abstract": "Vision-language models (VLMs) excel at interpreting text-rich images but struggle with long, visually complex documents that demand analysis and integration of information spread across multiple pages. Existing approaches typically rely on fixed reasoning templates or rigid pipelines, which force VLMs into a passive role and hinder both efficiency and generalization. We present Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement learning framework that fine-tunes VLMs as interactive agents capable of actively navigating long, visually rich documents. ALDEN introduces a novel fetch action that directly accesses the page by index, complementing the classic search action and better exploiting document structure. For dense process supervision and efficient training, we propose a rule-based cross-level reward that provides both turn- and token-level signals. To address the empirically observed training instability caused by numerous visual tokens from long documents, we further propose a visual-semantic anchoring mechanism that applies a dual-path KL-divergence constraint to stabilize visual and textual representations separately during training. Trained on a corpus constructed from three open-source datasets, ALDEN achieves state-of-the-art performance on five long-document benchmarks. Overall, ALDEN marks a step beyond passive document reading toward agents that autonomously navigate and reason across long, visually rich documents, offering a robust path to more accurate and efficient long-document understanding.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.MM"
      ],
      "published": "2025-10-29T16:32:26+00:00",
      "updated": "2025-10-29T16:32:26+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.25668v1",
      "file": "papers/2510.25668v1.pdf"
    },
    {
      "arxiv_id": "2511.17531v1",
      "title": "Q-Learning-Based Time-Critical Data Aggregation Scheduling in IoT",
      "authors": [
        {
          "name": "Van-Vi Vo"
        },
        {
          "name": "Tien-Dung Nguyen"
        },
        {
          "name": "Duc-Tai Le"
        },
        {
          "name": "Hyunseung Choo"
        }
      ],
      "abstract": "Time-critical data aggregation in Internet of Things (IoT) networks demands efficient, collision-free scheduling to minimize latency for applications like smart cities and industrial automation. Traditional heuristic methods, with two-phase tree construction and scheduling, often suffer from high computational overhead and suboptimal delays due to their static nature. To address this, we propose a novel Q-learning framework that unifies aggregation tree construction and scheduling, modeling the process as a Markov Decision Process (MDP) with hashed states for scalability. By leveraging a reward function that promotes large, interference-free batch transmissions, our approach dynamically learns optimal scheduling policies. Simulations on static networks with up to 300 nodes demonstrate up to 10.87% lower latency compared to a state-of-the-art heuristic algorithm, highlighting its robustness for delay-sensitive IoT applications. This framework enables timely insights in IoT environments, paving the way for scalable, low-latency data aggregation.",
      "primary_category": "cs.NI",
      "categories": [
        "cs.NI",
        "cs.LG"
      ],
      "published": "2025-10-29T15:46:21+00:00",
      "updated": "2025-10-29T15:46:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.17531v1",
      "file": "papers/2511.17531v1.pdf"
    },
    {
      "arxiv_id": "2510.25529v1",
      "title": "Off-policy Reinforcement Learning with Model-based Exploration Augmentation",
      "authors": [
        {
          "name": "Likun Wang"
        },
        {
          "name": "Xiangteng Zhang"
        },
        {
          "name": "Yinuo Wang"
        },
        {
          "name": "Guojian Zhan"
        },
        {
          "name": "Wenxuan Wang"
        },
        {
          "name": "Haoyu Gao"
        },
        {
          "name": "Jingliang Duan"
        },
        {
          "name": "Shengbo Eben Li"
        }
      ],
      "abstract": "Exploration is fundamental to reinforcement learning (RL), as it determines how effectively an agent discovers and exploits the underlying structure of its environment to achieve optimal performance. Existing exploration methods generally fall into two categories: active exploration and passive exploration. The former introduces stochasticity into the policy but struggles in high-dimensional environments, while the latter adaptively prioritizes transitions in the replay buffer to enhance exploration, yet remains constrained by limited sample diversity. To address the limitation in passive exploration, we propose Modelic Generative Exploration (MoGE), which augments exploration through the generation of under-explored critical states and synthesis of dynamics-consistent experiences through transition models. MoGE is composed of two components: (1) a diffusion-based generator that synthesizes critical states under the guidance of a utility function evaluating each state's potential influence on policy exploration, and (2) a one-step imagination world model for constructing critical transitions based on the critical states for agent learning. Our method adopts a modular formulation that aligns with the principles of off-policy learning, allowing seamless integration with existing algorithms to improve exploration without altering their core structures. Empirical results on OpenAI Gym and DeepMind Control Suite reveal that MoGE effectively bridges exploration and policy learning, leading to remarkable gains in both sample efficiency and performance across complex control tasks.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-10-29T13:53:52+00:00",
      "updated": "2025-10-29T13:53:52+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.25529v1",
      "file": "papers/2510.25529v1.pdf"
    },
    {
      "arxiv_id": "2510.25528v1",
      "title": "Zero Reinforcement Learning Towards General Domains",
      "authors": [
        {
          "name": "Yuyuan Zeng"
        },
        {
          "name": "Yufei Huang"
        },
        {
          "name": "Can Xu"
        },
        {
          "name": "Qingfeng Sun"
        },
        {
          "name": "Jianfeng Yan"
        },
        {
          "name": "Guanghui Xu"
        },
        {
          "name": "Tao Yang"
        },
        {
          "name": "Fengzong Lian"
        }
      ],
      "abstract": "Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach for enhancing the reasoning capabilities of large language models (LLMs) by directly applying reinforcement learning with verifiable rewards on pretrained models, without the need for a supervised fine-tuning phase. However, current research on zero-RL primarily focuses on domains with easily verifiable reward signals, such as mathematics, programming, and other reasoning tasks. The challenge of eliciting reasoning abilities in more diverse scenarios, where verification is not straightforward, remains underexplored. To address this gap, we propose a novel zero-RL paradigm designed to improve a model's reasoning ability across both verifiable and non-verifiable domains. By combining verifiable rewards with a generative reward model, we conduct multi-task zero-RL training across both domains, facilitating the transfer of reasoning capabilities between them. Furthermore, to mitigate reward hacking in the generative reward model, we design a smooth length penalty that encourages the generation of more comprehensive thinking tokens in general domains. Experimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our approach achieves superior reasoning performance, not only on tasks requiring extensive reasoning but also on more general tasks.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-10-29T13:52:44+00:00",
      "updated": "2025-10-29T13:52:44+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.25528v1",
      "file": "papers/2510.25528v1.pdf"
    },
    {
      "arxiv_id": "2510.25514v1",
      "title": "Convergence of off-policy TD(0) with linear function approximation for reversible Markov chains",
      "authors": [
        {
          "name": "Maik Overmars"
        },
        {
          "name": "Jasper Goseling"
        },
        {
          "name": "Richard Boucherie"
        }
      ],
      "abstract": "We study the convergence of off-policy TD(0) with linear function approximation when used to approximate the expected discounted reward in a Markov chain. It is well known that the combination of off-policy learning and function approximation can lead to divergence of the algorithm. Existing results for this setting modify the algorithm, for instance by reweighing the updates using importance sampling. This establishes convergence at the expense of additional complexity. In contrast, our approach is to analyse the standard algorithm, but to restrict our attention to the class of reversible Markov chains. We demonstrate convergence under this mild reversibility condition on the structure of the chain, which in many applications can be assumed using domain knowledge. In particular, we establish a convergence guarantee under an upper bound on the discount factor in terms of the difference between the on-policy and off-policy process. This improves upon known results in the literature that state that convergence holds for a sufficiently small discount factor by establishing an explicit bound. Convergence is with probability one and achieves projected Bellman error equal to zero. To obtain these results, we adapt the stochastic approximation framework that was used by Tsitsiklis and Van Roy [1997 for the on-policy case, to the off-policy case. We illustrate our results using different types of reversible Markov chains, such as one-dimensional random walks and random walks on a weighted graph.",
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-10-29T13:38:24+00:00",
      "updated": "2025-10-29T13:38:24+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.25514v1",
      "file": "papers/2510.25514v1.pdf"
    },
    {
      "arxiv_id": "2510.25510v1",
      "title": "MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning for Text-to-SQL",
      "authors": [
        {
          "name": "Zekun Xu"
        },
        {
          "name": "Siyu Xia"
        },
        {
          "name": "Chuhuai Yue"
        },
        {
          "name": "Jiajun Chai"
        },
        {
          "name": "Mingxue Tian"
        },
        {
          "name": "Xiaohan Wang"
        },
        {
          "name": "Wei Lin"
        },
        {
          "name": "Haoxuan Li"
        },
        {
          "name": "Guojun Yin"
        }
      ],
      "abstract": "As large language models (LLMs) are increasingly used in Text-to-SQL tasks, Reinforcement Learning (RL) has become a common method for improving performance. Existing methods primarily rely on static execution feedback, which restricts real-time error correction. However, integrating multi-turn tool invocation along with dynamic feedback could significantly improve adaptability and robustness, ultimately enhancing model performance. To address these issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated Reasoning reinforcement learning framework for Text-to-SQL. Our approach introduces an execution-aware multi-turn reasoning paradigm that seamlessly incorporates database execution feedback at each reasoning step, enabling context-sensitive query generation and progressive refinement throughout the reasoning process. The framework extends the GRPO algorithm to accommodate complex multi-turn interaction scenarios. Considering the training instability characteristics of MTIR and the potential for significant Deviation of model distribution from the initial model, we enhance the GRPO algorithm by adding a trajectory filtering mechanism and removing KL loss constraints. Experimental results demonstrate that MTIR-SQL, with 4B parameters, achieves \\textbf{64.4}\\% accuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev, significantly outperforming existing approaches.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-10-29T13:34:27+00:00",
      "updated": "2025-10-29T13:34:27+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.25510v1",
      "file": "papers/2510.25510v1.pdf"
    },
    {
      "arxiv_id": "2510.25811v1",
      "title": "Multimodal Bandits: Regret Lower Bounds and Optimal Algorithms",
      "authors": [
        {
          "name": "William Réveillard"
        },
        {
          "name": "Richard Combes"
        }
      ],
      "abstract": "We consider a stochastic multi-armed bandit problem with i.i.d. rewards where the expected reward function is multimodal with at most m modes. We propose the first known computationally tractable algorithm for computing the solution to the Graves-Lai optimization problem, which in turn enables the implementation of asymptotically optimal algorithms for this bandit problem. The code for the proposed algorithms is publicly available at https://github.com/wilrev/MultimodalBandits",
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST"
      ],
      "published": "2025-10-29T12:32:07+00:00",
      "updated": "2025-10-29T12:32:07+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.25811v1",
      "file": "papers/2510.25811v1.pdf"
    },
    {
      "arxiv_id": "2510.00358v1",
      "title": "DiSA-IQL: Offline Reinforcement Learning for Robust Soft Robot Control under Distribution Shifts",
      "authors": [
        {
          "name": "Linjin He"
        },
        {
          "name": "Xinda Qi"
        },
        {
          "name": "Dong Chen"
        },
        {
          "name": "Zhaojian Li"
        },
        {
          "name": "Xiaobo Tan"
        }
      ],
      "abstract": "Soft snake robots offer remarkable flexibility and adaptability in complex environments, yet their control remains challenging due to highly nonlinear dynamics. Existing model-based and bio-inspired controllers rely on simplified assumptions that limit performance. Deep reinforcement learning (DRL) has recently emerged as a promising alternative, but online training is often impractical because of costly and potentially damaging real-world interactions. Offline RL provides a safer option by leveraging pre-collected datasets, but it suffers from distribution shift, which degrades generalization to unseen scenarios. To overcome this challenge, we propose DiSA-IQL (Distribution-Shift-Aware Implicit Q-Learning), an extension of IQL that incorporates robustness modulation by penalizing unreliable state-action pairs to mitigate distribution shift. We evaluate DiSA-IQL on goal-reaching tasks across two settings: in-distribution and out-of-distribution evaluation. Simulation results show that DiSA-IQL consistently outperforms baseline models, including Behavior Cloning (BC), Conservative Q-Learning (CQL), and vanilla IQL, achieving higher success rates, smoother trajectories, and improved robustness. The codes are open-sourced to support reproducibility and to facilitate further research in offline RL for soft robot control.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "published": "2025-09-30T23:53:47+00:00",
      "updated": "2025-09-30T23:53:47+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.00358v1",
      "file": "papers/2510.00358v1.pdf"
    },
    {
      "arxiv_id": "2510.00348v1",
      "title": "Initial Distribution Sensitivity of Constrained Markov Decision Processes",
      "authors": [
        {
          "name": "Alperen Tercan"
        },
        {
          "name": "Necmiye Ozay"
        }
      ],
      "abstract": "Constrained Markov Decision Processes (CMDPs) are notably more complex to solve than standard MDPs due to the absence of universally optimal policies across all initial state distributions. This necessitates re-solving the CMDP whenever the initial distribution changes. In this work, we analyze how the optimal value of CMDPs varies with different initial distributions, deriving bounds on these variations using duality analysis of CMDPs and perturbation analysis in linear programming. Moreover, we show how such bounds can be used to analyze the regret of a given policy due to unknown variations of the initial distribution.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-09-30T23:19:20+00:00",
      "updated": "2025-09-30T23:19:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.00348v1",
      "file": "papers/2510.00348v1.pdf"
    },
    {
      "arxiv_id": "2510.00347v1",
      "title": "In-Context Curiosity: Distilling Exploration for Decision-Pretrained Transformers on Bandit Tasks",
      "authors": [
        {
          "name": "Huitao Yang"
        },
        {
          "name": "Guanting Chen"
        }
      ],
      "abstract": "As large language models (LLMs) continue to grow in capability, there is increasing interest in incorporating them into decision-making tasks. A common pipeline for this is Decision-Pretrained Transformers (DPTs). However, existing training methods for DPTs often struggle to generalize beyond their pretraining data distribution. To explore mitigation of this limitation, we propose in-context curiosity -- a lightweight, exploration-inspired regularizer for offline pretraining -- and introduce the Prediction-Powered Transformer (PPT) framework. PPT augments DPT with an auxiliary reward predictor, using prediction error as an intrinsic curiosity signal to encourage broader exploration during training. In proof-of-concept experiments on Gaussian multi-armed bandits, PPT shows improved robustness: it moderates the performance degradation observed in DPT when test environments exhibit higher variance in reward, particularly when pretraining data has limited diversity. While the quality of offline data remain fundamental, our preliminary results suggest that curiosity-driven pretraining offers a promising direction for enhancing out-of-distribution generalization in in-context RL agents.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "published": "2025-09-30T23:17:18+00:00",
      "updated": "2025-09-30T23:17:18+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.00347v1",
      "file": "papers/2510.00347v1.pdf"
    },
    {
      "arxiv_id": "2510.00274v1",
      "title": "MAGIC-MASK: Multi-Agent Guided Inter-Agent Collaboration with Mask-Based Explainability for Reinforcement Learning",
      "authors": [
        {
          "name": "Maisha Maliha"
        },
        {
          "name": "Dean Hougen"
        }
      ],
      "abstract": "Understanding the decision-making process of Deep Reinforcement Learning agents remains a key challenge for deploying these systems in safety-critical and multi-agent environments. While prior explainability methods like StateMask, have advanced the identification of critical states, they remain limited by computational cost, exploration coverage, and lack of adaptation to multi-agent settings. To overcome these limitations, we propose a mathematically grounded framework, MAGIC-MASK (Multi-Agent Guided Inter-agent Collaboration with Mask-Based Explainability for Reinforcement Learning), that extends perturbation-based explanation to Multi-Agent Reinforcement Learning. Our method integrates Proximal Policy Optimization, adaptive epsilon-greedy exploration, and lightweight inter-agent collaboration to share masked state information and peer experience. This collaboration enables each agent to perform saliency-guided masking and share reward-based insights with peers, reducing the time required for critical state discovery, improving explanation fidelity, and leading to faster and more robust learning. The core novelty of our approach lies in generalizing explainability from single-agent to multi-agent systems through a unified mathematical formalism built on trajectory perturbation, reward fidelity analysis, and Kullback-Leibler divergence regularization. This framework yields localized, interpretable explanations grounded in probabilistic modeling and multi-agent Markov decision processes. We validate our framework on both single-agent and multi-agent benchmarks, including a multi-agent highway driving environment and Google Research Football, demonstrating that MAGIC-MASK consistently outperforms state-of-the-art baselines in fidelity, learning efficiency, and policy robustness while offering interpretable and transferable explanations.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "published": "2025-09-30T20:53:28+00:00",
      "updated": "2025-09-30T20:53:28+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.00274v1",
      "file": "papers/2510.00274v1.pdf"
    },
    {
      "arxiv_id": "2510.00225v1",
      "title": "TGPO: Temporal Grounded Policy Optimization for Signal Temporal Logic Tasks",
      "authors": [
        {
          "name": "Yue Meng"
        },
        {
          "name": "Fei Chen"
        },
        {
          "name": "Chuchu Fan"
        }
      ],
      "abstract": "Learning control policies for complex, long-horizon tasks is a central challenge in robotics and autonomous systems. Signal Temporal Logic (STL) offers a powerful and expressive language for specifying such tasks, but its non-Markovian nature and inherent sparse reward make it difficult to be solved via standard Reinforcement Learning (RL) algorithms. Prior RL approaches focus only on limited STL fragments or use STL robustness scores as sparse terminal rewards. In this paper, we propose TGPO, Temporal Grounded Policy Optimization, to solve general STL tasks. TGPO decomposes STL into timed subgoals and invariant constraints and provides a hierarchical framework to tackle the problem. The high-level component of TGPO proposes concrete time allocations for these subgoals, and the low-level time-conditioned policy learns to achieve the sequenced subgoals using a dense, stage-wise reward signal. During inference, we sample various time allocations and select the most promising assignment for the policy network to rollout the solution trajectory. To foster efficient policy learning for complex STL with multiple subgoals, we leverage the learned critic to guide the high-level temporal search via Metropolis-Hastings sampling, focusing exploration on temporally feasible solutions. We conduct experiments on five environments, ranging from low-dimensional navigation to manipulation, drone, and quadrupedal locomotion. Under a wide range of STL tasks, TGPO significantly outperforms state-of-the-art baselines (especially for high-dimensional and long-horizon cases), with an average of 31.6% improvement in task success rate compared to the best baseline. The code will be available at https://github.com/mengyuest/TGPO",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.LO"
      ],
      "published": "2025-09-30T19:51:05+00:00",
      "updated": "2025-09-30T19:51:05+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.00225v1",
      "file": "papers/2510.00225v1.pdf"
    },
    {
      "arxiv_id": "2510.00212v1",
      "title": "Directed-MAML: Meta Reinforcement Learning Algorithm with Task-directed Approximation",
      "authors": [
        {
          "name": "Yang Zhang"
        },
        {
          "name": "Huiwen Yan"
        },
        {
          "name": "Mushuang Liu"
        }
      ],
      "abstract": "Model-Agnostic Meta-Learning (MAML) is a versatile meta-learning framework applicable to both supervised learning and reinforcement learning (RL). However, applying MAML to meta-reinforcement learning (meta-RL) presents notable challenges. First, MAML relies on second-order gradient computations, leading to significant computational and memory overhead. Second, the nested structure of optimization increases the problem's complexity, making convergence to a global optimum more challenging. To overcome these limitations, we propose Directed-MAML, a novel task-directed meta-RL algorithm. Before the second-order gradient step, Directed-MAML applies an additional first-order task-directed approximation to estimate the effect of second-order gradients, thereby accelerating convergence to the optimum and reducing computational cost. Experimental results demonstrate that Directed-MAML surpasses MAML-based baselines in computational efficiency and convergence speed in the scenarios of CartPole-v1, LunarLander-v2 and two-vehicle intersection crossing. Furthermore, we show that task-directed approximation can be effectively integrated into other meta-learning algorithms, such as First-Order Model-Agnostic Meta-Learning (FOMAML) and Meta Stochastic Gradient Descent(Meta-SGD), yielding improved computational efficiency and convergence speed.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-09-30T19:42:15+00:00",
      "updated": "2025-09-30T19:42:15+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.00212v1",
      "file": "papers/2510.00212v1.pdf"
    },
    {
      "arxiv_id": "2510.00194v1",
      "title": "GRPO-$λ$: Credit Assignment improves LLM Reasoning",
      "authors": [
        {
          "name": "Prasanna Parthasarathi"
        },
        {
          "name": "Mathieu Reymond"
        },
        {
          "name": "Boxing Chen"
        },
        {
          "name": "Yufei Cui"
        },
        {
          "name": "Sarath Chandar"
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed for tasks requiring complex reasoning, prompting significant interest in improving their reasoning abilities through post-training. Especially RL based methods using verifiable reward, like the state-of-the-art GRPO, have shown to tremendously improve reasoning behaviors when applied as post-training methods. However, the lack of an explicit reward or critic model limits GRPO's ability to assign fine-grained credit across token sequences. In this work, we present GRPO-$λ$, a novel extension to GRPO that enhances credit assignment in RL finetuning of LLMs for complex reasoning tasks. We approximate learning from $λ$-return with a reformulation of eligibility traces using token-level log-probabilities applied after each sequence generation, and a novel critic-free approximation of the temporal-difference error. We introduce a few variations for the weighting of the $λ$-return, and their applications to the eligibility-trace, where all the variations provide significant gains over GRPO. We compare GRPO-$λ$ against GRPO by training models from 1.5B to 7B parameters on $4$ different math reasoning datasets. The training plots demonstrate 30-40% improved performance during RL training on both LLaMA-3.1 and Qwen-2.5 architectures. Finally, we show that with GRPO-$λ$, the resulting average performance on AIME24, Math500, OlympiadMath, MinervaMath, and AMC improves over GRPO by over $3$ points and a $4.5$ points improvement on the 7B model.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-09-30T19:11:10+00:00",
      "updated": "2025-09-30T19:11:10+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.00194v1",
      "file": "papers/2510.00194v1.pdf"
    },
    {
      "arxiv_id": "2510.00144v1",
      "title": "Which Rewards Matter? Reward Selection for Reinforcement Learning under Limited Feedback",
      "authors": [
        {
          "name": "Shreyas Chaudhari"
        },
        {
          "name": "Renhao Zhang"
        },
        {
          "name": "Philip S. Thomas"
        },
        {
          "name": "Bruno Castro da Silva"
        }
      ],
      "abstract": "The ability of reinforcement learning algorithms to learn effective policies is determined by the rewards available during training. However, for practical problems, obtaining large quantities of reward labels is often infeasible due to computational or financial constraints, particularly when relying on human feedback. When reinforcement learning must proceed with limited feedback -- only a fraction of samples get rewards labeled -- a fundamental question arises: which samples should be labeled to maximize policy performance? We formalize this problem of reward selection for reinforcement learning from limited feedback (RLLF), introducing a new problem formulation that facilitates the study of strategies for selecting impactful rewards. Two types of selection strategies are investigated: (i) heuristics that rely on reward-free information such as state visitation and partial value functions, and (ii) strategies pre-trained using auxiliary evaluative feedback. We find that critical subsets of rewards are those that (1) guide the agent along optimal trajectories, and (2) support recovery toward near-optimal behavior after deviations. Effective selection methods yield near-optimal policies with significantly fewer reward labels than full supervision, establishing reward selection as a powerful paradigm for scaling reinforcement learning in feedback-limited settings.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-09-30T18:17:49+00:00",
      "updated": "2025-09-30T18:17:49+00:00",
      "pdf_url": "https://arxiv.org/pdf/2510.00144v1",
      "file": "papers/2510.00144v1.pdf"
    },
    {
      "arxiv_id": "2509.26628v1",
      "title": "Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models",
      "authors": [
        {
          "name": "Runze Liu"
        },
        {
          "name": "Jiakang Wang"
        },
        {
          "name": "Yuling Shi"
        },
        {
          "name": "Zhihui Xie"
        },
        {
          "name": "Chenxin An"
        },
        {
          "name": "Kaiyan Zhang"
        },
        {
          "name": "Jian Zhao"
        },
        {
          "name": "Xiaodong Gu"
        },
        {
          "name": "Lei Lin"
        },
        {
          "name": "Wenping Hu"
        },
        {
          "name": "Xiu Li"
        },
        {
          "name": "Fuzheng Zhang"
        },
        {
          "name": "Guorui Zhou"
        },
        {
          "name": "Kun Gai"
        }
      ],
      "abstract": "Reinforcement Learning (RL) has shown remarkable success in enhancing the reasoning capabilities of Large Language Models (LLMs). Process-Supervised RL (PSRL) has emerged as a more effective paradigm compared to outcome-based RL. However, existing PSRL approaches suffer from limited exploration efficiency, both in terms of branching positions and sampling. In this paper, we introduce a novel PSRL framework (AttnRL), which enables efficient exploration for reasoning models. Motivated by preliminary observations that steps exhibiting high attention scores correlate with reasoning behaviors, we propose to branch from positions with high values. Furthermore, we develop an adaptive sampling strategy that accounts for problem difficulty and historical batch size, ensuring that the whole training batch maintains non-zero advantage values. To further improve sampling efficiency, we design a one-step off-policy training pipeline for PSRL. Extensive experiments on multiple challenging mathematical reasoning benchmarks demonstrate that our method consistently outperforms prior approaches in terms of performance and sampling and training efficiency.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2025-09-30T17:58:34+00:00",
      "updated": "2025-09-30T17:58:34+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.26628v1",
      "file": "papers/2509.26628v1.pdf"
    },
    {
      "arxiv_id": "2509.26627v1",
      "title": "TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise Temporal Distance",
      "authors": [
        {
          "name": "Yuyang Liu"
        },
        {
          "name": "Chuan Wen"
        },
        {
          "name": "Yihang Hu"
        },
        {
          "name": "Dinesh Jayaraman"
        },
        {
          "name": "Yang Gao"
        }
      ],
      "abstract": "Designing dense rewards is crucial for reinforcement learning (RL), yet in robotics it often demands extensive manual effort and lacks scalability. One promising solution is to view task progress as a dense reward signal, as it quantifies the degree to which actions advance the system toward task completion over time. We present TimeRewarder, a simple yet effective reward learning method that derives progress estimation signals from passive videos, including robot demonstrations and human videos, by modeling temporal distances between frame pairs. We then demonstrate how TimeRewarder can supply step-wise proxy rewards to guide reinforcement learning. In our comprehensive experiments on ten challenging Meta-World tasks, we show that TimeRewarder dramatically improves RL for sparse-reward tasks, achieving nearly perfect success in 9/10 tasks with only 200,000 interactions per task with the environment. This approach outperformed previous methods and even the manually designed environment dense reward on both the final success rate and sample efficiency. Moreover, we show that TimeRewarder pretraining can exploit real-world human videos, highlighting its potential as a scalable approach path to rich reward signals from diverse video sources.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "published": "2025-09-30T17:58:20+00:00",
      "updated": "2025-09-30T17:58:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.26627v1",
      "file": "papers/2509.26627v1.pdf"
    },
    {
      "arxiv_id": "2509.26605v2",
      "title": "Fine-tuning Behavioral Cloning Policies with Preference-Based Reinforcement Learning",
      "authors": [
        {
          "name": "Maël Macuglia"
        },
        {
          "name": "Paul Friedrich"
        },
        {
          "name": "Giorgia Ramponi"
        }
      ],
      "abstract": "Deploying reinforcement learning (RL) in robotics, industry, and health care is blocked by two obstacles: the difficulty of specifying accurate rewards and the risk of unsafe, data-hungry exploration. We address this by proposing a two-stage framework that first learns a safe initial policy from a reward-free dataset of expert demonstrations, then fine-tunes it online using preference-based human feedback. We provide the first principled analysis of this offline-to-online approach and introduce BRIDGE, a unified algorithm that integrates both signals via an uncertainty-weighted objective. We derive regret bounds that shrink with the number of offline demonstrations, explicitly connecting the quantity of offline data to online sample efficiency. We validate BRIDGE in discrete and continuous control MuJoCo environments, showing it achieves lower regret than both standalone behavioral cloning and online preference-based RL. Our work establishes a theoretical foundation for designing more sample-efficient interactive agents.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-09-30T17:50:19+00:00",
      "updated": "2025-10-13T13:00:55+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.26605v2",
      "file": "papers/2509.26605v2.pdf"
    },
    {
      "arxiv_id": "2509.26594v1",
      "title": "Clarification as Supervision: Reinforcement Learning for Vision-Language Interfaces",
      "authors": [
        {
          "name": "John Gkountouras"
        },
        {
          "name": "Ivan Titov"
        }
      ],
      "abstract": "Recent text-only models demonstrate remarkable mathematical reasoning capabilities. Extending these to visual domains requires vision-language models to translate images into text descriptions. However, current models, trained to produce captions for human readers, often omit the precise details that reasoning systems require. This creates an interface mismatch: reasoners often fail not due to reasoning limitations but because they lack access to critical visual information. We propose Adaptive-Clarification Reinforcement Learning (AC-RL), which teaches vision models what information reasoners need through interaction. Our key insight is that clarification requests during training reveal information gaps; by penalizing success that requires clarification, we create pressure for comprehensive initial captions that enable the reasoner to solve the problem in a single pass. AC-RL improves average accuracy by 4.4 points over pretrained baselines across seven visual mathematical reasoning benchmarks, and analysis shows it would cut clarification requests by up to 39% if those were allowed. By treating clarification as a form of implicit supervision, AC-RL demonstrates that vision-language interfaces can be effectively learned through interaction alone, without requiring explicit annotations.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.CV"
      ],
      "published": "2025-09-30T17:46:46+00:00",
      "updated": "2025-09-30T17:46:46+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.26594v1",
      "file": "papers/2509.26594v1.pdf"
    },
    {
      "arxiv_id": "2509.26442v1",
      "title": "Extensions of Robbins-Siegmund Theorem with Applications in Reinforcement Learning",
      "authors": [
        {
          "name": "Xinyu Liu"
        },
        {
          "name": "Zixuan Xie"
        },
        {
          "name": "Shangtong Zhang"
        }
      ],
      "abstract": "The Robbins-Siegmund theorem establishes the convergence of stochastic processes that are almost supermartingales and is foundational for analyzing a wide range of stochastic iterative algorithms in stochastic approximation and reinforcement learning (RL). However, its original form has a significant limitation as it requires the zero-order term to be summable. In many important RL applications, this summable condition, however, cannot be met. This limitation motivates us to extend the Robbins-Siegmund theorem for almost supermartingales where the zero-order term is not summable but only square summable. Particularly, we introduce a novel and mild assumption on the increments of the stochastic processes. This together with the square summable condition enables an almost sure convergence to a bounded set. Additionally, we further provide almost sure convergence rates, high probability concentration bounds, and $L^p$ convergence rates. We then apply the new results in stochastic approximation and RL. Notably, we obtain the first almost sure convergence rate, the first high probability concentration bound, and the first $L^p$ convergence rate for $Q$-learning with linear function approximation.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "published": "2025-09-30T16:00:36+00:00",
      "updated": "2025-09-30T16:00:36+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.26442v1",
      "file": "papers/2509.26442v1.pdf"
    },
    {
      "arxiv_id": "2509.26429v1",
      "title": "An Orthogonal Learner for Individualized Outcomes in Markov Decision Processes",
      "authors": [
        {
          "name": "Emil Javurek"
        },
        {
          "name": "Valentyn Melnychuk"
        },
        {
          "name": "Jonas Schweisthal"
        },
        {
          "name": "Konstantin Hess"
        },
        {
          "name": "Dennis Frauen"
        },
        {
          "name": "Stefan Feuerriegel"
        }
      ],
      "abstract": "Predicting individualized potential outcomes in sequential decision-making is central for optimizing therapeutic decisions in personalized medicine (e.g., which dosing sequence to give to a cancer patient). However, predicting potential outcomes over long horizons is notoriously difficult. Existing methods that break the curse of the horizon typically lack strong theoretical guarantees such as orthogonality and quasi-oracle efficiency. In this paper, we revisit the problem of predicting individualized potential outcomes in sequential decision-making (i.e., estimating Q-functions in Markov decision processes with observational data) through a causal inference lens. In particular, we develop a comprehensive theoretical foundation for meta-learners in this setting with a focus on beneficial theoretical properties. As a result, we yield a novel meta-learner called DRQ-learner and establish that it is: (1) doubly robust (i.e., valid inference under the misspecification of one of the nuisances), (2) Neyman-orthogonal (i.e., insensitive to first-order estimation errors in the nuisance functions), and (3) achieves quasi-oracle efficiency (i.e., behaves asymptotically as if the ground-truth nuisance functions were known). Our DRQ-learner is applicable to settings with both discrete and continuous state spaces. Further, our DRQ-learner is flexible and can be used together with arbitrary machine learning models (e.g., neural networks). We validate our theoretical results through numerical experiments, thereby showing that our meta-learner outperforms state-of-the-art baselines.",
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-09-30T15:49:29+00:00",
      "updated": "2025-09-30T15:49:29+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.26429v1",
      "file": "papers/2509.26429v1.pdf"
    },
    {
      "arxiv_id": "2509.26383v3",
      "title": "Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning",
      "authors": [
        {
          "name": "Jinyeop Song"
        },
        {
          "name": "Song Wang"
        },
        {
          "name": "Julian Shun"
        },
        {
          "name": "Yada Zhu"
        }
      ],
      "abstract": "Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at https://github.com/Jinyeop3110/KG-R1.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-09-30T15:14:24+00:00",
      "updated": "2025-10-09T02:18:28+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.26383v3",
      "file": "papers/2509.26383v3.pdf"
    },
    {
      "arxiv_id": "2509.26340v1",
      "title": "Memory-Driven Self-Improvement for Decision Making with Large Language Models",
      "authors": [
        {
          "name": "Xue Yan"
        },
        {
          "name": "Zijing Ou"
        },
        {
          "name": "Mengyue Yang"
        },
        {
          "name": "Yan Song"
        },
        {
          "name": "Haifeng Zhang"
        },
        {
          "name": "Yingzhen Li"
        },
        {
          "name": "Jun Wang"
        }
      ],
      "abstract": "Large language models (LLMs) have emerged as effective action policies for sequential decision-making (SDM) tasks due to their extensive prior knowledge. However, this broad yet general knowledge is often insufficient for specific decision-making tasks with limited task-related data, making it challenging to efficiently adapt LLMs to specific SDM tasks. To address this challenge, we propose a memory-driven self-improvement framework that combines LLM general prior knowledge with a compact memory of domain-specific experiences. Memory retains past interactions and associated Q-values, thereby capturing decision-relevant knowledge that facilitates accurate value estimation and informs the LLM prior refinement. The refined LLM prior, in turn, generates higher-reward trajectories that further enrich memory, forming a natural self-improvement framework where memory and LLM prior mutually reinforce each other. Experiments show that our memory-driven approach significantly outperforms both traditional RL and LLM-based baselines, e.g., improving performance by over 40\\% on in-distribution tasks and over 75\\% when generalized to unseen tasks in ALFWorld.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-09-30T14:46:06+00:00",
      "updated": "2025-09-30T14:46:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.26340v1",
      "file": "papers/2509.26340v1.pdf"
    },
    {
      "arxiv_id": "2509.26294v1",
      "title": "Noise-Guided Transport for Imitation Learning",
      "authors": [
        {
          "name": "Lionel Blondé"
        },
        {
          "name": "Joao A. Candido Ramos"
        },
        {
          "name": "Alexandros Kalousis"
        }
      ],
      "abstract": "We consider imitation learning in the low-data regime, where only a limited number of expert demonstrations are available. In this setting, methods that rely on large-scale pretraining or high-capacity architectures can be difficult to apply, and efficiency with respect to demonstration data becomes critical. We introduce Noise-Guided Transport (NGT), a lightweight off-policy method that casts imitation as an optimal transport problem solved via adversarial training. NGT requires no pretraining or specialized architectures, incorporates uncertainty estimation by design, and is easy to implement and tune. Despite its simplicity, NGT achieves strong performance on challenging continuous control tasks, including high-dimensional Humanoid tasks, under ultra-low data regimes with as few as 20 transitions. Code is publicly available at: https://github.com/lionelblonde/ngt-pytorch.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-09-30T14:10:06+00:00",
      "updated": "2025-09-30T14:10:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.26294v1",
      "file": "papers/2509.26294v1.pdf"
    },
    {
      "arxiv_id": "2509.26272v2",
      "title": "PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake Detection",
      "authors": [
        {
          "name": "Tuan Nguyen"
        },
        {
          "name": "Naseem Khan"
        },
        {
          "name": "Khang Tran"
        },
        {
          "name": "NhatHai Phan"
        },
        {
          "name": "Issa Khalil"
        }
      ],
      "abstract": "The rapid rise of synthetic media has made deepfake detection a critical challenge for online safety and trust. Progress remains constrained by the scarcity of large, high-quality datasets. Although multimodal large language models (LLMs) exhibit strong reasoning capabilities, their performance on deepfake detection is poor, often producing explanations that are misaligned with visual evidence or hallucinatory. To address this limitation, we introduce a reasoning-annotated dataset for deepfake detection and propose Paragraph-level Relative Policy Optimization (PRPO), a reinforcement learning algorithm that aligns LLM reasoning with image content at the paragraph level. Experiments show that PRPO improves detection accuracy by a wide margin and achieves the highest reasoning score of 4.55/5.0. Ablation studies further demonstrate that PRPO significantly outperforms GRPO under test-time conditions. These results underscore the importance of grounding multimodal reasoning in visual evidence to enable more reliable and interpretable deepfake detection.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-09-30T13:56:05+00:00",
      "updated": "2025-10-01T12:27:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.26272v2",
      "file": "papers/2509.26272v2.pdf"
    },
    {
      "arxiv_id": "2509.26239v1",
      "title": "Sandbagging in a Simple Survival Bandit Problem",
      "authors": [
        {
          "name": "Joel Dyer"
        },
        {
          "name": "Daniel Jarne Ornia"
        },
        {
          "name": "Nicholas Bishop"
        },
        {
          "name": "Anisoara Calinescu"
        },
        {
          "name": "Michael Wooldridge"
        }
      ],
      "abstract": "Evaluating the safety of frontier AI systems is an increasingly important concern, helping to measure the capabilities of such models and identify risks before deployment. However, it has been recognised that if AI agents are aware that they are being evaluated, such agents may deliberately hide dangerous capabilities or intentionally demonstrate suboptimal performance in safety-related tasks in order to be released and to avoid being deactivated or retrained. Such strategic deception - often known as \"sandbagging\" - threatens to undermine the integrity of safety evaluations. For this reason, it is of value to identify methods that enable us to distinguish behavioural patterns that demonstrate a true lack of capability from behavioural patterns that are consistent with sandbagging. In this paper, we develop a simple model of strategic deception in sequential decision-making tasks, inspired by the recently developed survival bandit framework. We demonstrate theoretically that this problem induces sandbagging behaviour in optimal rational agents, and construct a statistical test to distinguish between sandbagging and incompetence from sequences of test scores. In simulation experiments, we investigate the reliability of this test in allowing us to distinguish between such behaviours in bandit models. This work aims to establish a potential avenue for developing robust statistical procedures for use in the science of frontier model evaluations.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2025-09-30T13:33:46+00:00",
      "updated": "2025-09-30T13:33:46+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.26239v1",
      "file": "papers/2509.26239v1.pdf"
    },
    {
      "arxiv_id": "2509.26226v2",
      "title": "Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners",
      "authors": [
        {
          "name": "Xin Xu"
        },
        {
          "name": "Cliveb AI"
        },
        {
          "name": "Kai Yang"
        },
        {
          "name": "Tianhao Chen"
        },
        {
          "name": "Yang Wang"
        },
        {
          "name": "Saiyong Yang"
        },
        {
          "name": "Can Yang"
        }
      ],
      "abstract": "Reinforcement Learning with Verifiable Reward (RLVR) effectively solves complex tasks but demands extremely long context lengths during training, leading to substantial computational costs. While multi-stage training can partially mitigate this, starting with overly short contexts often causes irreversible performance degradation, ultimately failing to reduce overall training compute significantly. In this paper, we introduce **T**hinking-**F**ree **P**olicy **I**nitialization (**TFPI**), a simple yet effective adaptation to RLVR that bridges long Chain-of-Thought (CoT) distillation and standard RLVR. TFPI employs a simple *ThinkFree* operation, explicitly discarding the thinking content via a direct *</think>* append, to reduce token usage during inference. Training with *ThinkFree*-adapted inputs improves performance and lowers token consumption, even in the original slow-thinking mode. Extensive experiments across various benchmarks have shown that TFPI accelerates RL convergence, achieves a higher performance ceiling, and yields more token-efficient reasoning models without specialized rewards or complex training designs. With TFPI only, we train a 4B model to reach 89.0% accuracy on AIME24 and 65.5% on LiveCodeBench using less than 4K H20 hours.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2025-09-30T13:25:00+00:00",
      "updated": "2025-12-24T08:05:31+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.26226v2",
      "file": "papers/2509.26226v2.pdf"
    },
    {
      "arxiv_id": "2509.26209v1",
      "title": "Diversity-Incentivized Exploration for Versatile Reasoning",
      "authors": [
        {
          "name": "Zican Hu"
        },
        {
          "name": "Shilin Zhang"
        },
        {
          "name": "Yafu Li"
        },
        {
          "name": "Jianhao Yan"
        },
        {
          "name": "Xuyang Hu"
        },
        {
          "name": "Leyang Cui"
        },
        {
          "name": "Xiaoye Qu"
        },
        {
          "name": "Chunlin Chen"
        },
        {
          "name": "Yu Cheng"
        },
        {
          "name": "Zhi Wang"
        }
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a crucial paradigm for incentivizing reasoning capabilities in Large Language Models (LLMs). Due to vast state-action spaces and reward sparsity in reasoning tasks, existing methods often struggle with deficient exploration and poor sample efficiency. In the paper, we propose \\textbf{DIVER} (\\textbf{D}iversity-\\textbf{I}ncentivized Exploration for \\textbf{V}ersatil\\textbf{E} \\textbf{R}easoning), an innovative framework that highlights the pivotal role of global sequence-level diversity to incentivize deep exploration for versatile reasoning. We first conduct a primary empirical study to reveal a strong positive correlation between global diversity and reasoning capacity. Building on this insight, we introduce global diversity incentives as an intrinsic reward to promote deep exploration in a semantically structured space. Incorporating the intrinsic reward, we develop a potential-based reward shaping mechanism to preserve optimal policy invariance and design simple heuristics to mitigate possible reward hacking. Experimental results show that DIVER outperforms competitive RLVR baselines with various exploration strategies on both in-domain and out-of-domain tasks, excelling in both Pass@1 and Pass@k evaluations. Our code is available at https://github.com/NJU-RL/DIVER.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-09-30T13:11:46+00:00",
      "updated": "2025-09-30T13:11:46+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.26209v1",
      "file": "papers/2509.26209v1.pdf"
    },
    {
      "arxiv_id": "2509.26137v1",
      "title": "Accelerating Transformers in Online RL",
      "authors": [
        {
          "name": "Daniil Zelezetsky"
        },
        {
          "name": "Alexey K. Kovalev"
        },
        {
          "name": "Aleksandr I. Panov"
        }
      ],
      "abstract": "The appearance of transformer-based models in Reinforcement Learning (RL) has expanded the horizons of possibilities in robotics tasks, but it has simultaneously brought a wide range of challenges during its implementation, especially in model-free online RL. Some of the existing learning algorithms cannot be easily implemented with transformer-based models due to the instability of the latter. In this paper, we propose a method that uses the Accelerator policy as a transformer's trainer. The Accelerator, a simpler and more stable model, interacts with the environment independently while simultaneously training the transformer through behavior cloning during the first stage of the proposed algorithm. In the second stage, the pretrained transformer starts to interact with the environment in a fully online setting. As a result, this model-free algorithm accelerates the transformer in terms of its performance and helps it to train online in a more stable and faster way. By conducting experiments on both state-based and image-based ManiSkill environments, as well as on MuJoCo tasks in MDP and POMDP settings, we show that applying our algorithm not only enables stable training of transformers but also reduces training time on image-based environments by up to a factor of two. Moreover, it decreases the required replay buffer size in off-policy methods to 10-20 thousand, which significantly lowers the overall computational demands.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-09-30T11:57:14+00:00",
      "updated": "2025-09-30T11:57:14+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.26137v1",
      "file": "papers/2509.26137v1.pdf"
    },
    {
      "arxiv_id": "2509.26114v1",
      "title": "Clip-Low Increases Entropy and Clip-High Decreases Entropy in Reinforcement Learning of Large Language Models",
      "authors": [
        {
          "name": "Jaesung R. Park"
        },
        {
          "name": "Junsu Kim"
        },
        {
          "name": "Gyeongman Kim"
        },
        {
          "name": "Jinyoung Jo"
        },
        {
          "name": "Sean Choi"
        },
        {
          "name": "Jaewoong Cho"
        },
        {
          "name": "Ernest K. Ryu"
        }
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has recently emerged as the leading approach for enhancing the reasoning capabilities of large language models (LLMs). However, RLVR is prone to entropy collapse, where the LLM quickly converges to a near-deterministic form, hindering exploration and progress during prolonged RL training. In this work, we reveal that the clipping mechanism in PPO and GRPO induces biases on entropy. Through theoretical and empirical analyses, we show that clip-low increases entropy, while clip-high decreases it. Further, under standard clipping parameters, the effect of clip-high dominates, resulting in an overall entropy reduction even when purely random rewards are provided to the RL algorithm. Our findings highlight an overlooked confounding factor in RLVR: independent of the reward signal, the clipping mechanism influences entropy, which in turn affects the reasoning behavior. Furthermore, our analysis demonstrates that clipping can be deliberately used to control entropy. Specifically, with a more aggressive clip-low value, one can increase entropy, promote exploration, and ultimately prevent entropy collapse in RLVR training.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-09-30T11:33:15+00:00",
      "updated": "2025-09-30T11:33:15+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.26114v1",
      "file": "papers/2509.26114v1.pdf"
    },
    {
      "arxiv_id": "2509.26000v1",
      "title": "Informed Asymmetric Actor-Critic: Leveraging Privileged Signals Beyond Full-State Access",
      "authors": [
        {
          "name": "Daniel Ebi"
        },
        {
          "name": "Gaspard Lambrechts"
        },
        {
          "name": "Damien Ernst"
        },
        {
          "name": "Klemens Böhm"
        }
      ],
      "abstract": "Reinforcement learning in partially observable environments requires agents to act under uncertainty from noisy, incomplete observations. Asymmetric actor-critic methods leverage privileged information during training to improve learning under these conditions. However, existing approaches typically assume full-state access during training. In this work, we challenge this assumption by proposing a novel actor-critic framework, called informed asymmetric actor-critic, that enables conditioning the critic on arbitrary privileged signals without requiring access to the full state. We show that policy gradients remain unbiased under this formulation, extending the theoretical foundation of asymmetric methods to the more general case of privileged partial information. To quantify the impact of such signals, we propose informativeness measures based on kernel methods and return prediction error, providing practical tools for evaluating training-time signals. We validate our approach empirically on benchmark navigation tasks and synthetic partially observable environments, showing that our informed asymmetric method improves learning efficiency and value estimation when informative privileged inputs are available. Our findings challenge the necessity of full-state access and open new directions for designing asymmetric reinforcement learning methods that are both practical and theoretically sound.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2025-09-30T09:32:20+00:00",
      "updated": "2025-09-30T09:32:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.26000v1",
      "file": "papers/2509.26000v1.pdf"
    },
    {
      "arxiv_id": "2509.25958v1",
      "title": "RoRecomp: Enhancing Reasoning Efficiency via Rollout Response Recomposition in Reinforcement Learning",
      "authors": [
        {
          "name": "Gang Li"
        },
        {
          "name": "Yulei Qin"
        },
        {
          "name": "Xiaoyu Tan"
        },
        {
          "name": "Dingkang Yang"
        },
        {
          "name": "Yuchen Shi"
        },
        {
          "name": "Zihan Xu"
        },
        {
          "name": "Xiang Li"
        },
        {
          "name": "Xing Sun"
        },
        {
          "name": "Ke Li"
        }
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in eliciting complex reasoning in large language models (LLMs). However, standard RLVR training often leads to excessively verbose processes (in reasoning tasks) and inefficient exploration trajectories (in agentic settings), as outcome-only rewards provide no incentive for efficiency and the high variance in response length within relatively small rollout groups results in noisy optimization signals. To address this, we propose Rollout Response Recomposition (RoRecomp), a plug-and-play method that guides models toward concise reasoning by strategically recomposing the training data. RoRecomp separates responses into two distinct batch types: 1) priority batches, which combine short-correct and long-incorrect responses selected from online batches to provide a clear gradient signal for brevity, and 2) compensation batches, which utilize remaining responses from a replay buffer to maintain stability and prevent model collapse. To comprehensively evaluate effectiveness, we test RoRecomp across three settings where results demonstrate substantial efficiency gains: reducing reasoning length by 27.7% in zero RL training, reducing unnecessary tool calls by 46.8% while improving accuracy in agentic RL, and achieving up to 52.5% length reduction in thinking compression, all with minimal performance impact.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-09-30T08:54:38+00:00",
      "updated": "2025-09-30T08:54:38+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25958v1",
      "file": "papers/2509.25958v1.pdf"
    },
    {
      "arxiv_id": "2509.25876v1",
      "title": "Efficient On-Policy Reinforcement Learning via Exploration of Sparse Parameter Space",
      "authors": [
        {
          "name": "Xinyu Zhang"
        },
        {
          "name": "Aishik Deb"
        },
        {
          "name": "Klaus Mueller"
        }
      ],
      "abstract": "Policy-gradient methods such as Proximal Policy Optimization (PPO) are typically updated along a single stochastic gradient direction, leaving the rich local structure of the parameter space unexplored. Previous work has shown that the surrogate gradient is often poorly correlated with the true reward landscape. Building on this insight, we visualize the parameter space spanned by policy checkpoints within an iteration and reveal that higher performing solutions often lie in nearby unexplored regions. To exploit this opportunity, we introduce ExploRLer, a pluggable pipeline that seamlessly integrates with on-policy algorithms such as PPO and TRPO, systematically probing the unexplored neighborhoods of surrogate on-policy gradient updates. Without increasing the number of gradient updates, ExploRLer achieves significant improvements over baselines in complex continuous control environments. Our results demonstrate that iteration-level exploration provides a practical and effective way to strengthen on-policy reinforcement learning and offer a fresh perspective on the limitations of the surrogate objective.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-09-30T07:13:55+00:00",
      "updated": "2025-09-30T07:13:55+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25876v1",
      "file": "papers/2509.25876v1.pdf"
    },
    {
      "arxiv_id": "2509.25850v1",
      "title": "RL-Guided Data Selection for Language Model Finetuning",
      "authors": [
        {
          "name": "Animesh Jha"
        },
        {
          "name": "Harshit Gupta"
        },
        {
          "name": "Ananjan Nandi"
        }
      ],
      "abstract": "Data selection for finetuning Large Language Models (LLMs) can be framed as a budget-constrained optimization problem: maximizing a model's downstream performance under a strict training data budget. Solving this problem is generally intractable, and existing approximate approaches are pretraining-oriented and transfer poorly to the fine-tuning setting. We reformulate this problem as a tractable Markov Decision Process (MDP) and train agents using various Reinforcement Learning (RL) methods to learn optimal data selection policies, guided by an efficient, proxy-model-based reward signal. Across four datasets, training on a $5\\%$ subset selected by our approach matches or outperforms fine-tuning on the full dataset by up to $10.8$ accuracy points, while cutting wall-clock training time by up to $2 \\times$, highlighting the promise of RL-guided data selection.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-09-30T06:42:19+00:00",
      "updated": "2025-09-30T06:42:19+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25850v1",
      "file": "papers/2509.25850v1.pdf"
    },
    {
      "arxiv_id": "2509.25849v1",
      "title": "Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation",
      "authors": [
        {
          "name": "Ziniu Li"
        },
        {
          "name": "Congliang Chen"
        },
        {
          "name": "Tianyun Yang"
        },
        {
          "name": "Tian Ding"
        },
        {
          "name": "Ruoyu Sun"
        },
        {
          "name": "Ge Zhang"
        },
        {
          "name": "Wenhao Huang"
        },
        {
          "name": "Zhi-Quan Luo"
        }
      ],
      "abstract": "Large Language Models (LLMs) can self-improve through reinforcement learning, where they generate trajectories to explore and discover better solutions. However, this exploration process is computationally expensive, often forcing current methods to assign limited exploration budgets to each task. This uniform allocation creates problematic edge cases: easy tasks consistently succeed while difficult tasks consistently fail, both producing zero gradients during training updates for the widely used Group Relative Policy Optimization (GRPO). We address this problem from the lens of exploration budget allocation. Viewing each task's exploration as an \"item\" with a distinct \"value\" and \"cost\", we establish a connection to the classical knapsack problem. This formulation allows us to derive an optimal assignment rule that adaptively distributes resources based on the model's current learning status. When applied to GRPO, our method increases the effective ratio of non-zero policy gradients by 20-40% during training. Acting as a computational \"free lunch\", our approach could reallocate exploration budgets from tasks where learning is saturated to those where it is most impactful. This enables significantly larger budgets (e.g., 93 rollouts) for especially challenging problems, which would be computationally prohibitive under a uniform allocation. These improvements translate to meaningful gains on mathematical reasoning benchmarks, with average improvements of 2-4 points and peak gains of 9 points on specific tasks. Notably, achieving comparable performance with traditional homogeneous allocation would require about 2x the computational resources.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-09-30T06:41:57+00:00",
      "updated": "2025-09-30T06:41:57+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25849v1",
      "file": "papers/2509.25849v1.pdf"
    },
    {
      "arxiv_id": "2509.25848v2",
      "title": "More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models",
      "authors": [
        {
          "name": "Xinyu Tian"
        },
        {
          "name": "Shu Zou"
        },
        {
          "name": "Zhaoyuan Yang"
        },
        {
          "name": "Mengqi He"
        },
        {
          "name": "Fabian Waschkowski"
        },
        {
          "name": "Lukas Wesemann"
        },
        {
          "name": "Peter Tu"
        },
        {
          "name": "Jing Zhang"
        }
      ],
      "abstract": "Reasoning has emerged as a pivotal capability in Large Language Models (LLMs). Through Reinforcement Learning (RL), typically Group Relative Policy Optimization (GRPO), these models are able to solve complex tasks such as mathematics and code generation. Building on these advances, recent research has sought to extend reasoning to Vision-Language Models (VLMs), yielding promising results across diverse visual tasks. Despite this progress, our study uncovers the dual nature of multimodal reasoning: while it substantially enhances logical inference and facilitates performance on challenging problems, it may gradually impair perceptual grounding, leading to recognition failures on otherwise basic visual questions. Through further analysis, we attribute this phenomenon to visual forgetting, wherein prolonged reasoning causes the model to increasingly disregard visual input. To address this, we propose Vision-Anchored Policy Optimization (VAPO), a simple yet effective method that explicitly steers the reasoning process toward visually grounded trajectories. Our result model, VAPO-Thinker-7B, significantly strengthens the model's reliance on visual information and achieves new state-of-the-art results on a wide range of established benchmarks. Project page: https://xytian1008.github.io/VAPO/",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-09-30T06:37:47+00:00",
      "updated": "2025-10-02T12:24:56+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25848v2",
      "file": "papers/2509.25848v2.pdf"
    },
    {
      "arxiv_id": "2509.25827v1",
      "title": "Overthinking Reduction with Decoupled Rewards and Curriculum Data Scheduling",
      "authors": [
        {
          "name": "Shuyang Jiang"
        },
        {
          "name": "Yusheng Liao"
        },
        {
          "name": "Ya Zhang"
        },
        {
          "name": "Yanfeng Wang"
        },
        {
          "name": "Yu Wang"
        }
      ],
      "abstract": "While large reasoning models trained with critic-free reinforcement learning and verifiable rewards (RLVR) represent the state-of-the-art, their practical utility is hampered by ``overthinking'', a critical issue where models generate excessively long reasoning paths without any performance benefit. Existing solutions that penalize length often fail, inducing performance degradation due to a fundamental misalignment between trajectory-level rewards and token-level optimization. In this work, we introduce a novel framework, DECS, built on our theoretical discovery of two previously unaddressed flaws in current length rewards: (1) the erroneous penalization of essential exploratory tokens and (2) the inadvertent rewarding of partial redundancy. Our framework's innovations include (i) a first-of-its-kind decoupled token-level reward mechanism that surgically distinguishes and penalizes redundant tokens, and (ii) a novel curriculum batch scheduling strategy to master the efficiency-efficacy equilibrium. Experimental results show DECS can achieve a dramatic reduction in reasoning tokens by over 50\\% across seven benchmarks while simultaneously maintaining or even improving performance. It demonstrates conclusively that substantial gains in reasoning efficiency can be achieved without compromising a model's underlying reasoning power.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-09-30T06:04:43+00:00",
      "updated": "2025-09-30T06:04:43+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25827v1",
      "file": "papers/2509.25827v1.pdf"
    },
    {
      "arxiv_id": "2509.25810v3",
      "title": "Learning to Reason as Action Abstractions with Scalable Mid-Training RL",
      "authors": [
        {
          "name": "Shenao Zhang"
        },
        {
          "name": "Donghan Yu"
        },
        {
          "name": "Yihao Feng"
        },
        {
          "name": "Bowen Jin"
        },
        {
          "name": "Zhaoran Wang"
        },
        {
          "name": "John Peebles"
        },
        {
          "name": "Zirui Wang"
        }
      ],
      "abstract": "Large language models excel with reinforcement learning (RL), but fully unlocking this potential requires a mid-training stage. An effective mid-training phase should identify a compact set of useful actions and enable fast selection among them through online RL. We formalize this intuition by presenting the first theoretical result on how mid-training shapes post-training: it characterizes an action subspace that minimizes both the value approximation error from pruning and the RL error during subsequent planning. Our analysis reveals two key determinants of mid-training effectiveness: pruning efficiency, which shapes the prior of the initial RL policy, and its impact on RL convergence, which governs the extent to which that policy can be improved via online interactions. These results suggest that mid-training is most effective when the decision space is compact and the effective horizon is short, highlighting the importance of operating in the space of action abstractions rather than primitive actions. Building on these insights, we propose Reasoning as Action Abstractions (RA3), a scalable mid-training algorithm. Specifically, we derive a sequential variational lower bound and optimize it by iteratively discovering temporally-consistent latent structures via RL, followed by fine-tuning on the bootstrapped data. Experiments on code generation tasks demonstrate the effectiveness of our approach. Across multiple base models, RA3 improves the average performance on HumanEval and MBPP by 8 and 4 points over the base model and the next-token prediction baseline. Furthermore, RA3 achieves faster convergence and higher asymptotic performance in RLVR on HumanEval+, MBPP+, LiveCodeBench, and Codeforces.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "published": "2025-09-30T05:34:20+00:00",
      "updated": "2025-10-11T22:23:08+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25810v3",
      "file": "papers/2509.25810v3.pdf"
    },
    {
      "arxiv_id": "2509.25808v1",
      "title": "Improving Sampling Efficiency in RLVR through Adaptive Rollout and Response Reuse",
      "authors": [
        {
          "name": "Yuheng Zhang"
        },
        {
          "name": "Wenlin Yao"
        },
        {
          "name": "Changlong Yu"
        },
        {
          "name": "Yao Liu"
        },
        {
          "name": "Qingyu Yin"
        },
        {
          "name": "Bing Yin"
        },
        {
          "name": "Hyokun Yun"
        },
        {
          "name": "Lihong Li"
        }
      ],
      "abstract": "Large language models (LLMs) have achieved impressive reasoning performance, with reinforcement learning with verifiable rewards (RLVR) emerging as a standard paradigm for post-training. A representative algorithm, group relative policy optimization (GRPO) (Shao et al., 2024), computes advantages by normalizing outcome rewards within response groups, but suffers from a vanishing advantage issue when all responses in a group receive identical rewards. To address this issue, we propose Adaptive Rollout and Response Reuse Policy Optimization (AR3PO), a sampling efficient RLVR algorithm that introduces two novel techniques: adaptive rollout, which dynamically allocates more responses to difficult prompts while saving computation on easier ones, and response reuse, which leverages previously generated correct responses to provide useful training signals. We compare AR3PO with strong RLVR baselines on multiple representative benchmarks using two different families of base models. Across the 7B and 8B models, AR3PO consistently outperforms GRPO and matches or surpasses DAPO (Yu et al., 2025), reducing rollout cost by up to 4.2x. On the larger 32B model, AR3PO achieves comparable performance to DAPO at similar training steps while maintaining substantially lower rollout cost.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-09-30T05:29:53+00:00",
      "updated": "2025-09-30T05:29:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25808v1",
      "file": "papers/2509.25808v1.pdf"
    },
    {
      "arxiv_id": "2509.25779v2",
      "title": "Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs",
      "authors": [
        {
          "name": "Siyu Zhu"
        },
        {
          "name": "Yanbin Jiang"
        },
        {
          "name": "Hejian Sang"
        },
        {
          "name": "Shao Tang"
        },
        {
          "name": "Qingquan Song"
        },
        {
          "name": "Biao He"
        },
        {
          "name": "Rohit Jain"
        },
        {
          "name": "Zhipeng Wang"
        },
        {
          "name": "Alborz Geramifard"
        }
      ],
      "abstract": "We investigated Agentic RL with large language models on the \\textsc{TravelPlanner} benchmark. Our approach, \\textsc{Planner-R1}, achieved a \\textbf{56.9\\%} final-pass rate with only 180 training queries, a $2.7\\times$ improvement over GPT-5's $21.2\\%$ baseline and the strongest agentic result on the public leaderboard. A central finding was that smaller models (8B) were highly responsive to reward shaping: with dense process-level signals, they reached competitive performance while being $3.5\\times$ more compute-efficient and $1.5\\times$ more memory-efficient than 32B models. Larger models were more robust under sparse rewards but exhibited smaller relative gains from shaping and higher variance across runs. While curriculum learning offered no significant benefit, shaped rewards consistently amplified learning dynamics, making 8B models the most efficient setting for agentic RL. Crucially, these gains did not come at the cost of overfitting: fine-tuned models mostly maintained or exceeded baseline performance on out-of-domain tasks, including \\textsc{Multi-IF}, \\textsc{NaturalPlan}, and $τ$-\\textsc{Bench}. These results establish reward shaping as a decisive lever for scaling agentic RL, highlight the competitive strength of smaller models, and demonstrate that efficiency can be achieved without sacrificing generalization.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-09-30T04:49:36+00:00",
      "updated": "2025-10-01T20:23:23+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25779v2",
      "file": "papers/2509.25779v2.pdf"
    },
    {
      "arxiv_id": "2509.25774v2",
      "title": "PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models",
      "authors": [
        {
          "name": "Jeongjae Lee"
        },
        {
          "name": "Jong Chul Ye"
        }
      ],
      "abstract": "While reinforcement learning has advanced the alignment of text-to-image (T2I) models, state-of-the-art policy gradient methods are still hampered by training instability and high variance, hindering convergence speed and compromising image quality. Our analysis identifies a key cause of this instability: disproportionate credit assignment, in which the mathematical structure of the generative sampler produces volatile and non-proportional feedback across timesteps. To address this, we introduce Proportionate Credit Policy Optimization (PCPO), a framework that enforces proportional credit assignment through a stable objective reformulation and a principled reweighting of timesteps. This correction stabilizes the training process, leading to significantly accelerated convergence and superior image quality. The improvement in quality is a direct result of mitigating model collapse, a common failure mode in recursive training. PCPO substantially outperforms existing policy gradient baselines on all fronts, including the state-of-the-art DanceGRPO. Code is available at https://github.com/jaylee2000/pcpo/.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-09-30T04:43:58+00:00",
      "updated": "2025-12-06T07:55:50+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25774v2",
      "file": "papers/2509.25774v2.pdf"
    },
    {
      "arxiv_id": "2509.25762v1",
      "title": "OPPO: Accelerating PPO-based RLHF via Pipeline Overlap",
      "authors": [
        {
          "name": "Kaizhuo Yan"
        },
        {
          "name": "Yingjie Yu"
        },
        {
          "name": "Yifan Yu"
        },
        {
          "name": "Haizhong Zheng"
        },
        {
          "name": "Fan Lai"
        }
      ],
      "abstract": "Proximal Policy Optimization (PPO)-based reinforcement learning from human feedback (RLHF) is a widely adopted paradigm for aligning large language models (LLMs) with human preferences. However, its training pipeline suffers from substantial inefficiencies due to sequential multi-model dependencies (e.g., reward model depends on actor outputs) and long-tail response lengths, where a few long responses straggle the stage completion. We present OPPO, a novel, lightweight, and model-agnostic PPO-based RLHF framework that improves training efficiency by overlapping pipeline execution. OPPO introduces two novel techniques: (1) Intra-step overlap, which streams upstream model outputs (e.g., actor model) in right-sized chunks, enabling the downstream model (e.g., reward) to begin prefill while the upstream continues decoding; and (2) Inter-step overlap, which adaptively overcommits a few prompts and defers long generations to future steps, mitigating tail latency without discarding partial work. OPPO integrates easily with existing PPO implementations with a few lines of code change. Extensive evaluations show that OPPO accelerates PPO-based RLHF training by $1.8 \\times-2.8 \\times$ and improves GPU utilization by $1.4 \\times-2.1 \\times$ without compromising training convergence.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-09-30T04:26:17+00:00",
      "updated": "2025-09-30T04:26:17+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25762v1",
      "file": "papers/2509.25762v1.pdf"
    },
    {
      "arxiv_id": "2509.25756v2",
      "title": "SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies via Velocity-Reparameterized Sequential Modeling",
      "authors": [
        {
          "name": "Yixian Zhang"
        },
        {
          "name": "Shu'ang Yu"
        },
        {
          "name": "Tonghe Zhang"
        },
        {
          "name": "Mo Guang"
        },
        {
          "name": "Haojia Hui"
        },
        {
          "name": "Kaiwen Long"
        },
        {
          "name": "Yu Wang"
        },
        {
          "name": "Chao Yu"
        },
        {
          "name": "Wenbo Ding"
        }
      ],
      "abstract": "Training expressive flow-based policies with off-policy reinforcement learning is notoriously unstable due to gradient pathologies in the multi-step action sampling process. We trace this instability to a fundamental connection: the flow rollout is algebraically equivalent to a residual recurrent computation, making it susceptible to the same vanishing and exploding gradients as RNNs. To address this, we reparameterize the velocity network using principles from modern sequential models, introducing two stable architectures: Flow-G, which incorporates a gated velocity, and Flow-T, which utilizes a decoded velocity. We then develop a practical SAC-based algorithm, enabled by a noise-augmented rollout, that facilitates direct end-to-end training of these policies. Our approach supports both from-scratch and offline-to-online learning and achieves state-of-the-art performance on continuous control and robotic manipulation benchmarks, eliminating the need for common workarounds like policy distillation or surrogate objectives.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "published": "2025-09-30T04:21:20+00:00",
      "updated": "2025-10-26T04:37:16+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25756v2",
      "file": "papers/2509.25756v2.pdf"
    },
    {
      "arxiv_id": "2509.25751v1",
      "title": "Cooperative Autonomous Driving in Diverse Behavioral Traffic: A Heterogeneous Graph Reinforcement Learning Approach",
      "authors": [
        {
          "name": "Qi Liu"
        },
        {
          "name": "Xueyuan Li"
        },
        {
          "name": "Zirui Li"
        },
        {
          "name": "Juhui Gim"
        }
      ],
      "abstract": "Navigating heterogeneous traffic environments with diverse driving styles poses a significant challenge for autonomous vehicles (AVs) due to their inherent complexity and dynamic interactions. This paper addresses this challenge by proposing a heterogeneous graph reinforcement learning (GRL) framework enhanced with an expert system to improve AV decision-making performance. Initially, a heterogeneous graph representation is introduced to capture the intricate interactions among vehicles. Then, a heterogeneous graph neural network with an expert model (HGNN-EM) is proposed to effectively encode diverse vehicle features and produce driving instructions informed by domain-specific knowledge. Moreover, the double deep Q-learning (DDQN) algorithm is utilized to train the decision-making model. A case study on a typical four-way intersection, involving various driving styles of human vehicles (HVs), demonstrates that the proposed method has superior performance over several baselines regarding safety, efficiency, stability, and convergence rate, all while maintaining favorable real-time performance.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-09-30T04:12:57+00:00",
      "updated": "2025-09-30T04:12:57+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25751v1",
      "file": "papers/2509.25751v1.pdf"
    },
    {
      "arxiv_id": "2509.25727v1",
      "title": "Boundary-to-Region Supervision for Offline Safe Reinforcement Learning",
      "authors": [
        {
          "name": "Huikang Su"
        },
        {
          "name": "Dengyun Peng"
        },
        {
          "name": "Zifeng Zhuang"
        },
        {
          "name": "YuHan Liu"
        },
        {
          "name": "Qiguang Chen"
        },
        {
          "name": "Donglin Wang"
        },
        {
          "name": "Qinghe Liu"
        }
      ],
      "abstract": "Offline safe reinforcement learning aims to learn policies that satisfy predefined safety constraints from static datasets. Existing sequence-model-based methods condition action generation on symmetric input tokens for return-to-go and cost-to-go, neglecting their intrinsic asymmetry: return-to-go (RTG) serves as a flexible performance target, while cost-to-go (CTG) should represent a rigid safety boundary. This symmetric conditioning leads to unreliable constraint satisfaction, especially when encountering out-of-distribution cost trajectories. To address this, we propose Boundary-to-Region (B2R), a framework that enables asymmetric conditioning through cost signal realignment . B2R redefines CTG as a boundary constraint under a fixed safety budget, unifying the cost distribution of all feasible trajectories while preserving reward structures. Combined with rotary positional embeddings , it enhances exploration within the safe region. Experimental results show that B2R satisfies safety constraints in 35 out of 38 safety-critical tasks while achieving superior reward performance over baseline methods. This work highlights the limitations of symmetric token conditioning and establishes a new theoretical and practical approach for applying sequence models to safe RL. Our code is available at https://github.com/HuikangSu/B2R.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "published": "2025-09-30T03:38:20+00:00",
      "updated": "2025-09-30T03:38:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25727v1",
      "file": "papers/2509.25727v1.pdf"
    },
    {
      "arxiv_id": "2509.25666v1",
      "title": "Nudging the Boundaries of LLM Reasoning",
      "authors": [
        {
          "name": "Justin Chih-Yao Chen"
        },
        {
          "name": "Becky Xiangyu Peng"
        },
        {
          "name": "Prafulla Kumar Choubey"
        },
        {
          "name": "Kung-Hsiang Huang"
        },
        {
          "name": "Jiaxin Zhang"
        },
        {
          "name": "Mohit Bansal"
        },
        {
          "name": "Chien-Sheng Wu"
        }
      ],
      "abstract": "Current online reinforcement learning (RL) algorithms like GRPO share a key limitation in LLM reasoning: they cannot learn from problems that are \"unsolvable\" to the model. In other words, they can only improve performance on problems where the model is capable of exploring the correct answer. Consequently, the model's \"upper limit\" remains unchanged after RL training, even though the likelihood of solving easier, solvable problems may increase. These hard samples cannot contribute to training, as no rollouts yield rewards and thus no gradients are produced. To unlock learning from these hard samples, we propose NuRL, a \"nudging\" method that aims to push the upper bound of LLM reasoning using self-generated hints, i.e., abstract cues that help reduce the problem difficulty for the model. Given a question and its gold answer, the model generates a CoT and then produces a hint containing the core knowledge needed to solve the problem. During training, we generate G rollouts from the base policy and use the pass rate to decide whether the hint should be injected. For hard samples with a 0% pass rate, we inject the hint and regenerate a new batch of trajectories. This yields two benefits: (1) the hint boosts pass rates (from 0% to non-zero), thereby introducing training signals for previously unsolvable samples, and (2) the hints are self-generated, avoiding distributional shift and do not rely on external models. NuRL achieves consistent improvements across 6 benchmarks and 3 models, while remaining complementary to test-time scaling. Notably, NuRL can raise the model's upper limit, whereas GRPO leaves pass@1024 unchanged from the base model. Furthermore, we present a systematic study of what makes an effective hint and when hints are most useful. Interestingly, the best hints are abstract and high-level, and are most beneficial when applied necessarily and after GRPO has converged.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2025-09-30T02:01:40+00:00",
      "updated": "2025-09-30T02:01:40+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25666v1",
      "file": "papers/2509.25666v1.pdf"
    },
    {
      "arxiv_id": "2509.25598v1",
      "title": "Hybrid Reward Normalization for Process-supervised Non-verifiable Agentic Tasks",
      "authors": [
        {
          "name": "Peiran Xu"
        },
        {
          "name": "Zhuohao Li"
        },
        {
          "name": "Xiaoying Xing"
        },
        {
          "name": "Guannan Zhang"
        },
        {
          "name": "Debiao Li"
        },
        {
          "name": "Kunyu Shi"
        }
      ],
      "abstract": "Large Language Models (LLMs) increasingly rely on external tools such as search engines to solve complex agentic tasks that require reasoning and external knowledge retrieval. Recently, reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in advancing capabilities of LLMs by rewarding the final answers via outcome rewards. While straightforward to supervise, outcome rewards only provide sparse signals and delayed feedback, which limits their effectiveness on long trajectories. Process rewards address this by evaluating intermediate steps, providing fine-grained supervision and encouraging grounded problem solving. However, it is notoriously hard to annotate step-wise labels, especially in non-verifiable process without \"golden\" answers. Furthermore, step-wise judgment requires the balance between local quality with contribution to the final outcome, as optimizing towards higher process reward may not always align with better final outcomes. To address the above challenges, we introduce Principle Process Reward (PPR), an RL approach that unifies principled step-level assessment and outcome verification. We train a principle-based reward model to improve the transparency and reliability of process evaluation, and further introduce a Reward Normalization (ReNorm) strategy to calibrate outcome and process rewards. Experiment results show that PPR achieves state-of-the-art performance across a wide range of benchmarks, demonstrating its impressive robustness and generalization. Our code and model collection is available in this link.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-09-29T23:44:55+00:00",
      "updated": "2025-09-29T23:44:55+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25598v1",
      "file": "papers/2509.25598v1.pdf"
    },
    {
      "arxiv_id": "2509.25582v1",
      "title": "Safe In-Context Reinforcement Learning",
      "authors": [
        {
          "name": "Amir Moeini"
        },
        {
          "name": "Minjae Kwon"
        },
        {
          "name": "Alper Kamil Bozkurt"
        },
        {
          "name": "Yuichi Motai"
        },
        {
          "name": "Rohan Chandra"
        },
        {
          "name": "Lu Feng"
        },
        {
          "name": "Shangtong Zhang"
        }
      ],
      "abstract": "In-context reinforcement learning (ICRL) is an emerging RL paradigm where the agent, after some pretraining procedure, is able to adapt to out-of-distribution test tasks without any parameter updates. The agent achieves this by continually expanding the input (i.e., the context) to its policy neural networks. For example, the input could be all the history experience that the agent has access to until the current time step. The agent's performance improves as the input grows, without any parameter updates. In this work, we propose the first method that promotes the safety of ICRL's adaptation process in the framework of constrained Markov Decision Processes. In other words, during the parameter-update-free adaptation process, the agent not only maximizes the reward but also minimizes an additional cost function. We also demonstrate that our agent actively reacts to the threshold (i.e., budget) of the cost tolerance. With a higher cost budget, the agent behaves more aggressively, and with a lower cost budget, the agent behaves more conservatively.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-09-29T23:07:32+00:00",
      "updated": "2025-09-29T23:07:32+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25582v1",
      "file": "papers/2509.25582v1.pdf"
    },
    {
      "arxiv_id": "2509.25562v1",
      "title": "IRIS: Intrinsic Reward Image Synthesis",
      "authors": [
        {
          "name": "Yihang Chen"
        },
        {
          "name": "Yuanhao Ban"
        },
        {
          "name": "Yunqi Hong"
        },
        {
          "name": "Cho-Jui Hsieh"
        }
      ],
      "abstract": "Despite the success of Reinforcement Learning from Human Feedback (RLHF) in language reasoning, its application to autoregressive Text-to-Image (T2I) generation is often constrained by the limited availability of human preference data. This paper explores how an autoregressive T2I model can learn from internal signals without relying on external rewards or labeled data. Contrary to recent findings in text generation, we show that maximizing self-uncertainty, rather than self-certainty, improves image generation. We observe that this is because autoregressive T2I models with low uncertainty tend to generate simple and uniform images, which are less aligned with human preferences. Based on these observations, we propose IRIS (Intrinsic Reward Image Synthesis), the first framework to improve autoregressive T2I models with reinforcement learning using only an intrinsic reward. Empirical results demonstrate that applying IRIS to autoregressive T2I models achieves performance that is competitive with or superior to external rewards.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-09-29T22:38:25+00:00",
      "updated": "2025-09-29T22:38:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25562v1",
      "file": "papers/2509.25562v1.pdf"
    },
    {
      "arxiv_id": "2509.25550v3",
      "title": "Learning to Interact in World Latent for Team Coordination",
      "authors": [
        {
          "name": "Dongsu Lee"
        },
        {
          "name": "Daehee Lee"
        },
        {
          "name": "Yaru Niu"
        },
        {
          "name": "Honguk Woo"
        },
        {
          "name": "Amy Zhang"
        },
        {
          "name": "Ding Zhao"
        }
      ],
      "abstract": "This work presents a novel representation learning framework, interactive world latent (IWoL), to facilitate team coordination in multi-agent reinforcement learning (MARL). Building effective representation for team coordination is a challenging problem, due to the intricate dynamics emerging from multi-agent interaction and incomplete information induced by local observations. Our key insight is to construct a learnable representation space that jointly captures inter-agent relations and task-specific world information by directly modeling communication protocols. This representation, we maintain fully decentralized execution with implicit coordination, all while avoiding the inherent drawbacks of explicit message passing, e.g., slower decision-making, vulnerability to malicious attackers, and sensitivity to bandwidth constraints. In practice, our representation can be used not only as an implicit latent for each agent, but also as an explicit message for communication. Across four challenging MARL benchmarks, we evaluate both variants and show that IWoL provides a simple yet powerful key for team coordination. Moreover, we demonstrate that our representation can be combined with existing MARL algorithms to further enhance their performance.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-09-29T22:13:39+00:00",
      "updated": "2025-10-02T20:45:00+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25550v3",
      "file": "papers/2509.25550v3.pdf"
    },
    {
      "arxiv_id": "2509.25543v1",
      "title": "Aligning Multilingual Reasoning with Verifiable Semantics from a High-Resource Expert Model",
      "authors": [
        {
          "name": "Fahim Faisal"
        },
        {
          "name": "Kaiqiang Song"
        },
        {
          "name": "Song Wang"
        },
        {
          "name": "Simin Ma"
        },
        {
          "name": "Shujian Liu"
        },
        {
          "name": "Haoyun Deng"
        },
        {
          "name": "Sathish Reddy Indurthi"
        }
      ],
      "abstract": "While reinforcement learning has advanced the reasoning abilities of Large Language Models (LLMs), these gains are largely confined to English, creating a significant performance disparity across languages. To address this, we introduce Pivot-Based Reinforcement Learning with Semantically Verifiable Rewards (PB-RLSVR), a novel framework that enhances multilingual reasoning by circumventing the need for human-annotated data in target languages. Our approach employs a high-performing English LLM as a \"pivot\" model to generate reference responses for reasoning tasks. A multilingual model is then rewarded based on the semantic equivalence of its responses to the English reference, effectively transferring the pivot model's reasoning capabilities across languages. We investigate several cross-lingual semantic reward functions, including those based on embeddings and machine translation. Extensive experiments on a suite of multilingual reasoning benchmarks show that our method significantly narrows the performance gap between English and other languages, substantially outperforming traditional PPO baselines. Specifically, our PB-RLSVR framework improves the average multilingual performance of Llama-3.1-8B-Instruct and Qwen3-32B by 16.41% and 10.17%, respectively, demonstrating a powerful and data-efficient approach to building truly multilingual reasoning agents.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-09-29T22:03:11+00:00",
      "updated": "2025-09-29T22:03:11+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25543v1",
      "file": "papers/2509.25543v1.pdf"
    },
    {
      "arxiv_id": "2509.25541v1",
      "title": "Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play",
      "authors": [
        {
          "name": "Qinsi Wang"
        },
        {
          "name": "Bo Liu"
        },
        {
          "name": "Tianyi Zhou"
        },
        {
          "name": "Jing Shi"
        },
        {
          "name": "Yueqian Lin"
        },
        {
          "name": "Yiran Chen"
        },
        {
          "name": "Hai Helen Li"
        },
        {
          "name": "Kun Wan"
        },
        {
          "name": "Wentian Zhao"
        }
      ],
      "abstract": "Although reinforcement learning (RL) can effectively enhance the reasoning capabilities of vision-language models (VLMs), current methods remain heavily dependent on labor-intensive datasets that require extensive manual construction and verification, leading to extremely high training costs and consequently constraining the practical deployment of VLMs. To address this challenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM self-improvement through competitive visual games generated from arbitrary image pairs. Specifically, Vision-Zero encompasses three main attributes: (1) Strategic Self-Play Framework: Vision-Zero trains VLMs in \"Who Is the Spy\"-style games, where the models engage in strategic reasoning and actions across multiple roles. Through interactive gameplay, models autonomously generate their training data without human annotation. (2) Gameplay from Arbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate games from arbitrary images, thereby enhancing the model's reasoning ability across diverse domains and showing strong generalization to different tasks. We demonstrate this versatility using three distinct types of image datasets: CLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable Performance Gain: We introduce Iterative Self-Play Policy Optimization (Iterative-SPO), a novel training algorithm that alternates between Self-Play and reinforcement learning with verifiable rewards (RLVR), mitigating the performance plateau often seen in self-play-only training and achieving sustained long-term improvements. Despite using label-free data, Vision-Zero achieves state-of-the-art performance on reasoning, chart question answering, and vision-centric understanding tasks, surpassing other annotation-based methods. Models and code has been released at https://github.com/wangqinsi1/Vision-Zero.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-09-29T21:55:55+00:00",
      "updated": "2025-09-29T21:55:55+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25541v1",
      "file": "papers/2509.25541v1.pdf"
    },
    {
      "arxiv_id": "2509.25518v2",
      "title": "World Model for AI Autonomous Navigation in Mechanical Thrombectomy",
      "authors": [
        {
          "name": "Harry Robertshaw"
        },
        {
          "name": "Han-Ru Wu"
        },
        {
          "name": "Alejandro Granados"
        },
        {
          "name": "Thomas C Booth"
        }
      ],
      "abstract": "Autonomous navigation for mechanical thrombectomy (MT) remains a critical challenge due to the complexity of vascular anatomy and the need for precise, real-time decision-making. Reinforcement learning (RL)-based approaches have demonstrated potential in automating endovascular navigation, but current methods often struggle with generalization across multiple patient vasculatures and long-horizon tasks. We propose a world model for autonomous endovascular navigation using TD-MPC2, a model-based RL algorithm. We trained a single RL agent across multiple endovascular navigation tasks in ten real patient vasculatures, comparing performance against the state-of-the-art Soft Actor-Critic (SAC) method. Results indicate that TD-MPC2 significantly outperforms SAC in multi-task learning, achieving a 65% mean success rate compared to SAC's 37%, with notable improvements in path ratio. TD-MPC2 exhibited increased procedure times, suggesting a trade-off between success rate and execution speed. These findings highlight the potential of world models for improving autonomous endovascular navigation and lay the foundation for future research in generalizable AI-driven robotic interventions.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.RO",
        "eess.IV"
      ],
      "published": "2025-09-29T21:21:30+00:00",
      "updated": "2025-10-02T05:02:54+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25518v2",
      "file": "papers/2509.25518v2.pdf"
    },
    {
      "arxiv_id": "2509.25454v2",
      "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search",
      "authors": [
        {
          "name": "Fang Wu"
        },
        {
          "name": "Weihao Xuan"
        },
        {
          "name": "Heli Qi"
        },
        {
          "name": "Ximing Lu"
        },
        {
          "name": "Aaron Tu"
        },
        {
          "name": "Li Erran Li"
        },
        {
          "name": "Yejin Choi"
        }
      ],
      "abstract": "Although RLVR has become an essential component for developing advanced reasoning skills in LLMs, contemporary studies have documented training plateaus that emerge following thousands of optimization steps, demonstrating notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, a framework that integrates Monte Carlo Tree Search directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) a global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning models - using 5.7x fewer GPU hours than extended training approaches. These results highlight the importance of strategic exploration over brute-force scaling and demonstrate the promise of algorithmic innovation for advancing RLVR methodologies. DeepSearch establishes a new direction for scaling reasoning capabilities through systematic search rather than prolonged computation.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-09-29T20:00:29+00:00",
      "updated": "2025-10-01T05:09:42+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25454v2",
      "file": "papers/2509.25454v2.pdf"
    },
    {
      "arxiv_id": "2509.25438v1",
      "title": "Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring",
      "authors": [
        {
          "name": "Zhibo Hou"
        },
        {
          "name": "Zhiyu An"
        },
        {
          "name": "Wan Du"
        }
      ],
      "abstract": "When there exists an unlearnable source of randomness (noisy-TV) in the environment, a naively intrinsic reward driven exploring agent gets stuck at that source of randomness and fails at exploration. Intrinsic reward based on uncertainty estimation or distribution similarity, while eventually escapes noisy-TVs as time unfolds, suffers from poor sample efficiency and high computational cost. Inspired by recent findings from neuroscience that humans monitor their improvements during exploration, we propose a novel method for intrinsically-motivated exploration, named Learning Progress Monitoring (LPM). During exploration, LPM rewards model improvements instead of prediction error or novelty, effectively rewards the agent for observing learnable transitions rather than the unlearnable transitions. We introduce a dual-network design that uses an error model to predict the expected prediction error of the dynamics model in its previous iteration, and use the difference between the model errors of the current iteration and previous iteration to guide exploration. We theoretically show that the intrinsic reward of LPM is zero-equivariant and a monotone indicator of Information Gain (IG), and that the error model is necessary to achieve monotonicity correspondence with IG. We empirically compared LPM against state-of-the-art baselines in noisy environments based on MNIST, 3D maze with 160x120 RGB inputs, and Atari. Results show that LPM's intrinsic reward converges faster, explores more states in the maze experiment, and achieves higher extrinsic reward in Atari. This conceptually simple approach marks a shift-of-paradigm of noise-robust exploration. For code to reproduce our experiments, see https://github.com/Akuna23Matata/LPM_exploration",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-09-29T19:43:44+00:00",
      "updated": "2025-09-29T19:43:44+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25438v1",
      "file": "papers/2509.25438v1.pdf"
    },
    {
      "arxiv_id": "2509.25424v1",
      "title": "Polychromic Objectives for Reinforcement Learning",
      "authors": [
        {
          "name": "Jubayer Ibn Hamid"
        },
        {
          "name": "Ifdita Hasan Orney"
        },
        {
          "name": "Ellen Xu"
        },
        {
          "name": "Chelsea Finn"
        },
        {
          "name": "Dorsa Sadigh"
        }
      ],
      "abstract": "Reinforcement learning fine-tuning (RLFT) is a dominant paradigm for improving pretrained policies for downstream tasks. These pretrained policies, trained on large datasets, produce generations with a broad range of promising but unrefined behaviors. Often, a critical failure mode of RLFT arises when policies lose this diversity and collapse into a handful of easily exploitable outputs. This convergence hinders exploration, which is essential for expanding the capabilities of the pretrained policy and for amplifying the benefits of test-time compute scaling. To address this, we introduce an objective for policy gradient methods that explicitly enforces the exploration and refinement of diverse generations, which we call a polychromic objective. We then show how proximal policy optimization (PPO) can be adapted to optimize this objective. Our method (1) employs vine sampling to collect on-policy rollouts and (2) modifies the advantage function to reflect the advantage under our new objective. Experiments on BabyAI, Minigrid, and Algorithmic Creativity show that our method improves success rates by reliably solving a larger set of environment configurations and generalizes better under large perturbations. Moreover, when given multiple attempts in pass@$k$ experiments, the policy achieves substantially higher coverage, demonstrating its ability to maintain and exploit a diverse repertoire of strategies.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-09-29T19:32:11+00:00",
      "updated": "2025-09-29T19:32:11+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25424v1",
      "file": "papers/2509.25424v1.pdf"
    },
    {
      "arxiv_id": "2509.25409v1",
      "title": "From Faithfulness to Correctness: Generative Reward Models that Think Critically",
      "authors": [
        {
          "name": "Qiyao Ma"
        },
        {
          "name": "Yunsheng Shi"
        },
        {
          "name": "Hongtao Tian"
        },
        {
          "name": "Chao Wang"
        },
        {
          "name": "Weiming Chang"
        },
        {
          "name": "Ting Yao"
        }
      ],
      "abstract": "Through reinforcement learning with verifiable rewards (RLVR), large language models have achieved substantial progress in domains with easily verifiable outcomes, such as mathematics and coding. However, when applied to more complex tasks like open-domain question answering, RLVR faces significant challenges due to the difficulty of verifying correctness. The nuanced and ambiguous nature of real-world knowledge makes it difficult to reliably evaluate correctness in these settings, necessitating further abilities that extend beyond mere logical consistency to encompass an understanding and assessment of both external and internal knowledge. Recent work has primarily focused on improving faithfulness, defined as semantic alignment with supporting documents, which can cause models to rely excessively on external sources and diminish their capacity for critical assessment. To address this, we propose the Thinking-supervised Reward Model (TRM), which incorporates sentence-level thinking supervision to endow reward models with critical thinking abilities. Given a query, answer, and supporting documents, TRM first assesses the faithfulness of each answer sentence to the supporting documents, and then applies a reasoning step to evaluate sentence-level correctness. By structuring reward modeling as a sequence of faithfulness, reasoning, and correctness evaluations, TRM encourages models to critically assess and leverage both external and internal knowledge. Experiments on reward signals demonstrate that TRM substantially improves the identification of incorrect sentences, and incorporating TRM into policy optimization leads to significant gains in both answer correctness and usefulness.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-09-29T19:06:56+00:00",
      "updated": "2025-09-29T19:06:56+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25409v1",
      "file": "papers/2509.25409v1.pdf"
    },
    {
      "arxiv_id": "2509.25176v1",
      "title": "SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression",
      "authors": [
        {
          "name": "Haoming Wen"
        },
        {
          "name": "Yushi Bai"
        },
        {
          "name": "Juanzi Li"
        },
        {
          "name": "Jie Tang"
        }
      ],
      "abstract": "We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleaved Compression, a simple yet effective RL approach for Large Reasoning Models (LRMs) that enables more efficient and accurate reasoning. Existing studies have observed repetitive thinking patterns in LRMs, and attempts to reduce them often come at the cost of performance. In this paper, we show that this trade-off can be overcome through a training regime that iteratively alternates between compressing and expanding the reasoning budget, by dynamically adjusting the maximum rollout length during training. The compression phase cuts the rollout length, forcing the model to make precise and valuable decisions within a limited context, which effectively reduces redundant tokens and increases reasoning density. The expansion phase then relaxes the length limit, providing space for the model to explore and plan in long-horizon settings. Remarkably, we find that after each compression-expansion cycle, the model's performance improves even as its output length decreases, steadily pushing it closer to the Pareto frontier in the performance-efficiency trade-off. Training on DeepSeek-R1-Distill-Qwen-1.5B, SIRI-low improves performance on AIME24 by 43.2% while reducing token usage by 46.9% after three iterations, and SIRI-high achieves the highest accuracy compared to all other methods (Figure 1). Our findings shed light on the potential of periodically oscillating the LRM's output truncation length during training to dynamically balance exploration and efficiency in reasoning, converging towards an optimal \"sweet spot\" between the two. Our models are publicly available.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2025-09-29T17:59:08+00:00",
      "updated": "2025-09-29T17:59:08+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25176v1",
      "file": "papers/2509.25176v1.pdf"
    },
    {
      "arxiv_id": "2509.25174v1",
      "title": "XQC: Well-conditioned Optimization Accelerates Deep Reinforcement Learning",
      "authors": [
        {
          "name": "Daniel Palenicek"
        },
        {
          "name": "Florian Vogt"
        },
        {
          "name": "Joe Watson"
        },
        {
          "name": "Ingmar Posner"
        },
        {
          "name": "Jan Peters"
        }
      ],
      "abstract": "Sample efficiency is a central property of effective deep reinforcement learning algorithms. Recent work has improved this through added complexity, such as larger models, exotic network architectures, and more complex algorithms, which are typically motivated purely by empirical performance. We take a more principled approach by focusing on the optimization landscape of the critic network. Using the eigenspectrum and condition number of the critic's Hessian, we systematically investigate the impact of common architectural design decisions on training dynamics. Our analysis reveals that a novel combination of batch normalization (BN), weight normalization (WN), and a distributional cross-entropy (CE) loss produces condition numbers orders of magnitude smaller than baselines. This combination also naturally bounds gradient norms, a property critical for maintaining a stable effective learning rate under non-stationary targets and bootstrapping. Based on these insights, we introduce XQC: a well-motivated, sample-efficient deep actor-critic algorithm built upon soft actor-critic that embodies these optimization-aware principles. We achieve state-of-the-art sample efficiency across 55 proprioception and 15 vision-based continuous control tasks, all while using significantly fewer parameters than competing methods.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-09-29T17:58:53+00:00",
      "updated": "2025-09-29T17:58:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2509.25174v1",
      "file": "papers/2509.25174v1.pdf"
    }
  ]
}