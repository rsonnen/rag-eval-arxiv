{
  "corpus_name": "Computational Biology Papers",
  "corpus_path": "/mnt/x/rag_datasets/arxiv_papers/computational_biology",
  "scenario": "graduate_exam",
  "mode": "textual",
  "questions": [
    {
      "question": "In the INFUSSE interpretability/ablation analysis that quantifies the contribution of structural graphs to B-factor prediction, how is the per-residue performance gain from adding the graph block defined (give the exact error difference, and state which secondary-structure types showed improvement vs no significant change when graph information was added)?",
      "answer": "INFUSSE defines per-residue error with and without the graph block as \u03b5^(q)_{Sblock,j} = ([Sblock(x^(q)_s, E^(q)_s)]_j \u2212 B^(q)_j)^2 and \u03b5^(q)_{INFUSSE,j} = ([INFUSSE(x^(q)_s, r^(q))]_j \u2212 B^(q)_j)^2. The gain from adding graph information is the difference \u0394^(q)_{graph,j} := \u03b5^(q)_{Sblock,j} \u2212 \u03b5^(q)_{INFUSSE,j}, collected over residues into \u0394_graph. Using this analysis across antigens\u2019 secondary structures, adding graph information improved performance for \u03b1-helices and for loops, but produced no significant change (relative to sequence-only prediction) for \u03b2-strands.",
      "source_document": "papers/2510.23975v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the INFUSSE training procedure for residue-level B-factor prediction, what are the two training steps (which parameters are learned/optimized in each step), and which component of the model is explicitly kept frozen throughout training?",
      "answer": "INFUSSE is trained in two steps by minimizing the mean squared error of B-factor prediction. (1) First, only the sequence block Sblock is trained: the learnable non-linear layers T1, T2, and T3 are learned to predict B-factors from sequence alone, using the one-hot sequence input x_s and the ProtBERT embedding E_s. (2) Second, the graph block Gblock is added and training optimizes the diffusive GCN parameters (t and W^(l)) jointly with T1\u2013T3, initializing T1\u2013T3 from the values obtained in step 1. The ProtBERT language model is kept frozen the entire time and is used only to compute the input embeddings E_s.",
      "source_document": "papers/2510.23975v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In ANTIPASTI\u2019s evaluation setup, what two constraints are imposed when constructing the random train/test splits to make the test set \u201cunseen,\u201d and where do the curated test datasets come from?",
      "answer": "The random splits are constructed so that (1) antibodies in the training and test sets share less than 90% sequence identity, and (2) the antigens in the training and test sets are different. The curated unseen test datasets are taken from SAbDab.",
      "source_document": "papers/2510.23975v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In ANTIPASTI\u2019s *model-dependent* interpretability analysis, the authors reshape the learned weights into an \u201cinterpretable 2D map\u201d \\(F\\). How is \\(F_{jk}\\) defined (what sum/indexing does it use and what quantities does it combine), and how do the *signs* of entries in \\(F\\) relate to binding affinity when rewriting the output equation for \\(\\log_{10}(\\hat K_D)\\)?",
      "answer": "They define an interpretable map \\(F\\) by reshaping the parameters of the bias-free output layer so that each entry aggregates the contribution of a specific (pooled) spatial position across convolutional filters. Concretely, \\(F_{jk}\\) is defined as a negative sum over the convolutional filters of the product of the learned fully-connected weight corresponding to that pooled position and the corresponding pooled activation (so \\(F_{jk}:= -\\sum_{\\ell=0}^{n_f-1} w_{(j,k,\\ell)}\\, z_{(j,k,\\ell)}\\), with indices \\(j,k\\) running over the pooled spatial grid and \\(\\ell\\) over filters). With this construction, equation (8) can be rewritten as \\(\\log_{10}(\\hat K_D)= -\\sum_{j,k} F_{jk}\\). Therefore, positive entries of \\(F\\) correspond to residue\u2013residue correlations that contribute to *high* binding affinity (lower \\(K_D\\)), while negative entries correspond to correlations driving *low* binding affinity (higher \\(K_D\\)); the authors refer to these \\(F_{jk}\\) pairs as \u201caffinity-relevant correlations.\u201d",
      "source_document": "papers/2510.23975v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In INFUSSE\u2019s graph block, what mathematical operator is used to modulate message passing, what matrix is it built from, and what does the learnable parameter that appears in it control?",
      "answer": "INFUSSE uses the graph diffusion operator \\(e^{-tL}\\) to modulate message passing. It is built from the graph Laplacian matrix \\(L\\) of the geometric structure graph, and the learnable scalar \\(t>0\\) controls the effective range (scale) of information flow / propagation on the graph.",
      "source_document": "papers/2510.23975v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In FolDE\u2019s few-shot mode, the authors train the MLP top-layer with a Bradley\u2013Terry ranking loss and describe a nontrivial way to split the directed training pairs into train/validation. How exactly do they construct and split the directed pairs, and what specific condition do they enforce so the validation set cannot be \u201ctrivially solved\u201d from training pairs (give the example they mention)?",
      "answer": "They form directed pairs from the data (all ordered pairs within a batch, where the direction corresponds to the higher-activity sequence) and train the Bradley\u2013Terry ranking loss on these directed pairs. For both the warm-start step and the activity-training step, they split the directed pairs 80/20 into train and validation using a breadth-first-search procedure that prevents transitive leakage: the validation set must not contain a pair that is implied by training pairs. Their explicit example is that if training contains (A,B) and (B,C) with A > B and B > C, then the validation set will not include (A,C).",
      "source_document": "papers/2510.24053v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In FolDE\u2019s constant-liar (CL) batch selection, what specific \u201clie\u201d value do the authors use for the imagined observation after choosing the first (UCB-maximizing) candidate, and what part of the CL update do they emphasize is independent of that lie value?",
      "answer": "They use a pessimistic lie equal to the minimum predicted label among the candidate set (y_lie = min y_i). They emphasize that the covariance-matrix posterior update (\u03a3\u2032 = \u03a3_{M\\i} \u2212 v v^T / \u03c3_i^2) is independent of the value of the lie; only the mean update depends on y_lie.",
      "source_document": "papers/2510.24053v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In FolDE\u2019s \u201cfew-shot mode\u201d (round-2 and beyond), the authors specify an explicit MLP top-layer architecture and training protocol. What embedding model and embedding dimensionality do they use as input, what are the hidden-layer sizes and key architectural choices (normalization/activations/dropout/final-layer bias), and which optimizer + early-stopping settings do they use for (i) the naturalness warm-start phase vs (ii) the activity-training phase?",
      "answer": "Input embeddings are produced by a protein language model using ESMC-300M; sequences are embedded by a forward pass through the PLM followed by mean-pooling the final hidden layer, yielding 960-dimensional embeddings.\n\nThe ensemble model is five MLPs mapping 960-d embeddings to a scalar with hidden layers 960 \u2192 100 \u2192 50 \u2192 1. They apply batch normalization after each hidden layer, use ReLU activations and dropout with p = 0.2, use no bias in the final linear layer, and train with a Bradley\u2013Terry ranking loss.\n\nOptimization uses Adam with learning rate 3e\u22124 and weight decay 1e\u22125, with automatic mixed precision.\n\nEarly stopping differs by phase: during warm-start they train up to 50 epochs with early stopping on validation loss (patience 20 epochs; validation every 5 epochs). During activity training they train up to 200 epochs with early stopping on validation loss (patience 40 epochs; validation every 10 epochs).",
      "source_document": "papers/2510.24053v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In FolDE\u2019s few-shot mode, the authors average predictions across an ensemble of 5 Bradley\u2013Terry-trained MLPs. Because the Bradley\u2013Terry objective is translation-invariant, what exact post-processing step do they apply to the per-model predictions before averaging, and what issue is this intended to correct?",
      "answer": "They de-mean (center) each ensemble member\u2019s predictions within that model before computing the consensus as the mean across models. This corrects the translation invariance of the Bradley\u2013Terry ranking loss\u2014i.e., the fact that each model\u2019s outputs are only identifiable up to an additive constant\u2014so raw outputs from different seeds can be on arbitrary offsets and shouldn\u2019t be averaged without centering.",
      "source_document": "papers/2510.24053v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In FolDE\u2019s few-shot mode, how does the paper define and expand the pool of candidate mutants across rounds, and what is the stated reason this strategy supports \u201cbacktracking\u201d during optimization? Be explicit about what sequences are mutated in round-2 versus round-3.",
      "answer": "The candidate pool is expanded each round by generating single mutants of any previously *measured high-performing* sequence. Specifically, round-2 considers mutations of the wild-type sequence and the round-1 hits; round-3 adds mutations of the round-2 hits (in addition to what was already considered). The paper states this progressive expansion allows exploration to move outward while \u201cmaintaining the ability to backtrack,\u201d i.e., because candidates are always single-step neighbors of already-measured good variants (including earlier hits), the search can return to/branch from earlier promising sequences rather than being forced to continue only along the most recent path.",
      "source_document": "papers/2510.24053v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Appendix A.4, the authors argue that evaluating a cofolding model with best@k avoids conflating pose generation with pose selection. According to the document, what does best@k mean operationally, and what specific procedure do the authors use in practice to estimate best@k from a finite set of generated poses (include the values of n and how k is handled)?",
      "answer": "Operationally, best@k means generating k random samples (poses) from the model and then selecting the best one according to the metric of interest (e.g., RMSD success/validity). In practice, they pre-generate n = 20 poses per test complex and then compute an unbiased estimator of best@k (for k such as 1, 5, and 20) from those 20 samples, following the estimator proposed in reference [40].",
      "source_document": "papers/2510.24670v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the conditional (pocket-aware) cofolding evaluation protocol, how do the authors automatically choose which pocket residues to reveal to the model at inference time, and what specific constraints do they impose to prevent information leakage (include the distance cutoff, how many residues, and the sequence-separation rule)?",
      "answer": "They reveal at most two pocket residues per test complex. To pick the first, they compute for each residue the distances from each ligand atom to any residue atom within 6 \u00c5, then choose the residue with the smallest median distance to the ligand. To pick a second residue, they repeat the same \u201csmallest median distance\u201d procedure but require that this residue be separated from the first by at least 8 residues in protein sequence. The same \u22642 selected residues are provided to all evaluated models for that complex to avoid unfair leakage.",
      "source_document": "papers/2510.24670v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Pearl\u2019s \u201ctraining recipe,\u201d the authors describe a five-stage curriculum. What concrete changes do they say occur as training progresses from the initial to the later stages (name at least two changes, e.g., in crop sizes, data mixture, and/or use of templating/structural priors), and what are the three distinct data sources mixed for training overall?",
      "answer": "They train on a mixture of (1) curated PDB structures, (2) monomer distillation data (from OpenFold and the AlphaFold Database), and (3) a large-scale synthetic protein\u2013ligand dataset generated from public data with physics-based methods and diverse virtual ligands. The five-stage curriculum progressively increases task complexity and data diversity: early stages use smaller crop sizes and simpler mixtures (non-templated PDB plus monomer distillation), while later stages introduce more complex structural priors and templating information across the datasets (applying structural templates to vary the structural context).",
      "source_document": "papers/2510.24670v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "Pearl adopts a \u201cconservative\u201d bf16 mixed-precision strategy rather than training the whole model in bf16. According to the paper (Methods + Appendix B.2), which parts of the computation are run in bf16, and which specific components/operations are kept or upcast to fp32 to preserve numerical stability?",
      "answer": "Pearl executes most of the computationally heavy *trunk* in bfloat16 (bf16)\u2014including triangle operations (implemented via NVIDIA cuEquivariance kernels) and LayerNorm (via a custom CUDA kernel). In contrast, numerically sensitive pieces are kept in fp32: loss calculations and final/input/output coordinate projections (and other \u201cconditioned transitions\u201d noted in Appendix B.2). Additionally, numerically unstable operations such as softmax/exp/log are automatically upcast to fp32, and all model weights are stored in fp32 (with the Methods section also noting that softmax and weights remain fp32).",
      "source_document": "papers/2510.24670v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Appendix A.1 (\u201cDatasets and baselines\u201d), the authors impose rules on what PDB structures may be used as templates during evaluation to avoid unfairness/leakage. What are the two explicit template-eligibility constraints they state (include both the absolute release-date cutoff and the relative \u201cdays before test structure\u201d rule)?",
      "answer": "They allow a PDB structure to be used as a template only if (1) it was released prior to 2021-09-30, and (2) it was released more than 60 days before the release date of the test structure being evaluated.",
      "source_document": "papers/2510.24670v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Gosling Designer\u2019s supported-file-format description, what is the recommended workaround when a BED file lacks its required index, and what trade-off does the paper explicitly warn about for that workaround?",
      "answer": "The paper states that if a BED file is missing its required Tabix index, it can instead be added through the CSV/TSV file type; however, it warns this may have performance limitations for large files.",
      "source_document": "papers/2510.24888v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Gosling Designer\u2019s \u201cView Linking\u201d editing subpanel, how does the paper say a user should link more than two views, and what concrete example does it give of doing this pairwise linking?",
      "answer": "The paper states that view linking is done by specifying a pair of views to be linked; if more than two views need to be linked, the user creates an additional link for each pair (i.e., pairwise links). The given example is linking View 1 and View 2 and then linking View 2 and View 3.",
      "source_document": "papers/2510.24888v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the \u201cSharing Workspaces\u201d section, what is the specific UI action the paper says a user should take to share a workspace, and what three collaborator roles does it list as configurable in that sharing view?",
      "answer": "To share a workspace, the user clicks the \u201cCollaborators\u201d button on the top right corner in the header. In the resulting view, collaborators have roles that can be set as viewer, editor, or admin (administrator).",
      "source_document": "papers/2510.24888v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s \u201cSystem Structure\u201d description of Gosling Designer, what are the two major components in its backend\u2013frontend model, and what distinct responsibilities does the paper assign to each (including what the Visualization Editing Component manipulates to update the visualization)?",
      "answer": "The backend\u2013frontend model has two major components: (1) the Visualization Editing Component (VEC) and (2) the Data Management Component (DMC). The VEC handles user interactions for editing and exploration, uses Gosling.js to render interactive genomics visualizations, and it updates what\u2019s displayed by manipulating the underlying Gosling specification (JSON). The DMC manages information about users, workspaces, and data sources (including public URLs and metadata), enabling users to add data sources, workspaces, and collaborators to their accounts.",
      "source_document": "papers/2510.24888v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s \u201cVisualization Examples\u201d section, the authors describe a \u201cCoverage, Sequence, and Pileup Plots\u201d visualization where the sequence track changes representation as you zoom out. What specific transformation does the sequence track undergo upon zooming out, and what quantity is that transformed view summarizing?",
      "answer": "Upon zooming out, the sequence track transitions from color-coded bars for nucleotide types into a stacked bar chart that summarizes the frequency of bases.",
      "source_document": "papers/2510.24888v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In scMRDR\u2019s total training objective, what are the four loss components summed to form \\(L_{\\text{total}}\\), and which hyperparameters weight the alignment and structure-preservation terms?",
      "answer": "The paper defines the total objective as \\(L_{\\text{total}} = L_{\\text{recon}} + \\beta L_{\\text{KL}} + \\lambda L_{\\text{alignment}} + \\gamma L_{\\text{preserve}}\\). The alignment (adversarial) term is weighted by \\(\\lambda\\), and the structure-preservation (isometric) term is weighted by \\(\\gamma\\).",
      "source_document": "papers/2510.24987v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In scMRDR\u2019s \u201cmasked reconstruction loss for missing features\u201d (Sec. 3.5), what two concrete problems does the paper say are caused by (i) restricting training to only features overlapping across all modalities and (ii) naively imputing unmeasured features with zeros\u2014and what is the specific remedy scMRDR introduces to avoid these issues during backpropagation?",
      "answer": "(i) Restricting to overlapping features across all modalities causes substantial information loss; (ii) imputing unmeasured features with zeros severely distorts the data distribution. To avoid these issues, scMRDR introduces a binary feature-availability mask b\u2208{0,1}^G that prevents gradients from back-propagating through unmeasured features in the reconstruction loss (and rescales by the proportion of available features so reconstruction losses are comparable across samples).",
      "source_document": "papers/2510.24987v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Appendix A.6, how does scMRDR perform spatial location imputation after integrating scRNA, scATAC, and spatial transcriptomics (merFISH) data in mouse primary motor cortex\u2014i.e., between which two sets of samples is optimal transport computed, and which specific latent representation is used for this transport?",
      "answer": "After integrating the three modalities, the paper imputes spatial locations by performing optimal transport in the aligned shared latent space using z_u: it computes optimal transport between the z_u embeddings of samples that have spatial coordinates (the merFISH spatial transcriptomics cells) and the samples without spatial information (the scRNA and scATAC cells), then uses this to interpolate missing spatial locations for the single-cell (non-spatial) samples.",
      "source_document": "papers/2510.24987v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In scMRDR\u2019s adversarial regularization (Sec. 3.3), what is the discriminator\u2019s prediction task and label space, and how are the VAE encoders optimized relative to the discriminator objective (i.e., do they minimize the same loss or optimize it in the opposite direction)?",
      "answer": "The method introduces an m-class discriminator D(z_u): R^{d_u} -> {0,1,...,m} whose task is to predict which omics modality a shared embedding z_u comes from. The discriminator is trained to minimize the cross-entropy-style objective L_discriminator, while the VAE encoders q(z_u|x) are trained to fool it by optimizing in the opposite direction (maximizing the discriminator objective), equivalently minimizing the alignment loss L_alignment defined as the supremum over D of the same log-likelihood term.",
      "source_document": "papers/2510.24987v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In scMRDR\u2019s isometric structure-preservation regularizer (Sec. 3.4), which two pairwise Euclidean distance matrices are matched, what latent quantities are used to compute each distance (be precise about whether it is the posterior mean and which latent subspace), and how is this term incorporated into the overall VAE training objective (include the weighting coefficient name)?",
      "answer": "The isometric loss matches, within each modality, (i) the pairwise Euclidean distances between cells computed from the shared latent embedding and (ii) the pairwise Euclidean distances computed from the full latent embedding. Concretely, it uses posterior means: distances ||\u03bc_zu(x_i)\u2212\u03bc_zu(x_j)||_2 are computed from the posterior mean of q(zu|x) (shared subspace), and distances ||\u03bc_z(x_i)\u2212\u03bc_z(x_j)||_2 are computed from the posterior mean of the concatenated latent (q(zu|x), q(zs|x,m)) (full latent z=(zu,zs)). This discrepancy is summed over pairs i,j within each modality to form L_preserve, and it enters the total VAE objective as L_total = L_recon + \u03b2 L_KL + \u03bb L_alignment + \u03b3 L_preserve, i.e., weighted by \u03b3.",
      "source_document": "papers/2510.24987v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In EnzyBind\u2019s dataset construction, the authors argue against a chronological train/test split and instead use a functionally meaningful split. What exact procedure do they use to create disjoint training and test sets, and what is clustered to achieve this disjointness?",
      "answer": "They split the data by sequence similarity: enzyme sequences are clustered with CD-HIT, and clusters are then randomly assigned to training or testing so that enzymes in the train and test sets are disjoint. Enzyme\u2013substrate pairs are sampled according to these cluster assignments.",
      "source_document": "papers/2510.25132v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In EnzyControl\u2019s *two-stage training paradigm*, which specific submodules are updated vs. frozen in Stage 1, and what changes in Stage 2 (including which components are LoRA-fine-tuned and which are still directly updated)?",
      "answer": "Stage 1 trains only the substrate-conditioning components: the projector and the EnzyAdapter, while keeping the rest of the prediction network frozen. Stage 2 then LoRA-fine-tunes the entire prediction network and the enzyme embedder, while continuing to update the projector and EnzyAdapter (now guided by the generation loss) so all components stay aligned.",
      "source_document": "papers/2510.25132v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In EnzyControl\u2019s evaluation protocol, the authors define a single \u201csuccess rate\u201d meant to capture a practical multi-objective enzyme design goal. What are the four criteria a generated enzyme backbone must satisfy to be counted as a success, and which specific structural/functional metrics are used for each criterion?",
      "answer": "A generated enzyme backbone is counted as successful only if it simultaneously: (i) matches the native enzyme\u2019s EC number (measured by the EC Match Rate, based on predicted EC numbers), (ii) has structural self-consistency scTM > 0.5, (iii) has scRMSD < 2 \u00c5, and (iv) has better (lower) predicted binding affinity than its native counterpart (binding affinity computed via Gnina; lower is better).",
      "source_document": "papers/2510.25132v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In EnzyControl\u2019s evaluation pipeline, the authors enforce a \u201cunified evaluation\u201d across all backbone generators by post-processing each generated backbone before computing metrics. What are the two post-processing steps applied to every generated backbone (name the specific tools/models used), and why do the authors state this is necessary for fair comparison?",
      "answer": "Every generated backbone is (1) inverse-folded with ProteinMPNN to produce sequences, and then (2) those sequences are structure-predicted with ESMFold to obtain all-atom structures. The authors state that all metrics are computed on the ESMFold-predicted structures so that differences in evaluation outcomes reflect genuine differences in backbone design quality rather than differences in sequence modeling or structure prediction protocols across methods.",
      "source_document": "papers/2510.25132v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s \u201cDocking-Aware Optimization Strategies\u201d discussion, the authors propose two substrate-aware ways to improve docking compatibility without explicitly modeling substrate geometry during generation. What are these two strategies, and what quantitative change in predicted binding affinity do they report in their prototype sampling-based optimization example (before vs. after)?",
      "answer": "They propose (1) sampling-based selection: generate multiple enzyme backbones per substrate, dock all candidates, and select the structure with the best predicted binding affinity; and (2) motif-branching beam search: start from the annotated catalytic motif, stochastically extend short N- and C-terminal fragments to create diverse partial scaffolds, complete and dock each candidate, and use the best-scoring motif variant as the seed for further generation. In their prototype sampling-based optimization example, predicted binding affinity improves from \u22126.92 before optimization to \u22128.38 after optimization (a 1.46 unit decrease, indicating stronger binding).",
      "source_document": "papers/2510.25132v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s cross-dataset stability analysis (Section 4.4), how do the authors align (match) concepts learned by Top\u2011k SAEs trained on Tabula Sapiens Immune versus Cross\u2011tissue Immune Cell Atlas, and what does a cosine-similarity score of 1 mean in their matching evaluation?",
      "answer": "They match (align) the two SAE dictionaries by pairing concept vectors to minimize cosine distance using the Hungarian algorithm. The cosine similarity of the resulting matched pairs quantifies alignment; a score of 1 indicates the two learned dictionaries are identical up to a permutation (i.e., perfectly aligned after reordering concepts).",
      "source_document": "papers/2510.25807v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Section 4.5 (\u201cTowards useful concept representations\u201d), the authors probe whether SAE concept activations preserve biological signal for downstream prediction. What two classification tasks do they train logistic-regression models on, what datasets/models are used for this experiment, and what conclusion do they draw from the fact that concept-based and neuron-based models achieve similar accuracies?",
      "answer": "They train logistic-regression classifiers on (1) cell cycle phase (3 classes: G1, S, G2M) and (2) cell type (7 selected classes). The experiment is run using scGPT embeddings on the Tabula Sapiens Immune dataset (cell-cycle labels obtained with Scanpy\u2019s \u201cscore genes cell cycle\u201d). Because logistic regression trained on concept activations reaches essentially the same accuracy as logistic regression trained on neuron activations (reported as 0.86 vs 0.87 for cell type and 0.49 vs 0.48 for cell cycle), they conclude that the concept activations preserve the predictive biological signal present in the original neuron/embedding representations while offering more interpretable features (e.g., high-coefficient concepts correspond to known marker genes, especially G2M markers).",
      "source_document": "papers/2510.25807v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the gene-level attribution method (Section 3.2), how do the authors choose the counterfactual \u201cbaseline\u201d cell for a prototype cell when explaining a given concept, and what two averaging steps (including the specific hyperparameters used in the expert study) do they apply to make the attribution scores more robust?",
      "answer": "For concept i and prototype cell x_p, the baseline x_c is defined as the closest cell that does *not* activate concept i (a counterfactual baseline). To improve robustness, they (1) use multiple counterfactuals: select the N_c closest non-activating cells and average the attribution scores over these counterfactual perturbations, and (2) use multiple prototypes: compute attribution for N_p prototype cells with highest concept activation and average across prototypes to obtain a ranked gene list C_i. In the expert interpretation study they used N_c = 3 counterfactual perturbations and N_p = 10 prototype cells (top-activation cells).",
      "source_document": "papers/2510.25807v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Appendix E (\u201cLimits of pathway enrichment\u201d), the authors argue that interpreting SAE concepts via GSEA on Gene Ontology biological processes can be systematically biased. What three concrete limitations do they list, and what specific gene-annotation coverage numbers do they give for scGPT versus scVI to motivate the first limitation?",
      "answer": "They list three limitations: (1) Many genes that appear to influence concept activation are not mapped to any GO biological process yet; they quantify this as 270/1200 genes for scGPT and 5113/8000 genes for scVI lacking GO biological-process mapping. (2) The tree structure/term hierarchy can be misleading because \u201csister\u201d GO terms sharing parents may be semantically close or very dissimilar, especially if one neglects the Gene Ontology\u2019s underlying graph structure and ignores edge types linking biological processes. (3) Incompleteness of gene annotation and of the ontology leads to a \u201cstreetlight effect,\u201d skewing interpretation toward what is already known; this produces a heterogeneous ontology with uneven annotation depth that can bias enrichment assays.",
      "source_document": "papers/2510.25807v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Appendix C (\u201cDifferential Gene Expression (DEG) analysis\u201d), how do the authors construct and compare attribution-based vs DEG-based gene sets using a deletion-curve evaluation? Specify (i) how genes are ranked for deletion in each method (including any statistical threshold), (ii) what perturbation procedure is applied to the cells during deletion, and (iii) what qualitative result they report when comparing the two deletion curves and what that implies about the identified genes.",
      "answer": "(i) For the attribution-based gene set, genes are ranked by their attribution value (highest to lowest). For the DEG-based gene set, genes are ranked by the absolute fold-change from differential expression, considering only genes with p-value \u2264 5e\u22123.\n(ii) They iteratively perturb cells using their counterfactual-perturbation procedure, deleting/perturbing genes from most important to least important according to the chosen ranking, and at each step compute the resulting (perturbed) concept activation.\n(iii) They report that the attribution-based deletion curve lies below the DEG-based deletion curve, meaning that deleting genes prioritized by attribution reduces concept activation more strongly; this implies attribution identifies genes with a bigger causal impact on concept activation than DEG-based rankings.",
      "source_document": "papers/2510.25807v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s comparison of baseline models for *multiple peptide-bond cleavage prediction*, Prosit shows an unusual precision\u2013recall behavior. What specific explanation do the authors give for Prosit achieving the highest recall but the lowest precision, and how does PBCLA contribute to this outcome?",
      "answer": "They explain that Prosit\u2019s predicted theoretical spectra contain ion fragments with intensities > 0 for the vast majority of m/z values, so when PBCLA is applied to those spectra it matches many fragments and labels most peptide bonds as cleaved (positive). This inflates recall (many true cleavages are included) but produces many false positives, driving precision down.",
      "source_document": "papers/2510.25814v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the DBond model, the authors group inputs into four feature sets before embedding and prediction. What are these four feature groups, and which specific variables (as listed in the paper) are included in the *state* feature group?",
      "answer": "The four feature groups are: (1) state features, (2) bond features, (3) env (environment) features, and (4) sequence features. The state feature group specifically includes the precursor ion charge, precursor ion m/z, and the absolute intensity of the precursor ion.",
      "source_document": "papers/2510.25814v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Experimental setup section, why do the authors explicitly choose *not* to include traditional machine-learning baselines (e.g., XGBoost), and what two concrete limitations of mirror-image peptide data/tools do they cite to justify this choice?",
      "answer": "They exclude traditional ML baselines like XGBoost because they are not well-suited to handling the sequence features of mirror-image peptides. The authors give two specific reasons: (1) mirror-image peptide sequences are complex\u2014large in number, variable in length, and with high internal dependencies\u2014so standard categorical encodings such as one-hot encoding are ineffective; and (2) existing peptide feature-extraction tools/models (e.g., iFeature and AlphaFold3) do not support direct processing of mirror-image peptides composed of D-amino acids or non-standard amino acids, making conventional feature-engineering pipelines infeasible.",
      "source_document": "papers/2510.25814v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Results/Discussion, the authors argue that the single-label strategy (DBond-s applied sequentially across bonds) outperforms the multi-label strategy (DBond-m) even though it ignores label dependencies. What specific limitation of the multi-label formulation on MiPD513 do they give to explain this, and which evaluation metric do they cite as evidence that \u201cexactly\u201d predicting the full cleavage pattern is particularly hard?",
      "answer": "They attribute DBond-m\u2019s weaker performance to the multi-label setup having relatively few instances but many labels per instance, which makes the solution space sparse and reduces learning effectiveness for the multi-label model. As evidence that exactly matching the full cleavage pattern is difficult, they point to the subset accuracy metric, which measures the proportion of peptides whose predicted cleavage statuses match the true pattern across all bonds.",
      "source_document": "papers/2510.25814v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In PBCLA (the peptide bond cleavage labeling algorithm), what *exact* fragment ion types are considered during the fragment-ion matching step, and what two matching constraints does PBCLA apply regarding (i) maximum fragment-ion charge state and (ii) the m/z tolerance?",
      "answer": "PBCLA\u2019s fragment-ion matching step considers exactly six ion types: b, y, b\u2013H2O, b\u2013NH3, y\u2013H2O, and y\u2013NH3. It restricts fragment ions to a maximum charge state of 2 and uses an m/z matching tolerance of 20 ppm.",
      "source_document": "papers/2510.25814v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s description of the incremental AmpliconHunter2 implementations, what specific low-level OS and I/O strategy does AHv2.\u03b1 use to keep memory usage low when processing large FASTA inputs (name the mapping mode and the two posix_madvise hints), and what is the stated purpose of those hints?",
      "answer": "AHv2.\u03b1 compresses FASTA into 2-bit batches and memory-maps those batches with MAP_PRIVATE. It then calls posix_madvise with POSIX_MADV_SEQUENTIAL and POSIX_MADV_DONTNEED to tell the kernel access will be sequential and that pages can be dropped once consumed, reducing peak resident set size (RSS).",
      "source_document": "papers/2511.00170v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In AmpliconHunter2\u2019s supplementary description of \u201cAmplicon calling and orientation (AHv2)\u201d, what exact rule is used to prevent invalid overlaps when pairing primer hits, and which orientation codes are emitted by default versus only when the user enables the off-target option?",
      "answer": "The algorithm sorts candidate primer-hit sites and pairs opposite-sense hits within the user\u2019s length bounds; it stops pairing when a same-sense site appears, which is stated to prevent invalid overlaps. Orientation codes are FR, RF, FF, RR; by default it emits FR+RF, while enabling --include-offtarget additionally emits FF and RR.",
      "source_document": "papers/2511.00170v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the \u201cDiscussion and future directions\u201d section, the authors explain why AmpliconHunter2\u2019s parallel speedup stops improving past a certain point. According to the paper, beyond how many threads does AHv2\u2019s scaling saturate, what are the two main causes given for this saturation, and what two concrete system/architecture-level strategies do they suggest could address it?",
      "answer": "The paper states that AHv2 \u201cscales well to dozens of cores, but scaling saturates beyond 64 threads\u201d due to \u201cI/O bottlenecks and memory contention.\u201d It suggests that further improvements could involve \u201cexplicit NUMA-aware scheduling and prefetching (non-uniform memory access).\u201d",
      "source_document": "papers/2511.00170v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Supplementary Information section describing \u201cPrimer trimming and barcode extraction (AHv2)\u201d, how does AmpliconHunter2 define/extract the forward and reverse barcodes in the FR orientation versus the RF orientation, and what extra transformation is applied in the RF case?",
      "answer": "For FR amplicons, barcodes are fixed-length flanks taken upstream of the forward primer (--fb-len) and downstream of the reverse primer (--rb-len). For RF amplicons, the barcodes are extracted from the opposite sides relative to the primer hits, and both extracted barcodes are reverse-complemented.",
      "source_document": "papers/2511.00170v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Results/benchmarking methodology description, the authors state that Tm-based filtering was disabled during the six-test comparison, yet they still report Tm information. Which two AmpliconHunter versions support Tm filtering (and thus had it disabled), and what does the paper say the implementations still do regarding melting temperature annotation despite filtering being off?",
      "answer": "Tm filtering is supported (and therefore disabled for the benchmarks) in AHv1.1 and AHv2. Even with filtering disabled, both implementations automatically compute and annotate each amplicon with melting temperature (Tm) using the same nearest-neighbor model.",
      "source_document": "papers/2511.00170v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the structure-guided de novo peptide design workflow described around RFdiffusion, what are the two distinct generative stages, and what specific non-diffusion models are named for the second stage?",
      "answer": "The workflow is a two-step hybrid. (1) RFdiffusion (a diffusion model) first generates a peptide/protein backbone as continuous 3D coordinates conditioned on the target structure/surface and desired interface residues. (2) A separate sequence design (inverse folding) model then designs a compatible amino-acid sequence for that backbone; the paper specifically names ProteinMPNN and ESM-IF as examples of this non-diffusion inverse-folding tool.",
      "source_document": "papers/2511.00209v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the small-molecule property-based design/optimization section, the authors argue that synthetic accessibility (SA) scores are insufficient as a guarantee of practical synthesis. What two concrete strategies do they list for making diffusion-based generation more synthesis-aware beyond using SA scores, and what is the key limitation of SA scores they cite?",
      "answer": "They state that SA scores only provide a rough estimate of synthesizability and do not guarantee that a practical synthesis route exists. To go beyond SA scores, they describe incorporating retrosynthesis models into the generation process either (1) by using retrosynthesis feasibility as an additional optimization objective, or (2) by generating molecules in a retrosynthetically aware way\u2014assembling them from commercially available building blocks using known reaction templates (they also mention evaluating synthesizability by combining retrosynthetic planning with forward reaction prediction to verify route feasibility).",
      "source_document": "papers/2511.00209v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the section on discrete diffusion models for functional peptide sequence generation, the authors describe three alternative noise processes for corrupting amino-acid sequences. What are these three noise processes, and what specific practical implication or use-case do the authors highlight for the uniform transition matrix and for the absorbing-state (MASK) model?",
      "answer": "The three noise processes are: (1) uniform transition matrices (randomly replace tokens uniformly), (2) absorbing-state models, and (3) learned transition matrices that respect amino-acid similarity.\n\nPractical implications/use-cases highlighted:\n- Uniform transition matrix: simplest to implement, but it ignores biochemical similarity between amino acids (e.g., treats Ala\u2192Val the same as Ala\u2192Lys).\n- Absorbing-state (MASK) model: particularly suited to sequence inpainting or other constrained-generation settings because the MASK token cleanly separates fixed regions from regions to be generated.",
      "source_document": "papers/2511.00209v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s discussion of small-molecule datasets/benchmarks, what three specific limitations of the CrossDocked2020 benchmark do the authors call out, and what broader implication do they draw from these limitations for how the field should improve its benchmarks?",
      "answer": "They note that (1) CrossDocked2020 docking scores are computational estimates rather than experimental measurements, (2) the dataset is biased toward certain protein families (with kinases and proteases over-represented), and (3) the ligands are primarily known drugs/drug-like molecules, which limits chemical diversity. From these limitations, they argue that the field needs more diverse and experimentally validated benchmarks (i.e., benchmarks with stronger experimental grounding rather than relying on docking proxies and biased/limited chemical spaces).",
      "source_document": "papers/2511.00209v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Section 4.2, the authors argue that \u201cclosing the loop\u201d with an automated Design\u2013Build\u2013Test\u2013Learn (DBTL) cycle is critical for translating diffusion-based generative design into practice. According to the text, what specific failure modes of current diffusion-model workflows motivate this (name at least two), and what concrete benefit does the DBTL integration provide in terms of how information flows between computational design and experiments?",
      "answer": "The motivation is that current workflows rely on imperfect computational scoring proxies (e.g., docking scores/predicted affinities) that often correlate poorly with experimental reality, producing high false-positive rates, and they are further limited by scarcity of high-quality labeled data pairing structures with experimentally validated activity/affinity; additionally, models can overfit and generalize poorly outside the training distribution. The DBTL cycle provides a direct, rapid, iterative pathway from in silico hypothesis generation to automated experimental validation and back, where data from one experimental round directly informs the next design round (accelerating what would otherwise be a slow, sequential process).",
      "source_document": "papers/2511.00209v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In SCUDDO, two different per-diagonal feature types are computed from each cell\u2019s diffused intrachromosomal Hi-C maps before the cosine-similarity distance calculation. What are these two feature types, and what evidence does the paper give (including the dataset and the reported performance) that using only one of them can be insufficient for good clustering?",
      "answer": "The two feature types are (1) the diffused (normalized) diagonal values themselves (the embedding vectors \\(\\vec e'^w_s\\)), and (2) the trinarized/signed-difference vectors along those diffused diagonals (\\(\\vec f^w_s = \\mathrm{sgn}(\\nabla(\\vec e'^w_s))\\), giving entries in \\{\u22121,0,1\\}). The paper states in the Discussion that for some datasets both features are necessary; specifically, for the Flyamer et al. dataset, using only one feature yields at best an ARI of only 0.5.",
      "source_document": "papers/2511.00278v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In SCUDDO, the authors perform a spectral decomposition step before running K-means++ because clusters in the SCUDDO embedding may be non-convex. Describe exactly how SCUDDO constructs the similarity matrix from the MDS embedding V, and then specify (i) which Laplacian/eigenvectors are used to form the final spectral embedding C, and (ii) what neighborhood parameter is used in the Shi\u2013Malik construction.",
      "answer": "After obtaining the a\u00d7\u03f5 embedding V, SCUDDO converts it to a similarity matrix by first computing pairwise distances Zij = |\\vec V_{i*} \u2212 \\vec V_{j*}| (where \\vec V_{i*} is row i of V) and then setting Aij = exp(\u2212Zij^2). It then forms the final a\u00d7l spectral embedding C by taking the smallest l eigenvectors of the random-walk Laplacian built from A using the Shi\u2013Malik algorithm, with log(a) nearest neighbors in the neighborhood graph.",
      "source_document": "papers/2511.00278v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In SCUDDO\u2019s preprocessing/imputation stage, how is each intrachromosomal Hi-C map transformed before diagonal features are extracted? Specify (i) how maps of different chromosome sizes are made comparable (including the definition of the target size r), (ii) the exact Gaussian-smoothing kernel size and standard deviation and what padding rule is used, and (iii) the mathematical form and interpretation of the subsequent diffusion step (i.e., how B_s^k is computed from the smoothed map).",
      "answer": "(i) Each intrachromosomal matrix A_s^k is resized to a common r\u00d7r matrix A\u2032_s^k using bicubic interpolation, where r = (\u2211_{k=1}^b n_k)/b (the average chromosome matrix size across the b sampled chromosomes). (ii) The resized map is convolved with a 2D 9\u00d79 Gaussian kernel G with standard deviation \u03c3 = 0.5, using replicate padding (out-of-bounds values are set to the nearest border entry), producing A\u2033_s^k. (iii) The map is then normalized by its total sum and passed through a matrix exponential: B_s^k = exp( \u2212 A\u2033_s^k / (\u2211_{ij} A\u2033_s,ij^k) ), which the paper describes as representing backwards diffusion over A\u2033_s^k.",
      "source_document": "papers/2511.00278v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Results analysis where the authors vary the number of intrachromosomal maps used per cell, how is the sampling parameter \\(b\\) defined operationally (i.e., which chromosomes are included when a given \\(b\\) is chosen), and at what values of \\(b\\) does SCUDDO first surpass the next-best method in clustering accuracy for (i) the Li et al. dataset and (ii) the Collombet et al. dataset?",
      "answer": "The parameter \\(b\\) is implemented by sampling all chromosomes with an index \\(\\le b\\); e.g., \\(b=4\\) means only chromosomes 1, 2, 3, and 4 are used. Using this protocol, SCUDDO first exceeds the next-best method\u2019s ARI/NMI at \\(b\\approx 4\\) for the Li et al. dataset and at \\(b=2\\) for the Collombet et al. dataset.",
      "source_document": "papers/2511.00278v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In SCUDDO\u2019s feature construction, each pooled-and-normalized diagonal embedding vector \\(\\vec e'^{\\,w}_s\\) is converted into a \u201csigned difference\u201d vector \\(\\vec f^{\\,w}_s\\) using Eq. (6). What is the exact operation applied (including the definition of the discrete gradient \\(\\nabla\\)), what value is assigned to the *last* entry of \\(\\vec f^{\\,w}_s\\), and what set of values can \\(\\vec f^{\\,w}_s\\) take after this transformation?",
      "answer": "SCUDDO defines the signed-difference vector as \\(\\vec f^{\\,w}_s = \\mathrm{sgn}(\\nabla(\\vec e'^{\\,w}_s))\\), where the discrete gradient is \\(\\big(\\nabla(\\vec e'^{\\,w}_s)\\big)_\\alpha = (\\vec e'^{\\,w}_s)_\\alpha - (\\vec e'^{\\,w}_s)_{\\alpha+1}\\). The sign function \\(\\mathrm{sgn}\\) is then applied elementwise. Because \\(\\alpha+1\\) is undefined at the end, SCUDDO sets the last entry explicitly to \\((\\vec f^{\\,w}_s)_{b(r-w)} = (\\vec e'^{\\,w}_s)_{b(r-w)}\\). After Eq. (6), \\(\\vec f^{\\,w}_s\\) is a ternary vector whose entries are in \\{1, 0, \u22121\\} (with the last entry as defined above).",
      "source_document": "papers/2511.00278v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the ablation/feature-importance analysis of STELLAR-koff, which input/component is identified as the most critical, and what quantitative drops in Pearson correlation (Rp) does the paper report when this component is removed\u2014(i) in five-fold cross-validation and (ii) on the p38 MAP kinase external test set?",
      "answer": "The most critical component is the protein\u2013ligand interaction landscape (the conformation-ensemble landscape input). When it is omitted and the model relies only on the global protein feature, the Pearson correlation in five-fold cross-validation drops from 0.729 to 0.667, and on the p38 MAP kinase external set it drops to 0.107.",
      "source_document": "papers/2511.01171v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "During structure preparation/model training, how did the authors filter the expanded PDBbind koff dataset after generating docking-based conformation ensembles, and what was the resulting number of protein\u2013ligand complexes used for subsequent training/evaluation? (Answer should state the specific conformation-count criterion and the final dataset size after this filtering.)",
      "answer": "After preparing structures, they generated conformation ensembles via molecular docking and removed complexes that could not generate more than 125 conformations; after this filtering, 920 protein\u2013ligand complexes remained and were processed for training/evaluation.",
      "source_document": "papers/2511.01171v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s hyperparameter-optimization setup, the authors define a custom Optuna objective called `maximize_score` as a weighted combination of classification metrics. What is the exact formula (including weights and denominator), and what specific evaluation priority do the authors give as the reason for choosing these weights?",
      "answer": "They optimize `maximize_score = (1\u00d7Accuracy + 3\u00d7Precision + 1\u00d7Recall + 2\u00d7F1) / 7`. Precision is given the highest weight (3\u00d7) to prioritize minimizing false positives, which they state is critical for identifying high-quality, clean data (weights chosen based on domain requirements, not empirically tuned).",
      "source_document": "papers/2511.01277v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s FullSignalModel post-processing, how are per-window capture probabilities converted into contiguous capture-phase boundaries? State (i) the approximate decision threshold used, (ii) the exact label-smoothing rule (including the two example patterns that get flipped), and (iii) what is done after smoothing to produce start/end indices.",
      "answer": "(i) The window-level confidence scores are thresholded at approximately 0.52 (~0.524 for the best model) to produce binary labels. (ii) A label-smoothing heuristic flips isolated windows that are surrounded by the opposite label: [0, 1, 0] is flipped to [0, 0, 0] and [1, 0, 1] is flipped to [1, 1, 1]. (iii) After smoothing, the final array of window-level binary predictions is aggregated to determine the start and end indices (boundaries) of each predicted capture phase, which constitute the FullSignalModel output.",
      "source_document": "papers/2511.01277v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "The authors address severe class imbalance between capture and non-capture windows during training. Describe their two-step strategy in enough detail to reproduce it: (i) how they decide which non-capture windows are eligible to include (include the exact overlap criterion), and (ii) how they constrain the class ratio in the final training set, and (iii) what loss function they train with and why it is appropriate here.",
      "answer": "(i) They define non-capture windows as segments with less than 50% overlap with any annotated capture section, and only such non-capture windows are eligible. (ii) They include non-capture windows only as needed to maintain a 1:1 ratio between capture and non-capture windows, yielding a balanced training set without synthetic sampling. (iii) They train using Binary Cross-Entropy (BCE) loss, which is well-suited for binary classification problems.",
      "source_document": "papers/2511.01277v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the authors\u2019 comparison of architectures on the held-out test set, they interpret a precision\u2013recall tradeoff between CaptureNet-Deep and the CNN\u2013LSTM hybrid. Which specific model is reported as having the *highest precision*, what qualitative behavior do the authors infer from its precision/recall pattern (i.e., what kinds of errors it tends to make), and what hypothesis do they give for why the pure CNN approaches generally outperform histogram-based methods?",
      "answer": "The CNN\u2013LSTM hybrid model (named CaptureCNNWithLSTM) is reported as having the highest precision (94.36%) but lower recall than CaptureNet-Deep. The authors infer that it is conservative in its predictions\u2014minimizing false positives but tending to miss some subtle capture phases (false negatives). They hypothesize that pure CNNs generally outperform histogram-based methods because convolutions capture local temporal patterns in the signal (temporal pattern recognition) that provide additional discriminative information beyond purely statistical/histogram features, even though statistical features are still informative.",
      "source_document": "papers/2511.01277v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "The authors argue that using *run-level* (rather than window-level) data splits gives a more realistic estimate of generalization for capture-phase detection. According to the Methods/Discussion text, (i) what specific form of data leakage does the run-level split prevent, and (ii) what exact train/validation/test percentages do they use for this run-level split?",
      "answer": "(i) It prevents windows from the same experimental run appearing in both the training set and the test set (i.e., leakage/overfitting due to shared run-specific signal characteristics). (ii) They split runs into 72% training, 18% validation, and 10% test.",
      "source_document": "papers/2511.01277v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Results section, the authors describe two alternative algorithms implemented for interval overlap during tokenization. What are the names of these two overlap methods as implemented in gtars-tokenizers (including their module names), and what underlying data structure/algorithm does each one use?",
      "answer": "The two overlap methods are: (1) **gtars/bits**, which uses **binary interval tree search (BITS)**; and (2) **gtars/alist**, which uses an **Augmented Interval List (AIList)**.",
      "source_document": "papers/2511.01555v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Results section, the authors argue that Hugging Face compatibility makes gtars-tokenizers usable as a \u201cnear-drop-in replacement\u201d in standard NLP-style training pipelines. According to the paper, what specific downstream frameworks/tools do they name as examples that can then interoperate seamlessly with models trained using gtars-tokenizers because they rely on the Hugging Face tokenizers standard?",
      "answer": "They explicitly name PyTorch Lightning and AllenNLP, and the evaluation/training ecosystem tools Evaluate, PEFT, and Weights & Biases as examples of frameworks/tools that can interoperate seamlessly because they rely on the Hugging Face tokenizers standard.",
      "source_document": "papers/2511.01555v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Results section describing Hugging Face integration, the authors include a short PyTorch example that tokenizes two query intervals and feeds them into an embedding layer. According to that example, what key name do they use to extract the token IDs from the tokenizer output, and what PyTorch module do they instantiate to consume those IDs (including its input and output dimensions as written)?",
      "answer": "They extract token IDs using the key \"input_ids\" (i.e., `tokenizer.tokenize(query_intervals)[\"input_ids\"]`). They instantiate a PyTorch embedding layer: `torch.nn.Embedding(tokenizer.vocab_size, 64)`, which takes indices in `[0, vocab_size)` and outputs 64-dimensional embeddings.",
      "source_document": "papers/2511.01555v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Results section where the authors demonstrate a simple PyTorch workflow, what is the *specific* sequence of steps they describe for building a tokenizer and producing embeddings from genomic intervals (include: how the tokenizer is constructed, how intervals are provided to it, what output field is used, and what neural network layer consumes the tokens)?",
      "answer": "They (1) create a tokenizer from a BED file using `tokenizer = Tokenizer.from_bed(\"path/to/file.bed\")`, (2) define a PyTorch embedding layer with `network = torch.nn.Embedding(tokenizer.vocab_size, 64)`, (3) provide the genomic intervals as a Python list of tuples like `query_intervals = [(\"chr1\", 100, 200), (\"chr2\", 300, 400)]`, (4) tokenize them and extract token IDs from the tokenizer output via `tokens = tokenizer.tokenize(query_intervals)[\"input_ids\"]`, and (5) pass the token IDs into the embedding layer (shown as `out = network(torch.tensor(input_ids))`).",
      "source_document": "papers/2511.01555v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Results section discussing performance, the authors benchmark gtars-tokenizers against three existing interval-overlap tools. Which tools are they, and what specific comparative performance claim do they make for large universes with >1 million intervals (include both the speedup range and which baseline tools it applies to, as well as which tool it is said to be comparable to)?",
      "answer": "They benchmark against bedtools, bedops, and bedtk. For large universes with >1 million intervals, they report gtars-tokenizers is about 2\u20133\u00d7 faster than bedtools and bedops, while being comparable to bedtk.",
      "source_document": "papers/2511.01555v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s description of the advanced \u201cPart III\u201d notebook pipelines, what specific three-model workflow is used in \u201cNotebook 9: Designing Proteins,\u201d and what is the stated role of each model in the pipeline?",
      "answer": "Notebook 9 uses a three-model pipeline: (1) RFDiffusion is used to create/generate novel protein backbones (including for unconditional generation and motif scaffolding); (2) ProteinMPNN is then used to predict/design amino-acid sequences that fit the given backbone structures; and (3) AlphaFold2 is used afterward to verify/validate that the designed sequences fold to the intended structures (i.e., to check folding of the designs).",
      "source_document": "papers/2511.02128v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the description of Notebook 10 (RFDiffusion All-Atom / RFDiff-AA), what types of molecular components does the paper say are integrated by the model\u2019s multi-track architecture for all-atom design, and what downstream design capability is this integration meant to enable compared with backbone-only protein design?",
      "answer": "The paper states that RFDiffusion All-Atom uses a multi-track architecture that integrates amino acids, nucleic acids, and small molecules for all-atom design. This integration is meant to enable designing proteins that can bind molecules beyond proteins\u2014specifically guiding the design of new protein sequences for small-molecule binding (including handling small molecules and covalent modifications), extending beyond backbone-only protein design.",
      "source_document": "papers/2511.02128v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the module descriptions for Part III, how does Notebook 5 (\u201cLanguage Models for Transfer Learning\u201d) propose mitigating a key practical risk of fine-tuning in low-data regimes, and what specific downstream biological prediction task does it apply the fine-tuned (or adapted) protein language model to in the notebook?",
      "answer": "It highlights overfitting as a challenge when fine-tuning in low-data regimes and then explores Low-Rank Adaptation (LoRA) as a more effective/efficient fine-tuning approach, visually comparing performance outcomes. In the notebook, the adapted protein language model is applied to classify residues in protein binding sites and to use language-model embeddings for ligand binding-site prediction.",
      "source_document": "papers/2511.02128v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the section describing the programming environment/resource constraints, what specific Google Colab hardware/session specifications do the authors cite (GPU model and memory, RAM, disk space, CPU speed, and maximum continuous session length), and what two practical barriers to running many deep-learning protein models do they say Colab mitigates?",
      "answer": "They cite Google Colab providing free GPUs such as an NVIDIA Tesla T4 with 15 GB of memory, ~13 GB RAM, ~110 GB disk space, a 2.25 GHz CPU, and continuous sessions lasting up to 12 hours. They argue Colab mitigates (1) dependency/environment conflicts by handling packages discretely within a single notebook (supporting a notebook-per-model approach), and (2) the need for high-performance computing (HPC)/expensive GPU resources by offering accessible GPU compute within the session limits (so the notebooks fit within the GPU allocation).",
      "source_document": "papers/2511.02128v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the \u201cLearning outcomes: Results from teaching our material\u201d section, how was the DL4Proteins notebook series used in a Spring 2025 Johns Hopkins course (i.e., what did students do in the second half of the semester), and what are the three example project themes the authors list that student groups completed?",
      "answer": "The notebooks were piloted in the Spring 2025 Johns Hopkins course \u201cComputational Protein Structure Prediction and Design.\u201d In the second half of the semester, students developed a project that either incorporated the protein prediction/design models used throughout the semester or trained their own models from architectures introduced in the class. The authors list three example project themes: (1) structurally analyzing the behavior of protein self-assembling colloidal systems with AlphaFold3, (2) designing proteins to novel DNA or protein targets, and (3) improving the predicted binding affinity of existing therapeutic antibody candidates.",
      "source_document": "papers/2511.02128v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In LA-MARRVEL\u2019s reranking ensemble, how are LLM \u201cpartial rankings\u201d aggregated into a single consensus gene order using Tideman\u2019s Ranked Pairs method? Describe (i) how pairwise wins are counted when an item is unranked in a ballot, (ii) how the directed victories are sorted (including all tie-break keys), and (iii) what tie-breaks are used when producing the final topological order.",
      "answer": "(i) For each ballot, pairwise win counts Wins[a][b] are incremented when a is ranked and b is unranked (ranked items are treated as preferred over any unranked candidate), and also when both are ranked and a has a better (smaller) rank than b; if a is unranked and b is ranked, nothing is added.\n\n(ii) For each unordered pair {a,b}, a directed victory is formed for the candidate with more wins, with margin equal to the difference in wins. The list of directed pairs P is then sorted in descending order by: (1) victory margin, (2) winner\u2019s votes, then ascending lexicographic tie-breaks by (3) winner name and (4) loser name.\n\n(iii) After greedily locking directed edges that do not create cycles, the final topological ordering is generated by repeatedly selecting among candidates with in-degree 0, sorted by descending Borda score and then by name; Avail is re-sorted by the same criteria after each removal/insertion.",
      "source_document": "papers/2511.02263v3.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Discussion\u2019s proposal for improving evaluation of RNA secondary-structure predictors, what specific kind of benchmark does the paper argue the field lacks, what existing community effort is it explicitly compared to, and what three concrete benefits does the paper claim such a benchmark would provide?",
      "answer": "It argues the field lacks a community-wide, **prospective** benchmarking system\u2014a regular, **blind challenge**\u2014explicitly analogous to **CASP** (Critical Assessment of protein structure prediction). The paper states this would (1) provide **unbiased evaluation of true generalization capabilities**, (2) **accelerate progress** on persistent hurdles like **pseudoknots and modified bases**, and (3) **build community consensus** on the genuine state-of-the-art.",
      "source_document": "papers/2511.02622v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the review\u2019s section on hybrid approaches (later in the paper), two specific designs are described for combining machine learning with classical RNA folding algorithms. Describe (i) what \u201cthermodynamic regularization\u201d is used for in MXfold2 and how it constrains the learned model, and (ii) how the Calonaci et al. hybrid model incorporates SHAPE (1D probing) and DCA (2D co-evolutionary) signals into RNAfold, including the role of differentiability in their training pipeline.",
      "answer": "(i) For MXfold2, \u201cthermodynamic regularization\u201d is a training strategy that penalizes deviations of the network\u2019s learned folding scores from established physical (Turner) free-energy parameters. The model\u2019s deep network computes four types of folding scores that are combined with Turner\u2019s parameters, and the regularization encourages the learned scores to stay close to the thermodynamic parameters to reduce overfitting.\n\n(ii) In the Calonaci et al. hybrid model, a convolutional neural network learns a mapping from 1D chemical probing data such as SHAPE and 2D co-evolutionary information from direct-coupling analysis (DCA) into pseudo-energy penalties. These learned penalties are then injected directly into the RNAfold thermodynamic algorithm. A key stated advantage is that the whole pipeline is differentiable, so the thermodynamic model can be included in end-to-end training; this was reported to increase the population of the native structure in their tests.",
      "source_document": "papers/2511.02622v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the review\u2019s discussion of MSA-based deep learning (Section 4.2), SPOT-RNA2 is described as improving accuracy by adding evolutionary features but also having situations where it is *less* reliable than a single-sequence model. What two specific evolutionary features does the paper say SPOT-RNA2 adds, and what two concrete limitations/tradeoffs does the review highlight for this MSA-driven pipeline (including when the original SPOT-RNA can be preferable)?",
      "answer": "It says SPOT-RNA2 augments inputs with (1) a Position Specific Score Matrix (PSSM) and (2) a 2D Direct Coupling Analysis (DCA) map, both derived from an MSA produced by the RNAcmap pipeline. The review highlights two main tradeoffs: (i) performance depends on having many homologous sequences\u2014so for orphan RNAs with very few homologs the original single-sequence SPOT-RNA can be more reliable; and (ii) the method is computationally demanding and, due to its feature-generation pipeline, is currently limited to sequences shorter than 1000 nucleotides.",
      "source_document": "papers/2511.02622v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "The review argues that many early deep-learning RNA secondary-structure predictors reported \u201cimpressive but misleading\u201d accuracy because of how they were evaluated. According to the text, what *specific* benchmarking practice led to this, what is the \u201cgold standard\u201d replacement evaluation protocol the community re-adopted (name it and define what is held out), and what quantitative generalization drop does the review cite when switching from the flawed to the rigorous setup?",
      "answer": "They were often evaluated using less rigorous splits based on simple sequence-similarity cutoffs, effectively allowing intra-family leakage and inflating performance. The replacement \u201cgold standard\u201d is rigorous homology-aware benchmarking via family-fold cross-validation, where entire RNA families are held out for testing. The review cites Szikszai et al. (2022) showing a model\u2019s F1-score could drop by 36% when moving from a flawed intra-family test to a rigorous inter-family (cross-family) evaluation.",
      "source_document": "papers/2511.02622v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the section \u201cEvolving Prediction Targets: From Static Blueprints to Dynamic Ensembles,\u201d the review argues that summarizing an RNA\u2019s folding ensemble with a 2D matrix (either a base-pairing probability matrix or a learned contact map) causes a specific kind of information loss. What *exactly* is the lost information, and what two examples of \u201csingle-molecule\u201d chemical probing + downstream analysis methods does the paper name as enabling ensemble deconvolution to recover state-specific structure/populations?",
      "answer": "The lost information is the *correlational structure of the ensemble*: marginal base-pair probabilities do not encode whether structural elements co-occur or are mutually exclusive across conformations, so important dependencies between pairs/motifs are obscured. As examples of single-molecule probing and analysis that enable ensemble deconvolution, the paper cites SHAPE-MaP and DMS-MaPseq as single-molecule probing approaches, and names deconvolution/analysis methods including DREEM, DRACO, and DANCE-MaP (any two of these satisfy the question).",
      "source_document": "papers/2511.02622v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Tartarus docking-score conditional generation experiment, what numeric docking-score value do the authors use as the target condition when sampling molecules with the CVAE, and how do they validate that conditioning improves docking for specific targets (name the statistical test and report which targets show significant differences)?",
      "answer": "They condition CVAE sampling on a docking-score target of \u221210.0. To validate improved docking, they dock the generated molecules with QVina and compare mean docking scores between VAE and CVAE samples using a two-sample t-test; significant differences are reported for targets 1syh and 6y2f (not for 4lde).",
      "source_document": "papers/2511.02769v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In STAR-VAE\u2019s LoRA-based conditioning mechanism, how do the authors modify each self-attention projection matrix (Q, K, V, O) to inject property information, and what is the role of the scalar \\(\\lambda\\) at inference time (including what common LoRA scaling it can absorb)?",
      "answer": "They keep the pretrained attention projection weights frozen and replace each projection \\(W_\\bullet\\) (for \\(\\bullet\\in\\{Q,K,V,O\\}\\)) with a property-conditioned version \\(\\tilde W_\\bullet(\\lambda)=W^{(0)}_\\bullet+\\lambda\\,A_\\bullet B_\\bullet\\), where \\(A_\\bullet\\) and \\(B_\\bullet\\) are learned low-rank matrices (\\(r\\ll d\\)) used for conditioning. The scalar \\(\\lambda\\) controls the strength of conditioning at inference time and can optionally absorb the usual LoRA scaling factor \\(\\alpha/r\\).",
      "source_document": "papers/2511.02769v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In STAR-VAE\u2019s conditional generative formalism, how do the authors define the three key conditional distributions (prior, approximate posterior/inference network, and decoder likelihood), what parametric family do they assume for the prior and posterior, and what is the exact conditional ELBO objective they maximize (including where the \u03b2 coefficient appears and what KL is taken against what)?",
      "answer": "They formalize conditional generation with three distributions: (1) the property-conditioned latent prior p\u03b8(z|y), (2) the approximate posterior/inference network q\u03d5(z|x,y), and (3) the conditional decoder likelihood p\u03b8(x|z,y), where the decoder factorizes autoregressively as p\u03b8(x|z,y)=\u220f_{t=1}^T p\u03b8(x_t|x_{<t},z,y). Both q\u03d5(z|x,y) and p\u03b8(z|y) are modeled as Gaussians with diagonal covariance: q\u03d5(z|x,y)=N(\u03bc\u03d5(x,y), diag(\u03c3^2_\u03d5(x,y))) where \u03bc\u03d5,\u03c3\u03d5 come from the Transformer encoder, and p\u03b8(z|y)=N(\u03bc\u03b8(y), diag(\u03c3^2_\u03b8(y))) where \u03bc\u03b8,\u03c3\u03b8 are predicted from the property embedding y. Training maximizes the conditional ELBO L(x,y)=E_{q\u03d5(z|x,y)}[log p\u03b8(x|z,y)] \u2212 \u03b2\u00b7KL(q\u03d5(z|x,y) \u2225 p\u03b8(z|y)), i.e., a reconstruction term under the decoder minus a \u03b2-weighted KL between the approximate posterior and the conditional prior.",
      "source_document": "papers/2511.02769v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In STAR-VAE\u2019s inference-time \u201cclassifier guidance\u201d mechanism, what exact modification do the authors make to the decoder logits during generation (write the formula), what quantity is differentiated to obtain the guidance signal, and what does the guidance-strength hyperparameter \\(\\lambda\\) control in this context?",
      "answer": "They add a gradient-based guidance term to the decoder logits:\n\n\\[\\text{logits}_\\lambda = \\text{logits}(z, y) + \\lambda\\,\\nabla_z f_\\psi(z, y).\\]\n\nThe guidance signal is the gradient with respect to the latent \\(z\\) of a differentiable property predictor \\(f_\\psi\\) (i.e., \\(\\nabla_z f_\\psi(z,y)\\)). The hyperparameter \\(\\lambda\\) controls the strength of this guidance (how strongly the logits are steered toward regions associated with the desired property values) at inference time.",
      "source_document": "papers/2511.02769v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In STAR-VAE\u2019s Methods section on molecular representation and preprocessing, what specific tokenization and sequence-handling choices do the authors make for SELFIES, and what design rationale do they give for using absolute positional encodings (as opposed to rotary/relative schemes)? Your answer should include: (i) the special tokens added, (ii) the maximum sequence length and how shorter/longer sequences are handled, and (iii) the stated reason absolute positional encodings are appropriate here.",
      "answer": "(i) They tokenize by splitting each SELFIES string into discrete tokens from a fixed vocabulary and add the special tokens <pad>, <sos>, <eos>, and <unk> for sequence handling during training and generation.\n(ii) They cap SELFIES sequences at a maximum length of 71 tokens; shorter sequences are padded with <pad> tokens and longer sequences are truncated.\n(iii) They use absolute positional encodings because the maximum sequence length is relatively short, so absolute encodings provide a stable inductive bias without the added complexity of rotary or relative positional encoding schemes.",
      "source_document": "papers/2511.02769v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In NABench\u2019s supervised-learning evaluation on DMS datasets, what are the two 5-fold cross-validation splitting strategies used, and what specific kind of generalization is each intended to test according to the paper?",
      "answer": "NABench uses (i) **Random Cross-Validation**, where variants are randomly partitioned into 5 folds, intended to test **interpolative generalization** on variants similar to the training set; and (ii) **Contiguous Cross-Validation**, where the wild-type sequence is split into 5 contiguous blocks and, for each fold, variants with mutations in one block are held out as the test set, intended to test **extrapolative generalization** to unseen, positionally distinct mutational regions (i.e., out-of-distribution by sequence position).",
      "source_document": "papers/2511.02888v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In NABench\u2019s evaluation metrics, how does the benchmark binarize sequences when computing AUC and MCC, and what is the exact cutoff used to define the positive class?",
      "answer": "For AUC and MCC, NABench treats fitness prediction as a binary classification problem by labeling the top 10% of sequences (by fitness within an assay) as the positive class; all remaining sequences are negative.",
      "source_document": "papers/2511.02888v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In NABench\u2019s transfer-learning evaluation, how is the train/test procedure defined at the assay level, and what metric format is used to report transfer-learning performance?",
      "answer": "Transfer learning is defined as training a predictive model on one or more complete fitness assays and then evaluating it on a different, held-out assay. Performance is reported as a Spearman\u2019s \u03c1 correlation matrix (with matrix entries giving the correlation when training on one assay and testing on another).",
      "source_document": "papers/2511.02888v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In NABench\u2019s raw sequencing preprocessing pipeline, which tool and specific parameter settings are used for sequence clustering to reduce redundancy, and how are cluster member frequencies handled when constructing the final dataset used for embedding and evaluation?",
      "answer": "NABench performs redundancy reduction by clustering sequences with CD-HIT-est using the options `-c 0.95 -n 8 -T 32`. After clustering, a representative sequence is chosen for each cluster and the frequencies of all sequences in that cluster are summed/aggregated and assigned to the representative to reflect its overall abundance in the processed dataset.",
      "source_document": "papers/2511.02888v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In NABench\u2019s evaluation pipeline, how are sequence embeddings extracted differently for (i) BERT-like models, (ii) GPT-like (auto-regressive) models, and (iii) Evo models that can output sequence-level embeddings? Give the exact embedding choices described.",
      "answer": "(i) For BERT-like models, NABench concatenates the <cls> embedding with a mean-pooled embedding over tokens to form the sequence embedding used for evaluation. (ii) For GPT-like auto-regressive models, it uses the last hidden state before the output as the sequence embedding. (iii) For Evo models that output sequence-level embeddings, it directly uses those provided embeddings without additional processing.",
      "source_document": "papers/2511.02888v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the mutation-inference part of the pipeline, what model is used as the final classification layer, and what specific evidence does the paper give (including the reported AUC value) to argue that deep image features are necessary beyond handcrafted morphometric features for predicting TP53 mutation status?",
      "answer": "The final classification layer for gene mutation prediction is a Random Forest classifier. The paper supports the need for deep image features by contrasting performance: when using the fused morpho-deep feature set (deep CNN/ViT embeddings plus morphometrics) the TP53 prediction achieves an AUC of 0.82 \u00b1 0.02, whereas using purely handcrafted morphometric features for TP53 yields a non-diagnostic, near-random AUC of 0.44\u2014showing that the deep, non-linear pixel-level features are necessary to decode the genomic signal.",
      "source_document": "papers/2511.03365v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s interpretability/verification step using Grad-CAM, what specific histopathologic regions does the author report the hybrid CNN\u2013ViT model consistently attends to when making its classifications and mutation predictions?",
      "answer": "The Grad-CAM heatmaps are reported to consistently focus on regions of high nuclear crowding, nuclear atypia, and areas of active epithelial invasion within the H&E slides.",
      "source_document": "papers/2511.03365v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Discussion\u2019s \u201cLimitations and Future Directions\u201d section, what specific methodological change does the author propose to address the limitation of training on image patches that may miss whole-slide architectural context, and what additional data modalities does the author suggest fusing to push mutation-prediction performance toward a clinically actionable AUC threshold (state the threshold)?",
      "answer": "The author proposes integrating multi-instance learning (MIL) to enable true whole-slide image (WSI) analysis instead of patch-only training. They also suggest multi-modal fusion with transcriptomics and clinical data to improve mutation-prediction performance toward a clinically actionable range of AUC \u2265 0.90.",
      "source_document": "papers/2511.03365v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Results section\u2019s subtype analysis, what two quantitative validation steps does the author use to argue that the handcrafted nuclear morphometric features are internally consistent and discriminative across subtypes, and what specific numerical evidence is reported for one of these steps?",
      "answer": "The author cites (1) a feature-correlation analysis via a Feature Correlation Heatmap showing expected internal relationships among morphometric features, and (2) a PCA of the morphometric feature vectors showing distinct, well-separated clustering of subtypes. The specific numerical evidence reported is that Area and Perimeter are strongly correlated with \u03c1 \u2248 0.98.",
      "source_document": "papers/2511.03365v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s mutation-prediction evaluation, the author reports a TP53 confusion-matrix breakdown and derived operating characteristics. What are the reported counts of true positives and true negatives, and what sensitivity and specificity do these correspond to?",
      "answer": "For TP53 mutation prediction with the fusion model, the confusion matrix is reported to have 52 true positives (TP) and 52 true negatives (TN), corresponding to a sensitivity of 75.4% and a specificity of 83.9%.",
      "source_document": "papers/2511.03365v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s \u201cPrevention of Data Leakage\u201d strategy, what two specific dataset-splitting/design choices are used to ensure the evaluation reflects future evolution rather than memorization, and what concrete dates/tree snapshots define the split?",
      "answer": "They (1) apply a strict temporal split: the training set is fixed to sequences released up to 2025-02-12 and the evaluation uses only sequences collected after that training-set release date (i.e., collected after 2025-02-12 / from 2025-02-13 onward), and (2) use distinct UShER phylogenetic-tree snapshots to avoid leakage from later tree optimizations: training uses the UShER tree updated on 2025-02-12, while evaluation uses the UShER tree snapshot updated on 2025-07-16 (including using the variant definitions and mutation trajectories from that 2025-07-16 snapshot).",
      "source_document": "papers/2511.03976v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In PETRA\u2019s training setup, sequences are tokenized into three parts and the loss is computed over a specific span of tokens; during evaluation/inference, prediction is restricted to a subset of those tokens. What are the three tokenized parts (in order), and on which parts does PETRA compute loss during training versus what does it predict during evaluation?",
      "answer": "Each sequence is tokenized (in order) into: (1) location/time information, (2) the variant mutations, and (3) the sequence mutations. During training, PETRA computes the loss on the full mutation trajectory\u2014i.e., both the variant-mutation tokens and the sequence-mutation tokens (given the preceding context including location/time). During evaluation/inference, it predicts only the sequence mutations on top of a provided variant (not the variant mutations).",
      "source_document": "papers/2511.03976v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the ablation study on weighted sampling, PETRA is compared under four settings (PETRA-NW, PETRA-TW, PETRA-RW, PETRA). What does each setting include in terms of (i) temporal weighting and (ii) representative weighting during training, and which single weighting method (temporal vs representative) gives the larger gain in weighted nucleotide Recall@1 over PETRA-NW?",
      "answer": "PETRA-NW uses no weighted sampling (no temporal weighting, no representative weighting). PETRA-TW uses temporal weighting only (temporal \u2713, representative \u2717). PETRA-RW uses representative weighting only (temporal \u2717, representative \u2713). PETRA uses both temporal and representative weighting (temporal \u2713, representative \u2713). Comparing weighted nucleotide Recall@1: PETRA-NW = 3.04%, PETRA-TW = 8.94% (+5.90 points), PETRA-RW = 4.25% (+1.21 points), so temporal weighting provides the larger gain over PETRA-NW.",
      "source_document": "papers/2511.03976v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Appendix A.4.1, the authors justify using a hand-made, fixed tokenizer rather than inheriting UShER\u2019s encoding scheme. What specific change do they make to UShER\u2019s mutation-token encoding (describe both what UShER does and what PETRA does instead), and what concrete practical capability does this change enable for running a trained model on future data?",
      "answer": "UShER \u201cshuffles the encoding for each token on every update,\u201d whereas PETRA unifies the encoder by assigning a fixed token to each of the 29,903\u00d75 = 149,515 possible mutation end-states (encoding only the final/mutated status per codon and treating mutations like A1T and G1T as the same token). Using a fixed mapping allows a PETRA model trained on an older UShER snapshot to be used for inference on newer UShER updates (and even on manually uploaded new sequences) without breaking due to remapped tokens.",
      "source_document": "papers/2511.03976v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Appendix A.2 the authors describe a cross-validation schedule to reconcile disagreements in variant-defining mutations across UShER, Nextstrain/Nextclade, and Cov-Spectrum. What are the three steps of this schedule, and in Step 3 what exact frequency criterion is used to decide when a nucleotide state should be treated as a defining mutation (include both the percentage threshold and the fold-enrichment condition)?",
      "answer": "The schedule is:\n1) For each variant, locate on the UShER tree the node belonging to that variant that is closest to the root, and take that node\u2019s mutations as the base defining set.\n2) Check the Nextstrain definition and directly add insertions and deletions to the defining set, because UShER does not contain indel information.\n3) For non-deleted codons where UShER and Nextclade disagree, query Cov-Spectrum to adjudicate: if a particular nucleotide state (including deletion) is present in >50% of sequences for the variant and appears at least 10\u00d7 more frequently than any other state, treat it as a defining mutation; otherwise (if neither side meets this condition) default to the UShER definition.",
      "source_document": "papers/2511.03976v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Algorithm 1 (Dynamic Selection Module) of DSRPGO, how are \u201cactive experts\u201d selected and how are their mixture weights computed before concatenating expert outputs? Specify the thresholding rule and the normalization formula used for the weights.",
      "answer": "Active experts are selected by thresholding the expert confidence coefficients: after computing \\(\\hat{p}\\leftarrow \\mathrm{Softmax}(\\mathrm{MLP}(X_{dsm}))\\), the active set is \\(S\\leftarrow \\{E_i\\mid \\hat{p}_i\\ge t\\}\\), where \\(t\\) is the threshold. For each selected expert \\(E_i\\in S\\), the mixture weight is the normalized confidence \\(W_i = \\hat{p}_i / \\sum_{E_j\\in S} \\hat{p}_j\\). The DSM output concatenates the weighted expert outputs \\(\\mathrm{Concat}(W_i\\cdot E_i(X_{dsm}))\\).",
      "source_document": "papers/2511.04040v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the fine-tuning stage, the paper replaces standard binary cross-entropy with an asymmetric loss to address class imbalance in GO-term multi-label prediction. Write the exact asymmetric loss formula used (including the two focusing parameters), and state what the symbols y_i^m and p_i^m represent in this equation.",
      "answer": "The prediction loss is the asymmetric loss (Eq. 8):\n\nL = (1/(N M)) * \\sum_{i=1}^{N} \\sum_{m=1}^{M} \\Big[ - y_i^m (1 - p_i^m)^{\\gamma_{+}} \\log(p_i^m) \\, - \\, (1 - y_i^m) (p_i^m)^{\\gamma_{-}} \\log(1 - p_i^m) \\Big].\n\nHere y_i^m is the ground-truth label for the i-th protein on GO term m, and p_i^m is the predicted score for that protein/term. The symbols \\gamma_{+} and \\gamma_{-} (written as {y+} and {y\u2212} in the text) are the positive and negative focusing parameters, respectively.",
      "source_document": "papers/2511.04040v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s \u201cFeature Effectiveness Analysis,\u201d how are clusters defined when computing the Davies\u2013Bouldin (DB) score, and what does a lower DB score indicate about the learned protein representations?",
      "answer": "When computing DB scores, the GO terms are used as the cluster labels: proteins sharing the same GO-term set are grouped into the same cluster. A lower DB score indicates more compact clusters and clearer separation between clusters (i.e., more discriminative representations).",
      "source_document": "papers/2511.04040v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the reconstructive pre-training stage, how does the paper incorporate the pretrained ProtT5 model into the Protein Sequence Information (PSeI) encoder-decoder pipeline\u2014specifically, what is done to ProtT5\u2019s parameters, and where is it connected for the subsequent pre-training?",
      "answer": "The model uses ProtT5 to parse protein sequences, freezes ProtT5\u2019s parameters, and connects ProtT5 to the PSeI encoder for further pre-training.",
      "source_document": "papers/2511.04040v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the reconstructive pre-training stage for spatial structure information (PSSI), what is the exact sample-wise binary cross-entropy objective \\(L_{sp}\\) minimized by the encoder\u2013decoder, and what do \\(N\\), \\(K\\), \\(H_i^k\\), \\(x_{ij}^{h(k)}\\), and \\(\\bar{x}_{ij}^{h(k)}\\) denote in that equation?",
      "answer": "The paper defines the PSSI encoder\u2013decoder reconstruction loss as a sample-wise binary cross-entropy over all proteins, input sources, and feature dimensions:\n\n\\[\nL_{sp}=\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{k=1}^{K}\\sum_{j=1}^{H_i^k}-\\Big[x_{ij}^{h(k)}\\log \\bar{x}_{ij}^{h(k)}+\\big(1-x_{ij}^{h(k)}\\big)\\log\\big(1-\\bar{x}_{ij}^{h(k)}\\big)\\Big].\n\\]\n\nHere, \\(N\\) is the total number of proteins, \\(K\\) is the number of input sources/modalities used in PSSI reconstruction, \\(H_i^k\\) is the feature dimension of the \\(k\\)-th input source for protein \\(i\\), \\(x_{ij}^{h(k)}\\) is the original (ground-truth) high-dimensional input feature value at index \\(j\\) for protein \\(i\\) from source \\(k\\), and \\(\\bar{x}_{ij}^{h(k)}\\) is the reconstructed feature value output by the decoder for that same entry.",
      "source_document": "papers/2511.04040v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the section on network-based inference (middle of the paper), the authors discuss two newer methods that integrate cell-type-specific expression with mutational burden. Name these two methods and state (i) what modeling idea each uses to incorporate expression information and (ii) a key limitation the authors explicitly note for VBASS.",
      "answer": "The two methods are VBASS and DYNATE. VBASS uses Bayesian modeling to weight rare-variant (mutational) burden by expression levels in relevant cell types, whereas DYNATE dynamically aggregates genomic regions via an aggregation-tree framework to test for enrichment of disease-associated variants while incorporating functional context. A key limitation noted for VBASS is that it is designed exclusively for de novo variation, limiting applicability to inherited variants.",
      "source_document": "papers/2511.04637v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Discussion, the authors propose an alternative way to quantify penetrance using EHR-linked biobank data and machine learning. What two quantities are contrasted to make penetrance a continuous, individual-level trait, and what kinds of modifiers does this framing allow researchers to identify?",
      "answer": "They describe predicting an individual\u2019s *expected phenotype* from clinical features using machine learning and then contrasting it with the *observed outcome*; the difference between predicted and observed is treated as a continuous penetrance measure for that individual. This framing enables identification of modifiers that buffer risk, including polygenic background, environmental exposures, and developmental timing.",
      "source_document": "papers/2511.04637v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the review\u2019s discussion of experimentally validating GWAS-implicated common/low-frequency variants, what specific MPRA study design details do the authors cite for non\u2013small cell lung cancer loci (variant frequency threshold, scale of variants tested, and main outcomes), and what two practical limitations do they highlight that complicate applying MPRAs to other complex disorders?",
      "answer": "They cite an MPRA that tested >1,200 common and low-frequency variants from non\u2013small cell lung cancer GWAS loci, using a minor allele frequency threshold of MAF > 0.5%, by cloning each allele into reporter constructs and measuring allele-specific effects on gene expression in lung cancer cell lines. The assay found 82 functional regulatory variants and narrowed these to 30 likely causal alleles. The authors note MPRAs are (1) labor-intensive and (2) constrained by cell-type specificity and developmental context, which makes transfer to other complex disorders challenging.",
      "source_document": "papers/2511.04637v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the review\u2019s section on phenotype-focused approaches (intermediate-frequency discovery), the authors cite a large UK Biobank association study comparing discovery rates for binary vs quantitative outcomes. What specific contrasts do they report for (i) how much more common variant-level associations were for quantitative traits and (ii) the proportions of phenotypes with at least one gene-level association for quantitative vs binary traits?",
      "answer": "They report that variant-level associations were nearly eight times more common for quantitative traits, and that gene-level associations were found in 54% of quantitative phenotypes versus 5.4% of binary phenotypes.",
      "source_document": "papers/2511.04637v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Section 2.1 (\u201cJoint searches\u201d), the authors argue that polygenic risk scores (PRS) could be incorporated into gene-discovery frameworks for intermediate-effect risk genes. According to the text, (i) what kinds of phenotypic features can PRS modulate among carriers of high-impact variants, and (ii) what specific gap do the authors identify in existing rare+common integration methods such as CORAC, RICE, and CLIN_SKAT with respect to *gene mapping*?",
      "answer": "(i) PRS can modulate phenotypic expression among carriers, including severity, age of onset, and comorbid features. (ii) While methods like CORAC, RICE, and CLIN_SKAT integrate rare and common variant signals to improve risk prediction and uncover shared mechanisms, they do not extend this integration to gene-mapping/discovery; the authors state that no current approaches explicitly leverage PRS to identify novel risk genes whose penetrance may be modulated by polygenic background.",
      "source_document": "papers/2511.04637v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the ESCAPE dataset compilation pipeline, how do the authors construct the negative (non-AMP) set from UniProt, and what specific keyword-based exclusion rule do they apply to reduce contamination by potentially antimicrobial sequences?",
      "answer": "They sample non-antimicrobial peptides from UniProt following TransImbAMP\u2019s methodology and apply strict exclusion filtering: they discard any UniProt sequences containing keywords such as \u201cmembrane,\u201d \u201ctoxic,\u201d \u201csecretory,\u201d \u201cdefensive,\u201d \u201cantibiotic,\u201d \u201canticancer,\u201d \u201cantiviral,\u201d or \u201cantifungal\u201d to improve the quality of the negative class.",
      "source_document": "papers/2511.04814v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the ESCAPE benchmark\u2019s implementation details, how do the authors adapt AVP-IFT\u2019s original contrastive-learning objective to support five-label multilabel AMP classification, and what continuous target do they use in place of the original binary similarity label?",
      "answer": "They modify AVP-IFT\u2019s contrastive loss by replacing the original binary similarity label with a continuous value computed from the multilabel annotations: the target is the fraction of similarity and dissimilarity across the five classes in the multilabel vector (antibacterial, antiviral, antifungal, antiparasitic, antimicrobial).",
      "source_document": "papers/2511.04814v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the ESCAPE Baseline\u2019s bidirectional cross-attention module, what tensors are used as queries/keys/values when (i) the sequence attends to structure and (ii) the structure attends to sequence, and how are the fused modality outputs used to produce the final multilabel prediction?",
      "answer": "(i) When the sequence attends to structure, the queries come from the sequence embeddings Qx = X W_Q^x, while keys and values come from the structure embeddings Ky = Y W_K^y and Vy = Y W_V^y (yielding Ax = softmax(Qx Ky^T / sqrt(d)) Vy). (ii) When the structure attends to sequence, the queries come from structure Qy = Y W_Q^y, while keys and values come from the sequence embeddings Kx = X W_K^x and Vx = X W_V^x (Ay = softmax(Qy Kx^T / sqrt(d)) Vx). After cross-attention, the model concatenates the updated [CLS] token from each modality and feeds that concatenated vector through a classification head (linear layer) to generate the final 5-dimensional multilabel prediction (with sigmoid later used for independent class probabilities).",
      "source_document": "papers/2511.04814v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s ablation study on sensitivity to predicted 3D structures, how do the authors set up the train/test replacement of experimental crystal structures with AlphaFold-predicted structures (which folds are replaced vs. left unchanged), and what absolute changes in mAP and F1 do they report when training with only predicted structures compared to using experimental+predicted structures?",
      "answer": "They replace the experimental 3D structures in the training data (Fold 1 and Fold 2; 1,671 peptides) with their AlphaFold predictions, while leaving the Test set unchanged. Using only generated/predicted structures yields mAP 71.2 and F1 67.5 versus 72.7 mAP and 69.4 F1 for experimental+generated structures\u2014an absolute drop of 1.5 mAP points and 1.9 F1 points when relying only on predicted structures.",
      "source_document": "papers/2511.04814v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the ESCAPE benchmark\u2019s standardized evaluation protocol, how do the authors combine the two cross-validation models at inference time, and at what point (relative to the sigmoid activation) is this combination performed?",
      "answer": "They ensemble the two cross-fold models by averaging their output logits from both trained instances, and this averaging is done before applying the sigmoid activation (i.e., logits are averaged prior to sigmoid).",
      "source_document": "papers/2511.04814v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In SIGMADOCK\u2019s late-stage sampling/selection procedure, how do the authors define the \u201cmixed score\u201d used to rank poses across multiple random seeds, what does each term represent, and what value do they set for the hyperparameter that controls the PoseBusters penalty strength?",
      "answer": "They rank sampled poses i using a mixed score\ns_i = \u2212b_i \u00b7 p_i^\u03b2.\nHere b_i is the (Vinardo) protein\u2013ligand binding energy of sample i (lower is better, so \u2212b_i makes higher scores better), and p_i\u2208[0,1] is the average PoseBusters validity across several stereochemical checks (bond lengths/angles, chirality, steric clash, minimum distance to the protein), so p_i^\u03b2 penalizes chemically implausible poses. They set \u03b2 = 4 for the PoseBusters penalty strength.",
      "source_document": "papers/2511.04854v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the \u201cSoft geometric constraints\u201d / triangulation conditioning scheme, for a torsional bond BC connecting adjacent fragments A and D, what two cross-fragment distances are added, and which two bond angles does Lemma 1 state become uniquely determined as a result (while not restricting changes in the dihedral \u2206\u03d5ABCD)?",
      "answer": "They add the cross-fragment distances ||A\u2212C|| and ||B\u2212D||. With these distances (together with the existing bond lengths around BC), the adjacent bond angles \u2220(A,B,C) and \u2220(B,C,D) become uniquely determined, without restricting changes in the dihedral \u2206\u03d5ABCD.",
      "source_document": "papers/2511.04854v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Appendix G.3, SIGMADOCK modifies EquiformerV2 to avoid instabilities caused by dynamic (distance-cutoff) edges appearing/disappearing as z is perturbed. What specific architectural change do the authors make to the MLPs (RadialFunction) that process edge features for these dynamic edges, and what two smoothness properties is this change intended to enforce as the edge distance approaches the cutoff?",
      "answer": "They remove the bias terms inside the MLPs (the RadialFunction) used for dynamic edges (and use a different MLP per edge type). This is intended to ensure (1) the messages passed along these dynamic edges (including via the edge degree embedder / attention blocks) smoothly decay to zero as the distance approaches the cutoff, and (2) the gradient norms also smoothly decay to zero as the edge distance approaches the cutoff, yielding smooth force predictions despite topology changes.",
      "source_document": "papers/2511.04854v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s *global processing* module, how are low-confidence nuclei candidates turned into segmented instances, and what specific numeric thresholds/criteria are used at each step (i.e., the heatmap filtering criterion and the probability threshold used when binarizing before instance-level FP removal)?",
      "answer": "Global processing first performs **heatmap filtering** by computing a per-instance confidence (average of the heatmap probabilities over pixels in the predicted instance) and **removes highly confident instances with average probability > 0.95** from the subsequent local-maxima detection stage, producing a filtered heatmap P\u2032. It then applies **local maxima detection** using a Laplacian-of-Gaussian to find candidate nuclei locations; the **centroids** of these candidates are used to **seed a watershed** algorithm to delineate boundaries. Finally, it **binarizes** the filtered heatmap (and appends back the confident instances) using a **low probability threshold Tp = 0.35** to form candidate instances for the last self-supervised instance-classification step that removes false positives.",
      "source_document": "papers/2511.04892v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the experimental setup, how do the authors tune LG-NuSegHop\u2019s hyperparameters across modules\u2014specifically, which evaluation metrics are used to (i) adjust the local processing module, (ii) tune the NuSegHop + global processing modules, and (iii) tune the final self-supervised instance-classification step, and what is the stated rationale for using a different metric in (iii)?",
      "answer": "They first adjust the local processing module using AJI and F1 computed from the intermediate pseudolabel output. Then they adjust NuSegHop together with the global processing modules by tuning hyperparameters to maximize AJI. For the last global-processing module (self-supervised instance classification), they instead try to maximize the F1 score, because that stage is an ROI-wise classification task.",
      "source_document": "papers/2511.04892v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s final \u201cself-supervised instance classification\u201d step for false-positive reduction, what image representation and feature set are extracted from each predicted nucleus instance before classification, and what classifier (including kernel) is used to assign a true-positive probability?",
      "answer": "For each predicted instance, they use the hematoxylin (H) image, convert it to HSI color space, perform feature extraction separately on each HSI channel and concatenate the features. The features are first-order statistics (to capture color characteristics) plus gray-level zone matrix (GLZM) texture features (to capture rough texture). They then use an SVM classifier with a radial basis function (RBF) kernel to output the probability that an instance is a true positive.",
      "source_document": "papers/2511.04892v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In NuSegHop\u2019s feature construction (after spatial-wise and spectral-wise Saab/PCA feature extraction), what is the final feature dimensionality fed to the pixel-wise classifier and how is it obtained from the full concatenated feature set? State the exact number of features used and the selection criterion described.",
      "answer": "After concatenating all spatial and spectral features from both layers into the final feature vector X, NuSegHop performs feature selection and keeps the top 100 discriminant features. Thus, the classifier is trained on 100 features, obtained by selecting the \u201ctop 100 discriminant features\u201d from the full concatenated spatial\u2013spectral feature set (citing [65]).",
      "source_document": "papers/2511.04892v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s \u201cLimitations & Discussion\u201d section, the authors compare deployment complexity of LG-NuSegHop to a deep learning baseline. What (i) total parameter count do they report for NuSegHop\u2019s feature extraction, and (ii) what are the reported per-image inference times and hardware used for HoVer-Net versus LG-NuSegHop when predicting a 1000\u00d71000 histology tile?",
      "answer": "(i) NuSegHop\u2019s feature-extraction module is reported to have 40K parameters. (ii) HoVer-Net is reported to take about 11.04 seconds per 1000\u00d71000 image using a GPU with 12GB memory, whereas LG-NuSegHop takes about 9.38 seconds on average per 1000\u00d71000 image using a multi-thread implementation on an Intel Xeon CPU E5-2620 v3 at 2.40\u202fGHz.",
      "source_document": "papers/2511.04892v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In GastroDL-Fusion\u2019s protein\u2013ligand complex encoder, the authors add an explicit 3D geometric constraint during GIN training. What geometric quantity do they compute, and how is it used during training to improve affinity prediction (describe the mechanism stated in the paper, not just \u201cit helps performance\u201d)?",
      "answer": "They compute the 3D Euclidean distance between pairs of molecular graph nodes, defined as d_ij = ||r_i \u2212 r_j||_2. During training, they add a regularization term that enforces consistency between the learned/predicted representation and these actual geometric relationships (i.e., the model must align its representation with the true inter-atomic distances), which improves binding-affinity prediction accuracy.",
      "source_document": "papers/2511.05726v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In GastroDL-Fusion\u2019s gene-sequence branch, after ProtBERT/ESM produces residue-level embeddings and a pooled global sequence vector, the authors add an extra module to emphasize disease-related mutations near binding sites. What is this additional module (name the two model components in order), and what specific kind of information does each component capture according to the paper?",
      "answer": "They apply a 1\u2011D CNN followed by a BiLSTM on a \u201cmutation window\u201d around the site. The 1\u2011D CNN is used to capture local physicochemical changes in the mutation window (local patterns), and the BiLSTM then models bidirectional context across that window to enhance detection of functional alterations; together this yields a sequence embedding reflecting both global gene function and local sensitivity to disease\u2011relevant mutations.",
      "source_document": "papers/2511.05726v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Experiment section, what exact data split and training configuration does GastroDL-Fusion use (percentages for train/validation/test, optimizer, initial learning rate, batch size, and the criterion used for early stopping)?",
      "answer": "The datasets are split into training 70%, validation 15%, and test 15%. The model is optimized with Adam using an initial learning rate of 1e-4, batch size 64, and early stopping is applied based on validation loss.",
      "source_document": "papers/2511.05726v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Conclusion\u2019s limitations/future-work discussion, what specific biological or experimental factors does the paper say GastroDL-Fusion *excludes* that could affect protein\u2013ligand binding affinity, and what two concrete future validation/extension steps do the authors propose to address these limitations?",
      "answer": "The paper states that GastroDL-Fusion is trained on curated protein\u2013ligand and gene-sequence data and excludes other factors that could affect binding affinity, specifically **conformational flexibility**, **epigenetics**, and **environmental influences**. To address these limitations, it proposes (1) **validating the model on larger, more heterogeneous datasets** to improve robustness/generalizability, and (2) extending the biological/physical context by **incorporating multi-omics data** and **integrating molecular dynamics simulations** to move beyond static structures when predicting protein\u2013ligand dynamics.",
      "source_document": "papers/2511.05726v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the dataset-preparation description for GastroDL-Fusion, what are the stated sources and approximate sizes of the two modalities (protein\u2013ligand complex data and GI-disease gene-sequence data), and what specific additional annotations/attributes does each sample include beyond the raw structure/sequence (as described in the text)?",
      "answer": "Protein\u2013ligand complex modality: obtained from the Protein Data Bank (PDB) and BindingDB; after cleaning/standardization the authors compile ~15,000 high-quality protein\u2013ligand complex samples. Each sample includes the 3D protein structure (atomic coordinates and amino-acid residue information), a molecular graph of the ligand (atom-node features and bond types), and the corresponding binding-energy/affinity value.\n\nGene-sequence modality: retrieved from NCBI Gene and Ensembl; after preprocessing they obtain ~5,000 gene-sequence samples. Sequences are provided in FASTA format, and the preprocessing extracts nucleotide sequences plus exon segments and disease-relevant variant information; the dataset also includes gene-expression data under different disease states. Each sequence is converted to a vector representation with residue-level embeddings extracted using pre-trained protein language models (ProtBERT or ESM).",
      "source_document": "papers/2511.05726v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s \u201clower-set bipartitioning\u201d strategy for complex splicing bubbles, what exact constraint defines a *valid* bipartition of paths, and what is the asymptotic time complexity the authors give for enumerating all such valid bipartitions using the containment graph (state it in terms of n, m, and L(g))?",
      "answer": "A bipartition is valid only if it respects subset relations among paths: if a path (viewed as a set of exonic-part edges) is included in one partition, then all of its subset paths must also be in that same partition\u2014i.e., the chosen side is a proper downward-closed subset (a lower set) of the containment graph, with the split being the lower set versus its complement (nonempty and not full). The enumeration algorithm runs in overall time O((n+m)*L(g)), where n is the number of path-nodes, m the number of subset-relation edges in the containment graph, and L(g) the number of lower sets.",
      "source_document": "papers/2511.05992v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the authors\u2019 scheme for comparing two partitions of paths within a complex splicing bubble, how are the *distinct* exonic parts for each partition and the *shared* exonic parts between partitions defined using set operations (i.e., in terms of the intersections/unions of paths within each partition), and what additional shared exonic parts do they include beyond those universally common within the bubble?",
      "answer": "Let partition 1 contain paths {P1,\u2026,Pa} with exonic-part edge sets A_i, and partition 2 contain paths {Q1,\u2026,Qb} with edge sets B_j. Define within-partition intersection and union as I1 = \u22c2_{i=1..a} A_i, U1 = \u22c3_{i=1..a} A_i, and I2 = \u22c2_{j=1..b} B_j, U2 = \u22c3_{j=1..b} B_j. The distinct exonic parts are D1 = I1 \\ U2 (edges present in all paths of partition 1 but absent from every path in partition 2) and D2 = I2 \\ U1. The shared exonic parts are S = I1 \u2229 I2 (edges universally common to all paths in both partitions). In addition, they also count as shared the incident edges incoming to the bubble\u2019s source and outgoing from the sink that are shared by all transcripts going through the bubble.",
      "source_document": "papers/2511.05992v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "The authors propose an optional \u201ccollapse internal bubbles\u201d step to reduce redundant re-analysis when traversing nested bubbles. According to the text, in what order are bubbles processed for this traversal, and after an inner bubble is examined, what specific rule is used to choose the single path that is retained when collapsing that inner bubble before analyzing the next (outer) bubble?",
      "answer": "Bubbles are processed in hierarchical order from inner to outer: \u201cshorter and deeper (inner nested) bubbles come first,\u201d followed by longer, outer bubbles. After examining an inner bubble, they collapse it by retaining only the single path that is followed by the largest number of transcripts (based on the annotation) before moving on to the outer bubble.",
      "source_document": "papers/2511.05992v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s description of the multinomial (n-way) comparison strategy for an n-path splicing bubble, what specific condition makes the multinomial comparison \u201cnot applicable,\u201d and what is the stated reason this condition can arise under the one-vs-rest formulation?",
      "answer": "The multinomial comparison is not applicable if path-unique exonic parts are missing for even a single path. The authors state this can arise because the multinomial strategy treats each path as a singleton outcome in a one-vs-rest setup, and many paths may lack exonic parts that are unique to that path (i.e., distinguishing exonic parts are not available for some paths under OVR).",
      "source_document": "papers/2511.05992v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the section defining shared/distinct exonic parts for comparing two *partitions* of paths within a splicing bubble, what is the paper\u2019s proposed differential-usage test statistic, expressed in terms of read counts mapped to distinct vs. shared exonic parts?",
      "answer": "They test the ratio (fraction) of reads mapping to the distinct exonic parts divided by the reads mapping to (distinct + shared) exonic parts\u2014i.e., reads(distinct) / [reads(distinct) + reads(shared)].",
      "source_document": "papers/2511.05992v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the review\u2019s discussion of nonequilibrium generalization (Section 5.4), it argues that a learned force field can match equilibrium free energies yet still be physically wrong under driven conditions. According to the document, name the three specific nonequilibrium/thermodynamic constraints such a model may violate, and list the three corresponding mitigation strategies the author proposes to address these failures.",
      "answer": "Potential failures (even if equilibrium free energies are reproduced): (1) violation of the Jarzynski equality \u27e8e^{\u2212\u03b2W}\u27e9 \u2260 e^{\u2212\u03b2\u0394F}; (2) prediction of negative entropy production \u27e8\u03a3\u27e9 < 0 in a driven steady state; (3) violation of Crooks\u2019 theorem via incorrect forward/reverse path probability ratios. Proposed mitigations: (i) augment training with nonequilibrium trajectories (e.g., pulling, flow, temperature ramps); (ii) add path-integral loss terms enforcing Jarzynski/Crooks identities; (iii) constrain the learned drift and diffusion so they satisfy detailed balance at equilibrium and obey fluctuation\u2013dissipation coupling out of equilibrium.",
      "source_document": "papers/2511.06585v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s Appendix B, how do the authors (i) construct the empirical Okazaki fragment (OF) size distribution from the raw T4 bacteriophage fragment-length measurements, including the binning choice and how P_exp(z) is defined, and (ii) set up the maximum-likelihood objective (distributional assumption for counts, error model, and the explicit likelihood form) used to estimate the model parameters q and r?",
      "answer": "(i) They take the individual OF lengths measured by electron microscopy at 8 nM and 64 nM primase concentrations (N=221 and N=261, with z_max=10.6 kbp and 8.2 kbp, and shortest OF 0.1 kbp) and bin each dataset with bin width dz = 300 bp. The number of bins is I = 1 + \u230az_max/dz\u230b (36 bins for 8 nM and 28 bins for 64 nM). From the binned counts n(z) in [z, z+dz), they define the empirical distribution as P_exp(z) = n(z)/(N\u00b7dz), i.e., the fraction of OFs in the bin per unit length.\n(ii) They treat the bin counts n(z) as Poisson random variables, giving standard errors \u03c3(z)=\u221an/(N\u00b7dz). For fitting, they use a Gaussian likelihood for the binned P_exp values around the model prediction Q(z) (Eq. 1), with\nL = \u220f_{i=1}^{I} [1/\u221a(2\u03c0 \u03c3(i dz))] \u00b7 exp(\u2212[P_exp(i dz) \u2212 Q(i dz)]^2 / (2 \u03c3(z)^2)).\nThey then maximize the log-likelihood log L over q and r to obtain the optimal parameters.",
      "source_document": "papers/2511.06904v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Results section where the authors analyze the predicted gap-size distribution R(g), they emphasize that the mean gap size behaves counterintuitively. According to Eqs. (6)\u2013(7) and the surrounding discussion, (i) what is the expression for the mean gap size \u27e8g\u27e9 when averaging over all gaps (including zero-sized gaps from collisions), and (ii) what is the expression for the mean gap size \u27e8g\u27e9+ when averaging only over non-zero gaps, and why does this change whether the mean depends on the spontaneous dissociation rate \u03f5?",
      "answer": "(i) Averaging over all gaps (including the delta-function mass at g=0 from collision-triggered dissociation), the mean gap size is \u27e8g\u27e9 = \u222b0^\u221e g R(g) dg = 1/r (Eq. 6), i.e., it depends only on the (re-)binding rate parameter r and is independent of the spontaneous dissociation rate \u03f5.\n\n(ii) If the mean is computed only over non-zero gaps (i.e., conditioning on gaps drawn from G(g)), then \u27e8g\u27e9+ = [\u222b0^\u221e g G(g) dg]/[\u222b0^\u221e G(g) dg] = 1/[r(1\u2212f_c)] (Eq. 7). This conditional mean can depend on \u03f5 because f_c (the fraction of collision-induced dissociations) depends on p=r/q and hence on q, which is proportional to \u03f5 (q=\u03f5/v). Including zero-sized gaps removes that dependence because those events contribute probability mass but zero length to the average, yielding the simpler \u27e8g\u27e9=1/r.",
      "source_document": "papers/2511.06904v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the section where the authors map their fitted, *rescaled* parameters to physical kinetic rates for the T4 bacteriophage replisome, what explicit conversion do they use to obtain the polymerase binding rate (\\u03c0*) and spontaneous dissociation/unbinding rate (\\u03f5*) from the fitted parameters (q*, r*) and the polymerase speed v, and what numerical value do they take for v from the cited experiment?",
      "answer": "They convert the dimensionless/rescaled fit parameters back to rates using \\u03c0* = v r* for the binding rate and \\u03f5* = v q* for the spontaneous dissociation (unbinding) rate. For T4 bacteriophage they take the polymerase speed to be v = 400 bp\\u00b7s\\u22121 (from Ref. [13]).",
      "source_document": "papers/2511.06904v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the derivation of the transition probability and the steady-state Chapman\u2013Kolmogorov equation (around Eqs. (8)\u2013(11)), how do the authors model the polymerase\u2019s rebinding waiting time and use it to derive the explicit form of the transition probability \\(W(z,g\\mid z',g')\\)? State (i) the waiting-time distribution \\(\\phi(\\Delta t)\\) including the role of the step function, and (ii) the final expression for \\(W(z,g\\mid z',g')\\) after averaging over \\(\\Delta t\\).",
      "answer": "(i) The waiting time \\(\\Delta t\\) to rebind is assumed exponentially distributed with rate \\(\\pi\\), restricted to nonnegative times: \\(\\phi(\\Delta t)=\\theta(\\Delta t)\\,\\pi e^{-\\pi\\Delta t}\\), where \\(\\theta(\\Delta t)=1\\) for \\(\\Delta t\\ge 0\\) and 0 otherwise.\n\n(ii) Averaging the joint distribution over \\(\\Delta t\\) yields the transition probability (their Eq. 9):\n\\[\nW(z,g\\mid z',g')=\\theta(g+z-z')\\, r q\\, e^{-(q+r)z - r(g-z')} \n+ \\theta(z-z')\\,\\delta(g)\\, r\\, e^{-r(z-z')-q z}.\n\\]\n(Here \\(q=\\epsilon/v\\) and \\(r=\\pi/v\\) are the rescaled spontaneous dissociation and binding rates.)",
      "source_document": "papers/2511.06904v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the section where the authors reduce the 2D Chapman\u2013Kolmogorov equation for the steady-state joint distribution P(z,g) to a 1D equation for the Okazaki-fragment size distribution Q(z), what are the explicit \u201cbackward\u201d and \u201cforward\u201d hopping kernels wb(z|z\u2032) and wf(z|z\u2032) in Eq. (11), and what biased-random-walk interpretation do the authors give for this equation (i.e., what plays the role of space and what are the hopping probabilities)?",
      "answer": "After integrating the steady-state Chapman\u2013Kolmogorov equation over g, they obtain\n\nQ(z) = \\int_0^z Q(z\u2032)\\,wb(z|z\u2032)\\,dz\u2032 + \\int_z^\\infty Q(z\u2032)\\,wf(z|z\u2032)\\,dz\u2032.\n\nThe kernels are (Eqs. 11 and the definitions immediately below it):\n- Backward kernel: wb(z|z\u2032) = q\\,e^{\u2212qz}.\n- Forward kernel: wf(z|z\u2032) = (q + r)\\,e^{\u2212qz \u2212 r(z \u2212 z\u2032)}.\n\nThey interpret Eq. (11) as the Chapman\u2013Kolmogorov equation of a continuous-space biased random walker where the \u201cspace\u201d variable is the fragment size z, and wb(z|z\u2032) and wf(z|z\u2032) act as the space-dependent backward and forward hopping probabilities, respectively.",
      "source_document": "papers/2511.06904v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s two-tier active-learning (AL) fine-tuning workflow, what exact criteria are used in the *outer loop* to move molecules from the temporal-specific set into the permanent-specific set during (i) the first four outer-loop cycles and (ii) the subsequent four cycles? Include both the docking-score threshold(s) and any required binding-interaction constraint(s).",
      "answer": "Outer-loop transfer from the temporal-specific set to the permanent-specific set requires both (1) a Glide docking-score cutoff and (2) presence of the critical hinge hydrogen bond to Ala145. For cycles 1\u20134, molecules must have a docking score \u2264 \u22127.0 kcal/mol and form an Ala145 hydrogen bond. For cycles 5\u20138, the docking cutoff is tightened to \u2264 \u22127.5 kcal/mol (while still requiring the Ala145 hydrogen bond).",
      "source_document": "papers/2511.06930v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Methods section\u2019s molecular docking protocol, what software/toolchain and specific setup parameters are used to dock the temporal-specific-set molecules to SIK3 (include: docking engine and precision mode, ligand preparation settings, protein structure source/PDB code, grid-box size and center coordinates, and the imposed interaction constraint and residue) ?",
      "answer": "Docking was performed with Schr\u00f6dinger\u2019s GLIDE using the Standard Precision (SP) protocol. Ligands were prepared with LigPrep at pH 7.4 \u00b1 0.5, generating up to four tautomers per molecule. The SIK3 structure came from the RCSB PDB (PDB code 8r4v) and was prepared with the Protein Preparation Wizard (adding hydrogens). A docking grid box of 29 \u00c5 was built centered on the co-crystallized ligand with coordinates X = \u22121.90, Y = \u221260.06, Z = \u22124.14. A hydrogen-bond constraint was applied to the backbone NH of the hinge residue Ala145 (Ala145 H-bond).",
      "source_document": "papers/2511.06930v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s off-target binding assessment, how are potential off-target kinases selected for docking starting from the kinase structures\u2014specify (i) the source of kinase entries, (ii) which structure is used when no crystallographic PDB is available (including the pLDDT threshold), (iii) how binding-site similarity is computed, and (iv) the numeric similarity cutoff used to decide which kinase sites are \u201csimilar\u201d to SIK3?",
      "answer": "(i) Kinase entries are taken from KLIFS. (ii) If no crystallographic structure is available for a kinase entry, a predicted AlphaFold structure is used only if the mean pLDDT is \u226560. (iii) PickPocket is used to identify binding sites and extract binding-site embeddings; pairwise Euclidean distances are then computed between embeddings. (iv) Kinase-site pairs with embedding distance <10 are considered similar to SIK3 and selected for subsequent docking to assess specificity/off-target interactions.",
      "source_document": "papers/2511.06930v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s active-learning inner loop, how is kernel density estimation (KDE) used to bias molecular generation in latent space\u2014specify (i) what set of latent vectors KDE is fit to, (ii) how many new latent vectors are sampled from the resulting density per iteration, and (iii) the KDE bandwidth value used?",
      "answer": "(i) KDE is fit to the latent representations of molecules that passed the inner-loop physico-chemical property filters (i.e., filtered molecules\u2019 latent vectors). (ii) The resulting KDE-estimated PDF is used to sample 1,500 new latent vectors per iteration. (iii) The KDE bandwidth is 0.3.",
      "source_document": "papers/2511.06930v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Methods\u2019 \u201cMolecular Dynamics Simulations of Selected Candidates\u201d section, what are the key setup choices and staged protocol used for the all-atom MD runs\u2014specifically: (i) which software suites and force fields are used for protein and ligands (including how ligand parameters/charges are obtained), (ii) how the system is solvated/neutralized, (iii) the sequence of minimization/heating/equilibration restraint steps (what is restrained and how restraints are released), and (iv) the production run conditions (ensemble, temperature/pressure targets, timestep, constraints, simulation length, and trajectory saving frequency)?",
      "answer": "(i) MD was run with the Amber software suite. Ligand parameters were derived from quantum-mechanical calculations using Jaguar: geometry optimization at the B3LYP/6-31G(d) level and PCM-solvation-model\u2013based partial charges; ligands were parameterized with GAFF. Protein preparation used PDB4amber, and the protein force field was ff14SB.\n\n(ii) Systems were built in tLeap with TIP3P water and Na+/Cl\u2212 ions added to neutralize the system charge.\n\n(iii) Two-stage energy minimization was performed: first with restraints on protein and ligand atoms (residues 1\u2013276) at 50 kcal/mol/\u00c5\u00b2; then in the second stage hydrogen atoms were released while keeping restraints on non-hydrogen atoms of the protein\u2013ligand complex only. Heating was done in three stages from 100 K to 300 K under NVT with positional restraints, then transitioning to NPT (1 atm) in the final heating stage. Equilibration used stepwise release of protein restraints (residues 1\u2013275, from 50 to 0.5 kcal/mol/\u00c5\u00b2) while keeping ligand restraint (residue 276) at 50 kcal/mol/\u00c5\u00b2, followed by progressive release of the ligand restraints (50 to 0.1 kcal/mol/\u00c5\u00b2) with weak terminal residue restraints on residues 1 and 275 at 0.5 kcal/mol/\u00c5\u00b2.\n\n(iv) Production runs were NPT at 300 K and 1 atm with isotropic pressure coupling, 2 fs timestep, and SHAKE constraints for bonds involving hydrogens. Each system was simulated for a total of 200 ns, saving coordinates every 100 ps for analysis.",
      "source_document": "papers/2511.06930v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Monte Carlo replica-exchange simulations, the authors sometimes use a move set that *breaks* linear topology by allowing chain crossings. What prior result do they cite to justify that this non\u2013topology-preserving move set still yields correct equilibrium thermodynamic properties, and what is the practical computational reason they give for choosing it in this study of deeply knotted proteins?",
      "answer": "They cite their previous work showing that equilibrium properties computed with a move set that preserves linear topology and one that allows chain crossings are indistinguishable for simple C\u03b1 models (tested on unknotted Fn-III and \u03b22m, shallowly knotted MJ0366, and deeply knotted Rds3p). The practical reason for using the topology-breaking move set here is that for deeply knotted proteins it is \u201cextraordinarily advantageous,\u201d providing correct equilibrium results at a much lower computational cost.",
      "source_document": "papers/2511.07024v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the \u201cModel systems\u201d and \u201cProtein YibK and its control systems\u201d sections, the authors argue that disentangling topology from other effects requires carefully matched unknotted controls. Describe (i) how the in silico unknotted control IS\u2011YibK is constructed from YibK (which specific loop is moved across what knotted-core segment, and what MD restraints/relaxation protocol is then applied), and (ii) what specific observation about melting temperature differences across YibK vs IS\u2011YibK vs CP\u2011YibK leads them to conclude that a lower Tm in an unknotted variant cannot be attributed to topology alone.",
      "answer": "(i) IS\u2011YibK is generated from YibK (PDB 1mxi) by manually passing the loop comprising residues 114\u2013125 across the knotted core (amino acids 81\u201385). The manipulated structure is then refined by an MD relaxation in which segments 1\u201380, 86\u2013113, and 126\u2013156 are positionally restrained (1000 kJ/mol\u00b7nm^2), while the two loops (81\u201385 and 114\u2013125) are left unrestrained to relax, yielding the final IS\u2011YibK structure.\n\n(ii) They find IS\u2011YibK has a melting temperature about 7% lower than YibK, but IS\u2011YibK does not conserve the number of native contacts, so the decreased Tm cannot be assigned to the loss of the knot. In contrast, CP\u2011YibK is nearly structurally identical to YibK and retains almost the same number of native contacts; YibK and CP\u2011YibK show practically coincident thermodynamic behavior with only ~1% difference in thermal stability. This pattern supports the conclusion that differences in Tm arise from structural/native-contact changes rather than topology per se.",
      "source_document": "papers/2511.07024v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the MC\u2013replica-exchange analysis, how do the authors operationally define the melting temperature (Tm) and the \u201ccooperativity degree\u201d of the folding transition, and what specific numerical target do they use to tune the Go-model potential-well half-width w?",
      "answer": "They compute thermodynamic observables from MC-RE data using WHAM to estimate the density of states. The heat capacity is computed as CV = (\u27e8E^2\u27e9 \u2212 \u27e8E\u27e9^2)/T^2 (reduced units), and the melting temperature Tm is defined as the temperature at which CV reaches its peak. Cooperativity is quantified by the ratio FWHM/Tm, where FWHM is the full width at half maximum of the CV peak. They adjust the potential-well half-width w so that the simulated FWHM/Tm ratio falls between 4% and 5%.",
      "source_document": "papers/2511.07024v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the \u201cComparison with experimental data\u201d discussion, the authors argue the DSC/Circular Dichroism melting temperatures for YibK may reflect non\u2011equilibrium behavior. What two specific timescales (and for what processes) do they cite from the MTTTm study, and what methodological inference do they draw about how these timescales would bias the experimentally measured Tm for a deeply knotted protein like YibK?",
      "answer": "They cite that for protein MTTTm the transition to an untied (unknotted) state takes at least ~6 months, whereas the transition to unfolded states occurs in ~2 weeks. From this separation of timescales they infer that, for deeply knotted proteins, a DSC heating scan is unlikely to equilibrate knot untying at each temperature; the experimental ensemble would be depleted of unfolded-and-unknotted conformations, so the measured melting temperature would act as an upper bound on the true equilibrium Tm.",
      "source_document": "papers/2511.07024v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Results/Discussion section comparing YbeA to its unknotted circular permutant CP\u2011YbeA, what change in folding-transition mechanism do the authors infer from the free-energy profile at the melting temperature, and what two model-level differences do they cite as the reason this cannot be attributed purely to \u201cbeing unknotted\u201d (i.e., to topology alone)?",
      "answer": "They infer that the folding transition shifts from two\u2011state behavior for knotted YbeA to a downhill (barrierless) transition for the unknotted CP\u2011YbeA when examining F(Q) at Tm. They argue this cannot be assigned solely to loss of the knot because untying via circular permutation causes (1) drastic structural changes (large loss of structural similarity/geometry) and (2) a significant loss of native interactions (fewer native contacts), which together explain the reduced cooperativity and lower thermal stability.",
      "source_document": "papers/2511.07024v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s survival analysis for RQ3, how do the authors scale age acceleration (AA) when reporting Cox proportional hazards model results, and what mortality-risk change do they report for EPICAGE per 5-year increase in AA in the internal versus external cohorts?",
      "answer": "They divide age acceleration (AA) by 5 so the Cox model reports the hazard ratio per 5-year increase in AA. For EPICAGE, each 5-year increase in AA is reported to be associated with a 6.7% increase in mortality hazard in the internal cohorts and an 11.3% increase in the external cohorts.",
      "source_document": "papers/2511.07219v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the EPICAGE ablation study, how do the authors define the two ablated variants \u201cw/o Ensemble\u201d and \u201cw/o Skip Connection\u201d in terms of which fusion layer is used and what inputs it receives, and which of these removals do they state causes a substantial performance drop specifically on external cohorts?",
      "answer": "\u201cw/o Ensemble\u201d is defined as using only the first-layer fusion clock (i.e., not using the second-layer meta-level ensemble). \u201cw/o Skip Connection\u201d is defined as using the second-layer fusion clock but without incorporating the raw feature inputs\u2014so it relies only on the predictions from the first-layer clocks. The authors specifically note that removing the skip connection leads to a substantial performance drop on external cohorts.",
      "source_document": "papers/2511.07219v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the implementation details, how do the authors make EPICAGE-TabPFN compatible with an external clinical dataset that contains one additional categorical variable not seen in training\u2014what encoding setting do they use, how are unseen categories represented, and what is the stated consequence for how predictions are computed for those cases?",
      "answer": "They convert all categorical variables to the category type and use TabPFN\u2019s internal one\u2011hot encoder with `handle unknown=\"ignore\"`. Unseen categories in the external set are mapped to an all\u2011zero vector in the one\u2011hot representation (treated as \u201cunknown\u201d), so predictions for those samples rely only on the remaining known features, enabling inference on the external dataset without further modification.",
      "source_document": "papers/2511.07219v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the RQ2 analysis of aging-related CpG sites, the authors evaluate whether EPICAGE\u2019s selected CpGs reflect cancer-specific aging signals beyond existing epigenetic clocks. What three CpG sets do they compare in this experiment, and what conclusion do they draw from the performance of the 49-CpG subset that excludes the 8 overlapping sites with existing clocks?",
      "answer": "They compare: (1) CpGs taken from existing epigenetic clocks, (2) the full set of 57 CpGs selected by EPICAGE, and (3) a 49-CpG subset formed by removing the 8 sites that overlap with existing clocks (Horvath, Hannum, PhenoAge, YingCausAge). They conclude that even after removing the overlapping 8 CpGs, the remaining 49 CpGs maintain similar predictive accuracy, suggesting EPICAGE identifies additional aging-associated CpGs that may capture cancer-specific aging signals not included in existing clocks.",
      "source_document": "papers/2511.07219v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In EPICAGE\u2019s layer-2 fusion clock, the model concatenates first-layer predictions with a skip connection from raw inputs. What *specific* dimension-reduction method and retained dimensionality do the authors use to form the skip-connection DNAm representation \\(\\tilde X_i\\), and on what data split is this reduction fit to avoid leakage?",
      "answer": "They use Principal Component Analysis (PCA) to obtain the skip-connection DNAm representation, retaining the top r = 400 principal components. The PCA is fit on the training DNAm matrix \\(X_{train}\\) (i.e., performed on the training set) and then the same 400 components are applied during inference, avoiding information leakage from the test data.",
      "source_document": "papers/2511.07219v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In SCISOR\u2019s UniRef-scale training setup, how do the authors handle Xt sequences longer than 2048 tokens while keeping the ELBO a valid lower bound, and what additional step do they take so that deletion probabilities remain properly normalized across the full Xt?",
      "answer": "If |Xt| > 2048, they uniformly sample a contiguous 2048-token window X_t^{[w:w+2048]} to feed to the de-noiser. They then re-normalize the model\u2019s predicted deletion probabilities by a factor 2048/|Xt| and assign uniform deletion probabilities outside the selected window, ensuring the probabilities over all positions in Xt sum to 1 while preserving the ELBO as a valid likelihood lower bound.",
      "source_document": "papers/2511.07390v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Appendix E.5 ablation study, which two specific modeling choices are ablated when training SCISOR-S on UniRef50, and what qualitative conclusion do the authors draw about the effect of each choice on training efficiency (i.e., which yields only a slight speedup vs. which is crucial for learning efficiently)?",
      "answer": "They ablate (1) the structured forward noising process that samples inserted amino acids according to their prevalence in the training set (instead of a uniform amino-acid distribution), and (2) the Rao\u2013Blackwellization of the training objective/gradient estimator (integrating over all insertion paths). The conclusion is that using prevalence-weighted insertions in the forward process gives only a slight speedup/small improvement in training, whereas the Rao\u2013Blackwellized loss is crucial for the model to learn efficiently.",
      "source_document": "papers/2511.07390v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the protein-shrinking evaluation (Sec. 8), why do the authors say it is computationally infeasible to sample shrunk subsequences from ProGen2 \u201cproportional to ProGen2\u2019s likelihood,\u201d and what specific alternative baseline do they implement instead?",
      "answer": "They state that sampling all length-(L\u2212M) subsequences of a length-L protein proportional to ProGen2\u2019s likelihood would require evaluating ProGen2 on all \\(\\binom{L}{M}\\) possible deletion sets, which is prohibitively expensive. Instead, they use a tractable baseline that first scores all L single deletions independently (requiring L model evaluations) and then samples sets of M deletions with probabilities based on these single-deletion effects, effectively assuming independent deletion effects.",
      "source_document": "papers/2511.07390v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In SCISOR\u2019s unconditional sampling procedure with corrector steps (Algorithm 2), what is the exact sequence of operations performed inside each corrector step, and what is the stated purpose of using these corrector steps compared to purely iteratively deleting until the target length is reached?",
      "answer": "Inside each corrector step k, SCISOR (1) removes one letter from the current sequence X according to the de-noiser distribution q\u03b8(prev(X) | X, M), (2) inserts a random letter drawn from the insertion distribution \u03c0 into a random position in X, and then (3) removes one letter again according to q\u03b8(prev(X) | X, M). The purpose is to improve sample quality by repeatedly noising (inserting) and de-noising (deleting), allowing the sampler to more thoroughly search the space of deletions and potentially escape local minima, at the cost of extra computation, rather than only deleting until length L is reached.",
      "source_document": "papers/2511.07390v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s adaptation of the ProteinGym indels benchmark (Appendix B.5.2), what exact filtering criterion do the authors apply to the indels dataset, and how do they define the \u201csingle deletions\u201d vs. \u201cmultiple deletions\u201d benchmarks in terms of deletion distance from the target sequence?",
      "answer": "They filter ProteinGym\u2019s indels dataset to only cases where the mutant sequence is a strict subsequence of the target sequence. Within this filtered set, the single-deletions benchmark uses mutants that are exactly one deletion away from the target sequence, while the multiple-deletions benchmark uses mutants that are two or three deletions away from the target sequence.",
      "source_document": "papers/2511.07390v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In EntangledSBM\u2019s off-policy weighted cross-entropy training scheme, what *specific* choice do the authors make for the auxiliary control drift \\(v\\) to enable replay-buffer reuse, and how do they compute the intractable importance weight \\(w^\\star(X_{0:T})\\) in practice when \\(Z\\) is unknown? (State both (i) the definition of \\(v\\) and (ii) the practical estimator of \\(w^\\star\\) used for optimization.)",
      "answer": "(i) They set the auxiliary/off-policy control drift to the previous iteration\u2019s bias force with gradients stopped: \\(v := \\bar b = \\mathrm{stopgrad}(b_\\theta)\\), so trajectories generated earlier can be reused from a replay buffer.\n\n(ii) Because the normalizing constant \\(Z\\) in \\(w^\\star(X_{0:T}) = \\frac{e^{r(X_T)}}{Z}\\,\\frac{dP_0}{dP_v}(X_{0:T})\\) is intractable, they compute \\(w^\\star\\) via a batch softmax estimator: \\(w^\\star(X_{0:T}) := \\mathrm{softmax}_B\\big(r(X_T) + \\log p_0(X_{0:T}) - \\log p_{\\bar b}(X_{0:T})\\big)\\), i.e., a softmax over the batch of trajectories (and for very small batches/large systems they sample from the replay buffer using \\(\\mathrm{Cat}(\\mathrm{softmax}_R(w^\\star))\\)).",
      "source_document": "papers/2511.07406v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Sec. 4.1, the authors design a bias-force parameterization that guarantees the predicted force does not increase distance to the target distribution. State (i) the explicit decomposition they use for each particle\u2019s bias force (identify the \u201cparallel\u201d term and how its coefficient is constrained), and (ii) the non-negativity condition this parameterization is designed to satisfy (write it as an inner-product inequality).",
      "answer": "(i) For each particle i they parameterize\n\\[b^i_\\theta(R_t,V^t)=\\alpha^i_\\theta(R_t,V^t)\\,\\hat s_i\\; +\\; (I-\\hat s_i\\hat s_i^\\top)\\,h^i_\\theta(R_t,V^t),\\]\nwhere \\(s_i=\\nabla_{r_t^i}\\log\\pi_B\\), \\(\\hat s_i=s_i/\\|s_i\\|\\). The first term is the component parallel to \\(\\hat s_i\\); its scalar coefficient is constrained to be nonnegative via a softplus: \\(\\alpha^i_\\theta:=\\mathrm{softplus}(\\alpha^i_\\theta)\\ge 0\\). The second term is an orthogonal correction obtained by projecting \\(h^i_\\theta\\) onto the plane orthogonal to \\(\\hat s_i\\).\n(ii) The intended constraint is\n\\[\\langle b^i_\\theta(R_t,V^t),\\nabla_{r_t^i}\\log\\pi_B\\rangle \\ge 0\\]\nfor all particles i (equivalently \\(\\langle b^i_\\theta, s_i\\rangle\\ge 0\\)), ensuring the projection of the bias force onto the target-gradient direction is positive.",
      "source_document": "papers/2511.07406v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the transition path sampling (TPS) benchmark, how do the authors operationally define a \u201chit\u201d when computing Target Hit Percentage (THP), and what are the two alternative low-dimensional coordinate mappings \u03be(R) they use to determine whether a trajectory endpoint RT lies in the target set \u03c0B for (i) alanine dipeptide vs. (ii) the fast-folding proteins? State the explicit neighborhood criterion used for \u03c0B.",
      "answer": "A trajectory is counted as a hit if its final position RT lies in the target set \u03c0B, where \u03c0B is defined as a 0.75-radius ball around the target in a chosen low-dimensional coordinate space: \u03c0B = { R | \u2016\u03be(R) \u2212 \u03be(RB)\u2016 < 0.75 }. For alanine dipeptide, \u03be(R) is the pair of backbone dihedral angles (\u03d5, \u03c8). For the fast-folding proteins (Chignolin, Trp-cage, BBA), \u03be(R) is the first two TICA components. THP is then computed as (1/M) * \u03a3_{i=1..M} 1[Ri_T \u2208 \u03c0B].",
      "source_document": "papers/2511.07406v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the transition path sampling experiment setup (Appendix E.1), how do the authors use Kabsch alignment to make the bias-force model invariant to rigid-body transformations? Describe (i) what coordinate frame they choose as the \u201caligned frame,\u201d (ii) which atoms\u2019 positions/velocities are aligned into that frame before being fed to the model, and (iii) what transformation is applied to the model\u2019s predicted bias force before it is used in the simulator.",
      "answer": "(i) They define the aligned frame to be the coordinate frame of the target structure (the target coordinates). (ii) They align the input positions and velocities of the heavy atoms (non-hydrogen atoms) into this target-aligned frame before passing them to the model. (iii) The model predicts the optimal bias force in the aligned frame, and they then transform that predicted force back to the original frame of the input positions before applying it in simulation.",
      "source_document": "papers/2511.07406v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the cell-perturbation experiment setup (Sec. 5.1), how do the authors construct *unseen* target distributions for evaluation from the perturbed single-cell data, and what is the train/eval split strategy across these target populations for (i) Clonidine and (ii) Trametinib? (Answer should state the number of disjoint clusters created per drug and how many are used for training vs held out for evaluation.)",
      "answer": "They cluster the perturbed-cell data into multiple *disjoint* perturbed cell populations (clusters) to serve as different target distributions \u03c0B, then train on only one cluster and evaluate generalization on the remaining clusters. Specifically: (i) for Clonidine they create 2 clusters total\u2014train on 1 and hold out 1 unseen cluster for evaluation; (ii) for Trametinib they create 3 clusters total\u2014train on 1 and hold out the other 2 clusters as unseen targets.",
      "source_document": "papers/2511.07406v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s experiments on handling extreme class imbalance, what specific \u201cANN forgetting\u201d issue did the authors observe during continuous re-training on batches of the no-bind data, and what concrete mitigation strategy do they propose for future work to address it (as described in the Discussion section)?",
      "answer": "They observed that continuous batch-wise re-training on the large, highly imbalanced no-bind portion did not yield reliable improvement because the network exhibited the well-known ANN \u201cforgetting\u201d problem. As a mitigation for future work, they propose using more granular, overlapping partitioning of the no-bind training and test examples to help reduce forgetting.",
      "source_document": "papers/2511.08648v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Discussion, the author interprets ensemble behavior to infer what different ANN activations are learning. What specific observation about *Tanh* homogeneous ensembles under *consensus voting* is reported, and what does the author conclude this implies about the binding patterns learned by Tanh models (compared to KGate) and the motivation for exploring heterogeneous ensembles?",
      "answer": "The paper reports that Tanh homogeneous ensembles with consensus voting \u201cfound no common true positives across models.\u201d The author concludes that this indicates Tanh ANNs learn different (non-codependent) binding patterns, unlike KGate models which appear to learn similar patterns but with higher dispersion. This non-overlap motivates trying heterogeneous ensembles\u2014especially those including Tanh\u2014to leverage complementary patterns, which the author notes can markedly reduce false positives but tends to increase false negatives.",
      "source_document": "papers/2511.08648v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Methods describing dataset construction, how are *non-binding* (no-bind) examples generated from the protein and RNA \u201c.fa\u201d files relative to known binding positions, and what does the paper state determines how many no-bind examples you end up with for a given experiment?",
      "answer": "Non-binding examples are built by taking sequence regions outside the context windows around known binding positions in the protein and RNA \u201c.fa\u201d files, splitting those outside regions into non-overlapping fragments of the chosen context-window length, and then pairwise matching them (each protein fragment without a binding point is joined with all RNA fragments without a binding point from the corresponding file). The number of resulting no-bind examples depends on the context-window length (the window size used in the experiment).",
      "source_document": "papers/2511.08648v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Methods section defining evaluation metrics, how does the paper define the predicted-positive set \\(P\\) using the ANN\u2019s softmax output and the \u201ctrusted threshold\u201d \\(Th\\), and what specific \\(Th\\) value does it then fix for computing the reported Accuracy/Precision/Recall/Specificity/F1 metrics (as opposed to ROC/AUC)?",
      "answer": "The paper defines predicted positives as the set of observations whose bind-class softmax weight meets or exceeds the trusted threshold: \\(P = \\{ C : sm_1(o_i) \\ge Th \\}\\) (with \\(O=B\\cup NB\\) the test observations). For the headline metrics (Accuracy, Precision, Recall, Specificity, F1), it fixes a neutral trusted threshold of \\(Th = 0.5\\) (while ROC/AUC is computed across multiple thresholds).",
      "source_document": "papers/2511.08648v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Discussion\u2019s comparison of adding Dropout regularization across the three convergent activation types (ReLU, KGate, Tanh), what specific explanation does the paper give for why KGate models show little change when Dropout layers are added, and why Tanh models get worse?",
      "answer": "The paper states that KGate ANNs show little change with Dropout because their GLU-based architecture inherently behaves like a data-driven dropout mechanism. In contrast, Tanh ANNs perform worse with Dropout, which the author attributes to excessive stochastic variability introduced by dropout.",
      "source_document": "papers/2511.08648v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the \u201cComprehensive Development Roadmaps\u201d results section, what specific activities does the Decision Orchestration Agent list for the mid-term (3\u20136 months) phase, and how are they grouped by workstream?",
      "answer": "For the mid-term (3\u20136 months) phase, the Decision Orchestration Agent specifies three grouped workstreams:\n1) Preclinical validation: generate CAR-T cells with optimized constructs; perform in vitro efficacy studies; conduct off-tumor toxicity assessment; evaluate cytokine secretion profiles.\n2) Patent strategy development: evaluate CrosMab compatibility; assess novel signaling domain configurations; monitor competitive filing activity.\n3) Manufacturing planning: develop a lentiviral vector production protocol; establish T cell processing; implement quality control assays.",
      "source_document": "papers/2511.08649v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Results section\u2019s FcRH5 hepatotoxicity case study, what three distinct evidence streams does the Toxicity Prediction Agent integrate to flag hepatotoxicity risk, and what specific mitigation strategies does it recommend in response?",
      "answer": "The agent flags FcRH5 hepatotoxicity risk using three converging evidence streams: (1) normal-tissue expression evidence from GTEx showing detectable FcRH5 expression in liver tissue (median TPM = 2.3), especially in hepatocytes; (2) pharmacovigilance evidence from the FDA FAERS database showing hepatotoxicity signals associated with cevostamab (an FcRH5-targeting bispecific antibody); and (3) literature analysis identifying combination trials with enhanced toxicity profiles. It recommends mitigation strategies including careful dose escalation, use of controllable CAR systems, patient selection excluding individuals with pre-existing liver disease, and evaluation of antigen masking technologies.",
      "source_document": "papers/2511.08649v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s logic-gated dual-target CAR design proposed for the GPRC5D+FcRH5 combination, what signaling domains are included or omitted in the primary versus secondary CAR, and what is the stated rationale for this split-signaling architecture?",
      "answer": "The agent designs an AND-gate with split signaling: the primary CAR targets GPRC5D and includes the CD3 zeta signaling domain but omits costimulation; the secondary CAR targets FcRH5 and includes the 4-1BB costimulatory domain but omits CD3 zeta. The rationale is to constrain on-target/off-tumor activity against FcRH5-positive hepatocytes while preserving potent activity against double-positive myeloma cells (i.e., full activation only when both antigens are present).",
      "source_document": "papers/2511.08649v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Results section\u2019s GPRC5D-targeting CAR design example (Section 4.5.1), what specific CAR construct components and predicted properties does the system propose, and what immunogenicity risk does it identify along with the recommended mitigation?",
      "answer": "For the GPRC5D CAR, the agent proposes a construct with a high-affinity scFv (KD \u2248 5 nM), an IgG4 hinge\u2013CH2\u2013CH3 spacer of 229 amino acids, and a 4-1BB costimulatory domain (chosen for clinical track record and lower cytokine release syndrome risk). It reports predicted molecular weight of 48.7 kDa and isoelectric point pI = 8.4. It identifies potential MHC class II\u2013binding epitopes via computational epitope prediction, and recommends germline humanization to reduce immunogenicity.",
      "source_document": "papers/2511.08649v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Section 3.2.2 describing the Toxicity Prediction Agent, what specific data sources and modalities does the paper say are integrated for safety assessment, and what are the key components included in the agent\u2019s output (i.e., what does it produce beyond a simple toxicity label)?",
      "answer": "The Toxicity Prediction Agent integrates multiple modalities: (1) normal-tissue transcriptomics from GTEx (54 tissue types, 17,382 samples) and (2) protein immunohistochemistry from the Human Protein Atlas for tissue expression analysis; (3) pharmacovigilance data mining of the FDA FAERS database to identify reported toxicities; (4) literature-based reasoning via semantic analysis of 50 million PubMed abstracts; and (5) mechanistic toxicity modeling using pathway databases and protein interaction networks. Its output is a comprehensive toxicity risk profile that includes severity scoring, mechanistic hypotheses, supporting evidence, and actionable mitigation strategies (not just a binary/label prediction).",
      "source_document": "papers/2511.08649v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Methods, the authors explain why they cannot directly apply Fisher\u2019s combination test to the raw split conformal p-values when aggregating read-level evidence into a site-level p-value. What is the specific issue they cite, and what adjustment do they apply before using Fisher\u2019s test (and then BH across sites)?",
      "answer": "They note that split conformal p-values depend on (share) the calibration set, so in a multiple-testing/combination setting they are not always valid\u2014specifically Fisher\u2019s combination test becomes invalid unless the p-values are adjusted. They therefore adjust the split conformal p-values to obtain calibration-conditional conformal p-values (calibration-conditional adjustment) before combining them with Fisher\u2019s test, and then apply Benjamini\u2013Hochberg across sites.",
      "source_document": "papers/2511.08855v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Methods\u2019 feature-extraction section, the authors explain why they preprocess nanopore ionic current paths before computing signature features. What two preprocessing steps do they apply to the current time series to make the signature transform injective, and what truncation orders of the signature do they report using for (i) rRNA/mRNA analyses versus (ii) the DENV analyses?",
      "answer": "They preprocess the ionic current by (1) adding a monotonically increasing (time) coordinate and (2) applying the invisibility transform, which they state ensures the signature transform is injective (distinct signals yield distinct signatures). For truncation, they use signature level m = 3 for the rRNA and mRNA results, and level m = 4 for the DENV results.",
      "source_document": "papers/2511.08855v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Methods section on differential analysis between conditions, what distribution do the authors derive for the number (or rate) of \u201canomalous\u201d reads at a site when thresholding conformal p-values at a fixed level, and what resampling-based hypothesis test do they use to compare anomaly rates between two samples (e.g., ctr2 vs METTL3 knockout)?",
      "answer": "They show that the anomaly count (number of conformal p-values below the chosen threshold, out of n native reads, accounting for finite calibration size m) follows a Beta\u2013Binomial distribution, with parameters determined by n, m and the threshold (via the order-statistics/quantile mapping). To compare anomaly rates between two conditions they use a parametric bootstrap test for two proportions under this Beta\u2013Binomial model (a one-sided two-proportion test, used in the paper for ctr2 vs METTL3 knockout).",
      "source_document": "papers/2511.08855v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Methods description of how nearest-neighbor (NN) anomaly scores are computed from signature features, how do the authors make the distance site-specific, and what linear-algebra operation do they use to construct the metric (include what property the whitening is intended to enforce on the transformed IVT features)?",
      "answer": "They build a separate metric for each genomic site by using only IVT (unmodified) signature vectors from that exact position and computing a whitening transform specific to that site. Concretely, they perform an SVD of the centered IVT feature matrix at that site and use the singular values/eigenvectors to form a whitening matrix W that rescales components so that the whitened data have mean zero and identity covariance; NN scores are then computed as a site-specific Mahalanobis/whitened Euclidean nearest-neighbor distance in that metric.",
      "source_document": "papers/2511.08855v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the discussion of limitations and in the Methods, the authors justify using Uncalled4 in *signal-to-reference* alignment mode rather than aligning signals to basecalled reads. What two advantages do they state for signal-to-reference alignment in their pipeline, and what downside do they acknowledge (and what alternative alignment mode do they propose could mitigate it)?",
      "answer": "They state that signal-to-reference alignment (i) provides a shared coordinate system so experimental/native reads and IVT control reads can be aligned and compared at the same genomic/transcriptomic positions, and (ii) makes the pipeline independent of basecalling errors that could otherwise confound anomaly-score calculations. They acknowledge the downside that reference-based alignment can introduce errors when there are genetic variations/mismatches between the samples and the reference sequence; they suggest signal-to-read alignment as an alternative that could mitigate such reference-induced alignment errors.",
      "source_document": "papers/2511.08855v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the METHODS section describing the target-centric model KG\u2011MTL, what are the three major modules of the model, and what specific neural architectures are used in each module to represent (a) knowledge-graph entities, (b) protein sequences, and (c) drug molecular graphs?",
      "answer": "KG\u2011MTL consists of three major modules: (i) a DTI module, (ii) a CPI module, and (iii) a Shared Unit. In the DTI module, KG\u2011MTL uses a 3\u2011layer RGCN to extract semantic relations/topology from the DRKG subgraph and then uses an MLP to compute DTI probability from the learned drug/target embeddings. In the CPI module, it uses a CNN to learn protein sequence representations and a GCN to learn drug molecular graph representations. The Shared Unit combines (shares) task\u2011independent drug features by integrating the compound\u2019s molecular-graph representation with the corresponding drug-entity embedding from the KG via trainable weights (Wdd, Wdg, Wgg, Wgd) and linear transformations.",
      "source_document": "papers/2511.08921v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "DeepDR provides different interpretability outputs depending on whether the selected model is network-based or knowledge-graph-based. According to the paper\u2019s \u201cResult analysis and visualization\u201d description, what specific explanatory information does DeepDR present for (i) network-based models and (ii) knowledge graph-based models, and how can the user interact with the latter to inspect the explanation?",
      "answer": "(i) For network-based models, DeepDR shows the most relevant top-20 drugs across five relationship categories: therapeutic, chemical, Gene Ontology (GO) biological process, GO cellular component, and GO molecular function. (ii) For knowledge graph-based models, DeepDR provides a network visualization of the paths between the predicted drug and the queried gene or disease; the user can interact with this network by clicking on a node or an edge to view the corresponding entity or relationship type it represents.",
      "source_document": "papers/2511.08921v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the METHODS \u201cOverview and Guidance\u201d sections, the authors state that RotatE is used as an *unsupervised* knowledge-graph embedding method in both DisKGE (disease-centric) and TarKGE (target-centric). According to the text, what is the RotatE triple-distance (scoring) function they use, and how do DisKGE vs. TarKGE use this distance to produce candidate drug lists (i.e., what entity pairs are ranked for each task, and what extra selection step is described for TarKGE)?",
      "answer": "RotatE maps entities/relations into a complex vector space and expects t = h \u25e6 r for a KG triple (h,r,t), with \u25e6 the Hadamard product and |r_i| = 1. The triple distance is:\n\ndr(h,t) = || h \u25e6 r \u2212 t ||.\n\nDisKGE uses this learned embedding space on DRKG to compute distances between drugs and diseases and ranks drugs as candidate treatments for each disease.\n\nTarKGE likewise learns embeddings on DRKG but computes distances (via a score function) between drugs and targets to rank candidate drugs for each target, and then explicitly selects the top 20 drugs for every target in the KG.",
      "source_document": "papers/2511.08921v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the METHODS section describing the target-centric model AOPEDF, how is the arbitrary-order proximity matrix S defined from the adjacency matrix M, and what matrix-factorization objective does AOPEDF minimize to obtain low-dimensional embeddings (include the variables used in the objective and the norm) before training the deep forest classifier?",
      "answer": "AOPEDF defines the high-order proximity as a polynomial of the adjacency matrix: \nS = F(M) = w1 M + w2 M^2 + \u00b7\u00b7\u00b7 + wl M^l (with weights w1,\u2026,wl, order l, and wi > 0; l can go to +\u221e if the sum converges).\nTo preserve this proximity in a low-dimensional space, it uses matrix factorization by minimizing:\nmin_{U* , V*} || S \u2212 U* (V*)^T ||_F^2,\nwhere U*, V* \u2208 R^{n\u00d7fs} are context embedding matrices, n is the number of nodes, fs is the embedding dimensionality, and ||\u00b7||_F is the Frobenius norm.",
      "source_document": "papers/2511.08921v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In ScNucAdapt\u2019s partial domain adaptation procedure, after dynamically clustering the unlabeled target data, how does the method decide which labeled *source* subset corresponds to each predicted *target* cluster when forming the CS-divergence alignment loss L_cs (Eq. 12)? State the matching rule and how it is used to construct L_cs.",
      "answer": "For each predicted target cluster X_{j,t}, ScNucAdapt computes the Cauchy\u2013Schwarz (CS) divergence between that target cluster and every labeled source subset X_{i,s}. It then matches the target cluster to the source subset with the *lowest* CS divergence (denoted a_i for the i-th target cluster). The CS alignment loss is constructed by summing the CS divergences over all target clusters using these best-matching source subsets: L_cs = \\sum_{i=1}^{\\hat C} D_CS(X_{a_i,s}, X_{i,t}) (Eq. 12).",
      "source_document": "papers/2511.08996v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s comparison to the thermodynamic uncertainty relation (TUR) for stationary systems, what specific limitation of the TUR analysis do the authors highlight regarding their displacement-based statistics, and what is the corresponding \u201cdegenerate\u201d partitioning of their method that makes it directly comparable to the TUR bound?",
      "answer": "They point out that the TUR bound based on the mean and variance of the net displacement \\(\\ell\\) does not exploit *where* in space displacements occur (it ignores spatial heterogeneity/localization of dissipation). The directly comparable, degenerate version of their method is the partitioning in which **all displacements are assigned to a single class** (\u201cclass \\(I\\)\u201d), i.e., no spatial categorization of displacements.",
      "source_document": "papers/2511.09183v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Methods section, the authors note that FK steering\u2019s reward is formally deterministic, but their evaluation introduces stochasticity via ProteinMPNN sequence sampling and PyRosetta relaxation. What conditional distribution do they define for these refinement steps, and what two aggregations do they propose to obtain a scalar reward from multiple independent refinement realizations for the same denoised backbone?",
      "answer": "They model the refinement pipeline (ProteinMPNN + PyRosetta) as a conditional distribution over refined sequence\u2013structure pairs, written as (x\u03030, s) ~ P_ref(\u00b7 | x0) (equivalently, each realization (x\u03030^(i), s_t^(i)) ~ P_ref(\u00b7 | x0)). To aggregate multiple independent realizations into a scalar reward, they propose either the mean across samples r_mean(x0) = (1/n) \u03a3_i r(x\u03030^(i), s_t^(i)) or the maximum across samples r_max(x0) = max_i r(x\u03030^(i), s_t^(i)).",
      "source_document": "papers/2511.09216v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Methods (\u201cFeynman\u2013Kac implementation\u201d), what exact numerical-stability procedure do the authors apply before exponentiating rewards to compute the FK potentials/weights (i.e., what value is subtracted, and what clipping bound is used)?",
      "answer": "Before exponentiation, they subtract the maximum reward value (rmax) and then clip the shifted rewards with a lower bound of \u221210^3, i.e., they exponentiate after applying (rt \u2212 rmax) and enforcing (rt \u2212 rmax \u2265 \u221210^3).",
      "source_document": "papers/2511.09216v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Methods section (\u201cDocking and scoring\u201d), what *exact* Boltz-2/BoPep docking settings do the authors use to evaluate FK-steered designs (i.e., how many recycling steps and how many diffusion samples per model), and what chain is used as the docking template?",
      "answer": "They dock each designed peptide to the target using BoPep/Boltz-2 with **ten recycling steps** and **five diffusion samples per model**, using the **target structure as the template for the target chain**.",
      "source_document": "papers/2511.09216v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Methods section, how do the authors compute the secondary-structure steering reward (rSS): (i) what two information sources are combined to estimate helix/\u03b2-sheet/loop fractions, and what fixed weights are used for each source; and (ii) when steering toward a specific secondary-structure class, how are the term weights (w\u03b1, w\u03b2, w\u2113) adjusted relative to the others?",
      "answer": "They estimate secondary-structure composition by combining (1) DSSP assignments computed on the refined backbone (geometry-based) and (2) residue-based secondary-structure propensities derived from the designed sequence. These are combined with fixed weights of 0.8 for DSSP and 0.2 for the sequence-based estimate to produce the helix, \u03b2-sheet, and loop fractions (\u03b1, \u03b2, \u2113). The reward penalizes deviation from user targets via rSS = w\u03b1(1\u2212|\u03b1\u2212\u03b1*|)+w\u03b2(1\u2212|\u03b2\u2212\u03b2*|)+w\u2113(1\u2212|\u2113\u2212\u2113*|); when steering toward a specific secondary-structure type, the corresponding term weight is set fourfold (4\u00d7) larger than the other two.",
      "source_document": "papers/2511.09216v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Methods (\u201cSequence recovery and packing\u201d), how do the authors turn a denoised backbone x0 into a refined all-atom, sequence-specific structure for reward evaluation? Specify: (i) the conditional distribution they sample sequences from (including the sampling temperature and which ProteinMPNN weights are used), and (ii) the two-stage PyRosetta refinement procedure used to obtain the refined coordinates \\u02dcx0 (what is sampled discretely and what is minimized continuously).",
      "answer": "(i) They sample sequences from ProteinMPNN\u2019s conditional distribution \\(P_{\\mathrm{MPNN}}(s\\mid x_0)\\), sampling at temperature 0.2 using solubility-optimized model weights, then thread each sampled sequence onto the backbone to build an all-atom model.\n\n(ii) They refine to \\(\\tilde{x}_0\\) by (a) discrete side-chain rotamer sampling (packing), followed by (b) continuous minimization of side-chain torsional angles \\(\\chi_1, \\chi_2, \\ldots\\) under the Rosetta all-atom energy function to resolve clashes and optimize side-chain poses; together these steps define the refinement distribution \\(P_{\\mathrm{ref}}(\\cdot\\mid x_0)\\).",
      "source_document": "papers/2511.09216v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In GIF\u2019s intensity-prediction step, after the model outputs a JSON list of (m/z, intensity) peaks, what specific criteria does the paper use to post-process/clean this response before running the single refinement query, and what information is merged to form the refined prompt?",
      "answer": "The initial intensity-prediction response is post-processed by removing any peak entries where (i) the returned m/z is not one of the m/z values included in the query, (ii) the returned m/z is a duplicate, or (iii) the intensity value is invalid. The remaining intensity values are then merged with the previously queried list of generated fragments and their corresponding m/z values; this merged fragment\u2013m/z\u2013intensity information (along with the query molecule and experiment settings) is used to create the refined prompt for one iterative refinement step.",
      "source_document": "papers/2511.09571v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s supervised fine-tuning setup for GIF\u2019s intensity-prediction step, how do the authors implement curriculum learning over the training set\u2014i.e., what is their definition of example \u201ccomplexity,\u201d and how is it used to order the data during training?",
      "answer": "They sort (order) the training examples by increasing complexity to create a curriculum-learning schedule. Complexity is defined as the sum of (i) the normalized number of tokens in the prompt and (ii) the normalized number of fragments, where each of these two quantities is divided by its respective maximum value before summing.",
      "source_document": "papers/2511.09571v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In GIF\u2019s fragment-generation step, what two validity checks does the framework apply to each LLM-generated fragment before using the surviving fragments to build the next refinement prompt, and how many total fragment-generation refinement iterations does the paper run before producing the final fragment list?",
      "answer": "Each generated fragment is filtered (1) by whether it can be converted to a valid RDKit Mol object, and (2) by whether that Mol corresponds to a valid substructure of the query molecule (checked with RDKit). This fragment-generation prompting/refinement loop is run for 5 iterations; the output of the 5th query is filtered one final time to form the final fragment list.",
      "source_document": "papers/2511.09571v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the ablation discussion of GIF\u2019s intensity-prediction step, what specific failure mode do the authors report for models *without* fine-tuning (i.e., what do the responses fail to contain), and what design choice do they make about the number of intensity-prediction iterative-refinement steps as a result?",
      "answer": "They report that without fine-tuning the model often fails to include the exact queried m/z values in its response. Consequently, they limit intensity-prediction iterative refinement to a single step, noting that additional refinement steps lowered performance.",
      "source_document": "papers/2511.09571v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s benchmark comparison on the full MassSpecGym test set, which baseline method is described as the state-of-the-art and what two key methodological steps does the paper say it uses (i.e., how it generates fragments and how it assigns likelihoods/scores)?",
      "answer": "FraGNNet is identified as the state-of-the-art baseline. The paper describes it as (1) first performing combinatorial fragmentation to generate candidate fragments, and then (2) using a graph neural network (GNN) to learn a probability distribution over the molecule fragments (i.e., assign likelihoods/scores).",
      "source_document": "papers/2511.09571v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s uncertainty-quantification scheme, how is epistemic uncertainty computed from an ensemble of probabilistic networks\u2014give the exact definition of the \u201cdisagreement score,\u201d including (i) what divergence is averaged, (ii) how it is normalized, and (iii) the stated numerical range and baseline behavior on random noise?",
      "answer": "Epistemic uncertainty is quantified via a per-pixel \u201cdisagreement score\u201d computed from an ensemble of five independently trained probabilistic networks. For each pixel, they compute the average pairwise Kullback\u2013Leibler (KL) divergence among the predicted distributions and normalize it by the log of the number of models: mean(KL)/log(5). The score is defined to range from 0 (perfect agreement) to 1 (maximum disagreement), and as a baseline they report that random noise yields a disagreement score of 1 when evaluated on five sets of 100 image pairs.",
      "source_document": "papers/2511.09574v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Methods/Results description of spatial resolution (Fourier-based analysis), how do the authors turn an image into a single \u201cresolution\u201d number\u2014specifically: (i) what transformation is computed, (ii) how is the resulting spectrum reduced to a 1D profile, (iii) what exact threshold defines the cutoff, and (iv) how is that cutoff converted into a real-space resolution?",
      "answer": "(i) They compute the 2D Fast Fourier Transform (FFT) of each image and take its power spectrum. (ii) They reduce the 2D power spectrum to a 1D radial profile by azimuthally averaging power over increasing spatial-frequency radii. (iii) The cutoff is the spatial frequency where the logarithm of the radial power spectrum drops below a fixed threshold of 0.1 (interpreted as the transition from signal-dominated to noise-dominated frequencies). (iv) That cutoff frequency is converted to a real-space resolution (\u00b5m) by normalizing using the pixel size and the field of view.",
      "source_document": "papers/2511.09574v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the HAMscope deep-learning pipeline, the authors describe a pix2pix-inspired GAN training objective but note that the discriminator is often disabled. What is the composite loss used to train the generator (name the two terms), and what exact scaling factor do they apply to the discriminator\u2019s adversarial loss to keep it subordinate to the pixel-fidelity term (and what is the default setting for the discriminator-loss term in most experiments)?",
      "answer": "They use a composite objective consisting of (i) the discriminator\u2019s adversarial loss (to encourage realistic outputs) and (ii) an L1 loss (to enforce pixel-level fidelity). The adversarial/discriminator loss is scaled by 0.005 so it remains subordinate to the L1 term, and in most experiments the discriminator-loss term is set to 0 (i.e., the discriminator is deactivated unless otherwise specified).",
      "source_document": "papers/2511.09574v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the \u201cResolution\u201d and deconvolution methodology, how do the authors build the space-variant Wiener deconvolution PSFs, and what specific calibration scan and processing steps are used to ensure the PSFs capture *network-induced* aberrations (include: what is scanned/how many points, what model processes the calibration image, and what the output PSF stack represents)?",
      "answer": "They generate PSFs from a single calibration scan consisting of 100 randomly illuminated pixels. This calibration image is then run through the registration-trained single U\u2011Net, and the network\u2019s output is treated as a 30\u2011channel hyperspectral PSF stack. That stack provides a dense map of PSFs across all spectral channels that explicitly captures aberrations introduced by the reconstruction network, and these PSFs are then used for the calibrated space-variant Wiener deconvolution.",
      "source_document": "papers/2511.09574v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Methods section describing automated dataset acquisition, what is the exact sequencing and count of frames captured per acquisition cycle (distinguishing \u201cground-truth\u201d frames versus the miniscope frame), and what two hardware components does LabVIEW explicitly control/synchronize to implement this cycle?",
      "answer": "Each acquisition cycle captures six sequential ground-truth images followed by one miniscope image. LabVIEW controls/synchronizes (i) the motorized variable bandpass filter (for stepping through ground-truth spectral slices) and (ii) the dichroic mirror (triggering mirror movements before and after each miniscope capture) while coordinating triggers for the cycle.",
      "source_document": "papers/2511.09574v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s quality-control/target-creation stage, the authors report discovering direct target leakage that inflated performance. What specific field caused this leakage, what (inflated) accuracy did it produce, and what hierarchical fallback procedure did they use to create the multiclass targets once the leakage fields were removed (name the evidence sources in order)?",
      "answer": "The leakage was caused by the clinical significance field CLIN_SIG appearing both as an input feature and as part of the target definition, which inflated accuracy to 98%. After removing target-informative fields (reducing features from 57 to 56), they created the {Benign, VUS, Pathogenic} targets hierarchically: (1) use CLIN_SIG when available; otherwise (2) fall back to AlphaMissense class predictions; otherwise (3) fall back to VEP IMPACT severity rules.",
      "source_document": "papers/2511.09576v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s evaluation framework, what specific \u201cType 2 circularity\u201d risk do the authors say they prevent with their splitting procedure, and what exact constraint do they enforce on the data partitions to prevent it (describe the unit of overlap they disallow)?",
      "answer": "They aim to prevent Type 2 circularity arising from the same variant appearing in multiple dataset partitions, which would leak information and inflate performance. To prevent this, they enforce that there is no overlap of the variant key across train/validation/test splits\u2014i.e., the same (chr, pos, ref, alt) unique variant identifier cannot appear in more than one partition (implemented via stratified sampling with fixed random seeds).",
      "source_document": "papers/2511.09576v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s VEP annotation-and-correction pipeline, how do the authors resolve cases where VEP has concatenated multiple transcript-level values into a single ampersand-delimited record? Describe (i) how they detect and reconstruct the transcript records and (ii) the exact fixed priority order they use to choose the single representative transcript/value, including what they do with the residual (non-selected) evidence.",
      "answer": "They detect VEP\u2019s multi-transcript concatenation by finding ampersand-concatenated values in affected fields, then reconstruct per-transcript records from those concatenated entries. To select one representative transcript/value, they apply a fixed clinical priority: (1) MANE/canonical transcript, then (2) highest IMPACT/Consequence severity, then (3) presence of known variant IDs such as rsID/COSMIC/ClinVar. Any remaining evidence from non-selected transcripts is not discarded; it is summarized into counts/flags (e.g., number of DOMAINS and distinct VAR SYNONYMS), and corrections are logged.",
      "source_document": "papers/2511.09576v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "The authors note a methodological limitation in their benchmark\u2019s *hierarchical labeling strategy* that creates a potential dependency between labels and inputs for a subset of variants. What is this dependency, which tier/label source and tier/feature source does it involve, and what fraction of the dataset is affected according to their label-source breakdown?",
      "answer": "They acknowledge that AlphaMissense is used both as a label source and as an input feature source: in their three-tier hierarchical labeling, AlphaMissense predictions supply labels for 43.3% of variants, while AlphaMissense scores/classes are also included as model inputs in Tier 3 (AlphaMissense features). Thus, for the subset labeled via AlphaMissense (43.3% of variants), there is coupling/dependency between labels and features.",
      "source_document": "papers/2511.09576v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s attention/interpretability analysis of the trained TabNet model, the authors claim that the VEP-concatenation correction materially influences the model\u2019s reasoning. What quantitative evidence do they provide for this claim in terms of (i) the share of total attention allocated to VEP-corrected features and (ii) the specific top-ranked VEP-corrected feature and its reported importance percentage?",
      "answer": "They report that 51.1% of the model\u2019s attention is allocated to the VEP-corrected feature tier, and the top-ranked VEP-corrected feature is VAR SYNONYMS (variant identifiers) with 20.3% importance.",
      "source_document": "papers/2511.09576v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s derivation of the *practical upper bound* for the listwise preference objective, the authors cite two main sources of intractability and then apply two corresponding approximations to obtain a tractable training loss for flow-matching models. What are these two intractabilities, and what specific approximations does EPO use to address each one?",
      "answer": "The derivation identifies: (1) the Plackett\u2013Luce listwise choice probability term as hard to optimize directly; EPO uses the convexity of the log-sum-exp/choice-probability form and applies Jensen\u2019s inequality to obtain a tractable upper bound. (2) reverse-time transition probabilities needed for trajectory rewards (sampling from the reverse-time transition distribution p\u03b8(yt\u22121,t|y0)) as infeasible during training; EPO approximates this intractable reverse transition with the corresponding forward-process probability, and then further simplifies the resulting term using an MSE proxy between predicted and reference trajectories.",
      "source_document": "papers/2511.10165v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the experimental setup for EPO\u2019s online refinement, what specific physics-based energy model do the authors use to produce the energy rewards, and what implementation detail do they give about it (e.g., framework)?",
      "answer": "They use Madrax (Orlando et al. 2024) as the physics-based energy reward, described as a differentiable empirical force field implemented in PyTorch.",
      "source_document": "papers/2511.10165v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the ablation study section, what failure mode do the authors report when using too few denoising steps in the SDE sampler during online refinement, and what trade-off do they describe when increasing the number of denoising steps beyond an adequate level?",
      "answer": "They report that using insufficient denoising steps leads to unstable training and mode collapse because the reverse trajectory fails to generate valid conformations. Increasing the number of denoising steps beyond a certain point yields diminishing returns: it substantially raises computational cost without a corresponding improvement in sample quality or model performance, so the steps must be balanced between stability and tractability.",
      "source_document": "papers/2511.10165v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Experimental Settings section, what specific hyperparameter values do the authors choose for (i) \u03b2 and (ii) the SDE score norm for the Tetrapeptides experiments versus the ATLAS experiments, and what do these choices imply about how strongly the preference loss is scaled across the two datasets?",
      "answer": "They set \u03b2=1 with SDE score norm 0.01 for Tetrapeptides, and \u03b2=250 with SDE score norm 0.0001 for ATLAS. This indicates the preference objective is scaled much more strongly (larger \u03b2) on ATLAS than on Tetrapeptides, while ATLAS uses substantially lower SDE stochasticity (smaller score norm).",
      "source_document": "papers/2511.10165v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Fast-Folding discussion, the authors argue that EPO\u2019s advantage over Str2Str and ConfDiff comes from \u201conline guidance and online exploration.\u201d According to the paper, what specific limitation of Str2Str does EPO address, and what specific limitation of ConfDiff does EPO avoid?",
      "answer": "Str2Str uses online stochastic perturbations to improve diversity but lacks explicit physical guidance from an energy/force signal; EPO adds direct physics-based (energy) guidance. ConfDiff fine-tunes on a static, offline dataset with pre-computed energy/force labels, which limits its ability to dynamically adapt to distributional shifts during training; EPO instead applies physical guidance dynamically in real time during online sampling/refinement.",
      "source_document": "papers/2511.10165v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In MOSAIC, how is the %MinMax window size chosen and what biological/physical rationale is given for that specific value? State the parameter name, its numeric value, and the ribosome-related justification as described in the paper.",
      "answer": "The %MinMax window size is denoted as z, and the authors set z = 10. They justify this by noting that a translating ribosome covers about 28\u201330 nucleotides, i.e., approximately 10 codons, so a 10-codon window appropriately reflects translation efficiency/elongation context while retaining codon-specific resolution.",
      "source_document": "papers/2511.10708v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Methods, the authors state they made a deliberate design choice in the *baseline* EHR models used for the clinical utility (lift) comparison. What feature domains did they intentionally include and exclude, and what was the purpose of this restriction?",
      "answer": "They intentionally used only condition-based (diagnosis) features and excluded demographic variables (and, for this baseline setup, other EHR domains such as medications/procedures) so that the lift comparison would isolate the predictive signal contained in structured clinical condition history alone\u2014i.e., to assess EHR-derived condition information independent of demographics and other risk-factor-like covariates.",
      "source_document": "papers/2511.11293v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Statistical analysis for the clinical-utility comparison, what hypothesis test did the authors use to compare lift between the EHR-based model and traditional risk-factor stratification, what was the direction of the alternative hypothesis, and how should that direction be interpreted in terms of the claim they were testing?",
      "answer": "They used a one-sided Mann\u2013Whitney U test with alternative = \u201cless\u201d. This direction means they were testing whether lift values from the traditional risk-factor approach were statistically lower than those from the EHR-based model\u2014i.e., whether the EHR model achieved significantly higher lift (better enrichment) than the risk-factor stratification.",
      "source_document": "papers/2511.11293v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Methods section describing the EHR foundation models, the authors explain why simply adding laboratory/observation domains to CEHR-GPT yielded only marginal gains and could even degrade performance. What explanation do they give for this (in terms of information redundancy and sequence construction), and what specific filtering strategy do they cite from MOTOR as an example of handling this issue (including the approximate vocabulary reduction reported)?",
      "answer": "They argue that lab measurement and observation information is largely correlated with the condition/procedure/drug domains already used by CEHR-GPT, so naively appending all those additional data points to the patient sequence can add non-informative/redundant tokens and can degrade performance rather than help. As an example of mitigating this, they note that MOTOR uses entropy filtering to remove non-informative codes, shrinking its vocabulary from roughly ~50,000 concepts to ~9,000.",
      "source_document": "papers/2511.11293v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "The authors propose a post hoc procedure for choosing the \u201cflagging threshold\u201d (population coverage) at which an EHR-based model would define a high-risk group for screening. Describe this procedure as stated in the Discussion: (i) how they define an initial candidate threshold range using lift (include the pancreatic-cancer example values), and (ii) what specific quantities they plan to estimate in counterfactual retrospective analyses within that range to pick the target coverage.",
      "answer": "(i) They first use lift relative to traditional risk factors to define a candidate coverage (threshold) range. In pancreatic cancer, they take the best lift achieved by a known risk factor\u2014family history of pancreatic cancer\u2014reported as 4.10 at 2% coverage, and then look for the maximum coverage at which the EHR model\u2019s lift remains \u22654.10. This yields a candidate coverage range from 2% to some upper bound X% (where lift is still at least 4.10), acknowledging that lift typically decreases as coverage increases.\n\n(ii) Within that candidate range, they plan counterfactual retrospective analyses to quantify: the number of patients flagged; the number and timing of true cancers detected; false positive rates; the estimated number of screening tests and downstream workups; associated clinical and resource implications; and potential stage shifts at each threshold, to select the target coverage (flagging threshold) for each cancer type.",
      "source_document": "papers/2511.11293v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Methods section describing cancer cohort identification, how did the authors define the index date for controls (and why), and how was the 3-year prediction index derived from the 1-year index for both cases and controls?",
      "answer": "Controls were assigned an index date 24 months before their last recorded medical condition to reduce the chance that controls included individuals with undiagnosed cancer (a strategy used in prior studies). For the 3-year prediction index, they subtracted two additional years from the 1-year prediction index date for both cases and controls (i.e., shifting the index earlier by 2 years relative to the 1-year index).",
      "source_document": "papers/2511.11293v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In nnUNet-B\u2019s Multimodal Posterior Sampling (MPS) training scheme, how many learning-rate cycles and total epochs are used, and during which part of each cycle are checkpoints sampled to form the posterior ensemble for uncertainty estimation (be specific about the epoch window/phase)?",
      "answer": "Training uses 3 cyclic learning-rate cycles over 1200 total epochs (Tc = 400 epochs per cycle). Checkpoints are sampled late in each cycle\u2014during the low-learning-rate phase\u2014specifically during the final 20 epochs of each cycle (described as sampling from the last part of the cycle / low-LR phase).",
      "source_document": "papers/2511.11486v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s uncertainty calibration evaluation, how is the Uncertainty Calibration Error (UCE) computed from binned pixel uncertainties\u2014i.e., what are the per-bin quantities compared, and how are bins weighted in the final UCE formula?",
      "answer": "UCE is computed by binning pixel-wise uncertainty values into B bins and, for each bin b, computing (1) the average predicted uncertainty \\u0304u_b of pixels in that bin and (2) the empirical error rate \\u0304e_b of those pixels. The final UCE sums the absolute difference |\\u0304u_b \u2212 \\u0304e_b| across bins weighted by the bin\u2019s fraction of pixels: UCE = \\n\\n\\u00a0\\u00a0\\u2211_{b=1..B} (|S_b| / \\u2211_{j=1..B} |S_j|) \u00b7 |\\u0304u_b \u2212 \\u0304e_b|,\\n\\nwhere S_b is the set of pixels in bin b.",
      "source_document": "papers/2511.11486v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In nnUNet-B\u2019s inference procedure, how are (i) the final segmentation mask and (ii) the two pixel-wise uncertainty maps computed from the ensemble of N checkpointed models? Give the explicit aggregation steps and the formulas used for standard deviation and entropy as defined in the paper.",
      "answer": "(i) Each sampled model Mi produces a per-pixel softmax probability map Pi(x) over C classes. These are averaged across the N models to form the mean probability map: \\(\\bar P(x)=\\frac{1}{N}\\sum_{i=1}^N P_i(x)\\). The predicted segmentation mask is then computed voxel/pixel-wise by taking the argmax over classes of the averaged probabilities: \\(\\hat y = \\arg\\max_c \\bar P_c(x)\\).\n\n(ii) Two pixel-wise uncertainty measures are computed over the ensemble: the standard deviation and the entropy of the averaged distribution. The per-pixel standard deviation is \\(\\sigma(x)=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N (P_i(x)-\\bar P(x))^2}\\). The per-pixel entropy is computed from the averaged probabilities as \\(H(x) = -\\sum_{c=1}^C \\bar P_c(x)\\log \\bar P_c(x)\\).",
      "source_document": "papers/2511.11486v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the experiments section, the authors define two evaluation configurations (Setup I: noisy; Setup II: refined). For each setup, what Grassmann representation Gr(n, p) (i.e., the embedding dimension n and subspace rank p) do they use for (i) most datasets and (ii) the smallest dataset GSE57249, and what downstream clustering algorithm is paired with each setup?",
      "answer": "Setup I (noisy): after PCA to 200 dims, embeddings are n = 100. For most datasets they sample nscales = 25 giving p = 23, so Gr(100, 23); for GSE57249 they use nscales = 13 giving p = 12, so Gr(100, 12). Downstream algorithm: spectral clustering.\n\nSetup II (refined): for small/medium datasets they embed to n = 20 with p = 10, so Gr(20, 10); for larger datasets they use n = 50 with p = 19, so Gr(50, 19); for GSE57249 they use n = 15 with p = 8, so Gr(15, 8). Downstream algorithm: k-means clustering.",
      "source_document": "papers/2511.11717v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s \u201cExpert Agreement Metrics\u201d evaluation for comparing GEOBPE-derived segments to CATH FunFam domain annotations, how do the authors avoid unfairly penalizing the fact that predicted segments are typically shorter than curated domains, and what optimization criterion do they use to choose the predicted segments associated with a given true domain before computing per-domain scores?",
      "answer": "They do not perform a naive one-to-one segment\u2013domain match. Instead, for each true domain Di they first select a *single best consecutive block* of predicted segments Si (a contiguous span of segments Pm..Pn) and then compute domain-level scores on that block. The block is chosen by maximizing the Intersection-over-Union (IoU) between the domain and the union of that consecutive block: (ai,bi) \u2208 arg max_{1\u2264m\u2264n\u2264M} |Di \u2229 (\u22c3_{k=m}^n Pk)| / |Di \u222a (\u22c3_{k=m}^n Pk)|, with ties optionally preferring the shortest block/fewest segments.",
      "source_document": "papers/2511.11758v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Algorithm 9 (one GEOBPE merge iteration), how does GEOBPE prioritize which Geo-Pair key \u03ba to merge next, and what does each component of the priority tuple \u03c0(\u03ba) represent (including how \u03c1(\u03ba) is defined and used)?",
      "answer": "GEOBPE keeps an ordered map D from Geo-Pair keys \u03ba to their occurrence sets O(\u03ba), and selects the next key by taking FRONT(D), i.e., the smallest key under the ordering \u03c0(\u03ba) = (\u03c1(\u03ba), \u2212|O(\u03ba)|, \u03ba). Here \u03c1(\u03ba) = 1[\u03ba \u2209 dom(V)] indicates whether \u03ba has no prototypes yet in the current vocabulary V (\u03c1=1) or already has prototypes (\u03c1=0), so keys with existing prototypes are prioritized first. The second component \u2212|O(\u03ba)| prioritizes higher-frequency pairs (larger |O(\u03ba)| gives more negative value, hence earlier). The final component \u03ba is a deterministic tie-breaker by key value.",
      "source_document": "papers/2511.11758v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the SSLM-Eval setup (Appendix E.6), what specific architecture and training/configuration choices define the \u201csmall autoregressive Transformer\u201d used to model GEOBPE token sequences (include model size/depth, attention/FFN dimensions, embedding/output tying, padding strategy, and the early-stopping criterion)?",
      "answer": "SSLM-Eval trains a decoder-only (autoregressive) Transformer on GEOBPE\u2019s discretized geometry tokens with hidden size dmodel = 256, depth L = 8 layers, H = 8 attention heads, and feed-forward width dff = 1024, using GELU activations. Token and positional embeddings are summed; a LayerNorm is applied before the classifier; the output projection is weight-tied to the token embedding; and a causal attention mask enforces left-to-right prediction. Sequences are padded to a dataset-dependent maximum length set to the 95th percentile of training lengths (by default). Training optimizes cross-entropy with Adam (lr 1\u00d710\u22124), batch size 32, up to 100 epochs with early stopping based on validation perplexity.",
      "source_document": "papers/2511.11758v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Algorithm 14 (BUILDJOINTVOCAB), how is the final token dictionary \u03a3 constructed and indexed\u2014specifically, in what order are motif-medoid tokens and glue-angle bin tokens appended, and what are the exact index-offset formulas used to assign ids to the \u03b8, \u03c9, and \u03d5 glue bins relative to M = |\u03a3med|?",
      "answer": "\u03a3 is built by first appending all motif-medoid tokens \u03a3med in the GEOBPE key introduction order: for s = 1..S and for each key \u03ba(s), append each medoid \u27e8\u03ba(s), j\u27e9 for j = 1..K_{\u03ba(s)} and set idmed(\u03ba(s), j) to its position in \u03a3med. After all medoids, glue-bin tokens are appended after the medoids with offset M = |\u03a3med|, using: idbin(\u03b8, b) = M + b; idbin(\u03c9, b) = M + B\u03b8 + b; idbin(\u03d5, b) = M + B\u03b8 + B\u03c9 + b (with b indexing the bin center within that angle type). Finally \u03a3 = \u03a3med \u222a {all glue-bin tokens} (optionally BOS/EOS).",
      "source_document": "papers/2511.11758v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the \u201cPrinciples of Protein Structure Tokenization\u201d section, how do the authors formally define (i) the average distortion \\(\\Delta(T;D)\\) and (ii) the bits-per-residue objective \\(\\mathrm{BPR}(T;D)\\) for a tokenizer \\(T=(V,\\mathrm{Enc},\\mathrm{Dec})\\)? In your answer, specify what \\(L(T)\\), \\(L(\\mathrm{Enc}(x))\\), and \\(N(x)\\) represent, and state the simplification of \\(L(\\mathrm{Enc}(x))\\) under a uniform per-token code.",
      "answer": "They define distortion as the dataset-average reconstruction error under a distortion metric \\(d\\):\n\\[\\Delta(T;D)=\\frac{1}{|D|}\\sum_{x\\in D} d\\bigl(x,\\mathrm{Dec}(\\mathrm{Enc}(x))\\bigr).\\]\nThey define bits-per-residue as\n\\[\\mathrm{BPR}(T;D)=\\frac{L(T)+\\sum_{x\\in D} L(\\mathrm{Enc}(x))}{\\sum_{x\\in D} N(x)}\\quad\\text{(bits/res)}.\\]\nHere \\(L(T)\\ge 0\\) is the description length of the tokenizer itself \\((V,\\mathrm{Enc},\\mathrm{Dec})\\); \\(L(\\mathrm{Enc}(x))\\) is the description length of the token sequence produced for structure \\(x\\); and \\(N(x)\\) is the residue count (sequence length) of \\(x\\). Under a uniform per-token code, \\(L(\\mathrm{Enc}(x))=|\\mathrm{Enc}(x)|\\log_2|V|\\).",
      "source_document": "papers/2511.11758v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the ablation studies, the authors test (i) replacing chemically grounded similarity rewards with purely textual metrics (BLEU/METEOR) and (ii) removing the round-trip consistency objective. What quantitative effects do they report for each ablation\u2014specifically, how much does Morgan similarity drop on ChEBI-20 when chemical rewards are replaced, and how much does validity decrease on each of ChEBI-20, L+M-F, and Mol-Instruct-F when the round-trip objective is removed?",
      "answer": "They report that replacing chemical similarity rewards with BLEU/METEOR degrades chemical fidelity, with Morgan similarity dropping by about 2.3% on ChEBI-20. Removing the round-trip consistency objective reduces validity by approximately 4.1% on ChEBI-20, 3.2% on L+M-F, and 16.9% on Mol-Instruct-F.",
      "source_document": "papers/2511.12135v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In RTMol\u2019s coupled round-trip training, what dependency between the Captioner and Generator motivates training them \u201cin parallel,\u201d and what specific failure mode does the paper say this design choice is meant to prevent?",
      "answer": "The Captioner\u2019s learning depends directly on the Generator\u2019s feedback because the Generator serves as an evaluator of the Captioner\u2019s generated description by reconstructing a molecule and providing the reconstruction-based reward. The paper trains them in parallel so the Captioner learns from a stable, proficient evaluator, preventing degradation of the Generator\u2019s core text-to-molecule capability.",
      "source_document": "papers/2511.12135v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Appendix C, RTMol optimizes the Captioner with Group Relative Policy Optimization (GRPO). According to the paper, how is the per-completion advantage computed from the group rewards, and what are the key policy components appearing in the GRPO objective (Eq. 15) used for the clipped update (i.e., which policies are compared and what additional regularization term is included)?",
      "answer": "The advantage for completion i is computed by normalizing its reward within the sampled group: \nAi = (ri \u2212 mean{r1,\u2026,rG}) / std{r1,\u2026,rG} (Eq. 14).\nIn the GRPO objective (Eq. 15), the update uses the PPO-style clipped ratio between the current policy and the rollout policy, \u03c0\u03b8(yi,t|x,yi,<t) / \u03c0\u03b8old(yi,t|x,yi,<t), multiplied by Ai (with clipping parameter \u03f5), and it includes a KL regularization term with coefficient \u03b2 against a reference policy \u03c0ref (written as \u2212\u03b2 \u000eDKL[\u03c0ref || \u03c0r; x, yi, zt]).",
      "source_document": "papers/2511.12135v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In RTMol\u2019s training strategy, the authors explain why the final round-trip metric R(\u03b8,\u03d5) is not used as the sole reinforcement-learning reward early in training. What specific failure mode do they describe for an untrained Captioner, and what two reward components (beyond exact reconstruction) do they add to make the reward signal informative/smoother?",
      "answer": "They state that an initially untrained Captioner often generates syntactically invalid SMILES and captions that lead to a reconstruction score of zero, making the reward from using only the final round-trip metric sparse and uninformative (no meaningful gradient). To address this, they include (i) a validity-checking reward to enforce generating valid SMILES, and (ii) a similarity-based reward (fingerprint similarity) so the reward landscape is smoother/continuous even when an exact molecular match is not achieved.",
      "source_document": "papers/2511.12135v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the authors\u2019 alignment-accuracy comparison between LCPan and vg, what specific parameter choices did they make to ensure that node/segment lengths were comparable between the two graphs, and what average segment lengths did those choices yield for each graph?",
      "answer": "They set the vg graph\u2019s k-mer size to 64 and set LCPan\u2019s LCP level to 5 to make average node/segment lengths comparable. With these settings, the resulting graphs had average segment lengths of 56.25 bp for LCPan and 55.17 bp for vg.",
      "source_document": "papers/2511.12205v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the authors\u2019 alignment-accuracy evaluation (Section 2.4), what data and scoring procedure do they use as a proxy ground truth for alignment accuracy, and what breakpoint tolerance do they apply when computing precision/recall/F1?",
      "answer": "They use structural-variant (SV) call sets from the Genome in a Bottle project, generated with the pbsv tool, as a proxy for alignment accuracy. They compute precision, recall, and F1 using a \u00b1100 bp tolerance window around SV breakpoints (and also regenerate SV calls with pbsv and restrict variations to match the ground truth for fairness).",
      "source_document": "papers/2511.12205v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Methods, the authors use GradientSHAP to interpret the neural network and then define a \u201cblended score\u201d to rank genes for therapeutic target prioritization. Precisely (i) how is the blended score computed, including the value of the weighting parameter \u03b1 and how the probability and SHAP terms are normalized, and (ii) what specific GradientSHAP sampling design choices (number of empirical baselines, number of path samples, and batching strategy) do they state they used to obtain stable attributions over all features?",
      "answer": "(i) The blended score is defined as: **Blended Score = \u03b1 \u00d7 Pnorm + (1 \u2212 \u03b1) \u00d7 SHAPnorm**, with **\u03b1 = 0.7**. The normalized probability is **Pnorm = (P \u2212 Pmin) / (Pmax \u2212 Pmin)**, and the normalized attribution term is **SHAPnorm = (|SHAPsum| \u2212 |SHAPsum|min) / (|SHAPsum|max \u2212 |SHAPsum|min)**, where |SHAPsum| is the magnitude of the summed SHAP attributions for a gene.\n\n(ii) For GradientSHAP, they use **50 empirical baseline samples** randomly selected from the training data and compute attributions using **50 path samples**. They compute attributions across **all 134 features** (6 centrality + 128 embeddings) and, to manage compute, they **process genes in batches of 1,024** while covering the full dataset.",
      "source_document": "papers/2511.12463v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "The authors approximate one of the PPI network centrality measures for computational efficiency. Which centrality measure is approximated, what approximation scheme and parameter value do they use, and what asymptotic time-complexity reduction (from what to what) do they claim this yields?",
      "answer": "They approximate betweenness centrality using k-sampling with k = 500 randomly selected nodes, and state this reduces computation time from O(n^3) to O(k n^2).",
      "source_document": "papers/2511.12463v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the neural-network model described in Methods, what specific architecture and training/regularization choices do the authors state they used\u2014(i) the sizes of the hidden layers, (ii) where batch normalization is applied and what activation function follows, (iii) the dropout rate, and (iv) the optimizer settings including initial learning rate, weight decay, and the early-stopping criterion?",
      "answer": "(i) A multi-layer perceptron with two hidden layers of 256 and 128 neurons. (ii) Each hidden layer uses batch normalization, then a ReLU activation. (iii) Dropout regularization with rate 0.2. (iv) Adam optimizer with initial learning rate 0.001 and weight decay 0.0001, trained for 40 epochs with early stopping based on validation AUPRC.",
      "source_document": "papers/2511.12463v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the Methods, how do the authors operationally define (i) the positive class for supervised learning from DepMap CRISPR essentiality scores, and (ii) the separate \u201cessential gene\u201d threshold used for descriptive distribution comparisons in the Results? State the exact thresholds/percentiles and the corresponding gene counts they report (where given), and briefly explain why these two definitions are not identical.",
      "answer": "(i) For model training labels, they binarize DepMap median essentiality by taking the top 10% most essential genes\u2014i.e., genes with the lowest DepMap scores at the 10th percentile\u2014as the positive class. They report this corresponds to ~824 essential genes out of 8,236 total genes.\n(ii) In Results 3.1 (descriptive distributions), they also refer to \u201cessential genes\u201d using a fixed DepMap score cutoff of \u22120.5 (genes below \u22120.5), yielding 1,407 essential genes vs 6,829 non-essential.\nThese are not identical because (i) uses a percentile-based cutoff (forcing exactly 10% positives for the classification task), while (ii) uses an absolute score threshold (\u22120.5) for exploratory comparisons, which in their dataset selects a larger set than 10%.",
      "source_document": "papers/2511.12463v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s Node2Vec feature-construction step, what exact values do the authors choose for the return parameter p and the in\u2013out parameter q, and what random-walk sampling design do they report (walk length and number of walks started per node)? Briefly state the exploration behavior they say these settings are intended to achieve.",
      "answer": "They set Node2Vec\u2019s return parameter p = 1.0 and in\u2013out parameter q = 1.0. They use random walks of length 80 steps and generate 10 walks starting from each node. The authors state that setting p and q to 1.0 yields an \u201cunbiased exploration\u201d that balances breadth-first sampling (local neighborhood structure) and depth-first sampling (global network patterns).",
      "source_document": "papers/2511.12463v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the paper\u2019s ModelAngelo-based validation on EMPIAR-10648 (Atomic Model Recovery via ModelAngelo), how are reconstructed protein chains matched to the \u201cOriginal\u201d atomic model for structural comparison? State (i) the alignment method/tool used, (ii) the gap-penalty scoring scheme, and (iii) the minimum sequence-identity threshold applied before computing backbone RMSD.",
      "answer": "Chains are matched to the Original model using global sequence alignment implemented in Biopython\u2019s pairwise2 module. The gap-penalty scoring scheme is: match = 2, mismatch = \u22121, gap open = \u22122, gap extend = \u22120.5. Only alignments with sequence identity \u2265 40% are retained before computing backbone RMSD (from backbone atoms N, C\u03b1, C, O after superposition).",
      "source_document": "papers/2511.12931v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In cryoSENSE\u2019s Nesterov-accelerated measurement-consistency guidance for DDPM posterior sampling (Appendix 6 / Implementation of Generative Priors), what are the specific endpoint values used for (i) the measurement-consistency strength schedule \u03b6t (give \u03b6min and \u03b6max, including the special-case \u03b6max for the largest kernel size), and (ii) the momentum/extrapolation coefficient schedule \u03bat (give \u03bamin and \u03bamax)?",
      "answer": "The paper sets the guidance strength to \u03b6min = 10^-10, and uses \u03b6max = 1.0 for kernel sizes 2, 4, 8, and 16, but increases to \u03b6max = 10.0 for the largest kernel size 32. The Nesterov extrapolation/momentum coefficient is scheduled linearly from \u03bamin = 0.1 to \u03bamax = 0.9.",
      "source_document": "papers/2511.12931v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Appendix 5 (\u201cImplementation of Sparse Priors\u201d), how do the authors choose the two key hyperparameters for sparse-prior reconstruction\u2014(i) the regularization strength \u03bb and (ii) the learning rate \u03b1? Specify the exact candidate grids for \u03bb and \u03b1, how many images are used to tune them, and the number of proximal-gradient epochs used to solve each sparse-prior optimization problem.",
      "answer": "They solve each sparse-prior optimization (DCT L1, wavelet L1, and TV) with proximal gradient descent for 200 epochs. The learning rate is held constant across iterations (\u03b1t = \u03b1). For each protein, they tune \u03bb and \u03b1 by grid search over two training images, using \u03bb \u2208 [0.1, 0.01, 0.001, 0.0001] and \u03b1 \u2208 [0.001, 0.01, 0.1, 0.5, 1].",
      "source_document": "papers/2511.12931v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the CryoDRGN-based conformational heterogeneity validation (Section 3.3), describe the exact workflow the authors use to quantify whether cryoSENSE reconstructions preserve heterogeneity on EMPIAR-10076: include (i) the latent-space dimensionality used for CryoDRGN, (ii) the method used to visualize the latent space and the clustering method used to assign conformational clusters, and (iii) how \u201ccluster agreement\u201d is computed plus the reported agreement ranges (and averages) for DCT vs DDPM reconstructions at K = 16, C = 1.25 pixel-space masking.",
      "answer": "(i) CryoDRGN embeds particles into an 8-dimensional latent space. (ii) The latent space is visualized using UMAP, and conformational clusters are assigned using Gaussian mixture modeling (GMM). (iii) Cluster agreement is computed as the fraction of particles in each cryoSENSE cluster that match the corresponding cluster assignment from the original dataset (using the original dataset\u2019s GMM labels). At K = 16 and C = 1.25 pixel-space masking: DCT reconstructions achieve 74.2%\u201389.0% agreement (80.8% average), while DDPM reconstructions achieve 82.1%\u201391.4% agreement (87.9% average).",
      "source_document": "papers/2511.12931v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In compressive cryo-EM acquisition, what is the mathematical relationship between the compression factor and the number of acquired measurements for (i) pixel-space masking with b random binary masks and kernel-size-K pooling, and (ii) Fourier-space masking via subsampling Fourier coefficients? State the resulting expressions for the measurement dimension m and compression factor C in each case.",
      "answer": "(i) Pixel-space masking: after applying b random binary masks and non-overlapping K\u00d7K summation pooling, the measurement vector has dimension m = b n / K^2, so the compression factor is C = n/m = K^2 / b.\n\n(ii) Fourier-space masking: subsample m Fourier coefficients from an n-pixel image, with m = n / C (equivalently C = n/m).",
      "source_document": "papers/2511.12931v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In image-based GWAS, how can you quantitatively compare an unsupervised, learned set of imaging-derived phenotypes against (i) a small set of human-defined phenotypes and (ii) alternative deep-learning phenotyping pipelines, and what overlap patterns would indicate that the learned phenotypes recover known signals while also increasing discovery?",
      "answer": "One practical way is to run GWAS on each phenotype set and compare results at the level of independent association signals (e.g., lead SNPs grouped into loci) by counting how many lead SNPs/loci overlap between methods and how many loci are unique to the learned phenotypes.\n\nIn the human retinal fundus example, human-defined phenotypes (vascular branching complexity/fractal dimension, vascular density, and mean R/G/B color channels) showed substantial overlap with AIPheno for some lead SNPs (e.g., 6/7 and 10/13 lead SNPs overlapping for two vascular metrics; 49/53 overlapping for color channels) but still missed most loci found by the learned phenotypes (missing 94.3% and 90.6% of AIPheno\u2019s 106 loci for the two vascular metrics; missing 71.7% for the color-channel HDPs).\n\nFor alternative deep-learning phenotyping, iGWAS overlapped on 13/14 of its lead SNPs but still missed 88.7% of AIPheno\u2019s loci, and a transferGWAS pipeline using ImageNet-pretrained models (top 10 PCs per model) had 70.0% of its lead SNPs overlapping AIPheno\u2019s loci while still leaving 31.1% (33/106) of AIPheno\u2019s loci uncovered.\n\nThis pattern\u2014high overlap for known/related signals plus a large fraction of additional, non-overlapping loci\u2014supports the interpretation that the learned imaging-derived phenotypes recover established genetic associations while increasing genetic discovery by capturing additional biologically meaningful image variation (e.g., pigmentation, lesions, optic disc/vasculature morphology) beyond sparse human-defined metrics.",
      "source_document": "papers/2511.13141v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You have an unsupervised image-based GWAS pipeline that learns latent \u201cdirections\u201d as quantitative phenotypes and then uses a generator to visualize how a variant changes the image. What concrete design choices can you use to (i) make the learned phenotypes more disentangled and biologically interpretable across scales, and (ii) ensure that the generated visual interpretation isolates the effect of one phenotype axis rather than being confounded by other latent factors?",
      "answer": "(i) Improve disentanglement/interpretability by discovering statistically independent or orthogonal latent directions (e.g., ICA for independent components or PCA for orthogonal components) within the generator\u2019s latent space, and by exploiting a hierarchical generator (e.g., StyleGAN) that synthesizes images from coarse-to-fine so different layers capture different biological scales\u2014early layers tend to encode morphology while later layers encode color/texture, with intermediate layers mixing both. Training on large, diverse datasets (e.g., integrating images across time points) can also yield richer, more robust phenotype directions.\n\n(ii) To isolate an axis during interpretation, perform direction-wise latent traversal along only the selected significant direction (in standardized units) while holding other directions/latent factors constant, then use the generator to synthesize images from these controlled latent codes. Additionally, use \u201cstd images\u201d or analogous difference/variability maps to localize regions of significant change and confirm that the visualization reflects the targeted axis.",
      "source_document": "papers/2511.13141v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In intrinsic-dimension (ID) analysis of molecular dynamics trajectories, what complementary temporal ID summaries can you compute to (i) reduce sensitivity to early non-equilibrated frames and (ii) still detect transient conformational transitions, and how are these summaries defined?",
      "answer": "Three complementary temporal summaries are used:\n1) Overall ID: treat all frames as one point cloud to obtain a single ID value; to reduce influence of initial non-equilibrated segments, also compute overall ID on only the final portion of the trajectory (user-specified length).\n2) Instantaneous ID: a time series obtained by estimating ID in local neighbourhoods centered at each frame, enabling detection of transitions/heterogeneity over time.\n3) Averaged ID: summarize the instantaneous series by its mean over the full trajectory and by its mean over the final portion (both referred to as averaged).",
      "source_document": "papers/2511.13550v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In deep-learning scoring of protein\u2013protein docking models using interface \u201cimages\u201d and a Vision Transformer, what are the main preprocessing steps used to convert a 3D docked complex into the multi-channel 2D interface maps the model consumes, and how is patch-level geometric/biophysical information (including inter-surface distances and solvent accessibility) incorporated?",
      "answer": "The workflow constructs an interface map from a docked complex by (1) refining the docking pose and computing interaction energy terms (e.g., van der Waals, desolvation, hydrogen/disulfide bonds, electrostatics and related contacts), (2) cropping the complex to a neighborhood around the interaction center (geometric center of contact points), (3) triangulating the solvent-excluded surface and rescaling it to ~1 \u00c5 granularity, (4) defining surface \u201cpatches\u201d on each protein as vertices within a geodesic radius of the interaction center and computing per-surface-point features such as shape index, curvature, hydrogen-bond potential, charge, and hydropathy; additionally computing a patch-distance image from a grid of Euclidean distances between the two opposing surfaces and computing per-residue Relative Accessible Surface Area (RASA) via DSSP, and finally (5) projecting patch surface points to a 2D plane (via multidimensional scaling) and converting the patch features into a multi-channel 2D image where pixel intensities are proportional to feature values (one channel per feature type, including the distance and RASA-derived information).",
      "source_document": "papers/2511.13583v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When searching for rare transcriptomic subtypes within a single cancer using k-means on an autoencoder latent space, how can cluster-label permutation across random restarts be handled so that a stability metric like the Jaccard index is meaningful, and what rarity/stability criteria can be used to flag candidate rare subtypes?",
      "answer": "To make stability comparisons meaningful across k-means restarts, the cluster labels from each run should first be aligned to a reference labeling because k-means labels are only defined up to permutation. This can be done by solving a label-matching (assignment) problem with the Hungarian algorithm, aligning each run\u2019s clusters to the reference run, and then computing per-cluster Jaccard similarity between the reference cluster\u2019s sample set and its aligned counterpart across runs.\n\nCandidate rare subtypes can then be flagged using a simple discovery rule based on (i) prevalence and (ii) stability: mark a cluster as rare if its prevalence is <10% of samples, and mark it as stable if its Jaccard stability is \u22650.60 computed across multiple random seeds (e.g., 20 restarts) after Hungarian alignment.",
      "source_document": "papers/2511.13705v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In gene-incremental learning for single-cell transcriptomics, why can masked-gene regression loss be a poor basis for comparing performance across datasets, and what alternative evaluation can be used to obtain a more universally comparable measure of whether previously learned genes are retained across stages? Describe how this alternative is constructed and evaluated.",
      "answer": "Masked-gene regression (masked value prediction) loss can be hard to compare across datasets because different transcriptomic datasets can have different value scales and distributions, so the absolute regression loss is not a universally comparable metric. An alternative evaluation is gene-based downstream classification: pick sets of biologically/clinically crucial genes for specific downstream classification datasets, assign those gene sets to different incremental stages (so stage k learns the genes crucial for downstream dataset D_dk), and then at each stage evaluate retention by freezing the gene-incremental model as a feature extractor (using the extracted gene features e\u2032) and training only a simple linear classifier layer to predict the downstream labels. Performance on the downstream dataset in later stages reflects how well the model retains the previously learned stage-specific genes.",
      "source_document": "papers/2511.13762v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a trajectory/embedding method for time-resolved single-cell snapshot data where ground-truth dynamics are unavailable, what two neighborhood-based consistency metrics can be used to quantify (i) smoothness/coherence of the inferred velocity field in latent space and (ii) temporal separation/coherence of time points in the embedding, and how is each metric computed conceptually?",
      "answer": "Two metrics are used:\n\n1) **Velocity Consistency (VC):** For each embedded cell, find its neighborhood (defined by an r-distance neighborhood in latent space). Compute the average cosine similarity between that cell\u2019s latent velocity vector and the latent velocity vectors of its neighbors; then average this quantity across all cells. Higher VC indicates a smoother, more locally coherent velocity field in the embedding.\n\n2) **Temporal Consistency (TC):** For each embedded cell from time point i, find its r-distance neighborhood in latent space and compute the fraction of its neighbors that also come from the same time point i. Average this fraction across all cells/time points to obtain a global measure of temporal coherence/separation; higher TC indicates that cells cluster with temporally matched neighbors in the embedding.",
      "source_document": "papers/2511.13786v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In trajectory inference from time-resolved single-cell snapshot data, why might you prefer an *unbalanced* dynamical optimal transport (OT) formulation over a mass-conserving OT formulation, and how is this implemented mathematically in the continuity equation and the transport cost (including the role of the hyperparameter \u03b1)?",
      "answer": "Mass-conserving dynamical OT assumes total probability mass is preserved, which can be biologically inappropriate because cell populations can expand or shrink due to division or apoptosis. Unbalanced dynamical OT relaxes conservation by adding a growth term g(t,x), changing the continuity equation from \u2202t\u03c1 + \u2207x\u00b7(v\u03c1)=0 to \u2202t\u03c1 + \u2207x\u00b7(v\u03c1)=g\u03c1 (with the same boundary constraints \u03c1(0,\u00b7)=\u03c10 and \u03c1(T,\u00b7)=\u03c1T). The associated unbalanced transport cost uses the Wasserstein\u2013Fisher\u2013Rao objective: LWFR = \u222b0^T \u222bRd (||v(t,x)||^2 + \u03b1 g(t,x)^2) \u03c1(t,x) dx dt, where the first term penalizes transport/kinetic energy and the second penalizes growth/decay; \u03b1 controls the relative weight between transport and growth penalties.",
      "source_document": "papers/2511.13786v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a deep model that learns time-dependent single-cell embeddings together with an unbalanced dynamical optimal-transport (OT) flow, you need an objective that (i) enforces that the simulated cell population at each time point matches the observed population and (ii) still compares distributions even when total mass changes due to proliferation/apoptosis. How can a \u201cdata-matching\u201d loss be constructed by combining a mass-consistency term with an OT term, and what do the two terms measure conceptually (including why the OT term uses *normalized* weights)?",
      "answer": "Construct the data-matching loss as a weighted sum of a mass loss and an OT loss:\n\n- LMatch = \u03bbMass\u00b7LMass + \u03bbOT\u00b7LOT.\n\nLMass measures global mass (population-size) consistency across time points by penalizing the discrepancy between the total predicted mass at time i (sum of predicted weights \u0175_i) and the observed mass at that time point (proportional to N_i).\n\nLOT measures how different the *shapes* of the predicted vs observed distributions are at each time point using a Wasserstein-2 distance computed on *normalized* weights (\u0175_i/||\u0175_i||1 and w_i/||w_i||1). Normalization removes total-mass effects so LOT compares distributional geometry/composition while LMass separately accounts for changes in total population mass.",
      "source_document": "papers/2511.13786v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When interpreting a deep learning model that classifies protein sequences into enzyme functional groups, how can combining Integrated Gradients with 1D Grad-CAM provide complementary evidence that the model is using biologically meaningful motifs rather than spurious sequence patterns? In your answer, describe what each method attributes (granularity and mechanism) and what kind of consensus across different architectures would strengthen biological credibility of the explanations.",
      "answer": "Integrated Gradients (IG) provides fine-grained, residue/k-mer\u2013level importance by integrating gradients along a path from a baseline input to the actual sequence, yielding per-token contribution scores that can pinpoint critical residues or short motifs influencing the class decision. In contrast, 1D Grad-CAM is adapted to convolutional layers and produces a coarser, region-level attribution by highlighting contiguous sequence segments that most drive the predicted class through gradient-weighted activation maps. Using both together gives complementary interpretability\u2014IG can localize specific k-mers within a highlighted region, while Grad-CAM can show broader discriminative regions that convolutional filters use. Biological credibility is strengthened when different architectures (e.g., CNN, BiLSTM, CNN-BiLSTM, CNN-Attention) independently emphasize recurring motifs (such as HIDRL, HQLLH, LLHEL/DEEVK) and these motifs are enriched in residues like histidine, aspartate/glutamate, and lysine that are consistent with known catalytic or metal/cofactor-binding chemistry in enzymes (e.g., transferases). Such cross-model motif recurrence indicates the models learned conserved biochemical signatures rather than arbitrary patterns.",
      "source_document": "papers/2511.13791v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You observe that a CNN, a BiLSTM, a CNN\u2013BiLSTM hybrid, and a CNN-with-attention model all perform similarly on protein functional-group classification from sequence k-mers, but their post-hoc attribution maps differ (spiky local peaks vs broader distributed signal vs a single sharply focused region). What mechanistic properties of each architecture explain these different attribution patterns, and how would you use the fact that certain high-importance motifs recur across multiple architectures to argue that the model is learning biochemically meaningful transferase signatures rather than dataset-specific artifacts?",
      "answer": "CNN attributions appear as multiple sharp, localized peaks because convolutional filters act as motif detectors that respond strongly to short subsequences anywhere in the sequence. A BiLSTM tends to distribute relevance over broader regions because recurrent state integrates information across many positions, capturing long-range dependencies and spreading importance across connected motifs. The CNN\u2013BiLSTM hybrid shows an intermediate pattern: CNN layers detect local motifs while the BiLSTM contextualizes and links them across the sequence. A CNN-with-attention can concentrate attribution very strongly on one narrow region because the attention mechanism can assign most weight to a single subsequence deemed most discriminative.\n\nIf specific motifs (e.g., HIDRL, HQLLH, LLHEL/DEEVK) are consistently emphasized across these different architectures, that convergence provides evidence the signal is not an idiosyncratic artifact of one model type. Instead it suggests independent mechanisms have latched onto the same conserved biochemical signatures enriched in catalytically relevant residues (H, D/E, K) characteristic of transferase active/binding regions, supporting biological plausibility over memorization.",
      "source_document": "papers/2511.13791v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are building a protein functional-class classifier from raw PDB sequences using k-mer tokenization. What concrete preprocessing choices can you make to obtain fixed-size inputs suitable for CNN/BiLSTM models, and what is the rationale for each choice? Describe (i) the k-mer length and overlap scheme, (ii) how tokens are mapped to integers including how rare k-mers are handled, and (iii) how sequences of varying length are standardized before being fed to the network.",
      "answer": "A workable preprocessing pipeline is:\n(i) Segment each protein sequence into overlapping k-mers of length 5 (k=5) to capture local residue-level dependencies.\n(ii) Build a k-mer vocabulary where each unique k-mer is assigned an integer index using frequency ranking; rare k-mers are truncated/removed to keep a fixed vocabulary size.\n(iii) Convert each sequence to its integer indices and then pad or truncate it to a fixed maximum length of 300 so every example has the same input dimensionality. This fixed-length integer-encoded representation lets an embedding layer learn distributed vectors for k-mers while preserving sequence order/context for downstream CNN/BiLSTM models.",
      "source_document": "papers/2511.13791v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are designing an interpretable protein functional-class classifier that uses k-mer integer encoding and wants to keep inputs a fixed size for different deep architectures (CNN, BiLSTM, CNN\u2013BiLSTM, CNN-with-attention). What specific representation and training choices would you make to (i) capture local residue dependencies, (ii) keep the input dimensionality uniform, and (iii) reduce overfitting during training? Give concrete values/strategies for k-mer generation, vocabulary handling, sequence length standardization, and the regularization/early-stopping setup.",
      "answer": "Use overlapping k-mers of length 5 (k=5) to capture local residue-level dependencies. Assign each unique k-mer an integer index by frequency ranking, truncating rare k-mers to keep a fixed vocabulary size. Pad or truncate each integer-encoded sequence to a maximum length of 300 so all models receive uniform input dimensions. Train all architectures under a unified setup with Adam (learning rate 1\u00d710\u22123), batch size 64, up to 30 epochs, dropout 0.3, and early stopping with patience of 5 epochs (monitoring validation performance) to limit overfitting and improve generalization.",
      "source_document": "papers/2511.13791v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are comparing how a noncoding SNP might change transcription-factor binding through mechanics rather than protein sequence. Using normal mode analysis outputs and protein\u2013DNA interface mapping, what specific computational signatures would support the conclusion that the SNP allele produces a stiffer, less adaptable protein\u2013DNA complex, and what mechanistic interpretation follows for DNA binding?",
      "answer": "A stiffness/rigidity signature is a higher NMA eigenvalue (more energy required to deform the complex) together with reduced mobility in B-factor profiles (fewer high-flexibility peaks) and reduced cumulative variance captured by the first few modes, indicating more constrained collective motions. Interface mapping should concurrently show a more compact binding geometry (reduced cleft volume) and a reduced/diversified contact network (fewer non-bonded contacts and loss of key phosphate-mediated anchoring residues such as Arg55/Arg58, even if the total hydrogen bonds remain conserved). Mechanistically, these signatures indicate that the SNP allele yields a tighter but less flexible interface that restricts large-scale bending/adaptive motions of the DNA and HMG-box, impairing the biomechanical balance needed for efficient recognition and stable, geometry-optimized transcription-factor\u2013DNA binding, thereby perturbing downstream gene regulation.",
      "source_document": "papers/2511.13916v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In diffusion-based structure-based drug design that starts from an apo (unbound) protein structure, how can you design the forward/noising process so the model learns ligand generation while also learning an apo\u2192holo pocket refinement trajectory, and what specific representation choices help keep the pocket transformation physically coherent?",
      "answer": "Use a coupled forward diffusion where the ligand is progressively corrupted in both continuous coordinates and discrete atom types, while the pocket is not drawn from a generic prior but is driven toward the apo state via an explicit apo\u2013holo interpolation. Concretely, at time t the ligand positions are noised with a Gaussian (x_t^M ~ N(sqrt(alpha_t) x_0^M, (1\u2212alpha_t)I)) and atom types with a categorical noising process, whereas the pocket conformation at step T corresponds to the apo structure. Intermediate pocket states are generated by a residue-level interpolation that applies (i) noisy interpolated residue translations and chi-angle updates and (ii) quaternion-based rotations using spherical linear interpolation (Slerp) with an additional small quaternion perturbation for robustness. Pocket coordinates at time t are then obtained by applying these interpolated residue transformations to the holo structure. These choices\u2014residue-level transforms plus quaternion/Slerp rotations\u2014maintain structural coherence and avoid issues like gimbal lock while enabling smooth apo\u2194holo transitions for training the reverse denoising model.",
      "source_document": "papers/2511.14559v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building and validating a generative model that refines an apo binding pocket toward a holo state while generating a ligand, what dataset-splitting strategy and structural \u201cnon-overlap\u201d criterion can you use to reduce information leakage from highly similar pockets, and what similarity metric and threshold would you apply?",
      "answer": "Use a chronological split of apo\u2013holo\u2013ligand triplets to mimic real-world deployment, and enforce strict structural non-overlap by verifying that each test pocket has TM-score < 0.35 to all training pockets (TM-score used as the pocket-structure similarity metric, threshold 0.35).",
      "source_document": "papers/2511.14559v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an all-atom generative model that co-designs peptide sequence and 3D structure by integrating a learned ODE, what concrete mechanism can be used during training and sampling to enforce stereochemical validity throughout the trajectory (not only at the final step), and what does it penalize?",
      "answer": "Use mid-trajectory reconstruction of full all-atom (atom37) coordinates from the current multimodal state (backbone/side-chain dihedrals, rigid-body frames, and sequence) and add a stereochemical violation loss (inspired by OpenFold). The violation term penalizes geometries that violate stereochemical constraints, enforcing physical plausibility during training; during inference, reconstructing atom37 after each Euler ODE step maintains geometric consistency.",
      "source_document": "papers/2511.14663v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a peptide generator under two different conditioning contexts (e.g., binding pocket only vs whole receptor), it is possible to see better agreement with native *marginal* torsion-angle distributions in one setting but better agreement with the *joint* Ramachandran (\u03c6\u2013\u03c8) distribution in the other. What mechanistic explanation accounts for this apparent contradiction, and what does it imply about what each conditioning context is regularizing?",
      "answer": "Pocket-only conditioning acts as stronger local regularization on individual torsion angles, producing tighter marginal \u03c6/\u03c8/\u03c9 distributions that match natives well, but it loses the coupled basin structure (\u03b1-helix/\u03b2-sheet regions) in the joint \u03c6\u2013\u03c8 distribution. Whole-receptor conditioning provides richer global geometric context, so although its marginals are broader (worse univariate KS agreement), it better preserves the native joint Ramachandran basin structure by learning correlated backbone motions; i.e., it regularizes global conformational preferences rather than per-angle marginals.",
      "source_document": "papers/2511.14663v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using predicted antibody\u2013antigen complex structures to rank binding affinities via MM/GBSA-derived relative binding free energy (RBFE), what validation strategy can you use to (i) test whether the predicted structures give physically consistent energies relative to an experimentally solved complex, and (ii) quantify whether the method can correctly rank multiple antibodies by true affinity\u2014and what specific metrics/scoring approaches implement these two checks?",
      "answer": "A two-part validation is:\n(i) An RBFE consistency test against a complex with an experimentally determined structure: compute RBFE on the experimental structure (RBFE1), compute RBFE on each method\u2019s predicted \u201coptimal conformation\u201d (RBFE2), and evaluate the deviation \u0394RBFE = RBFE2 \u2212 RBFE1; smaller \u0394RBFE indicates better agreement. Methods can be converted into ranks and scored with a point system (e.g., 700/560/420/280/140 for ranks 1\u20135).\n(ii) A multi-antibody ranking test: predict complexes for several antibodies, compute RBFE for each, compare the predicted affinity ordering to the true ordering defined by experimental Kd values, and quantify global ranking accuracy using Spearman\u2019s rank correlation coefficient. Additionally, assess \u201clocal\u201d correctness of pairwise affinity relationships using a separate point-based scoring scheme that awards points for correct subject rankings and for correctly predicted within-pair affinity relationships.",
      "source_document": "papers/2511.14676v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You observe that a diffusion-based complex predictor sometimes produces antibody\u2013antigen models that are geometrically plausible and assigned high confidence, yet the antibody is docked onto the wrong face/region of the antigen (\u201creverse docking\u201d). Describe (i) a statistical protocol to show this is a systematic failure mode rather than random occasional misprediction (include how you would label outcomes and what hypothesis test(s) you would apply), and (ii) an input-level intervention that can reduce this error for at least one antibody family\u2014along with the trade-off this intervention introduces in the model\u2019s confidence/quality metrics.",
      "answer": "(i) Run many independent predictions for the same antibody\u2013antigen pair and classify each output as either a reasonable model (R) or antibody/antigen opposite orientation (Abo/Ago; \u201creverse docking\u201d). In the described protocol, 75 additional AF3 runs were added (81 total predictions) and the frequencies of reverse-docked vs reasonable models were compared; a t-test showed the mean confidence/score behavior differed strongly between groups (one-tailed p = 3.83\u00d710\u207b\u00b3\u00b3), supporting that reverse docking is not a coincidence. Similar replication across related antibodies in the same family (e.g., D2512, D2523, D2525, D2546) can be assessed with F- and t-tests, with a practical benchmark such as achieving 10 predictions in a row without reverse docking to claim \u201ccorrection.\u201d\n\n(ii) An input-level intervention is to extend the antigen input from just the extracellular domain to the full CD47 sequence including the transmembrane region (a 5-\u03b1-helix segment). This additional structural context can normalize docking direction for D2510 (most of 20 runs avoided reverse docking), but it introduces a trade-off: the resulting predictions have substantially lower confidence scores (ipTM\u22480.43, pTM\u22480.55\u20130.56) and can be harder to predict reliably due to the longer, more complex sequence; moreover, the same intervention did not consistently fix reverse docking for another family member (D2523 still bound the \u03b1-helix).",
      "source_document": "papers/2511.14676v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a long-context DNA language model that processes sequences in fixed windows and carries information forward via a small set of summary tokens, why do approximation errors tend to grow as input length increases, and what two hyperparameter changes can reduce this error accumulation? Explain the mechanism in terms of cross-window boundaries and summary granularity.",
      "answer": "Errors grow with input length because longer sequences span more windows, so the model must propagate information across more cross-window boundaries using only the retained summary-token KV states; small approximation differences introduced at each boundary can accumulate over many windows. Two changes reduce this accumulation: (1) increasing the window size W, which creates fewer boundaries and preserves more within-window detail, yielding uniformly smaller discrepancies; and (2) decreasing the k-mer cadence k (finer granularity), which inserts more summary tokens per window and provides finer summaries, improving fidelity across lengths at the cost of retaining more KV.",
      "source_document": "papers/2511.14694v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a windowed context-compression scheme for autoregressive DNA language models that inserts one learnable summary token after every k bases and drops ordinary-token KV states at window boundaries, what is the purpose of sharing (reusing) a boundary summary token between adjacent windows, and how does this design change what information later windows can access compared with standard sliding-window truncation?",
      "answer": "Sharing a boundary summary token (setting the first summary in a window equal to the last summary of the previous window) makes the cross-window interface stationary: every window sees the same canonical pattern of interactions among W summary tokens under causality, so long-range information can be propagated hop-by-hop through repeated application of the same within-window summarization/update rule. Because ordinary-token KV from previous windows is discarded, later windows cannot directly attend to earlier base tokens; they can only access remote history through the retained KV states of earlier summary tokens (including the carried boundary token). This differs from standard sliding-window truncation, which simply drops distal context entirely; here distal information is preserved in a compact channel (the retained summary KV) rather than being irrevocably removed.",
      "source_document": "papers/2511.14694v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a streaming inference setup for an autoregressive DNA Transformer that compresses long contexts by inserting learnable summary tokens every k bases and discarding ordinary-token KV at window boundaries, what training signal can be added to ensure the summary tokens actually encode predictive information, and how can you make this compression behavior robust to different compression strengths at test time?",
      "answer": "Add supervision on the summary (Focus) token positions by including them in the same next-token cross-entropy objective: for each summary token at position t, train it to predict the next real base to its right (i.e., the next real-base position \u03c0(t)), given only its visible past context where earlier windows contribute only retained summary-token KV. To improve robustness across compression strengths, randomize compression configurations during training\u2014for example, enable the summary-token attention/compression at different Transformer layers and/or slightly perturb k\u2014so the learned compressor performs stably across a range of compression ratios without changing the loss form.",
      "source_document": "papers/2511.14694v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a compressed DNA language model by comparing its next-base probability distributions to an uncompressed baseline, what set of distribution-distance metrics can be used to quantify fidelity, and what numerical-stability step should be applied before computing them?",
      "answer": "Fidelity can be quantified by comparing the baseline vs. compressed next-token distributions using five complementary measures: L1 distance, L2 distance, Hellinger distance, Jensen\u2013Shannon divergence, and KL divergence (taking the uncompressed/baseline distribution as the reference). For numerical stability, probabilities should be clipped at 10^\u221212 and then re-normalized before computing the metrics.",
      "source_document": "papers/2511.14694v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a streaming DNA language model that inserts learned summary tokens and only keeps summary-token KV states across fixed windows, what modification to the standard next-token cross-entropy objective can be used to ensure the summary tokens actually encode predictive information, and why does this work given that later windows cannot see ordinary-token KV from earlier windows?",
      "answer": "Augment the language-modeling cross-entropy so that it is applied not only at real-base positions but also at summary-token positions, supervising each summary token to predict the next *real* nucleotide to its right (i.e., for any position t\u2014base or summary\u2014train on the target x_{\u03c0(t)}, where \u03c0(t) is the nearest real-base position after t). For real bases this reduces to ordinary next-base prediction; for summary tokens it forces their representations, which are the only states retained across windows, to contain enough information (about their associated k-mer and accumulated history) to predict upcoming bases. Because subsequent windows cannot access earlier ordinary-token KV, the only way to reduce loss on distant bases is to route relevant information through these retained summary-token states, so the objective directly pressures the summaries to be predictive carriers of cross-window context.",
      "source_document": "papers/2511.14694v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an AlphaFold2/OpenFold-style Evoformer, what does an ablation study reveal about how reliance on MSA-derived features versus pairwise geometric features changes with protein sequence length, and how is this evidenced by the direction of the length\u2013performance correlation when you (i) skip MSA column attention, (ii) zero the MSA representation before the structure module, and (iii) skip triangle attention?",
      "answer": "Reliance shifts with length: longer proteins depend more on MSA-based features, while shorter proteins are more sensitive to triangle-based pair geometry. This is evidenced by positive length\u2013performance-loss correlations for MSA ablations and a negative one for triangle attention. Specifically, skipping MSA column attention produces increasing TM-score loss with increasing length (Spearman \u03c1 = 0.40, p = 1.9\u00d710\u22127), and zeroing the MSA representation also shows larger losses for longer proteins (\u03c1 = 0.46, p = 1.3\u00d710\u22129). In contrast, skipping triangle attention correlates negatively with length (\u03c1 = \u22120.19, p = 0.018), indicating greater reliance on triangle attention in shorter proteins.",
      "source_document": "papers/2511.14781v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When probing which internal representations a structure predictor like OpenFold actually uses, why is it informative to ablate the final MSA and Pair tensors right before the Structure Module by (i) zeroing them out and (ii) replacing them with random noise that preserves per-channel mean and variance\u2014and what qualitative conclusion does this kind of test support about the relative importance of the two representations?",
      "answer": "Ablating the representations right before the Structure Module isolates how much the downstream structure generator depends on the information content of each representation (rather than on upstream computation). Zeroing is an extreme removal of signal, while replacing with noise matched to the original per-channel mean and standard deviation controls for simple scale/normalization effects (i.e., it tests whether performance comes from meaningful features versus just having activations of the right magnitude). The results of both ablations are qualitatively consistent: corrupting the MSA representation (zero or scale-matched noise) has minimal effect relative to baseline for most proteins, whereas corrupting the Pair representation causes a large drop in TM-score\u2014supporting that OpenFold\u2019s Structure Module relies far more on the final Pair representation than on the final MSA representation.",
      "source_document": "papers/2511.14781v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In genome-scale masked-language pretraining, how can you use a differentiable, context-dependent tokenizer to focus learning on information-dense regions (e.g., motifs) while down-weighting repetitive/low-information regions? Describe (i) how salient tokens can be identified from token merging, and (ii) how this saliency can be turned into an adaptive masking distribution for the MLM loss.",
      "answer": "Use a hierarchical model with token merging to both learn variable-length tokens and derive an importance signal for masking.\n\n(i) Identify salient tokens via token merging: First apply a learnable local tokenizer that merges adjacent bases into L variable-length tokens and keeps a binary source matrix S indicating which bases belong to each token. Then, during pretraining, run a second merging step at the token level inside the long-range (latent) encoder using a ToMe-style attention that merges tokens globally, producing K < L latent tokens Z\u2032_K and a grouping/source matrix S\u2032 (K\u00d7L). The merging preferentially fuses redundant/less-salient tokens, while preserving distinct tokens carrying unique information; thus S\u2032 encodes which original tokens were grouped, and the remaining tokens/groups correspond to the more salient content.\n\n(ii) Turn saliency into adaptive masking: Compute group sizes g_i = \\sum_j S\u2032_{i,j}, i.e., how many original tokens were merged into latent group i. Assign each group an inverse-size weight w_i = 1/g_i so large merged groups (low information) get low weight and small/singleton groups (high information) get high weight. For each local token j belonging to group i, set an importance/masking probability proportional to w_i/g_i and normalize to form a distribution P_L over the L tokens. Sample exactly K tokens (without replacement) from P_L to mask; map this token-level mask M_L back to base positions using the source matrix (M_N = U(M_L, S)) so all bases within a selected merged token are masked. Finally, compute the masked-token modeling loss only on these masked, high-information base positions (an adaptive MLM loss), ignoring easy/redundant positions. This focuses learning on informative regions while down-weighting repetitive DNA.",
      "source_document": "papers/2511.14806v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When repurposing pathogen genome repository metadata for spatial epidemiology, how can you infer U.S. state-level locations from messy free-text \u201cgeographic location\u201d fields, and what auxiliary resource can be used to resolve entries that only list a major city or a state abbreviation?",
      "answer": "A practical approach is to clean the free-text location string using word-lookup/keyword extraction to identify an explicit state name when present (e.g., parsing \u201criver in jackson mississippi\u201d to extract \u201cMississippi\u201d). When records contain only major city names or state abbreviations, state information can be inferred by matching against an external lookup table of U.S. cities-to-states (e.g., a table of the 300 most populated U.S. cities with their associated states); if a city matches, replace the city entry with the corresponding state name.",
      "source_document": "papers/2511.14826v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building an automated pipeline to reuse NCBI genome-assembly metadata for U.S. state-level epidemiological mapping, what sequence of processing steps can turn raw NCBI downloads into an analysis-ready table with one row per assembly, and how can you infer the state when the \u201cgeographic location\u201d field contains only a major city name or a state abbreviation?",
      "answer": "A workable end-to-end workflow is: (1) select the pathogen/taxonomy and download raw NCBI assembly metadata with an initial filter; (2) convert the raw nested .jsonl output into a flat tabular file (e.g., .tsv/.xlsx) using NCBI\u2019s Dataformat tool while retaining only a small set of needed columns (accession, biosample attribute name/value, submitter); (3) pivot the long \u201cattribute name/attribute value\u201d table so each assembly becomes a single row and each attribute name becomes a column (using a pivot_table-style operation), then keep the key derived columns such as Geographic Location, Date of Collection, Strain, Host, Serotype, Isolation Source, and Source Type; (4) clean the pivoted fields (e.g., parse collection date into month/year and standardize location/source strings); and (5) export to .csv for downstream visualization/modeling and comparison with outbreak data.\n\nFor state inference from messy \u201cgeographic location\u201d entries, use word/lookup-based extraction: if the string contains a full state name, extract it; if it contains only a major city or a state abbreviation, map it to a state using an external lookup table (specifically, a Plotly GitHub lookup table listing the top ~300 most populous U.S. cities with their associated states), replacing the city (or resolving abbreviations) with the state name.",
      "source_document": "papers/2511.14826v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When converting NCBI pathogen assembly metadata into an analysis-ready table for downstream epidemiological modeling, why is it useful to pivot the long \u201cbiosample attribute name/value\u201d format into a wide one-row-per-assembly format, and which metadata fields are retained after pivoting to support spatial, temporal, and source/host analyses?",
      "answer": "NCBI\u2019s downloaded assembly metadata include many rows per assembly because biosample fields are stored as repeated (attribute-name, attribute-value) pairs; pivoting converts these into a single row per assembly with each unique attribute name becoming a column, which makes standard analyses (time series, state-level mapping, host/isolation-source stratification) straightforward. After pivoting, the pipeline keeps eight key fields: Geographic Location, Date of Collection, Strain, Host, Serotype, Isolation Source, and Source Type (along with the assembly accession/submitter metadata already present).",
      "source_document": "papers/2511.14826v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You\u2019ve curated pathogen genome-assembly metadata into a table with state-level location plus inferred \u201csource category\u201d (e.g., beef, fruit, human, pork, turkey, water) from the host and isolation-source fields. How can you statistically test whether certain source categories are disproportionately associated with particular states, and what are the key steps needed to construct the data for this test from the cleaned metadata?",
      "answer": "Construct a contingency table of counts for (state \u00d7 source-category) using the cleaned NCBI submissions: (1) classify each U.S. submission into a small set of broad categories derived from the host and isolation-source columns (beef, fruit, humans, pork, turkey, water); (2) restrict to submissions with state-level geographic location; (3) tabulate counts for each category\u2013state pairing; then (4) run a Chi-square test of independence on this contingency table to assess whether category and state are independent (i.e., whether some categories are over/under-represented in particular states).",
      "source_document": "papers/2511.14826v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In metagenomic read simulation, why might you want to weight each organism\u2019s contribution to the simulated read pool by both its target abundance and its genome length, and what alternative sampling assumption does this weighting replace?",
      "answer": "Weighting by both abundance and genome length makes the expected number of reads from a species proportional to how much DNA it contributes: larger genomes yield more sequenced fragments in real experiments, so they should contribute more reads even at the same relative abundance. This replaces an abundance-only sampling assumption where read share depends only on the specified relative abundances (ignoring genome length), which can be used to mimic simulators that do not account for genome size.",
      "source_document": "papers/2511.14909v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When simulating metagenomic Illumina-style reads from a reference that is fragmented into many contigs per organism, what strategy can you use to ensure each organism is treated as a single sampling unit (so reads are drawn across all of its contigs), and how is that grouping defined?",
      "answer": "Group contigs that belong to the same genome/organism by applying a regular expression to each reference entry\u2019s name; all sequences whose names share the same matching text are treated as one species, and reads are sampled from all contigs in that group.",
      "source_document": "papers/2511.14909v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In high-throughput metagenomic read simulation, why might multi-threading provide little benefit even if the simulator is CPU-efficient, and what system component can become the dominant bottleneck at very high simulation speeds?",
      "answer": "Once simulation is fast enough, the limiting factor can shift from computation to I/O: storage throughput (reading references and especially writing/compressing large output read files) becomes the main bottleneck, so adding CPU threads may yield little additional speedup.",
      "source_document": "papers/2511.14909v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When designing a high-throughput Illumina-style metagenomic read simulator, what implementation choices can substantially increase throughput while keeping the same biological error model, and what non-CPU bottleneck can dominate once generation is very fast?",
      "answer": "Throughput can be improved without changing the underlying error model by implementing the simulator in a statically typed, compiled language (rather than an interpreted one), designing the code to minimize dynamic/virtual calls, avoiding extra storage operations by reading reference inputs directly without making temporary copies (including reading compressed inputs directly), and compressing output files before writing them (instead of writing uncompressed output then compressing later). At very high simulation speeds, storage/I/O throughput becomes the dominant bottleneck rather than CPU, which is why multi-threading may not be planned or beneficial.",
      "source_document": "papers/2511.14909v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a weakly supervised whole-slide-image (WSI) prognostic pipeline, how can you systematically connect an image-derived risk score to candidate molecular driver genes, and what selection/graph steps would you use to identify a single \u201chub\u201d biomarker gene from thousands of transcripts?",
      "answer": "One workable strategy is a multi-level integration pipeline that (i) reduces high-dimensional deep-learning WSI representations to a compact set of \u201ccore\u201d pathomic features associated with the model\u2019s continuous risk score, and then (ii) links those features to prognosis-associated genes via a feature\u2013gene association network, ranking genes by a graph-centrality measure to nominate a hub.\n\nConcretely:\n1) Start from the slide-level deep learning feature vector (e.g., 512 dimensions from a pretrained WSI encoder). Screen features by correlation with the model-defined risk score (e.g., |Spearman r| \u2265 0.2 with FDR < 0.05), then use an elastic net regression to compress the remaining features and select a small core set (e.g., 28 features) that are robustly associated with the risk score.\n2) Independently identify prognosis-related genes from transcriptome-wide data using univariable Cox proportional hazards models (e.g., p < 0.01 and HR \u2260 1).\n3) Compute feature\u2013gene correlations between the core pathomic features and prognosis-related genes (again applying thresholds such as |r| \u2265 0.2 and FDR < 0.05) and build a pathology\u2013gene interaction network from significant feature\u2013gene pairs.\n4) Rank nodes in this network using eigenvector centrality; the top-ranked gene is taken as the hub biomarker candidate (in this study\u2019s application, MRPL37 was the highest-centrality hub gene).",
      "source_document": "papers/2511.15067v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using cross-attention to impute unmeasured genes in spatial transcriptomics from an scRNA-seq reference, how can you quantitatively diagnose whether each spatial spot is drawing information from many cell-type centroids versus being dominated by a single centroid, and what would high vs. low values of these diagnostics indicate?",
      "answer": "Compute two diagnostics from each spot\u2019s attention weight vector over the K scRNA-seq centroids: (1) the entropy E(a) = \u2212\u2211_{i=1..K} a_i log(a_i), which is upper-bounded by log(K); higher entropy means attention is more diffusely spread across many centroids, while lower entropy means attention is concentrated on fewer centroids. (2) the Top-2 probability gap F(a) = a_(1) \u2212 a_(2), where a_(1) and a_(2) are the largest and second-largest attention weights; a large gap indicates one centroid clearly dominates (more \u201csingle-centroid\u201d behavior), whereas a small gap indicates substantial mass is shared across at least two centroids (more \u201cmulti-centroid\u201d behavior).",
      "source_document": "papers/2511.15139v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In high-dimensional motif interaction discovery, why can a mean-field variational approximation be too restrictive for Bayesian probit regression with many interaction terms, and what dependency structure can be introduced in the variational family to better capture posterior skewness while retaining scalable coordinate-ascent updates (including what computational trick is used when p > n)?",
      "answer": "A mean-field factorization that forces independence between parameter blocks can be too restrictive when the number of predictors/interactions p is large, because it cannot flexibly represent the posterior dependence structure and resulting skewness. A more flexible partially factorized variational family relaxes the independence between the regression coefficients \u03b2 and the latent probit augmentation variables z by using a conditional factor q(\u03b2|z) (with separate factors for the remaining parameters), which yields a unified skew\u2013normal approximation for \u03b2 and still admits analytic coordinate-ascent updates in closed form. Each coordinate-ascent iteration requires inverting a p\u00d7p matrix for the covariance \u03a3(\u03b2); when p>n this is made scalable by applying the Woodbury matrix identity to invert a lower-dimensional n\u00d7n matrix instead.",
      "source_document": "papers/2511.15330v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a high-dimensional regression model that includes all pairwise interactions between d biological features, coefficients for interaction terms naturally belong to multiple groups (one per participating feature). Describe a Bayesian shrinkage prior construction that can enforce sparsity while respecting this *overlapping* group structure: specifically, how can you parameterize the prior variance of each coefficient using (i) a global shrinkage parameter, (ii) a coefficient-specific local shrinkage parameter, and (iii) feature-dependent group shrinkage parameters encoded via an indicator matrix?",
      "answer": "Use a Gaussian scale-mixture prior where each regression coefficient \\(\\beta_j\\) is centered at 0 with variance given by a product of a global, a local, and overlapping feature-dependent group factors. Let \\(J\\in\\mathbb{R}^{p\\times d}\\) be an indicator matrix with entry \\(J_{jl}=1\\) if feature \\(m_l\\) contributes to term \\(x_j\\) (main effect or interaction) and 0 otherwise. Then specify\n\\[\n\\beta_j\\mid \\tau,\\lambda,\\delta \\sim \\mathcal{N}\\Big(0,\\; \\tau\\,\\lambda_j\\,\\prod_{l=1}^d \\delta_l^{J_{jl}}\\Big),\\qquad j=1,\\dots,p,\n\\]\nso \\(\\tau\\) shrinks all coefficients globally, \\(\\lambda_j\\) provides coefficient-specific local adaptation, and each \\(\\delta_l\\) jointly shrinks all terms that involve feature \\(m_l\\); interaction terms inherit multiple \\(\\delta_l\\) factors via the product over participating features. The shrinkage parameters are given inverse-gamma hierarchical priors (a half-Cauchy style construction): \\(\\tau\\mid\\nu\\sim IG(1/2,1/\\nu),\\;\\nu\\sim IG(1/2,1)\\); \\(\\lambda_j\\mid c_j\\sim IG(1/2,1/c_j),\\;c_j\\sim IG(1/2,1)\\); and \\(\\delta_l\\mid t_l\\sim IG(1/2,1/t_l),\\;t_l\\sim IG(1/2,1).\\) Effects are standardized to unit scale for consistent shrinkage and interpretation.",
      "source_document": "papers/2511.15330v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In high-dimensional probit regression for discovering motif\u2013motif interactions from deep-learning attribution maps, how can you construct the design matrix from motif scanner matches and attribution scores, and how are interaction features and feature filtering defined to keep p manageable? Describe (i) how single-motif features are aggregated from position-wise attributions, (ii) how pairwise interaction features are computed, and (iii) the criterion used to drop rarely occurring features.",
      "answer": "A practical construction is:\n(i) Use a motif scanner (e.g., FIMO) to get start/end positions of each matched motif in each sequence, then aggregate the position-wise attribution scores over that matched subregion into a motif-wise contribution; the document uses the absolute average attribution over the matched positions and then scales the motif-wise scores within each sequence so that the aggregated motif scores sum to 1.\n(ii) Define interaction features via pairwise co-activation computed as the product of the (average) motif contribution scores for the two motifs.\n(iii) To control dimensionality, remove rarely occurring features using a cutoff quantile of 0.95 (user-adjustable), retaining only features above this occurrence threshold; in the described application this yields a design matrix with 755 single effects and 745 interaction effects (p = 1,500).",
      "source_document": "papers/2511.15330v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When comparing predicted microprotein structures to the few available experimental (NMR) structures, what evidence suggests that current predictors capture a stable structural core but diverge at the termini, and how is this quantified using the \u201cpruned\u201d versus \u201call-residue\u201d RMSD comparisons?",
      "answer": "The comparisons use two RMSDs: an RMSD over a best-overlapping contiguous segment longer than 3 residues (\u201cRMS Pruned\u201d) and an RMSD over the full sequence (\u201cRMS All\u201d). For the three microproteins with experimental NMR structures, the pruned overlaps show sub\u20111 \u00c5 agreement for large fractions of the sequence (about 50\u201397% pruned for AlphaFold and 54\u201382% for I\u2011TASSER; ROSETTA also has sub\u2011~1.4 \u00c5 pruned RMSD over 38\u201355%+), indicating that a core fold is reproduced well. In contrast, the full-length RMSDs are much larger (e.g., overall average RMS between predicted and experimental structures of ~4.43 \u00c5 for AlphaFold, 6.84 \u00c5 for I\u2011TASSER, and 12.20 \u00c5 for ROSETTA), implying substantial disagreement outside the core. The interpretation given is that predictors can reproduce the folding of a core region but have major dissimilarities in the end regions (termini), which are more disorganized/coil-like and therefore harder to model consistently.",
      "source_document": "papers/2511.15628v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "Microproteins are much shorter than the proteins used to develop most structure predictors. If you only have a handful of experimental microprotein structures, what evidence would you use to argue that a predictor is systematically mis-calibrated for microproteins due to training-set bias, and what corrective strategy follows from that diagnosis?",
      "answer": "A key indicator is that even the best-performing predictor shows substantially worse agreement to the available experimental microprotein structures than the sub-\u00c5 accuracy reported on larger, well-characterized proteins, suggesting the model is not well represented (and thus not well calibrated) in the microprotein regime. The likely cause is under-representation of microprotein structures in the predictor\u2019s training data. The corrective strategy is to obtain a much larger set of experimentally determined microprotein structures to enable robust benchmarking and to support retraining (or recalibration) of prediction models so microproteins are better represented.",
      "source_document": "papers/2511.15628v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a sequence-only predictor of protein\u2013protein binding affinity using protein language model (PLM) embeddings, what architectural mechanism can explicitly model inter-protein dependencies between the two partner sequences, and how is its output aggregated to produce a single affinity prediction?",
      "answer": "Use a bidirectional cross-attention module to model inter-protein dependencies: one cross-attention block computes the influence of protein A on protein B and a second computes the reverse (each with four attention heads). The outputs from the cross-attention layers are averaged along the sequence dimension and concatenated into a joint pair representation, which is then passed through a small feed-forward regression head (ReLU + dropout) to predict affinity (pKa).",
      "source_document": "papers/2511.16113v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In training a neural network to predict protein\u2013protein binding affinity from sequence-derived embeddings, why might you choose a Huber loss instead of mean squared error, and what role does the Huber delta parameter play in controlling the influence of occasional large prediction errors?",
      "answer": "Huber loss can be more stable than mean squared error when the dataset contains occasional large deviations (outliers): it behaves quadratically for small residuals (like MSE) but transitions to a linear penalty for larger residuals, preventing a few large errors from dominating the gradients and destabilizing training. The delta parameter (\u03b4) sets the cutoff between these regimes\u2014residuals with magnitude below \u03b4 are penalized quadratically, while those above \u03b4 are penalized approximately linearly\u2014thereby controlling how strongly large errors are down-weighted.",
      "source_document": "papers/2511.16113v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When curating a protein\u2013protein binding-affinity dataset to fairly assess generalization, how can homology reduction be applied, and what specific sequence-identity threshold is used here to reduce redundancy in the training set?",
      "answer": "Homology reduction is applied by filtering out redundant complexes based on sequence similarity so that closely related proteins don\u2019t appear multiple times in the training data. Here, complexes are filtered using a 25% sequence-identity cutoff (yielding a nonredundant training set of 1,741 complexes).",
      "source_document": "papers/2511.16113v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a sequence-only protein\u2013protein binding-affinity predictor that uses per-residue embeddings from a protein language model, what pooling and pairwise feature-construction steps can be used to turn variable-length residue embeddings into a fixed-size input suitable for a lightweight regression model, and what key information is lost or retained by this choice?",
      "answer": "A practical approach is to (1) average (mean-pool) the 1,024\u2011dimensional residue embeddings across sequence positions to obtain one fixed-length vector per protein, and then (2) concatenate the two protein vectors to form a single pair representation (e.g., 2,048 dimensions) for downstream modeling. Mean pooling discards position-specific/residue-level detail but retains an overall, global summary of biophysical trends captured by the language-model embeddings for each partner; concatenation keeps complementary information from both proteins while remaining computationally lightweight.",
      "source_document": "papers/2511.16113v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a sequence-only protein\u2013protein binding-affinity predictor that uses averaged per-protein ProtT5 embeddings, what is the purpose of optionally applying PCA before the interaction module, and what trade-off is it meant to probe?",
      "answer": "PCA is optionally applied as a dimensionality-reduction step to make the ProtT5 embedding representation more compact. It is used mainly to test whether compressing the embeddings (reducing feature dimensionality) affects predictive performance\u2014i.e., probing the trade-off between a smaller, computationally cheaper representation and potential loss of information that could reduce accuracy.",
      "source_document": "papers/2511.16113v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a coarse-grained elastic-network model that computes time-delayed (transfer-entropy) communication between residue fluctuations, what specific criterion indicates directional (nonreciprocal) allosteric information flow between two residues, and how can ranking residues by their total outgoing transfer be used to infer where signals originate and where they terminate in PDZ domains and TIM-barrel enzymes?",
      "answer": "Directional allosteric information flow is indicated when the time-delayed cross-correlation is asymmetric, i.e., \u27e8\u0394Ri(0)\u0394Rj(\u03c4)\u27e9 \u2260 \u27e8\u0394Rj(0)\u0394Ri(\u03c4)\u27e9; this nonreciprocity implies a net transfer from one residue to the other. To infer signal sources/sinks, one can compute each residue\u2019s total outgoing transfer to the rest of the protein by summing pairwise transfers (Ti\u2192\u2299(\u03c4)=\u2211j Ti\u2192j(\u03c4)) and then rank residues by this \u2018transfer capability.\u2019 In PDZ-2, the strongest emitters localize to helix \u03b11 and the \u03b22\u2013\u03b23 loop near the ligand-binding pocket, while the strongest receivers localize to the \u03b2-sheet region containing \u03b23/\u03b24/\u03b26/\u03b21, consistent with signal propagation along pathways that end at \u03b21. In the TIM-barrel (HisF-C9S), the best donors are predominantly peripheral, whereas the worst donors concentrate in the central/core region that contains catalytic residues (e.g., Asp11 and Asp130), indicating information flow from the outer regions toward the catalytic core.",
      "source_document": "papers/2511.16456v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a heterogeneous-graph model for cell-line\u2013specific drug synergy prediction, how can you ensure (i) the predicted synergy score is invariant to swapping the two drugs and (ii) the same drug can take different representations in different cellular contexts? Describe the two architectural mechanisms used and how they are applied at inference time.",
      "answer": "(i) Order invariance is achieved with a symmetric prediction module: for a drug pair (d_i,d_j) in cell line c_k, the model computes two scores using both input orders\u2014MLP([h\u2032_{d_i},h\u2032_{d_j},h\u2032_{c_k}]) and MLP([h\u2032_{d_j},h\u2032_{d_i},h\u2032_{c_k}])\u2014and averages them, so swapping the drugs yields the same final score.\n\n(ii) Context-specific drug representations are produced with a FiLM (Feature-wise Linear Modulation) mechanism conditioned on the triplet context. Modulation parameters are generated by an MLP from the concatenation of the propagated embeddings [h_{d_i},h_{d_j},h_{c_k}], yielding per-entity \u03b3 and \u03b2 vectors. Each embedding is then modulated as h\u2032 = \u03b3 \u2299 h + \u03b2 (applied to both drugs and the cell line), making the drug embeddings depend on the specific cell-line context (and partner drug) before scoring.",
      "source_document": "papers/2511.17695v3.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In drug-synergy prediction, you want to stress-test whether a model is truly learning transferable biological mechanisms rather than memorizing frequent drug pairs or cell-line\u2013specific patterns. Describe a suite of dataset splitting strategies that progressively increase this out-of-distribution difficulty, and for each split state exactly what is held out (e.g., unseen cell lines, unseen drug pairs, or unseen drugs).",
      "answer": "A progressively more challenging evaluation suite can be built with five splits:\n\n1) **Random split**: randomly partition all drug\u2013drug\u2013cell line samples into train/test (9:1), giving an in-distribution baseline.\n\n2) **CLine split**: split by **cell line**, so **all cell lines in the test set are completely unseen during training**, testing generalization to new cellular contexts.\n\n3) **DrugComb split**: split by **drug combination**, so **test-set drug pairs are entirely absent from training**, testing prediction for novel drug pairs.\n\n4) **DrugSingle split**: in the test set, **at least one drug in each combination has never appeared in training**, simulating generalization to new chemical entities.\n\n5) **DrugDouble split**: the most stringent; **both drugs in each test-set pair are completely absent from training**, testing performance on entirely novel drug combinations composed of unseen drugs.",
      "source_document": "papers/2511.17695v3.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are training a heterogeneous-graph model for cell-line\u2013contextualized drug synergy prediction, but your labeled \u201csynergistic\u201d edges (positive samples) are sparse and potentially incomplete. Describe a training-time strategy to mitigate class imbalance and a robustness/ablation protocol to test how sensitive the model is to missing positive edges. Based on this protocol, what qualitative conclusion can you draw about how much positive-edge sparsification the model can tolerate, especially in the hardest novel-drug (both drugs unseen) setting?",
      "answer": "Training-time, the framework addresses severe class imbalance by upsampling positive synergy samples with replacement to achieve a balanced 1:1 positive-to-negative ratio during training, while optimizing with BCEWithLogitsLoss and early stopping on validation AUROC.\n\nFor robustness to missing positives, it performs an edge-sampling ablation in which only a fraction of the positive synergy edges are kept (e.g., 60%, 40%, 20%) and the model is retrained/evaluated under multiple generalization splits (Random, CLine, DrugComb, DrugSingle, DrugDouble). The results show that substantial positive-edge sparsification causes only modest degradation overall, and in the hardest DrugDouble setting performance remains strong even when most positives are removed; with only 20% of positive edges, AUROC is essentially unchanged/slightly higher than the full model (\u224875.95% vs 75.74%). Qualitatively, the model can tolerate large losses of positive edges and appears to rely on a core subset of informative positives rather than dense positive connectivity.",
      "source_document": "papers/2511.17695v3.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are constructing a heterogeneous graph for cell-line\u2013contextualized drug synergy prediction using drug\u2013target edges, protein\u2013protein interaction edges, and cell line\u2013protein \u201cexpresses\u201d edges. Describe a principled procedure for (i) defining cell line\u2013protein edges from expression data using a quantile-based criterion, and (ii) selecting a computationally manageable but biologically important subset of proteins by combining PPI topology with expression prevalence. Specify the exact scoring components and how they are combined into a final importance score.",
      "answer": "(i) Cell line\u2013protein \u201cexpresses\u201d edges are created by ranking each gene\u2019s expression across all available cell lines and marking a protein as \u201chighly expressed\u201d in a given cell line if that gene\u2019s expression in the cell line exceeds 80% of other cell lines (i.e., is in the top 20% quantile). An edge is then added between that cell line node and the corresponding protein node.\n\n(ii) Proteins are selected using a composite importance score that integrates two PPI-topology measures plus an expression-prevalence measure:\n- PageRank centrality PR(p) on the PPI network (with damping factor d = 0.85).\n- Degree centrality Deg(p) = |N(p)| (number of PPI neighbors).\n- Cell line coverage Cov(p) = \u03a3_{c\u2208C} I[expr(p,c) > \u03b8], counting in how many cell lines protein p exceeds an expression threshold.\nThese are combined with log scaling for degree and coverage into a weighted score:\nS(p) = 0.6\u00b7PR(p) + 0.2\u00b7log(1 + Deg(p)) + 0.2\u00b7log(1 + Cov(p)).\nProteins are then ranked by S(p) and the top set (e.g., top 1,000) is retained to form the protein nodes, balancing pathway coverage and computational efficiency.",
      "source_document": "papers/2511.17695v3.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are given a rooted phylogenetic tree whose leaves have known taxonomic labels at a chosen rank (e.g., phylum). Describe how an entropy-based metric can quantify concordance between the tree\u2019s clade structure and the taxonomy, including what the extreme values of the scaled metric mean biologically.",
      "answer": "Compute Shannon entropy for (i) the taxonomy-defined groups at that rank (each leaf belongs to exactly one taxonomic unit): H_taxa = \u2212\u2211_i p_i log2 p_i, where p_i = n_i/N. Then, using the rooted, lineage-annotated tree, identify the clades corresponding to that rank (a node is a clade at a rank when its branch\u2019s taxonomic group rank is lower than its parent\u2019s), which also partition leaves; compute the clade entropy H_tree = \u2212\u2211_j q_j log2 q_j, where q_j = m_j/N and m_j is the number of leaves in clade j. Define a maximum clade entropy H_max corresponding to the worst case where each leaf is in a different taxonomic unit from its siblings. Scale to an entropy reduction ratio: \u0394\u0124 = (H_max \u2212 H_tree)/(H_max \u2212 H_taxa), with 0 \u2264 \u0394\u0124 \u2264 1. \u0394\u0124 = 1 indicates best concordance: all taxonomic units at that rank are monophyletic on the phylogenetic tree. \u0394\u0124 = 0 indicates worst concordance: effectively maximal mixing (each leaf is in a different taxonomic unit from its siblings).",
      "source_document": "papers/2511.17996v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In deep-learning-based analysis of molecular dynamics trajectories, what architectural constraint can be imposed so that the learned latent/representation dimensions form an isotropic space with identical scaling, and how does this constraint justify using a simple clustering method to partition kinetic states?",
      "answer": "Impose that the scalar order parameter used for dynamics\u2014here the learned effective energy\u2014is computed as the sum of all coordinates in the last hidden (representation) layer, with no activation on the output connection. Then the gradient of effective energy with respect to any representation dimension is constant (unity), so after training all dimensions share the same scale and exhibit intrinsic uniformity (isotropy). Because the representation is highly optimized and uniformly scaled, even a simple K-Means applied to vectors of the last hidden layer can stably partition the trajectory into kinetic states without fragmentation artifacts.",
      "source_document": "papers/2511.18010v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a comparative Mapper/TDA analysis of gene co-expression between control and Alzheimer\u2019s samples, how can you quantify and localize \u201cdiscontinuity hotspots\u201d (i.e., regions where AD co-expression topology diverges from the healthy reference), and what criterion can be used to select the genes for downstream functional enrichment?",
      "answer": "Construct a Mapper graph for the healthy (control) co-expression point cloud and assign each gene a numerical color from the healthy Mapper (a smooth reference gradient/labeling). Transfer these gene-level colors onto the AD Mapper graph built from the same gene set. Then, for each AD Mapper node, compute the standard deviation of the transferred color values of the genes it contains, \u03c3(n)=StdDev{color(g) | g\u2208n}, as a heterogeneity score: nodes with large \u03c3(n) indicate abrupt color jumps and thus localized topological misalignment/discontinuities relative to the healthy organization. Discontinuity hotspots are taken as the highest-variability nodes\u2014specifically, selecting the top 20% of AD Mapper nodes by \u03c3(n) within each brain region\u2014and genes contained in these nodes are extracted for GO Biological Process enrichment analysis.",
      "source_document": "papers/2511.18238v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using Mapper-based TDA to compare disease vs control gene co-expression, how can you (i) choose a distance metric and clustering method appropriate for correlation-based co-expression, and (ii) define an objective node-level score to select \u201cdiscontinuity hotspots\u201d whose genes should be carried forward to GO Biological Process enrichment\u2014while also guarding against missing discontinuities due to the orientation of the coloring/lens?",
      "answer": "(i) Convert the gene\u2013gene Pearson correlation matrix into a distance matrix with d = 1 \u2212 c, so highly correlated genes are close. Cluster points within each Mapper cover element using DBSCAN, which detects arbitrary-shaped dense clusters and is robust to noise.\n\n(ii) Treat the control (healthy) Mapper coloring as a \u201ccontinuous ground truth\u201d: assign each gene a numerical color from its control Mapper node (derived from the DBSCAN cluster labels), then transfer these gene-level colors onto the disease (AD) Mapper graph. For each disease Mapper node n, compute a heterogeneity score as the standard deviation of the transferred gene color values, \u03c3(n) = StdDev{color(g) | g \u2208 n}. Select nodes in the top 20% of \u03c3(n) as discontinuity hotspots, and use the genes contained in those nodes as the input list for GO BP enrichment.\n\nTo avoid missing discontinuities aligned orthogonally to the original 1D coloring gradient, apply a gradient-rotation relabeling: reorder cluster color labels by the vertical (second-coordinate) centroid of each cluster in the 2D lens space and reassign colors top-to-bottom, enabling detection regardless of orientation.",
      "source_document": "papers/2511.18238v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In spatial transcriptomics prediction from histology, how can you turn the otherwise combinatorial problem of choosing a useful subset of ~20,000 noisy auxiliary genes into a tractable, end-to-end learnable procedure that avoids overfitting to batch effects? Describe (i) the prior-knowledge ranking used, (ii) how the subset choice is relaxed to be differentiable, and (iii) how bi-level optimization updates the subset-size parameter using training vs validation data, including why the loss choice helps under batch effects.",
      "answer": "A tractable approach is to (i) rank all candidate auxiliary genes once using prior knowledge about signal quality\u2014specifically a highly-variable-gene (HVG) score computed from each gene\u2019s dispersion (variance divided by mean) with z-score normalization within mean-expression bins\u2014then sort genes by this score; (ii) replace hard subset selection with a temperature-controlled, differentiable soft top\u2011k mask \u03bb\u0303(k,\u03c4) so that a single scalar cutoff k (how many top-ranked genes are kept) can be optimized by gradient descent; and (iii) use bi-level optimization where, for a fixed k, the model weights \u03b8 are updated on training data for H inner steps using the combined loss of primary genes plus masked auxiliary-gene losses, then k is updated in an outer step by backpropagating the primary-gene loss on a separate validation mini-batch through the differentiable mask. This alternating \u201c\u03b8 on training, k on validation\u201d scheme learns k to minimize primary-gene validation error, reducing subset search to 1D. To mitigate batch-effect\u2013related scaling bias, both primary and auxiliary losses are based on a Pearson correlation coefficient loss, which is less sensitive to slide-to-slide scaling shifts than magnitude-based regression losses.",
      "source_document": "papers/2511.18336v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In adaptive kinetic proofreading (AKPR) models of T-cell receptor signaling, how does adding proofreading steps help resolve the biological problem that very abundant, very weak (self-like) ligands should not shut down responses to rare agonists? Explain in terms of how antagonism depends on the position (m) of the inhibitory feedback relative to the total number (N) of proofreading steps, and what class of antagonist ligands is predicted to produce the strongest antagonism.",
      "answer": "AKPR models include an inhibitory feedback triggered by an intermediate proofreading state m in an N-step kinetic proofreading cascade. For a mixture of agonists (L1, \u03c41) and antagonists (L2, \u03c42), the AKPR output can be approximated as CN = (\u03a3i Li \u03c4i^N)/(\u03a3i Li \u03c4i^m). Antagonism is quantified by a fold-change FC = Out(mix)/Out(Ag) and, in this approximation, antagonism occurs when \u03c42 < \u03c41. Critically, proofreading steps mitigate antagonism from very weak self-like ligands when the negative feedback is placed downstream (m>1): the FC curve becomes flat near \u03c42\u22480, so extremely weak ligands do not significantly antagonize. In contrast, when feedback is too early (small m, e.g., adaptive sorting with N=1), antagonism is too strong and would predict that the weakest ligands cause the strongest antagonism, which is inconsistent with the need to respond in the presence of abundant self. With larger m, AKPR predicts maximal antagonism for antagonists with binding times just below the activation threshold (\u03c42\u2248\u03c4c): the strongest antagonists are also weak agonists.",
      "source_document": "papers/2511.18626v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "Adaptive kinetic proofreading (AKPR) models are often analyzed by comparing the response to an agonist alone versus a mixture of agonist and subthreshold ligands. Define an appropriate quantitative metric for antagonism in such mixtures, and explain (i) how the AKPR output for a general ligand mixture can be approximated in terms of proofreading depth N and feedback position m, and (ii) what qualitative condition on ligand binding times determines whether antagonism occurs and which antagonists tend to be strongest.",
      "answer": "A convenient metric is the fold-change ratio\nFC = Out(mix)/Out(Ag),\nwhere Out(Ag) is the response to agonist ligands alone {L1,\u03c41} and Out(mix) is the response to a mixture { (L1,\u03c41);(L2,\u03c42) }. Antagonism corresponds to FC < 1 (equivalently log FC < 0).\n\nIn AKPR, for a mixture C = {(Li,\u03c4i)} the output at proofreading step N can be approximated as\nCN \u2248 (\u2211i Li \u03c4i^N) / (\u2211i Li \u03c4i^m),\nwhere the numerator is the KPR-like activating signal accumulated through N steps and the denominator represents an inhibitory feedback triggered at step m.\n\nFor two ligands, antagonism occurs when the added ligand has lower binding time than the agonist (\u03c42 < \u03c41). Proofreading mitigates antagonism by very weak self-like ligands (e.g., \u03c42\u21920 can give FC\u21921 in regimes with sufficient proofreading/late feedback), and AKPR predicts maximal antagonism for intermediate antagonist qualities\u2014typically ligands with binding times just below the activation threshold (weak agonists), rather than the very weakest binders. This behavior is promoted when the negative feedback is placed downstream (large m) of a proofreading cascade.",
      "source_document": "papers/2511.18626v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "High-throughput cytokine time-course measurements can be compressed into a low-dimensional latent space that still reconstructs the original data. In such a setting, what empirical relationship was observed between (i) the antagonist\u2019s own ability to elicit a minimal/partial response and (ii) its ability to antagonize an agonist in ligand mixtures across different molecular perturbations, and what normalization/rescaling was used to reveal this relationship as a single (collapsed) antagonism curve?",
      "answer": "Across perturbations, the mixture (agonist+antagonist) response shifted in coordination with the antagonist-only response: conditions that moved the onset of activation by antagonists also moved the point where antagonism ended. This was quantified by the alignment between the antagonist \u2018partial response EC50\u2019 (the antagonist quality where a minimal ~10% above-baseline response first appears) and the \u2018agonist crossover EC50\u2019 (the antagonist quality where the agonist-only and mixture responses become equal, i.e., antagonism disappears). When antagonist quality (EC50) was rescaled by dividing by the partial-response EC50 for each condition, the antagonism data from different perturbations collapsed onto a universal curve, revealing a strong link between antagonists\u2019 intrinsic signaling potential and their antagonistic strength in mixtures.",
      "source_document": "papers/2511.18626v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In high-throughput studies of T cell responses to many ligands, bulk cytokine time courses can be compressed into a low-dimensional latent space (\u201cantigen encoding\u201d). Explain (i) what criterion is used to justify that the latent representation retains the original cytokine information, and (ii) how changes in the shape/monotonicity of the second latent variable can be interpreted mechanistically in terms of the presence or absence of negative regulatory feedback (e.g., SHP-1\u2013mediated inhibition) that supports sharp ligand discrimination in adaptive kinetic proofreading\u2013type models.",
      "answer": "(i) The latent representation is justified by showing that the original high-dimensional cytokine dynamics can be reconstructed from the compressed (two-dimensional) latent trajectories, demonstrating equivalence of the latent variables to the initial data.\n\n(ii) In the wild-type system, the second latent dimension displays a non\u2011monotonic dependence on antigen quality, consistent with an underlying negative interaction/inhibitory feedback similar to the negative regulatory loops used in adaptive kinetic proofreading (AKPR). In a signaling-impaired, CD3\u03b6 ITAM\u2013deficient receptor, this second latent variable becomes a simple monotonic function of antigenic strength; this loss of non\u2011monotonicity is interpreted as loss/blunting of the negative regulatory loop. Biochemically, this aligns with reduced recruitment of the SHP\u20111 phosphatase, increased phosphorylation of CD3-associated chains, and increased ZAP70 recruitment\u2014consistent with weakened negative feedback and therefore reduced sharp antigen discernment.",
      "source_document": "papers/2511.18626v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In coarse-grained kinetic models of in vivo protein aggregation that include both aggregate formation and active clearance, what two mechanistic features are sufficient to produce (i) bistability between a low-aggregate \u201chealthy\u201d state and a high-aggregate \u201cpathological\u201d state and (ii) a seeding-triggered transition into the pathological state?",
      "answer": "Bistability with a seeding-triggered transition arises when the system has: (1) a self-replicating aggregation mechanism whose rate depends on monomer concentration, and (2) an aggregate removal (clearance) mechanism with limited capacity (i.e., it saturates/maxes out).",
      "source_document": "papers/2511.18893v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using a BWT-based index over a collection of highly similar genomes, exact matching can return thousands of suffix-array/BWT positions for a single biological locus. Describe a method to (i) compress these redundant occurrences into alignment coordinates and (ii) report, for a given BWT backward-search interval [l,r], the distinct alignment columns where the pattern occurs in time proportional to the number of reported columns (not to r\u2212l+1). What auxiliary arrays/bitvectors and query primitives are needed, and what is the key idea that makes the reporting output-sensitive?",
      "answer": "(i) Tag each BWT position with an MSA column via a TAG array: TAG[i] is the alignment column of the character immediately following BWT[i] in its original sequence (equivalently, if BWT[i] corresponds to some non-gap character in column c of some row, TAG[i] is the next column after c in that row containing a non-gap character). For similar genomes the TAG array is run-length compressible; boundaries of TAG runs are marked in a bitvector RB (RB[p]=1 iff p starts a TAG run), giving a run-length compressed TAG\u2032 storing one tag per run. For space, sample only 1/s of TAG runs: store sampled-run tags in an array L addressed by a bitvector S over TAG\u2032 runs; for unsampled runs encode the tag difference to a predecessor run reached by one LF step from the run head (run mapping e[i]=rank1(RB,LF[select1(RB,i)]+1)\u22121) using a unary-coded bitvector D. Querying an unsampled run recursively follows e[\u00b7] (at most s steps by construction) and adds back the encoded differences.\n\n(ii) To report distinct alignment columns for a BWT interval [l,r], first translate [l,r] to a run interval on TAG\u2032 using rank1 on RB (so the set of distinct tags is unchanged). Then apply a document-listing strategy (Muthukrishnan): conceptually maintain C[i]=previous occurrence index of TAG\u2032[i]. Use a succinct RMQ structure over C to find the position of the minimum C in a queried subinterval; if that minimum is < left boundary, its TAG\u2032 value is a new distinct tag to output, and recurse on the left/right subintervals around it; otherwise stop. To avoid storing C explicitly, equivalently track which TAG values have been output with a global static bitvector that can be reset proportional to the number of outputs. \n\nKey output-sensitivity idea: RMQ-driven recursion ensures the number of RMQ queries/recursion steps is proportional to the number of distinct TAG values reported, so runtime depends on output size rather than the raw interval length.",
      "source_document": "papers/2511.19068v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In MILP-based flow decomposition on cyclic assembly graphs, how can you encode a \u201csubset constraint\u201d stating that for each specified edge set Sj, at least one of the k source-to-sink walks must include every edge in Sj? Specify the auxiliary variables introduced and the key linear constraints that enforce the logic.",
      "answer": "Model each walk i with integer traversal variables x_{uv,i} (number of times edge uv is used). To express \u201cedge uv is present in walk i\u201d, introduce binary variables p_{uv,i} with p_{uv,i} = min(1, x_{uv,i}), enforced by:\n1) p_{uv,i} \u2264 x_{uv,i}  for all uv\u2208E, i\u2208{1,\u2026,k}\n2) x_{uv,i} \u2264 M3\u00b7p_{uv,i}  for all uv\u2208E, i\u2208{1,\u2026,k}, where M3 is a valid upper bound on x_{uv,i}.\n\nFor each subset constraint Sj and each walk i, introduce a binary indicator s_{i,j} that can be 1 only if walk i contains all edges of Sj. Enforce with:\n3)  \u03a3_{uv\u2208Sj} p_{uv,i} \u2265 |Sj|\u00b7s_{i,j}  for all i, j.\n\nFinally, require each Sj to be satisfied by at least one walk:\n4)  \u03a3_{i=1}^k s_{i,j} \u2265 1  for all j.\n\nTogether these linear constraints ensure: \u2200j, \u2203i such that x_{uv,i}\u22651 for every uv\u2208Sj (i.e., Sj is fully contained in at least one solution walk).",
      "source_document": "papers/2511.19153v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an MILP formulation for decomposing a weighted directed graph with cycles into k source-to-sink walks, how can you use the condensation DAG of strongly connected components (SCCs) and \u201csafe sequences\u201d to (i) safely fix some edge-traversal variables for a particular walk i to be at least 1 (or exactly 1), and (ii) infer that other edge-traversal variables for that same walk must be 0? State the conditions that determine each type of fixing, and explain why SCC boundaries matter.",
      "answer": "Use maximal safe sequences of edges that are guaranteed to occur (as a subsequence) in some walk of any feasible decomposition, and assign one such safe sequence S(e_i) to walk i. Work in the condensation graph G_SCC (DAG of SCCs):\n\n(i) Fixing x_{uv,i} to (at least) 1:\n- For every edge uv appearing in the safe sequence S(e_i), set the traversal variable x_{uv,i} based on whether uv crosses SCC boundaries.\n  \u2022 If uv connects two different SCCs, set x_{uv,i} = 1. This is safe because in the SCC condensation DAG an s\u2013t walk cannot traverse an inter-SCC edge more than once (G_SCC is acyclic).\n  \u2022 If uv lies inside a single SCC, set x_{uv,i} \u2265 mult_{S(e_i)}(uv), i.e., at least the number of times uv occurs in the safe sequence. This is safe because within an SCC a walk may repeat edges, so only a lower bound is implied by the safe sequence.\n- A corollary is that any x_{uv,i} corresponding to an inter-SCC edge can be declared binary (0/1), since it cannot be used more than once by any s\u2013t walk.\n\n(ii) Fixing x_{uv,i} to 0:\nAfter committing walk i to contain S(e_i), an edge uv cannot belong to walk i (so set x_{uv,i}=0) unless uv can be placed somewhere along an s\u2013t walk that contains S(e_i). The admissible placements are exactly those where at least one of the following reachability conditions holds:\n1) v can reach the first vertex of the safe sequence (uv could occur before S(e_i));\n2) the last vertex of the safe sequence can reach u (uv could occur after S(e_i));\n3) for some pair of consecutive edges ab and cd in S(e_i), b can reach u and v can reach c (uv could occur between those consecutive edges).\nIf none of (1)\u2013(3) holds, then uv cannot be spliced into any s\u2013t walk while still containing S(e_i) in order, hence x_{uv,i} can be fixed to 0.\n\nSCC boundaries matter because inter-SCC movement is constrained by the acyclicity of the condensation DAG (implying \u201cat most once\u201d usage and enabling equality/binarization), while inside SCCs walks can cycle/repeat, so only multiplicity lower bounds (not equalities) are implied and reachability-based exclusions must respect possible repetitions within SCCs.",
      "source_document": "papers/2511.19153v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In MILP formulations for decomposing an s\u2013t graph with cycles into k weighted s\u2013t walks, objective terms often include products like x_{uv,i}\\,w_i (edge-traversal count times walk weight), which are nonlinear. Describe the standard \u201cpower-of-two\u201d linearization used to model the product of an integer variable x (with known upper bound) and another variable y, including (i) the auxiliary binary variables introduced, (ii) the key constraints relating them to x, and (iii) how the remaining binary\u2013continuous products are linearized with additional variables and bounds. Also explain one graph-structural condition under which you can avoid this expensive linearization entirely by restricting some x_{uv,i} variables to be binary.",
      "answer": "To linearize a product x\u00b7y where x is an integer variable with known upper bound \\(\\bar x\\), set \\(t=\\lfloor \\log_2 \\bar x\\rfloor\\) and introduce binary variables \\(x_0,\\dots,x_t\\in\\{0,1\\}\\) such that\n1) \\(x=\\sum_{j=0}^{t} 2^j x_j\\).\nThen\n2) \\(x\\,y=\\sum_{j=0}^{t} 2^j (x_j y)\\), reducing the problem to linearizing each binary\u2013(general) product \\(x_j y\\).\nAssuming y is bounded in an interval \\([\\underline y,\\bar y]\\), introduce a new variable \\(z_j\\) for each j to represent \\(x_j y\\) and add standard big-M style linear constraints enforcing \\(z_j=x_j y\\) when \\(x_j\\in\\{0,1\\}\\):\n- \\(\\underline y\\,x_j \\le z_j \\le \\bar y\\,x_j\\)\n- \\(y-\\bar y(1-x_j) \\le z_j \\le y-\\underline y(1-x_j)\\).\nFinally substitute \\(x y\\) with \\(\\sum_{j=0}^t 2^j z_j\\) in the objective/constraints.\nA structural condition that can avoid this costly linearization is when an edge uv lies between two different strongly connected components (SCCs): because the SCC condensation graph is acyclic, such inter-SCC edges cannot be traversed more than once by any s\u2013t walk, so the corresponding traversal variables \\(x_{uv,i}\\) can be restricted to be binary \\(\\in\\{0,1\\}\\), removing the need to linearize products involving those x-variables.",
      "source_document": "papers/2511.19153v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In diffusion-based protein backbone generation, how can you enforce *exact* local backbone geometry (fixed bond lengths) while still improving global compactness, and what iterative two-step refinement procedure can adjust the Radius of Gyration without permanently violating the bond-length constraint?",
      "answer": "Use an internal-coordinate (torsion-angle) diffusion model that denoises dihedral angles (\u03d5, \u03c8, \u03c9) rather than Cartesian coordinates, then reconstruct coordinates with a differentiable forward-kinematics mapping that assumes a fixed adjacent C\u03b1 bond length b = 3.8 \u00c5. This makes the bond-length constraint hold by construction for any generated sample.\n\nTo improve global compactness, apply a post-sampling constrained refinement that iterates two steps:\n1) **Rg scaling**: compute the current radius of gyration Rcurr and scale all coordinates about the center of mass rcm by a factor s = 1 + \u03b7rg(Rtgt/Rcurr \u2212 1), i.e., C\u2032 = rcm + s\u00b7(C \u2212 rcm), moving the structure toward a target Rg Rtgt.\n2) **Bond restoration**: walk along the chain and correct each adjacent bond vector v so its length returns to 3.8 \u00c5, using vtgt = (v/||v||)\u00b73.8 and updating positions with a bond-correction step (learning rate \u03b7bond), repeating for multiple iterations.\n\nThis alternation compacts the overall fold while re-imposing the fixed bond length after each scaling operation.",
      "source_document": "papers/2511.19184v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a graph-based generative policy for molecule construction, how can you produce an intervention-based explanation that connects specific substructures to changes in a drug-likeness reward, and why is it useful to compute saliency on the terminal \u201cStop\u201d decision?",
      "answer": "One approach is to (1) compute atom-level saliency with Integrated Gradients on the log-probability of the Stop action, (2) aggregate high-saliency atoms into candidate motifs (connected components plus detected ring systems), and (3) apply chemically motivated counterfactual edits targeted to those motifs (e.g., ether\u2192thioether, methyl\u2192fluorine, chloro\u2192bromo, amide\u2192ester), then recompute the reward and report the reward change as \u0394QED = QED(m\u2032) \u2212 QED(m). This yields an intervention-based attribution: salient motifs whose edits produce the largest \u0394QED are interpreted as most influential for reward sensitivity.\n\nComputing saliency on the Stop action is useful because it occurs when the entire molecule is present and the model decides the structure is complete, allowing attribution to the full molecular graph (though it only explains the final step and not earlier intermediate decisions).",
      "source_document": "papers/2511.19264v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a graph-based generative policy for molecular design, how can you test whether the model\u2019s internal embeddings explicitly encode recognizable functional groups, and what evaluation setup indicates that these motifs are \u201clinearly decodable\u201d from the embeddings?",
      "answer": "Freeze the pretrained generative policy, extract pooled hidden embeddings h from a consistent layer/state, and train shallow supervised \u201cmotif probes\u201d to predict the presence/absence of SMARTS-defined motifs (functional groups, aromatic rings, halogens) labeled automatically with RDKit. Use a separate lightweight feedforward classifier per motif (standardizing embeddings; binary cross-entropy training with class-balanced sampling) and evaluate on a held-out test split using AUROC and average precision. High test AUROC (e.g., >0.9 for many motifs such as halogens, aromatic ring systems, and carbonyl-containing groups) indicates the motif information is readily accessible\u2014i.e., linearly/straightforwardly decodable\u2014from the embeddings by a shallow classifier.",
      "source_document": "papers/2511.19264v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You have a graph-based generative model for molecules trained with a drug-likeness reward (QED), and you want to show that its learned state embeddings are chemically grounded rather than an uninterpretable \u201cQED axis.\u201d How can you use a sparse autoencoder on frozen policy embeddings to test whether the embedding decomposes into interpretable physicochemical components, and what empirical pattern in downstream prediction would support the claim that QED is represented as a nonlinear combination of simpler axes (e.g., polarity/size/lipophilicity) rather than being directly linearly encoded?",
      "answer": "Train a sparse autoencoder (SAE) on frozen molecule/state embeddings h from the policy network (using a nonnegative sparse code z = ReLU(Wh + b) and reconstruction h\u0302 = W\u2032z + b\u2032, optimizing a loss with an \u21132 reconstruction term plus an \u21131 sparsity penalty to encourage axis-aligned factors). Then interpret factors by correlating each latent factor with standard molecular descriptors such as TPSA (polarity) and Crippen logP (lipophilicity), and evaluate how well the learned factors predict different reward/property signals.\n\nEvidence that the representation decomposes into interpretable physicochemical components\u2014and that QED is a nonlinear combination of them\u2014is the pattern that simple predictors trained on SAE factors achieve high predictive performance (high R\u00b2) for basic physicochemical properties like polarity and size (and also lipophilicity), while direct prediction of the composite QED score from the same factors is much harder (much lower R\u00b2). This gap supports the interpretation that QED is not captured as a single linearly decodable dimension, but is mediated by more linearly predictable axes (polarity/size/lipophilicity) that combine nonlinearly to produce QED.",
      "source_document": "papers/2511.19264v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building an ML model to predict how mutations shift the redox potential of a [4Fe\u20134S] cluster using electrostatic-field features, what symmetries/invariances should the input representation enforce, and what is the rationale for enforcing them in this setting?",
      "answer": "The representation should enforce (i) invariance to global rotations of the cluster/protein coordinate system and (ii) invariance to within-cluster atom permutations, specifically permutations within the Fe subgroup and within the S subgroup (reflecting the approximately D2d-symmetric [4Fe\u20134S] core). Enforcing these invariances regularizes learning by preventing the model from keying on arbitrary orientation or atom ordering, so predictions depend on physically meaningful geometric/electrostatic descriptors and can focus on mutation-induced/environment-induced distortions rather than coordinate conventions.",
      "source_document": "papers/2511.19423v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an agentic workflow that predicts how protein mutations tune the redox potential of a [4Fe\u20134S] cluster using electrostatic fields and geometry-derived descriptors, what training/regularization steps can you use to improve generalization and keep predictions numerically stable across features with different scales, and what is the role of a held-out validation set in this setup?",
      "answer": "Use training-set z-scoring (standardizing each feature component and the target using training-set statistics) so the network operates on normalized inputs while preserving the analytically enforced invariances; optimize with AdamW (decoupled weight decay) and include dropout; and apply early stopping that monitors validation loss on a held-out validation split and keeps the parameters achieving the minimum held-out error. The held-out validation set provides an unbiased signal for stopping/selection to prevent overfitting and choose the best-performing parameters.",
      "source_document": "papers/2511.19423v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using a MinHash-style sketch for k-mer\u2013based nucleotide search/dereplication, how does the sketch \u201cscale\u201d parameter affect (i) CPU/RAM usage and (ii) the minimum sequence length for reliable matching, and what concrete rule-of-thumb threshold implies that default settings can miss short sequences?",
      "answer": "Increasing the scale means retaining fewer hashes (a smaller sketch), which reduces CPU and RAM use but also reduces accuracy and raises the minimum sequence length where matching is effective. A stated rule of thumb is that the method works best on sequences about ~20\u00d7 longer than the chosen scale value; with the default scale of 100, sequences shorter than ~2000 bases are likely to be falsely missed (making it unsuitable for short reads unless the scale is lowered).",
      "source_document": "papers/2511.19769v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a transformer-based multi-omics integration model that keeps the backbone frozen and learns only lightweight modality-specific modules, what set of training objectives can be combined to (i) make embeddings discriminative for labeled tasks, (ii) preserve class-wise neighborhood structure, and (iii) enforce batch-/modality-invariant alignment? Name the objectives and briefly state the role of each.",
      "answer": "A composite multi-objective loss can be used that combines: (1) cross-entropy classification loss to make embeddings discriminative for the supervised labels; (2) supervised contrastive loss to preserve intra-class clustering while increasing inter-class separation (neighborhood/cluster structure); (3) a modality alignment loss to encourage consistent features across modalities (cross-modality invariance); and (4) an intra-class variance reduction loss to tighten the latent distribution for each label, improving robustness/generalization. In the described framework, these terms are weighted and optimized jointly while the transformer backbone is frozen and only the lightweight adapters/fusion components are trained, promoting batch-invariant alignments without overfitting the backbone.",
      "source_document": "papers/2511.20382v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In parameter-efficient multi-omics integration with a frozen transformer backbone, what are the four complementary loss terms that can be combined to (a) make embeddings predictive for labeled tasks, (b) preserve intra-class neighborhoods and separate classes, (c) align representations across different omics modalities, and (d) tighten each class\u2019 latent distribution to reduce within-class scatter? Name each term and state its role.",
      "answer": "A suitable composite objective combines four terms: (i) a cross-entropy classification loss to make embeddings discriminative/predictive for labeled tasks; (ii) a supervised contrastive loss to preserve intra-class clustering while increasing inter-class separation (neighborhood structure); (iii) a modality alignment loss to encourage consistent features across different omics modalities; and (iv) an intra-class variance reduction loss to tighten the latent distributions for each label (reduce within-class scatter).",
      "source_document": "papers/2511.20382v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a frozen-transformer PEFT model for single-cell multi-omics integration, how can a task-adaptive fusion module and an iterative refinement step be designed to (i) handle missing/weak modalities and (ii) remove batch effects without destroying biological signal? Describe the key computations and the intuition behind why each helps.",
      "answer": "A practical design uses (1) task-adaptive fusion and (2) residual, batch-aware iterative refinement.\n\n(i) Task-adaptive fusion: compute a per-modality embedding z_G from the frozen encoder, then form a fused representation as an element-wise weighted sum z_H = \\sum_G (v_G \\odot z_G), where v_G is a learnable attention/importance vector over features for modality G and \\odot is Hadamard product. Because weights are learned per task and per feature dimension, the model can upweight informative modalities and suppress noisy ones, which is especially helpful when some modalities are missing or weakly informative compared with naive concatenation/averaging.\n\n(ii) Batch-effect removal via iterative refinement: update the latent representation with a residual step that subtracts a learned batch embedding b_batch and then applies a feedforward refinement network, e.g., z^{(t+1)} = z^{(t)} + Refine(z^{(t)} \u2212 b_batch), repeated for multiple steps. Subtracting a batch-specific embedding targets technical shifts, while the residual formulation encourages only discrepancy correction (denoising/harmonization) and helps preserve task-relevant semantic/biological content.",
      "source_document": "papers/2511.20382v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a coupled mechano-chemical finite-element model of traumatic brain injury where ATP release is driven by tissue strain/strain-rate and downstream signaling is represented by diffusion\u2013reaction PDEs, how can you operationally define a \u201cchemical injury threshold\u201d and use it to build an injury map over loading conditions? Describe (i) which biomarker and spatial statistic you would threshold, (ii) how that threshold is linked to receptor activation, and (iii) what the resulting strain\u2013strain-rate injury curve represents.",
      "answer": "A practical definition of a chemical injury threshold is to use the model\u2019s extracellular ATP field and declare injury when ATP exceeds a receptor-activation cutoff. Specifically: (i) compute the maximum (spatial peak) extracellular ATP concentration over the domain for each imposed strain/strain-rate loading condition; (ii) compare that maximum ATP to a critical ATP level corresponding to P2X7 purinergic receptor activation (a cutoff of about 6\u00d710\u207b3 mM is used as the activation level); if the maximum ATP stays above this value the microenvironment is considered susceptible to pathway-driven chemical degradation (\u201cchemical injury\u201d); and (iii) by sweeping strain and strain-rate, record the maximum ATP at each point and plot it over the strain\u2013strain-rate plane to form an injury curve/map that partitions loading conditions into an \u2018injury\u2019 region (ATP above cutoff) versus an \u2018uninjured\u2019 region (ATP below cutoff), providing a first-order link from mechanical loading to downstream biomarker upregulation risk.",
      "source_document": "papers/2511.20392v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are designing a safe deduplication workflow for genomics storage where false positives could delete irreplaceable sequencing files. Describe a duplicate-detection strategy that (i) minimizes unnecessary I/O on large files, but (ii) only marks files as exact duplicates when there is cryptographic certainty. Include the sequence of filters/hashes used, how candidate clusters are formed, and what hash/blocking scheme is used for final verification.",
      "answer": "A safe, I/O-efficient strategy is a three-tier pipeline: (1) bucket files by exact byte size so different-sized files are never compared; (2) for files larger than 196 KB, compute a sampled fingerprint by hashing three 64 KB windows (first/middle/last) with MD5, concatenating the three 16-byte digests and re-hashing with MD5 to yield a single fingerprint; for files \u2264196 KB, use a single-window MD5 over the whole file. These MD5-based fingerprints are used only to cluster candidates (not to justify deletion). (3) For each candidate cluster, perform an exact match using a full-file cryptographic hash: stream SHA-256 over the entire content in 4 MiB read blocks, and label files as exact duplicates only when their 32-byte SHA-256 digests are identical.",
      "source_document": "papers/2511.20727v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a sequencing-data storage cleanup tool, how can you conservatively identify \u201cerasable intermediate\u201d files (i.e., files that can be removed without losing information) while preserving auditability and reproducibility? Describe the core decision rule, the kinds of file-type evidence used to avoid misclassification, and give at least three concrete examples of intermediate-file situations that would be flagged, including any dependencies needed for regeneration.",
      "answer": "Use a conservative, workflow-aware rule: only flag a file as erasable when an equivalent or superior downstream artifact exists from which the file can be deterministically regenerated. To avoid misclassification, classify files by inspecting header bytes (not just filename suffixes) and normalize multi-part extensions (e.g., FASTQ.GZ), then group related files by a sample stem to build a per-sample inventory. Apply conservative cases such as: (1) flag SAM if BAM/CRAM exists (SAM can be re-emitted), (2) flag uncompressed BAM when CRAM is retained (BAM is reconstructable, requiring the reference; dependency explicitly noted as CRAM and <ref.fa>), (3) flag redundant SRA when FASTQ(.gz) layers exist, and (4) flag trimmed FASTQ when raw FASTQ plus a manifest of trimming parameters exists. For each candidate, report rationale, stated fidelity (bit-equivalent or content-equivalent), explicit dependencies, and a ready-to-run regeneration command; do not delete automatically to keep the process auditable.",
      "source_document": "papers/2511.20727v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In model-based ancestral recombination graph (ARG) inference under the infinite-sites model, unrestricted recombination sampling can waste computation by generating events in non-ancestral material that do not affect the sample\u2019s genealogy. Describe a sampling strategy for recombination locations and breakpoints that preferentially generates only \u201cuseful\u201d events (i.e., avoids recombination types where one or both sides of the breakpoint contain no ancestral material), and explain why sampling the breakpoint uniformly on the *closure* of ancestral intervals still allows some events to fall in non-ancestral \u201choles.\u201d",
      "answer": "A practical strategy is: (1) sample a recombination edge with probability proportional to its (time) length; (2) sample the event\u2019s location on that edge conditional on the chosen edge; and (3) sample the breakpoint uniformly over the mathematical closure of the set of genomic intervals for which that edge is ancestral. By restricting the breakpoint to positions within the edge\u2019s ancestral material (in the closure sense), the breakpoint necessarily has ancestral material on both sides, so it avoids recombination types where only the left side is ancestral, only the right side is ancestral, or neither side is ancestral (types 3, 4, and 5). However, using the closure rather than the intervals themselves means the support can include points inside gaps (\u201choles\u201d) of non-ancestral material between ancestral sub-intervals; breakpoints landing in these gaps correspond to type 2 events (breakpoint placed in non-ancestral material even though ancestral material exists on both sides).",
      "source_document": "papers/2511.21124v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In protein sequence generative models, how can reinforcement-learning\u2013based fine-tuning steer generation toward high-scoring designs while preventing the tuned model from drifting too far away from the pretrained (natural-protein) distribution, and what concrete mechanism is used to enforce this constraint?",
      "answer": "RL-based fine-tuning treats the pretrained generator as a policy and updates it to maximize a scalar reward for the desired property, but adds an explicit constraint/penalty to limit deviation from the pretrained reference distribution. Concretely, this is implemented as KL-regularization (a KL-divergence penalty D_KL(\u03c0\u03b8||\u03c0ref) added to the objective/reward), and in PPO-style methods the update is further stabilized with a clipped surrogate objective (trust-region-like clipping of the policy ratio) while keeping the KL penalty to keep the new policy close to the reference model.",
      "source_document": "papers/2511.21476v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In inference-time control of protein generative models, how does Bayesian guidance combine a pretrained model\u2019s prior over sequences with an external property predictor or score to steer sampling toward a desired attribute, and what distribution is effectively reweighted?",
      "answer": "Bayesian guidance steers a fixed generative protein model by reweighting its output probabilities using Bayes-style updating: the model\u2019s prior over sequences (the likelihoods/probabilities encoded by the GMPD) is combined with external evidence such as a functional predictor or activity score, so sequences that score well under the external signal receive higher probability and are sampled more often. In other words, the probability distribution produced by the GMPD (its prior over sequences) is reweighted according to the property predictor/score to bias generation toward the target attribute.",
      "source_document": "papers/2511.21476v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In retrieval-augmented generation (RAG) for protein sequence design, what problem does adding retrieval solve compared with relying on a pretrained protein language model alone, and how is the retrieved information used during inference to influence the generated sequences?",
      "answer": "RAG addresses the limitation that a pretrained generative protein model only contains what it has encoded in its parameters, so it may lack task-specific functional/structural priors or broader evolutionary context needed for a design goal. At inference time it dynamically retrieves semantically related examples (e.g., homologous sequences) from large external databases and injects them into the model\u2019s context, which makes sampling more informed and enriches generation with functional/structural priors not explicitly represented in the base model (e.g., jointly retrieving homologs and modeling fitness to provide evolutionary context without explicit structural supervision).",
      "source_document": "papers/2511.21476v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When guiding a pretrained generative model to design proteins with properties that are rare in (or absent from) natural training data, why can both fine-tuning\u2013based control and inference-time steering fail in out-of-distribution regimes, and what two concrete mitigation strategies are proposed to improve reliability of designs far from the natural protein distribution?",
      "answer": "Both approaches can fail due to poor out-of-distribution generalization: inference-time control assumes the base model already learned structure in the relevant regions of sequence space and steering cannot create representations the model never learned; fine-tuning can also remain biased because curated fine-tuning datasets and in-silico scoring oracles often come from the same natural protein distribution and inherit its evolutionary/data biases. Two proposed mitigations are (1) incorporating physics-based scoring functions (e.g., RoseTTAFold and FoldX) into reinforcement-learning fine-tuning to reduce reliance on purely natural-distribution signals, and (2) using lab-in-the-loop frameworks with experimental testing/validation (supported by lab automation and scaled experimental characterization) to reliably evaluate designs as models explore farther from natural proteins.",
      "source_document": "papers/2511.21476v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a protein-motif localization pipeline that scans a sequence with overlapping windows, embeds each window with a pretrained protein language model, and scores motif presence by the inner product between the window embedding and a motif-specific concept vector, what specific evaluation procedure can be used to convert the ranked window scores into precision/recall/F1\u2014i.e., how are (i) the number of predicted windows per protein chosen and (ii) true positives defined in terms of overlap with curated motif intervals?",
      "answer": "Use a top-k window selection per protein, where k is set equal to the number of ground-truth motif instances in that protein. Then, define a predicted window as a true positive if its overlap with a ground-truth motif exceeds a chosen threshold, with overlap computed as |prediction \u2229 ground truth| / |ground truth| \u00d7 100 (and evaluate precision/recall/F1 across different overlap thresholds).",
      "source_document": "papers/2511.21614v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a motif-localization pipeline that learns a Concept Activation Vector (CAV) for each Pfam motif by training a linear classifier on window embeddings from a pretrained protein language model, how are (i) positive and negative training windows constructed and (ii) window embeddings computed before logistic regression\u2014and what is the rationale for embedding each window as a standalone sequence rather than embedding the full protein and then slicing out windows?",
      "answer": "(i) Training windows are built per motif as fixed-length subsequences: positive windows are extracted by centering a window of length w on each annotated motif instance, where w is set to the median annotated motif length plus a small buffer to allow natural length variation across homologs. Negative windows are the same length w but sampled from proteins that do not contain the motif, yielding a balanced positive/negative dataset.\n\n(ii) Each subsequence window is embedded with a pretrained PLM (ESM-C) at a chosen layer m, producing residue embeddings; these are reduced to a single fixed-size window vector using mean-pooling (the average of the residue embeddings across the window). These pooled window vectors are then used to train a logistic-regression classifier, whose normal vector toward the positive class is taken as the motif\u2019s CAV.\n\nRationale for embedding windows independently: motifs are local, and analyses of ESM models indicate that structural signals (e.g., residue\u2013residue contacts) are encoded through local sequence windows; embedding each window as a standalone sequence aims to capture local motif-associated signals without interference from distant regions of the protein that would be present if embedding the whole protein first.",
      "source_document": "papers/2511.21614v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using a pretrained protein language model (PLM) to build linear \u201cconcept vectors\u201d for motif localization, why might embeddings from mid-network layers be preferred over very early or very late layers, and what qualitative layer-wise pattern would you expect to observe in motif-alignment score profiles across the network?",
      "answer": "Mid-level PLM layers tend to yield the strongest, most discriminative motif signal: early layers show weak motif-specific structure, middle layers (roughly the low-to-late 20s; described as ~layers 21\u201331) produce sharper, higher signal-to-noise alignment peaks at motif locations, and the deepest layers (final few layers, e.g., ~32\u201336) show diminished performance again. This is interpreted as middle layers best balancing local biochemical features with longer-range dependencies before later layers collapse information into more abstract/task-summary representations.",
      "source_document": "papers/2511.21614v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In tertiary-structure\u2013based RNA inverse folding, how can you design a fitness evaluation that avoids spending expensive 3D folding/RMSD computations on obviously wrong candidates, while still tolerating non-canonical tertiary interactions? Describe a two-stage strategy that uses a secondary-structure filter (including when it can be imperfect) followed by a tertiary-structure metric, and state what structural alignment/atoms are used for the RMSD calculation and what complementary metric can be reported for global similarity.",
      "answer": "A practical approach is a two-stage fitness evaluation:\n\n1) **Fast secondary-structure filter via base-pair distance (BPD).** For a candidate sequence S, predict its secondary structure with ViennaRNA and compute the base-pair distance to the target secondary structure extracted from the target 3D structure. Candidates with mismatching base pairs are filtered out (assigned effectively infinite/very poor fitness) so you do not run expensive 3D folding. This filter can be **imperfect** when the target contains **wobble or non-canonical interactions** (e.g., Leontis\u2013Westhof classes): because ViennaRNA dot\u2013bracket encodes only canonical/wobble pairs, BPD may remain nonzero even for biologically consistent folds; small deviations can be tolerated at the filtering stage.\n\n2) **Expensive tertiary evaluation via RMSD after 3D folding.** Only for sequences that pass the secondary filter, fold the sequence with RhoFold and compute RMSD to the target 3D structure after optimal superposition using US-align (Kabsch least-squares alignment). The RMSD uses aligned atoms including the **backbone phosphorus (P)**, **sugar C4\u2032**, and **base nitrogen atoms involved in pairing** (**N1 for pyrimidines, N9 for purines**).\n\nA complementary global-similarity metric that can be reported alongside RMSD is **GDT-TS**, which averages the fractions of residues within 1, 2, 4, and 8 \u00c5 after superposition.",
      "source_document": "papers/2511.21781v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a metaheuristic RNA inverse-folding pipeline that uses an Artificial Bee Colony\u2013style population search, how can you keep exploration broad early but still encourage convergence later? Describe a concrete mechanism that (i) makes the mutation rate depend on the best structure-matching score seen so far and the sequence length, (ii) defines how many positions are mutated per move, and (iii) defines a Boltzmann-style onlooker selection probability with a temperature schedule that changes over iterations.",
      "answer": "Use an adaptive mutation schedule tied to the current best RMSD and sequence length, and a Boltzmann (softmax) onlooker selection rule with an iteration-dependent temperature.\n\n(i) Adaptive mutation rate: set an initial rate and decay it as better RMSD is achieved, scaled by length n:\n\nmutation rate = max(0.1, 0.095 \u00b7 exp(\u2212 best_RMSD / (5n))).\n\n(ii) Number of mutations per neighbor: mutate max(1, \u230amutation_rate \u00b7 n\u230b) positions (chosen randomly) to generate a neighbor sequence.\n\n(iii) Onlooker selection probability: for a candidate with RMSD r_i, select it with\n\np_i = exp(\u2212r_i/\u03c4) / \u03a3_j exp(\u2212r_j/\u03c4),\n\nwhere the temperature increases with iteration t over a total of T = 40 iterations, e.g.\n\n\u03c4 = 5.0 \u00b7 (1 + t/T),\n\nto modulate exploration vs exploitation across the run.",
      "source_document": "papers/2511.21781v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In voxel-based 3D CNN models for protein\u2013ligand binding affinity prediction, why can a \u201cshape-only\u201d (no atom identities) or density-derived input outperform an atom-type one in low-data training, and what does this imply about the dominant predictive signal in that regime?",
      "answer": "In low-data settings, atom-type inputs have many channels (separate element-type channels for ligand and pocket), making the voxel grids higher-dimensional and sparser, which increases overfitting risk when only ~100 complexes are available. By contrast, shape-only and density-derived grids emphasize where matter occupies space (steric/shape complementarity) and can reach near-peak performance even with little data. This implies that, for bound-state PDBbind complexes, geometric occupancy\u2014how well the ligand fills the pocket, reflecting largely hydrophobic/shape-complementary interactions\u2014can dominate the predictive signal for affinity in the low-data regime, while atom-type\u2013specific interactions add limited benefit and may hurt due to sparsity/overfitting.",
      "source_document": "papers/2511.21900v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building train/validation/test splits for protein\u2013ligand binding affinity prediction, what concrete similarity rules can you apply to reduce train\u2013test leakage from near-duplicate receptors and ligands, and how can you further construct a \u201charder\u201d test set that probes generalization across protein families?",
      "answer": "Use splits constrained by both receptor sequence identity and ligand similarity so that closely related complexes are grouped together: assign two complexes to the same split only if (1) their receptor sequence identity is >50%, or (2) their receptor sequence identity is >40% **and** their ligands have Tanimoto similarity >0.9. To make the test set more challenging and diverse across protein families, reserve for testing all targets that are similar to those in the DEKOIS 2.0 benchmark (a broad, multi-family target set), thereby reducing overlap with training targets and probing out-of-family generalization.",
      "source_document": "papers/2511.21900v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When predicting DFT-derived quantum properties (e.g., dipole moment, polarizability, HOMO/LUMO energies) from *approximate* electron densities computed at a cheaper semiempirical level, what are the two main sources of approximation error you must account for, and what empirical outcome indicates that voxelized density still carries useful electronic-structure signal beyond atom types/shape?",
      "answer": "Two main approximation errors are (1) a mismatch in the level of theory between inputs and labels\u2014QM9 targets come from DFT, while the input electron densities are generated with the lower-accuracy semiempirical GFN2-xTB method (which can introduce systematic bias, e.g., overestimated dipole moments), and (2) discretization error introduced by voxelizing a continuous electron-density field onto a grid. Despite these limitations, models using density-derived voxel inputs (raw density or density gradient magnitude) outperform atom-type and shape-only voxel representations when trained on the full QM9 dataset, showing that voxelized density retains informative electronic-structure features not captured by atom identity or geometry alone.",
      "source_document": "papers/2511.21900v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a stochastic hybrid (piecewise-deterministic) model of single-cell growth with division as random jump events, how can you set up a maximum-likelihood estimator for division-control parameters using time-lapse microscopy data, and what key conditional-independence assumption makes it possible to estimate parameters at each time point independently (even when parameters vary over time)?",
      "answer": "Use the per-cell observations at a given time point tn as data X = {(\u00b5(tn), s(tn), \u2206(tn))i} together with an indicator of whether each cell divides in the short interval (tn, tn+1). Define a division probability over that interval from the division propensity, then write a per-cell log-likelihood Li(\u00b5,\u2206,s;\u03b8) that takes one form if division occurs and another if it does not; the total log-likelihood is the sum over cells, and the MLE is \u03b8* = argmax\u03b8 \u2211i Li.\n\nA key assumption enabling time-local inference is that the division log-likelihood depends only on the current values of s, \u2206, and \u00b5 at time tn (and whether division occurs in (tn,tn+1)), and not on the size at birth sb or any past history. Under this Markov/time-local dependence, all cells observed at the same time point are assumed to share the same parameter values, so parameters can be estimated independently at each tn even if they change over time.",
      "source_document": "papers/2511.22145v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When predicting mutation-induced binding free-energy changes (\u0394\u0394G) for protein\u2013nucleic acid complexes, why is it important to evaluate a model using a complex-based split (i.e., no PDB complex appears in both train and test), and what does performance under this split indicate about robustness and data leakage?",
      "answer": "A complex-based split prevents data leakage by ensuring that mutations from the same PDB complex cannot appear in both training and test sets; otherwise the model could exploit complex-specific structural patterns rather than learning generalizable mutation\u2013energetics relationships. Under this no-overlap split (train mutations from 351 PDBs, test from 88 unique PDBs), the model still achieved reasonable accuracy (average PCC 0.47 with MAE 0.66 kcal/mol and RMSE 0.83 kcal/mol; best fold PCC 0.54), indicating it retains predictive power on entirely unseen complexes and is therefore robust beyond within-complex memorization.",
      "source_document": "papers/2511.22239v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a graph neural network that predicts mutation-induced binding free-energy changes (\u0394\u0394G) for protein\u2013nucleic acid complexes, what design choices let the model explicitly distinguish (i) atoms belonging to the mutated residue, (ii) the rest of the protein, and (iii) the nucleic acid, and how are different interaction contexts between these groups encoded and weighted during message passing?",
      "answer": "Use a heterogeneous, atomic-level graph centered on the mutation site. Nodes are atoms with basic chemical features and an explicit node-type label with three categories: atoms of the wild-type (mutated) residue, atoms of other protein residues, and nucleic-acid atoms. Edges are created from covalent bonds and from spatial proximity (atom pairs within 4 \u00c5) and are assigned one of six relation types capturing interaction context: wild-type\u2013wild-type, wild-type\u2013other-protein, wild-type\u2013nucleic-acid, other-protein\u2013other-protein, other-protein\u2013nucleic-acid, and nucleic-acid\u2013nucleic-acid. Message passing is done with an edge-aware RGCN: each relation has its own transformation (implemented via basis decomposition), and each edge instance receives a learned importance weight computed from a trainable embedding of its edge type passed through a small MLP with sigmoid. The incoming messages are scaled by this edge weight and normalized by neighbor degrees before aggregation, with residual/self-connection terms to stabilize deep propagation.",
      "source_document": "papers/2511.22239v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multimodal model that predicts mutation-induced binding free-energy changes (\u0394\u0394G) for protein\u2013nucleic acid complexes, what evidence would you use to justify fusing (concatenating) a graph-based structural encoder with a protein language-model sequence embedding, and how can an ablation study quantify the complementary contribution of each modality (including the effect of using relational/typed edges rather than an untyped GCN)?",
      "answer": "Justification comes from showing that each modality alone is insufficient and that combining them improves predictive accuracy. An ablation study can quantify this by training/evaluating: (i) a structural-only model using the edge-aware RGCN on the local atomic graph, (ii) a sequence-only model using ESM-2 embeddings from the local amino-acid window, and (iii) a fused model that concatenates the two representations. In this document\u2019s results, RGCN alone achieved an average PCC of 0.52, ESM-2 alone achieved 0.73, and concatenating them increased PCC to 0.76 while reducing error (MAE \u22480.51 kcal/mol, RMSE \u22480.64 kcal/mol), indicating complementary information from structure and sequence. The ablation also compared a simple untyped GCN (all edges treated the same) to a relational model with categorized/typed edges, and introducing relational edge types improved five-fold cross-validation performance by about 2.70%, supporting the value of relational/edge-typed structural encoding in addition to multimodal fusion.",
      "source_document": "papers/2511.22239v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a diffusion-based protein backbone generator, you want to steer the final secondary-structure composition (e.g., more helices vs. more strands) using interpretable internal features learned by a sparse autoencoder (SAE). Describe a principled intervention rule that uses (i) the sign of each SAE feature\u2019s correlation with the target property and (ii) a single scalar steering strength parameter to modulate activations during denoising. How does the rule behave for steering strength values of 0, positive, and negative, and how are positively vs. negatively correlated features scaled?",
      "answer": "A principled rule is to intercept the chosen block\u2019s activations during denoising, encode them with the SAE, and then multiplicatively modulate only the subset of SAE latents identified (e.g., via probing) as correlated with the target property. Introduce a scalar steering hyperparameter \u03bb controlling direction and magnitude: \u03bb=0 applies no intervention; \u03bb>0 steers toward the target; \u03bb<0 steers away (toward the opposite of the target). For each latent feature, multiply its activation by (1+\u03bb) if it is positively correlated with the target property, by (1\u2212\u03bb) if it is negatively correlated with the target, and by 1 if it is not selected/neutral; then decode the modified latents back to the activation space and continue the diffusion process.",
      "source_document": "papers/2511.22519v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When steering a diffusion-based protein backbone generator by routing a block\u2019s activations through a sparse autoencoder (SAE), why can the SAE reconstruction itself degrade downstream generation, and what concrete error-offsetting procedure can be used so that the only net change is the intended feature intervention (not the SAE\u2019s baseline reconstruction error)? Describe the steps and the final formula for the activations returned to the model.",
      "answer": "SAE reconstruction introduces reconstruction error; if the reconstructed activations are propagated to later blocks, this error causes a distribution shift that can degrade downstream performance. To ensure the net effect reflects only the intervention, compute and add back the baseline reconstruction error: (1) reconstruct the original activations \u0393 with no intervention to get \u007f\u0393\u0302; (2) compute the original reconstruction error E = \u0393\u0302 \u2212 \u0393; (3) reconstruct \u0393 with the intervention applied to get \u0393\u0302\u2032; (4) return the intervened reconstruction offset by the original error, i.e., \u0393\u0302\u2032 + E.",
      "source_document": "papers/2511.22519v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You want to identify a sparse set of internal directions in a protein-backbone diffusion model that causally separate helix from strand formation, and then use them for controllable generation. Describe a complete, methodologically sound pipeline that (i) localizes which network block to intervene on using block ablation, (ii) trains a sparse autoencoder on that block\u2019s per-residue activations (including how the activations are \u201cpatched\u201d and what top-K sparsity means), and (iii) selects helix-vs-strand discriminative SAE features using one-vs-rest logistic probes\u2014what coefficient sign/threshold pattern indicates a feature that differentiates helix from strand?",
      "answer": "Pipeline:\n(i) Localization by iterative block ablation: for each block m, \u201cablate\u201d it by substituting its output with the output of the previous (m\u22121) block, then measure how much a score function S for the desired property (e.g., helix content) changes. Choose the intervention block m* that maximizes S(M_orig) \u2212 S(M\\m), i.e., the ablation that produces the largest change in property strength.\n(ii) SAE training on localized block activations: take the chosen block\u2019s activations, flatten them (length l\u00d7d for l residues, embedding dimension d), then split into l sequential d-dimensional patches so each residue provides one input vector x\u2208R^d. Train a top-K sparse autoencoder with a single-layer encoder/decoder: z = TopK(ReLU(W_enc(x\u2212b))) and x\u0302 = W_dec z + b, where TopK keeps only the k largest latent activations per example and zeros the rest, enforcing sparsity. Train by minimizing reconstruction error ||x\u2212x\u0302||^2.\n(iii) Feature selection via probing: build a dataset by generating proteins with the SAE integrated, caching SAE encoder activations per residue (and timestep), and labeling residues with secondary structure (e.g., via Stride). Train one-vs-rest logistic regression probes (helix vs rest and strand vs rest, with class weighting and a held-out test split). To find helix-vs-strand discriminative SAE features, compare the two OvR probes and select feature indices where both probes\u2019 coefficients exceed a chosen magnitude threshold and have opposite signs\u2014e.g., strongly positive for helix probe and strongly negative for strand probe (or vice versa). This opposite-sign, above-threshold pattern indicates a shared latent feature that differentiates helix from strand formation.",
      "source_document": "papers/2511.22519v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When generating fixed-length embeddings from pretrained transformer encoders for DNA/protein sequences, what strategy is used to handle sequences that exceed the model\u2019s positional/input length limits, and what is a key modeling drawback of this strategy that should be considered when interpreting downstream results?",
      "answer": "Long sequences are handled with safeguards such as safe truncation or chunking (e.g., chunking/sliding windows) before pooling the resulting token embeddings into a per-sequence vector. A key drawback is that transformer models have fixed positional capacities, so chunking/sliding-window processing can attenuate long-range dependencies\u2014meaning signals that depend on very long-distance interactions in the sequence may be weakened or missed in the resulting embedding.",
      "source_document": "papers/2511.22821v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You generate a sequence-embedding feature matrix and see that many feature columns have near-zero variance across sequences, while a smaller set have extremely high variance. What conclusions can you draw from each case, and what downstream preprocessing choices are suggested to make subsequent ML models more stable and informative?",
      "answer": "Near-zero variance features carry little discriminatory information and can be removed (feature pruning/variance filtering). Extremely high variance features may reflect biologically meaningful heterogeneity but can also indicate artifacts from the feature extractor\u2019s settings; they warrant inspection and may motivate additional preprocessing such as normalization/clipping and/or dimensionality reduction (e.g., PCA) depending on the goal.",
      "source_document": "papers/2511.22821v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You generate a fixed-length feature matrix from biological FASTA sequences using an embedding-based extractor, and the QC diagnostics show (i) a long right tail in the per-sequence L2-norm distribution and (ii) many sequences with extremely small L2 norms. What do these two patterns typically indicate about the extracted representations, and what corrective preprocessing steps are recommended before downstream modeling?",
      "answer": "A long right tail in the per-sequence L2-norm distribution indicates that a subset of sequences produce unusually large-magnitude feature vectors (outliers or scaling drift) and can dominate downstream models; recommended corrections include per-row (per-sequence) normalization or clipping. Extremely small L2 norms indicate near-empty representations, which can arise from filtering or tokenization mismatches; these should be investigated as extractor/input issues and can also be mitigated by appropriate normalization after correcting the underlying mismatch.",
      "source_document": "papers/2511.22821v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You extract a feature matrix from multi-FASTA sequences and, after normalization, a heatmap preview shows strong horizontal bands and block-like structures. What do these two patterns typically indicate about the data, and what preprocessing actions should you consider before fitting downstream models?",
      "answer": "Horizontal bands suggest duplicated or highly similar sequences, while block-like patterns indicate correlated groups of features (e.g., shared motif signals). Before downstream modeling, consider preprocessing such as deduplication of sequences and correlation-based feature filtering/pruning (and more generally additional feature pruning) to reduce redundancy.",
      "source_document": "papers/2511.22821v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When learning graph-based embeddings from biological sequences, what design choices in graph construction and embedding (random-walk) hyperparameters most strongly control the runtime\u2013fidelity trade-off, and why do these choices make model performance sensitive to heuristics?",
      "answer": "Graph-based sequence embeddings depend heavily on (1) how the graph is constructed\u2014e.g., defining k-mer co-occurrence, choosing edge weights, and applying thresholding\u2014which directly affects graph density, and (2) the random-walk settings used to learn embeddings (e.g., window length and number of walks), which trade increased runtime for higher representation fidelity. Because these steps are based on construction heuristics, embedding quality can vary substantially with graph density and the chosen co-occurrence/weighting/threshold rules, making performance sensitive to those heuristic choices.",
      "source_document": "papers/2511.22821v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When inferring pathogen source\u2013sink dynamics from highly uneven genomic surveillance data, what bias-correction strategy can be used to make regions comparable in their ability to detect newly emerging low-frequency mutations, and how does the approach then order regions along the global circulation pathway without relying on a phylogenetic tree?",
      "answer": "Use an Equal Power Sampling (EPS) design to equalize statistical power across region\u00d7time strata: target a sample size that yields ~90% power (5% type I error) to detect rare variants when mutations first emerge (e.g., n\u22481705 for prevalence \u03c1=0.005), then switch to a smaller sample size once prevalence is higher (e.g., n\u2248163 for \u03c1=0.05); if a stratum has fewer sequences than the target, include all, and repeat random sampling B times to reduce Monte Carlo noise. After estimating mutation prevalence under EPS, infer circulation order via rank statistics: for each key mutation, define the \u2018first-detection with continuous circulation\u2019 sequence across regions, assign ranks (source is rank 1; sink is last), normalize ranks when the mutation is not observed in all regions, and aggregate rank frequencies/probabilities across many mutations (and resamples) to estimate each region\u2019s probability of being the source, intersections (e.g., ranks 2\u20133), or sink\u2014without reconstructing genealogies or introduction times.",
      "source_document": "papers/2511.22841v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a phylogeny-free surveillance framework that ranks regions by when key mutations are first observed, how can you turn early detection in presumed \u201csource\u201d regions into a quantitative early-warning rule for mutations that will later dominate elsewhere? Specify (i) the prevalence thresholds used to define \u201cdetected in the source\u201d versus \u201cpredominant in another region,\u201d and (ii) the resulting conditional probability and typical lead time reported for receptor-binding-domain amino-acid substitutions.",
      "answer": "Use a two-threshold rule: treat a mutation as \u201cdetected in the source\u201d once its prevalence exceeds 1% (screening threshold \u03c11 = 0.01) in the top source regions, and treat it as \u201cpredominant\u201d in another region once its prevalence exceeds 50% (predominance threshold \u03c12 = 0.50). With this rule applied to receptor-binding-domain amino-acid substitutions, the conditional probability that mutations passing the 1% source threshold later become predominant in at least one other region is 80.0%, with a median lead time of about 2 months (IQR 1\u20134 months).",
      "source_document": "papers/2511.22841v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a spectral (Koopman/resolvent) approximation of an exponentially ergodic stochastic reaction network, how can the dominant decay modes be identified from data using a convex optimization problem, and what constraints on the optimization variables ensure that the inferred modes correspond to stable (negative-real-part) generator eigenvalues?",
      "answer": "Choose a set of observables F and a set of complex frequencies S (with Re(s)>0), estimate the needed iterated resolvent evaluations R_s^m f(x) and stationary expectations E_\u03c0(f) from CTMC simulations, and then fit the coefficients \u03b2=(\u03b21,\u2026,\u03b2J) of a polynomial whose roots encode the (unknown) decay modes.\n\nFor fixed J, define the resolvent-based \u201cerror\u201d\nE_{f,J}(x,s) := ( \u03a0_{j=1}^J (s(R_s\u2212I)+\u03c3_j R_s) / \u03a0_{j=1}^J \u03c3_j ) f(x) \u2212 E_\u03c0(f),\nand reparameterize it by the polynomial\n1 + \u03a3_{j=1}^J \u03b2_j s^j = \u03a0_{j=1}^J (s+\u03c3_j)/\u03a0_{j=1}^J \u03c3_j,\nwhich yields an explicit form\nE_{f,J}(x,s) = R_s^J f(x) \u2212 E_\u03c0(f) + \u03a3_{j=1}^J \u03b2_j s^j C^{(s)}_{j,J}(f,x),\nwith C^{(s)}_{j,J}(f,x) a linear combination of iterated resolvents.\n\nMinimize the worst-case normalized L2(\u03c0) error over the chosen s and f:\nC_J(\u03b2) = max_{s\u2208S} max_{f\u2208F}  ||E_{f,J}(\u00b7,s)||_{L2(\u03c0)} / ||f||_{L2(\u03c0)},\nvia the convex program  min_{\u03b2\u2208R_+^J} C_J(\u03b2).\n\nAfter solving, form P_{\u03b2*}(s)=1+\u03a3_{j=1}^J \u03b2*_j s^j; the (negated) roots of this polynomial give the estimated decay modes \\bar{\u03c3}_1,\u2026,\\bar{\u03c3}_J. The constraint \u03b2\u2208R_+^J (strictly positive orthant) enforces, by the Routh\u2013Hurwitz criterion, that the polynomial\u2019s roots have strictly negative real parts, equivalently that the inferred decay modes have strictly positive real parts and thus correspond to stable generator eigenvalues.",
      "source_document": "papers/2511.23114v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When estimating cross-spectral densities (CSDs) between two observables from continuous-time Markov jump process trajectories, what are the two main sources of error/inefficiency in the common SSA + DFT approach, and what methodological change enables a Koopman/resolvent-based spectral method to avoid both?",
      "answer": "In the SSA\u2013DFT approach, the CTMC trajectory over [0,T] must be uniformly time-discretized with step size \u03b4=T/N before applying a DFT. This introduces (1) discretization/aliasing-related bias and limits frequency resolution to the discrete grid of sampled frequencies determined by \u03b4 and T, and (2) potentially high variance unless a large ensemble of SSA trajectories is simulated, making it computationally expensive. A Koopman/resolvent-based spectral method (SKA) avoids both by deriving an explicit frequency-domain expression for the CSD directly from an approximate Koopman operator, so no time discretization/DFT is required and reliable accuracy can be achieved with smaller sample sizes.",
      "source_document": "papers/2511.23114v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In single-cell experiments you might observe only the protein copy-number distribution at many time points and want to infer parameters of the *initial-state distribution* of a stochastic gene-expression SRN. Explain how a Koopman spectral approximation can make this inference problem computationally cheaper than directly matching moments at every measurement time point: what quantities are first estimated from data, how do they enter a reduced objective, and why does the complexity scale with the number of decay modes rather than the number of sampled times?",
      "answer": "Using a spectral (Koopman) approximation, the expected observables (e.g., protein moments) are represented as a stationary term plus a finite sum of exponentials with state-independent decay modes: for moments of protein X2 at times tn,\n\nE_{P_\\eta(x)}[X_{x,2}^k(t_n)] \\approx E_\\pi(f_k) + \\sum_{j=1}^J \\bar\\alpha_j(f_k,\\eta) e^{-\\bar\\sigma_j t_n},\n\nwhere f1(x)=x2 and f2(x)=x2^2, the decay modes \\bar\\sigma_j and stationary expectations E_\\pi(f_k) are assumed pre-estimated, and the unknown dependence on the initial distribution parameters \\eta enters only through the coefficients \\bar\\alpha_j(f_k,\\eta)=E_{P_\\eta(x)}[\\bar\\alpha_j(f_k,x)].\n\nInstead of minimizing a \u201cdirect\u201d nonconvex cost that sums discrepancies over all N measurement times (so cost/gradient evaluation scales linearly in N), one first estimates from the empirical time series the linear coefficients \\hat\\alpha_{jk} by regressing the measured moment trajectories \\hat f_k(t_n) onto the exponential basis {e^{-\\bar\\sigma_j t}}. Then \\eta is inferred by minimizing a reduced SKA-based objective\n\nC_SKA(\\eta)=\\tfrac12 \\sum_{j=1}^J \\sum_{k=1}^2 \\left( \\frac{\\bar\\alpha_j(f_k,\\eta)-\\hat\\alpha_{jk}}{E_\\pi(f_k)} \\right)^2,\n\nwhose dimension and computational complexity scale with J (typically \\ll N). Moreover, because only the coefficients need to be matched, one can choose a smaller set of time points over a shorter interval for simulation/regression when estimating \\bar\\alpha_j(f_k,\\eta) and its gradient, further reducing compute relative to simulating over all original measurement times.",
      "source_document": "papers/2511.23114v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In stochastic reaction networks, parameter sensitivities of observables are often written as time integrals involving the process along a trajectory, which makes them hard to compute accurately over long horizons. Describe how a spectral (Koopman) approximation can turn this sensitivity calculation into an explicit closed-form function of time. In your answer, (i) state the integral form of the sensitivity and identify the problematic term, (ii) explain the key approximation step that replaces that term using a Koopman spectral expansion on a finite span of observables, and (iii) indicate what quantities can be precomputed once per network versus what must be re-estimated for each new initial state.",
      "answer": "(i) The sensitivity of the Koopman expectation with respect to a parameter \u03b8 can be represented as a time integral whose integrand depends on the Koopman operator applied to auxiliary functions built from the \u03b8-derivative of propensities and finite-difference-like changes in the observable across reaction jumps; the difficult part is that along the trajectory one must evaluate differences of Koopman images across many visited states (i.e., terms involving K_{t-s} acting on a new function g_{\u03b8,j,f} and, in particular, state-dependent differences in the spectral coefficients such as \u0394_k \u03b1_j(f,x)=\u03b1_j(f,x+\u03b6_k)\u2212\u03b1_j(f,x)). (ii) A Koopman spectral approximation replaces K_{t} acting on any function in (or projected onto) the linear span of a chosen observable set F by a finite sum of exponentials: \\bar K_t h(x)=E_\u03c0(h)+\\sum_{j=1}^J \\bar \u03b1_j(h,x)e^{-\\bar \u03c3_j t}. By projecting each auxiliary integrand function g_{\u03b8,j,f} onto the same span L(\\bar F), one obtains an explicit integrand as a finite linear combination of e^{-\\bar \u03c3_j(t-s)}, so the time integral can be evaluated analytically, yielding a closed-form sensitivity trajectory t\u21a6S_{t,\u03b8}f(x) expressed in terms of the decay modes \\bar \u03c3_j and coefficient matrices (linear in the estimated Koopman coefficients). (iii) State-independent quantities that are computed once per SRN include the dominant decay modes (decay rates) \\bar \u03c3_1,\u2026,\\bar \u03c3_J, stationary expectations E_\u03c0(f), and the projection coefficients for the auxiliary functions used in sensitivities (coefficients c^{(n)}_{\u03b8,j,f} obtained by projecting g_{\u03b8,j,f} onto L(\\bar F) under the stationary distribution). For each new initial state x, one only needs to estimate the state-dependent Koopman spectral coefficients \\bar \u03b1_j(f,x) (assembled as \u03b1(F,x)) via regression from a small set of CTMC simulations; sensitivities then follow by plugging these coefficients into the explicit closed-form formula.",
      "source_document": "papers/2511.23114v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a Koopman-spectral approximation method for stochastic reaction networks that estimates state-dependent Koopman coefficients by solving an overdetermined linear system from Monte Carlo estimates of iterated resolvents, why is it beneficial to (i) solve the coefficient system with least squares and (ii) propagate the resulting covariance information forward? Explain how these choices support uncertainty quantification for predicted moment dynamics and for downstream quantities like parameter sensitivities and cross-spectral densities.",
      "answer": "(i) The coefficient matrix of the truncated Koopman expansion is obtained by combining the per-observable linear systems into one overdetermined system and solving it by least squares for robustness (ensuring J_tot \u2265 J). Least squares provides a stable estimate of the coefficient matrix rather than an exact solve that would be sensitive to Monte Carlo noise in the resolvent estimates.\n\n(ii) The least-squares solution yields not only point estimates of the coefficients but also an estimated covariance matrix for the stacked coefficients. This covariance can be propagated to compute standard deviations (uncertainty bands) for the approximate Koopman predictions \\bar K_t f(x) for all t \u2265 0. Because the sensitivity formulas and cross-spectral-density (CSD) formulas depend linearly on the same estimated coefficients, their standard deviations can be derived from the same covariance matrix as well, enabling uncertainty quantification for sensitivities and CSD/PSD estimates without additional resampling.",
      "source_document": "papers/2511.23114v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In retrieval-augmented protein inverse folding, how can you construct a per-residue evolutionary prior from structurally similar proteins and use it to condition a discrete denoising diffusion model? Describe (i) the two-stage structural retrieval criteria used to select candidates, (ii) how residue-level structural alignments are aggregated into a position-specific amino-acid probability profile, including what to do for unaligned positions, and (iii) how this profile is fused with structure embeddings to produce residue-wise amino-acid probabilities.",
      "answer": "(i) Retrieve structural neighbors with a hierarchical search: first run FoldSeek for a fast coarse filter and keep hits whose 3Di alignment has fident > 0.5; then run coordinate-based US-align on this reduced candidate set, compute the two asymmetric TM-scores (tm1, tm2) for each alignment, and retain hits with min(tm1, tm2) > 0.5 to preserve high-quality local/fragment matches.\n\n(ii) For each retrieved protein Pr=(Sr,Xr), use the residue correspondence from US-align to map each query position i either to an aligned residue index j in Sr or to \u201cunaligned\u201d. For each query position i, collect a multiset Ti of amino-acid identities from all retrieved proteins that align to i: Ti = {Sr[j] over all retrieved Pr where i aligns to j}. Convert these multisets into a position-specific amino-acid profile \u03a0\u2208R^{N\u00d720} by frequencies: \u03a0_{i,aa} = count(aa in Ti)/|Ti| if |Ti|>0, and assign a uniform distribution 1/20 when |Ti|=0 (no aligned residues), giving a non-informative prior at unaligned positions.\n\n(iii) Condition the diffusion model by integrating \u03a0 with the final-layer structure embeddings h_i^L from an EGNN using an extremely lightweight fusion module: project \u03a0_i into the node-feature hidden dimension and fuse with h_i^L via a residual addition, then apply MLPs and softmax to get the final residue-wise amino-acid distribution, e.g., p_i = softmax(\u03c62(\u03c61(\u03a0_i)+h_i^L)), trained with cross-entropy to the ground-truth sequence.",
      "source_document": "papers/2512.00126v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In retrieval-augmented protein inverse folding, you want to inject an external evolutionary prior into a structure-conditioned denoising model without retraining a huge protein language model. Describe one concrete way to fuse a per-residue amino-acid probability profile \u03a0 (built from aligned retrieved structures) with the model\u2019s final structure-derived residue embedding h\u1dab\u1d62 so that the output is a distribution p\u1d62 over 20 amino acids. What operations are applied to \u03a0\u1d62 and how is it combined with h\u1dab\u1d62 before the final softmax?",
      "answer": "Use a lightweight fusion module: for each residue i, first project the profile vector \u03a0\u1d62 into the hidden dimension with an MLP \u03d5\u2081, then add it to the final EGNN residue embedding h\u1dab\u1d62 via a residual (additive) connection, and map the summed vector through a second MLP \u03d5\u2082 followed by a softmax to obtain amino-acid probabilities: p\u1d62 = softmax( \u03d5\u2082( \u03d5\u2081(\u03a0\u1d62) + h\u1dab\u1d62 ) ). The network is trained with cross-entropy against the ground-truth sequence.",
      "source_document": "papers/2512.00126v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating retrieval-augmented protein inverse folding, what concrete protocol can you use to prevent information leakage from the external structure database (including the special case where test examples are protein domains/fragments), and why does each filter reduce the risk of trivial retrieval matches inflating performance?",
      "answer": "Use a strict two-part filtering protocol during retrieval: (1) **Identity filtering**\u2014exclude any structure in the retrieval database that is identical to a structure in the test sets. (2) **Substring filtering**\u2014for domain-based datasets where test samples can be fragments of full proteins (e.g., CATH domains), perform a sequence-based substring check and discard any database hit whose amino-acid sequence contains the query domain sequence as a substring, or where the query sequence contains the database sequence as a substring. Identity filtering removes exact duplicates of test structures, and substring filtering removes cases where the query is essentially a contiguous fragment of a database protein (or vice versa), preventing trivial near-copies from being retrieved and artificially boosting design metrics.",
      "source_document": "papers/2512.00126v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In retrieval-augmented protein inverse folding, you want an evaluation metric that gives partial credit when the designed residue is not identical to the native residue but is biochemically similar (e.g., conservative substitutions). How can you define such a \u201cnative sequence similarity recovery\u201d metric using a BLOSUM substitution matrix, and what is the criterion for counting a predicted\u2013native residue pair as a match?",
      "answer": "Define native sequence similarity recovery (NSSR) by comparing each predicted residue to the native residue using a BLOSUM substitution matrix (e.g., BLOSUM62 or BLOSUM90) and counting it as recovered if the BLOSUM score for that residue pair is positive; NSSR is the fraction (or percent) of positions that meet this positive-score criterion.",
      "source_document": "papers/2512.00126v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In drug\u2013gene\u2013ADR triad prediction, how can you design an evaluation protocol that tests *inductive* generalization to previously unseen entities (drugs, genes, or ADRs) while avoiding information leakage, and which performance metrics are appropriate if positives are rare and the downstream goal is to rank the correct missing entity for a given query pair?",
      "answer": "Use entity-held-out inductive splits: run separate protocols for drug-held-out, gene-held-out, and ADR-held-out evaluation where **all triads containing a particular held-out entity** are removed from training and reserved for test so the model must generalize to entities unseen during training; perform cross-validation such that held-out entities and their associated triads appear only in test folds. Report complementary metrics: **AUC** for overall discrimination, **AUPR** because the setting is highly imbalanced with rare true triads, and **MRR** to measure ranking quality\u2014e.g., for a fixed query like a drug\u2013ADR pair, MRR is high when the true gene is ranked near the top among candidates.",
      "source_document": "papers/2512.00137v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a triadic drug\u2013gene\u2013ADR prediction model that must answer \u201cgiven any two entities, rank the correct third entity,\u201d how can query-conditioned contrastive learning be set up so it (i) avoids manual negative sampling, and (ii) supports predicting any missing element of the triplet (drug vs gene vs ADR)? Describe the core idea of the in-batch objective and how you would apply it across query types.",
      "answer": "Use a CLIP-style in-batch contrastive objective over query\u2013response pairs. For each observed triplet, form a query embedding from two entities (e.g., drug+ADR) and compute conditional embeddings for all candidate third entities (e.g., genes) by modulating their base embeddings with the query via FiLM (feature-wise affine scaling/shifting). The contrastive loss pulls the query toward the conditional embedding of the ground-truth positive entity and repels it from all other candidates present in the minibatch, treating all non-matching samples as implicit negatives\u2014so no explicit/manual negative sampling is needed. To support predicting any missing element, run the same procedure for all three query configurations (drug\u2013ADR\u2192gene, drug\u2013gene\u2192ADR, gene\u2013ADR\u2192drug) and take the average of the contrastive losses across the three query types as the overall contrastive term (optionally combined with a supervised BCE link-prediction loss via a weighted sum).",
      "source_document": "papers/2512.00137v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When protein\u2013ADR edges are missing from curated resources, how can you construct biologically grounded surrogate connections in a drug\u2013gene\u2013ADR knowledge hypergraph to reduce data sparsity, and what concrete example illustrates the core assumption behind this strategy?",
      "answer": "Use a disease-mediated bridging strategy: exploit the idea that an ADR in one context can correspond to a disease state in another, so the ADR and the aligned disease share underlying molecular mechanisms. Embed ADR terms and disease terms into a shared semantic space (via SapBERT), compute ADR\u2013disease semantic similarity, and if similarity exceeds a threshold \u03b8, let the ADR inherit the disease\u2019s topological connections (reconstructing missing ADR\u2013protein edges). The document\u2019s illustrative example is Sildenafil: its side effect \u201cpenile erection\u201d corresponds to the disease Erectile Dysfunction, which is linked to PDE5; aligning the ADR to the disease allows inferring an otherwise missing \u201cErection\u2013PDE5\u201d connection.",
      "source_document": "papers/2512.00137v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are building a protein-language-model pipeline to assign kinase functional classes from sequence. Describe a validation and modeling setup that reduces overoptimistic performance due to sequence homology, and explain what embedding strategy (layer choice and pooling) and post-processing step can improve both accuracy and decision reliability.",
      "answer": "Use homology-aware evaluation by de-redundifying/splitting sequences with CD-HIT clustering at a low identity threshold (e.g., 40% sequence identity) and performing stratified cross-validation on those cluster-based splits. For representations, extract kinase domains (Pfam PF00069/PF07714) and compute ESM-2 embeddings by averaging mid-to-late transformer layers (layers 20\u201333) rather than using only the final layer; apply mean pooling over residues to obtain a sequence-level embedding (CLS pooling was competitive only for the final layer but worse for mid-layer embeddings). Train a supervised classifier such as multinomial logistic regression with standardized features and balanced class weights. For decision reliability, calibrate predicted probabilities using Platt scaling, which reduces miscalibration (ECE and log-loss) and allows flagging low-confidence predictions (e.g., probability < 0.7) for review.",
      "source_document": "papers/2512.00376v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You have a multiclass protein-function classifier that outputs probabilities from a linear model trained on protein-language-model embeddings. In a deployment setting where you want probability scores to reflect true likelihoods (so you can flag uncertain sequences for expert review), what post-hoc calibration method can you apply, what reliability metrics should you use to quantify improvement, and how would you operationalize a \u201clow-confidence\u201d triage rule based on the calibrated probabilities?",
      "answer": "Apply Platt scaling to the classifier scores to calibrate probabilities. Quantify reliability using Expected Calibration Error (ECE) and log-loss before vs. after calibration; calibration reduces ECE (e.g., 0.154\u21920.110) and log-loss (e.g., 1.07\u21920.77). Operationalize triage by flagging test/deployment sequences whose calibrated maximum class probability is below a threshold such as 0.7 as low-confidence for expert review (about 18% flagged in this setup).",
      "source_document": "papers/2512.00376v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You need a kinase functional-class prediction pipeline from protein sequences using a transformer PLM, but you also want predictions that are stable and interpretable across variable-length inputs and suitable for deployment. Describe (i) how to construct a homology-aware evaluation split, (ii) how to form a sequence embedding from multi-layer per-residue representations (including the recommended layer range and pooling), and (iii) how to handle sequences longer than the model\u2019s token limit to avoid bias. Explain the statistical intuition given for why multi-layer averaging improves robustness.",
      "answer": "(i) Build homology-aware train/test splits by clustering sequences with CD-HIT at a low identity threshold (40% identity is used) after extracting the kinase domain (Pfam PF00069/PF07714 via HMMER). Use these clusters to define non-overlapping splits and run stratified CV on those splits.\n\n(ii) Extract per-residue embeddings H^(\u2113) from ESM-2 across a selected set of transformer layers L and average them across layers (recommended: extended mid-to-late layers 20\u201333) to get \nH\u0304 = (1/|L|) \u03a3_{\u2113\u2208L} H^(\u2113), equivalently h\u0304_i = (1/|L|) \u03a3_{\u2113\u2208L} h_i^(\u2113) for each residue i. Then apply mean pooling across residues to obtain a length-invariant sequence embedding z = (1/Lseq) \u03a3_i h\u0304_i (mean pooling is reported to outperform [CLS] pooling, especially for mid-layer embeddings).\n\n(iii) For sequences exceeding the model input limit (L_max = 1022), split into overlapping windows with stride 900, embed each window to get z^(w), then combine with length-weighted averaging:\nz_final = (\u03a3_w n_w \u00b7 z^(w)) / (\u03a3_w n_w), where n_w is the window length. Weighting by window length prevents overlap-heavy regions from dominating.\n\nStatistical intuition: averaging across k layers reduces embedding variance by ~1/k under an independence assumption: Var(h\u0304_i) = (1/k^2) \u03a3_{\u2113\u2208L} Var(h_i^(\u2113)) = \u03c3^2/k. By the CLT, the averaged representation converges toward a Gaussian with reduced variance, improving stability/robustness.",
      "source_document": "papers/2512.00376v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multimodal model that predicts enzyme kinetic constants from (i) a protein sequence embedding and (ii) a substrate SMILES embedding, how can you combine contrastive learning and supervised regression in a single training objective to (a) align the correct enzyme\u2013substrate pairs in a shared latent space and (b) make robust numeric predictions in the presence of experimental outliers? Describe the specific loss terms used and how they are combined.",
      "answer": "Use a dual-encoder setup that produces projected protein and substrate embeddings in the same latent space, and train with two losses: (1) a symmetric InfoNCE contrastive loss computed from the similarity matrix between L2-normalized protein and chemical embeddings, scaled by a learnable temperature parameter, so that matched pairs in the batch are pulled together and mismatched pairs are pushed apart; and (2) a SmoothL1 (Huber) regression loss on the predicted log10-transformed kinetic value, which is quadratic for small errors and linear for large errors to reduce sensitivity to outliers. The total objective is an equal-weight sum of the two: L_total = L_InfoNCE + L_SmoothL1 (Huber).",
      "source_document": "papers/2512.00379v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You train a multimodal enzyme\u2013substrate model and want to (i) evaluate whether its predictions are unbiased/calibrated and (ii) reuse its learned multimodal representations in a second-stage regressor. Which evaluation statistics beyond standard regression scores should you compute to assess calibration/bias, and what specific embedding extraction and pooling procedure would you use to obtain fixed-length protein and substrate vectors suitable for downstream tree-based regression?",
      "answer": "Beyond standard regression metrics (MSE, RMSE, MAE, R2, Pearson r), compute residual-based statistics\u2014specifically the mean and standard deviation of residuals\u2014to assess calibration and systematic bias.\n\nFor second-stage regression, extract the latent representations from the model\u2019s projection layers for all splits, producing protein and chemical embeddings (e_prot and e_chem). Instead of CLS-token summarization, form fixed-length vectors by attention-mask\u2013weighted mean pooling over token-level hidden states:\n\ne = (\\sum_{t=1}^{L} h_t \u00b7 a_t) / (\\sum_{t=1}^{L} a_t),\n\nwhere h_t is the hidden state at position t and a_t is the attention-mask weight. Then pass the mean-pooled vectors through modality-specific projection layers into a shared 256-dimensional space to obtain e_prot and e_chem, and concatenate them to create a unified multimodal feature vector (e_combined) for downstream ensemble regression (e.g., XGBoost/CatBoost).",
      "source_document": "papers/2512.00379v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are building a multimodal enzyme\u2013substrate regressor using a CLIP-style dual encoder, but you want (i) a fixed-length embedding per modality that is robust to variable sequence/SMILES length and padding, and (ii) a single joint feature vector suitable for a downstream tree-ensemble regressor. What pooling and masking operation would you apply to token-level hidden states to obtain each modality embedding, and how would you combine the two modality embeddings into a unified representation for downstream regression?",
      "answer": "Use attention-mask\u2013weighted mean pooling over token-level hidden states to produce a fixed-length contextual vector for each modality:\n\n\\( e = \\frac{\\sum_{t=1}^{L} h_t \\cdot a_t}{\\sum_{t=1}^{L} a_t} \\)\n\nwhere \\(h_t\\) is the hidden state at position \\(t\\), \\(a_t\\) is the attention-mask weight (so padded/invalid tokens contribute 0), and \\(L\\) is sequence length. Pass the pooled vectors through modality-specific projection layers to get shared-space embeddings \\(e_{prot}\\) and \\(e_{chem}\\), then concatenate them to form the joint feature vector for downstream ensemble regression:\n\n\\(e_{combined} = \\mathrm{Concat}[e_{prot}, e_{chem}]\\).",
      "source_document": "papers/2512.00379v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "A property-conditioned molecular generator is trained only on (single-property prompt, synthesis-pathway) pairs, but at inference time you want molecules that satisfy a composite constraint like (C1 AND C2) or (C1 AND NOT C2), and you also want to support arbitrary Boolean formulas over properties. How can you combine the model\u2019s per-property next-token predictions to sample from these composite queries, and what additional steps are needed to handle OR and general logical expressions? State the key independence/uniform-prior assumptions required and one example of when they break down.",
      "answer": "Composite querying is done by composing conditional distributions that the model can already produce for each individual property prompt.\n\n\u2022 For AND (conjunction) of two property prompts C1 and C2, use a product-of-experts combination:\n  p(s|C1\u2227C2) \u221d p(s|C1)^\u03b1 p(s|C2)^\u03b2 (\u03b1,\u03b2>0).\n  Autoregressively at each decoding step i, combine next-token distributions by adding logits:\n  p(si|s<i,C1\u2227C2) = softmax(\u03b1 z1 + \u03b2 z2),\n  where z1 and z2 are the next-token logits under prompts C1 and C2.\n\n\u2022 For NOT (negation), sample molecules satisfying C1 but not C2 by flipping the sign of the second expert:\n  p(s|C1\u00acC2) \u221d p(s|C1)^\u03b1 / p(s|C2)^\u03b2,\n  giving per-step combination\n  p(si|s<i,C1\u00acC2) = softmax(\u03b1 z1 \u2212 \u03b2 z2).\n  Negation can be unified with conjunction by allowing \u03b2 to take negative values.\n\n\u2022 For OR (disjunction), use a mixture rather than a product:\n  p(s|C1\u2228C2) \u221d \u03b1 p(s|C1) + \u03b2 p(s|C2).\n  This form is not autoregressively factorable, so you sample full sequences separately from p(s|C1) and p(s|C2) and then merge the samples.\n\n\u2022 For general Boolean formulas, first rewrite the query into disjunctive normal form (DNF): an OR of terms where each term is an AND of (possibly negated) properties. For each conjunctive term, sample using the AND/NOT logit-composition rules above; then merge samples across the OR terms to obtain molecules satisfying the whole query.\n\nAssumptions: property prompts are mutually conditionally independent given the molecule, and the prior distribution over molecules is uniform; under these assumptions the joint conditional has a product-of-experts form.\n\nBreakdown example: correlated or contradictory constraints (e.g., simultaneously requiring molecular weight < 100 and number of heavy atoms > 50, or generally size-correlated descriptors like molecular weight and rotatable bonds) violate independence/consistency and can make the composition problematic.",
      "source_document": "papers/2512.00384v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In query-space optimization for synthesizable molecular design, you start from a seed property query and iteratively update it using black-box oracle feedback rather than directly editing molecular graphs or synthetic trees. Describe the core loop of this optimization procedure: (i) how a candidate molecule is converted into a property query with \u201coptimizable\u201d terms versus fixed \u201cconstraint\u201d terms, (ii) what role perturbing the optimizable terms plays before resampling, and (iii) how new candidates are accepted/replaced. Name at least two iterative optimization strategies that can implement this loop.",
      "answer": "Core loop:\n(i) Begin with a seed query Q0 to sample an initial population of candidate molecules. At iteration t, each candidate molecule M is mapped to a property query Qt that separates terms into optimizable conditions that depend on M and fixed constraints: in general Q = C1^(opt)(M) \u2227 C2^(opt)(M) \u2227 \u2026 \u2227 C1^(cstr) \u2227 \u2026 , where {C_i^(opt)(M)} are the molecule-dependent optimizable conditions and {C_i^(cstr)} are fixed constraint conditions.\n(ii) A perturbation is added to the optimizable terms to form an updated query Q\u2032 (e.g., C\u2032^(opt) terms while keeping the same constraint terms). This perturbed query is then used to recondition the generative model and resample new synthesizable molecules; even if the query does not correspond to an exact valid molecule, the model can generate molecules approximating the desired properties, smoothing the landscape.\n(iii) The newly generated candidates are evaluated by the black-box oracle, and candidates with better oracle score replace the previous candidates, yielding an improved population over iterations.\nTwo example strategies to implement the loop: genetic algorithms and Metropolis\u2013Hastings sampling (other iterative sampling methods are also possible).",
      "source_document": "papers/2512.00384v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In multimodal QSAR-style bioactivity prediction, why can predicting raw IC50 values directly destabilize regression training, and what target transformation and scaling steps can be applied to make optimization more stable?",
      "answer": "Raw IC50 values can span a very large range, so a regressor trained directly on IC50 produces very large losses early in training due to the huge difference between minimum and maximum IC50. A stabilizing approach is to transform IC50 to pIC50 using pIC50 = \u2212log10(IC50) and then standardize the pIC50 targets with a StandardScaler before training.",
      "source_document": "papers/2512.00521v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You\u2019re building a multimodal QSAR model that fuses (i) RDKit molecular descriptors, (ii) a Transformer SMILES embedding (e.g., ChemBERTa), and (iii) a GNN-derived graph representation. What specific preprocessing steps can you apply to the descriptor modality to reduce redundancy and improve training stability, and what criteria/thresholds would you use to decide which descriptors to remove?",
      "answer": "Start from a broad RDKit descriptor set, then (1) drop descriptors with extremely low variance across the dataset (variance < 0.01), since near-constant features add noise and little signal; (2) remove one descriptor from any highly correlated pair using an upper-triangular filtering strategy when the correlation coefficient exceeds 0.9 to reduce redundancy/multicollinearity; and (3) standardize the remaining descriptor values with a StandardScaler for stable optimization. Applying these steps reduced the descriptor set from 217 to a filtered set of 134 descriptors.",
      "source_document": "papers/2512.00521v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multimodal QSAR regressor that fuses (i) RDKit molecular descriptors, (ii) Transformer-based SMILES embeddings (e.g., ChemBERTa), and (iii) GNN-derived graph features, what experimental evidence would you provide (via an ablation study) to demonstrate that the three modalities are complementary rather than redundant, and what qualitative performance pattern should you expect when training with one modality vs. two vs. all three?",
      "answer": "An ablation study should train and evaluate the same regression pipeline under multiple input configurations\u2014each single modality alone (descriptors only; SMILES/ChemBERTa embeddings only; graph/GNN features only), each pairwise fusion (descriptors+SMILES; descriptors+graph; SMILES+graph), and the full fusion of all three\u2014using the same cross-validation protocol and metrics (e.g., MSE and Spearman).\n\nThe expected qualitative pattern is monotonic improvement as more complementary views are fused: (1) single-modality models perform worst (descriptors alone is weakest; SMILES embeddings improve; graph features perform best among single modalities), (2) combining any two modalities yields better performance than either constituent alone (lower MSE and higher Spearman), and (3) the full three-way fusion is best overall, indicating synergy/complementarity among descriptors (global physicochemical properties), SMILES embeddings (context/semantics), and graph features (structural/spatial reasoning).",
      "source_document": "papers/2512.00521v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When designing a GNN-based molecular regressor under limited training data, what architectural choices can you make in the graph encoder and readout to obtain a compact but information-rich whole-molecule representation, and why might combining a skip-connected graph convolution with a readout that concatenates a weighted-sum pooling and a max-pooling summary be beneficial?",
      "answer": "A compact design is to use a single graph convolution layer augmented with a skip connection to update node features, then apply dropout, and use a graph-level readout that concatenates two complementary summaries: (i) a weighted-sum pooling that aggregates node features with importance weights (capturing distributed/global contributions across the molecule) and (ii) a max-pooling that retains the strongest activations (capturing salient substructures). Concatenating these two pooled vectors yields a richer whole-graph embedding than either alone while keeping the encoder shallow (helpful in low-data settings and for computational efficiency); the skip connection helps preserve original node information and stabilizes learning.",
      "source_document": "papers/2512.00521v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When comparing graph-based molecular bioactivity predictors, why can a model with *fewer* trainable parameters still have *slower* inference than a larger multimodal model, and what concrete architectural factors can explain a larger model achieving lower latency per molecule?",
      "answer": "Inference latency depends on architectural operations, not just parameter count. A small-parameter graph-attention model can be slow because it uses multiple stacked attention layers and iterative readout operations with sequential attention steps that are not easily parallelizable, adding overhead despite few weights. Message-passing networks can also be slow due to multiple message-passing steps and complex neighborhood aggregation that require repeated graph traversals. In contrast, a larger multimodal model can be faster if it (i) uses only a single graph convolution layer with a canonical (static) featurizer, (ii) generates the other modalities (e.g., RDKit descriptors and ChemBERTa embeddings) in parallel rather than via iterative graph readout, and (iii) avoids deep attention stacks and repeated message-passing, reducing sequential overhead and graph-traversal costs.",
      "source_document": "papers/2512.00521v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When applying post-hoc residue-attribution methods to a GCN-based protein function predictor, how can a quantitative sparsity metric be used to reason about the trade-off between robustness and localization precision across different explainers (e.g., GradCAM vs Excitation Backpropagation vs PGExplainer), and what qualitative conclusions follow from their relative sparsity levels?",
      "answer": "Define sparsity as how many residues are effectively used by an explainer in forming an explanation (low sparsity = many residues weighted; high sparsity = few residues highlighted). Comparing explainers by sparsity supports reasoning about a robustness\u2013precision trade-off: \n- GradCAM is least sparse (it spreads relevance broadly across many residues), which implies greater robustness to small perturbations but poorer localization/active-site pinpointing.\n- Excitation Backpropagation is most sparse with binary masks (it highlights very few residues), giving an immediately interpretable include/exclude signal and sharper localization, but it can miss subtler distributed contributions.\n- PGExplainer has intermediate sparsity (per-residue scores), balancing hotspot identification with broader structural context; it can be used for more precise residue-level guidance than GradCAM while not being as all-or-nothing as EB.\nThus, relative sparsity provides a quantitative way to choose an explainer based on whether the goal is robust screening (favor lower sparsity) versus precise functional-site localization (favor higher sparsity), with PGExplainer offering a compromise.",
      "source_document": "papers/2512.00642v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a benchmark designed to test whether affinity predictors truly learn modification-specific effects (rather than just echoing wild-type behavior), what two wild-type\u2013based baselines can you use when evaluating predictions on modified protein\u2013ligand pairs, and what does failure to beat these baselines imply about the model?",
      "answer": "Two baselines are used:\n1) Wild-type ground truth (yWT): predict the modified pair\u2019s affinity by reusing the experimentally measured affinity of the corresponding wild-type protein\u2013ligand pair.\n2) Wild-type prediction (\u0177WT): predict the modified pair\u2019s affinity by reusing the model\u2019s own predicted affinity for the corresponding wild-type pair.\nIf a model does not outperform these baselines, it indicates the model is not capturing modification-specific effects beyond what is already implied by (or predictable from) the wild type\u2014i.e., it is effectively overfitting to / echoing wild-type information rather than learning how modifications change affinity.",
      "source_document": "papers/2512.00708v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When binding-affinity measurements are censored by an upper Kd cap (e.g., all Kd > 10 \u00b5M recorded at the cap), how can you categorize wild-type vs. modified protein\u2013ligand pairs so that some modification-induced affinity changes are exactly computable while others are only lower bounds or completely untrackable\u2014and what does each category imply about what you can infer about \u0394pKd?",
      "answer": "You can partition pairs into four cases based on whether each of the wild-type (WT) and modified measurements is capped (Kd > 10 \u00b5M) or uncapped (Kd < 10 \u00b5M):\n1) WT-uncapped & modification-uncapped: both Kd values are below 10 \u00b5M, so \u0394pKd = A(modified, ligand) \u2212 A(WT, ligand) is exactly trackable.\n2) WT-capped & modification-uncapped: WT is censored but the modified is not; this indicates an affinity increase due to the modification, and \u0394pKd is only a lower bound because the true WT affinity is unknown beyond the threshold.\n3) WT-uncapped & modification-capped: modified is censored but WT is not; this indicates an affinity decrease, and \u0394pKd is only the minimum possible magnitude (true decrease is untrackable).\n4) WT-capped & modification-capped: both are censored, so the exact change is completely untrackable; \u0394pKd is effectively set to 0 under the capped representation.",
      "source_document": "papers/2512.00708v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a few-shot setting where you have only a small number of binding-affinity measurements for modified (mutant/PTM) versions of a kinase, how can you design an evaluation to test whether an affinity predictor actually adapts to modification-specific effects (rather than just memorizing wild-type behavior)? Specify (i) how you would construct the train/fine-tune/test splits, (ii) two complementary task variants that probe sensitivity to modifications vs ligands, and (iii) what pattern of results would indicate that a model benefits from fine-tuning versus one that does not.",
      "answer": "A suitable few-shot modification generalization evaluation is:\n\n(i) Split design: pre-train the model on wild-type protein\u2013ligand pairs only, then fine-tune on a limited set of modified protein\u2013ligand pairs; use an 80%/20% split of the available modified pairs for fine-tuning vs evaluation (i.e., fine-tune on 80% of modified examples and test on the remaining 20%) to measure adaptation to unseen modified examples.\n\n(ii) Two task variants:\n\u2022 Same-ligand, different-modifications: fix the ligand and evaluate across additional modified variants of the same kinase (tests whether the model can distinguish affinity changes caused by different modifications).\n\u2022 Same-modification, different-ligands: fix a particular kinase modification and evaluate across distinct ligands (tests whether the model can rank/fit ligand-dependent affinities for a given modified protein).\n\n(iii) Interpretation pattern:\nA model benefits from few-shot adaptation if metrics on the modified test set improve after fine-tuning (lower MSE and higher Pearson correlation Rp and C-index when comparing post\u2013fine-tuning predictions to pre\u2013fine-tuning predictions). Conversely, if performance stagnates or degrades after incorporating few-shot modified examples, it indicates the model does not effectively leverage fine-tuning and needs a better adaptation strategy.",
      "source_document": "papers/2512.00708v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an analysis of animal vocalizations encoded as sequences of automatically discovered syllable types, how can you test whether the communication system exhibits associative (order-independent) versus combinatorial (order-dependent) syntax, and what outcome would support the associative-syntax conclusion?",
      "answer": "Train a behavioral-context classifier (e.g., a Random Forest) on features engineered from syllable sequences, then run a permutation test by shuffling the order of syllables within sequences and comparing classification performance (e.g., F1-scores) between original and permuted data. If performance is essentially unchanged\u2014here, F1 remains > 0.9 for both original and permuted sequences\u2014then syllable order does not contribute to contextual information, supporting associative rather than combinatorial syntax.",
      "source_document": "papers/2512.01033v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You extract Maximal Repeats (MRs) from symbolized animal vocal sequences to quantify long-range temporal structure. Describe (i) what statistical null model you would test MR-lengths against, (ii) what alternative distributional pattern would support long-range dependencies/greater combinatorial capacity, and (iii) what concrete statistical decision procedure can be used to choose between the two.",
      "answer": "(i) Test MR-lengths against an exponential distribution, which corresponds to a memory-less information-decay process where longer repeats become exponentially unlikely. (ii) Evidence for long-range dependencies/combinatorial capacity is a heavy-tailed MR-length distribution, e.g., a (truncated) power-law rather than exponential. (iii) Use a likelihood-ratio test comparing the exponential fit versus a power-law (or truncated power-law) fit; rejecting the exponential in favor of the heavy-tailed model (p < 0.05) supports the long-range-dependency interpretation.",
      "source_document": "papers/2512.01033v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When generating a protein conformational transition path with a Langevin-bridge/conditioned-dynamics integrator, what specific modeling choices in the coarse-grained potential help keep trajectories physically plausible (i.e., preserve backbone geometry and avoid steric clashes) while still providing a driving force toward large-scale motion\u2014and what failure mode does the method anticipate if these terms are omitted or simplified too much?",
      "answer": "Use a coarse-grained potential with (i) local geometric restraints on the C\u03b1 backbone (bonded distance term for consecutive C\u03b1 atoms and an angle term for three consecutive C\u03b1 atoms, using values from the initial conformation) to preserve native-like backbone geometry; (ii) an excluded-volume/steric term implemented as a repulsive Lennard\u2013Jones (vdW) interaction to prevent collisions/steric clashes during the transition; and (iii) a global elastic driving term based on a reference-free Rouse spring network, with a smooth Fermi-like cutoff g(r) so distant pairs do not create an unphysical r^2 penalty that would otherwise collapse the protein (minimizing radius of gyration). If steric and geometric terms are missing or the coarse graining is too simplified (e.g., C\u03b1-only without side chains), trajectories can become non-physical\u2014showing steric clashes and/or unrealistic \u2018breathing\u2019/compaction and geometry distortions because the elastic term alone would favor overly compact conformations and lacks detailed excluded volume.",
      "source_document": "papers/2512.01903v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a joint generative model that co-generates amino-acid identities and atomistic side-chain coordinates, why can including side-chain atoms for residues whose sequence tokens are still masked make the sequence prediction problem artificially easy, and what procedural constraint during training/generation avoids this information leakage?",
      "answer": "Because the presence/absence and identity-dependent layout of side-chain (non-backbone) atoms reveals the residue type: if masked positions still carry side-chain atoms (or even just indicate which atoms exist), the model can infer the amino-acid token directly, trivializing sequence denoising. To prevent this leakage, all non-backbone atoms for residues that are masked in the sequence are removed/zeroed during training, and during generation the model only begins denoising/generating a residue\u2019s non-backbone atoms after that residue has been unmasked in the sequence (newly unmasked residues get an initialization, otherwise masked ones keep z=0).",
      "source_document": "papers/2512.01976v3.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a large-scale training set for joint sequence\u2013structure protein generative modeling, why can simply replacing each natural sequence\u2019s structure with an alternative predicted structure (e.g., an ESMFold refold) or filtering to a \u201c100% designable\u201d structural subset still hurt model performance, and what data-construction procedure can restore useful sequence\u2013structure alignment while preserving structural diversity?",
      "answer": "Replacing AFDB structures with alternative predicted structures (or filtering to an ESMFold-designable subset) can reduce dataset diversity/volume and still leave a hard joint-learning problem because the sequences remain \u201cnatural\u201d while the structures are synthetic; this can drive sequence overfitting and degrade generation metrics unless the synthetic structures are paired with sequences that are actually recoverable for them. A more effective procedure is to resample sequences with an inverse-folding model (ProteinMPNN) for each diverse AFDB cluster-representative backbone, refold those new sequences to obtain compatible all-atom structures, select the sequence\u2013structure pair that best matches the original backbone geometry (lowest C\u03b1-RMSD to the AFDB structure) to preserve structural diversity, and filter by structure-prediction confidence (e.g., ESMFold pLDDT threshold), yielding aligned synthetic sequence\u2013structure pairs.",
      "source_document": "papers/2512.01976v3.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In lead-conditioned generative peptide design, how can you incorporate a known lead peptide\u2019s secondary-structure pattern and an optimal-transport coupling to improve sampling efficiency and structural fidelity compared with an unconditional (global) noise prior? Describe (i) how the class-conditional geometric prior is constructed from helix/sheet/loop annotations, and (ii) how multimodal optimal transport changes the pairing between noisy residues and target residues to yield shorter, more disentangled flow-matching trajectories.",
      "answer": "(i) Use the lead peptide\u2019s secondary-structure annotations (e.g., via DSSP) to assign each residue to a class such as helix, sheet, or loop, then compute a class-specific centroid (average C\u03b1 position) for each class in the lead structure. Initialize noisy C\u03b1 coordinates by sampling each residue from a Gaussian centered at the centroid of its assigned class (a class-conditional prior), rather than from a single global centroid; this reduces the average deviation from the reference geometry while remaining rotation-equivariant.\n\n(ii) Apply multimodal optimal transport between the lead peptide and the initialized noisy peptide to compute a coupling/permutation that reassigns noisy residues to their target counterparts across modalities (coordinates/orientations/angles/sequence). This OT-based residue pairing minimizes overall path length, producing shorter and more disentangled flow-matching/ODE trajectories, which improves stability and efficiency of generation versus unconditional pairing.",
      "source_document": "papers/2512.02030v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a lead-peptide optimization pipeline, you generate thousands of candidate peptide\u2013protein complexes with a conditional generative model and need to down-select a small set for synthesis. What multi-stage, multi-criterion in silico screening strategy can you use to bridge the generative output to wet-lab validation\u2014specifically, how can you combine (i) unsupervised clustering, (ii) selection based on similarity to the lead, and (iii) ranking by predicted binding affinity and peptide\u2013protein interaction profiles\u2014to arrive at a short list of candidates?",
      "answer": "Use an expert-system style funnel: first cluster the generated peptides into a moderate number of groups with an unsupervised peptide-sequence clustering/alignment method (e.g., GibbsCluster) to organize diversity; then choose the cluster whose representative sequence is most similar to the lead peptide (to stay in the lead-conditioned neighborhood); within that chosen cluster, evaluate a manageable subset by predicted binding affinity and by peptide\u2013protein interaction profiles (e.g., non-covalent contacts such as hydrogen bonds, hydrophobic contacts, salt bridges/other interaction types) and select the top-scoring peptides as the final set for experimental validation.",
      "source_document": "papers/2512.02030v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a lead-conditioned peptide structure generator, what set of complementary evaluation metrics can you use to capture (i) how tightly the model stays in the lead\u2019s local sequence\u2013structure neighborhood, (ii) whether it improves binding/complex energetics, and (iii) whether it preserves geometric correctness and binding-interface similarity? Define each metric and what it is intended to measure.",
      "answer": "A suitable metric suite includes:\n\n1) Similarity: the proportion of generated peptides that closely resemble the reference/lead peptides, defined by meeting both a structural threshold (TM-score \u2265 0.5) and a sequence threshold (sequence identity \u2265 0.5). Intended to measure staying near the lead in both structure and sequence.\n\n2) Compactness: a measure of variability across generated peptides, computed as 1 minus the product of pairwise (1 \u2212 TM-score) and (1 \u2212 sequence identity) across peptides generated for a target. Intended to quantify how concentrated the generated set is in the local lead-conditioned region.\n\n3) Affinity: the fraction of generated peptides predicted to have stronger binding affinity (i.e., lower binding energies) than the native peptide. Intended to measure improvement in binding energetics.\n\n4) Stability: the percentage of generated complexes that are more thermodynamically stable (lower total energy) than native complexes, computed with the Rosetta energy function. Intended to measure complex stability/overall energetics.\n\n5) RMSD: root-mean-square deviation between generated and native peptide structures, computed using aligned C\u03b1 distances. Intended to quantify geometric accuracy of generated peptide conformations.\n\n6) BSR (Binding Site Rate): overlap-based similarity of peptide\u2013target interactions, quantifying how similar the binding sites/interfaces are between generated and native complexes. Intended to assess preservation of the binding mode/interface.",
      "source_document": "papers/2512.02030v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a pharmacophore/shape-based virtual screening pipeline, how can you reduce the number of expensive 3D overlay comparisons against a large library without collapsing to near-duplicate analogs, and what is the resulting asymptotic reduction in 3D comparisons per query in terms of database size, ng (number of generated molecules), and na (number of 2D analogs selected per generated molecule)?",
      "answer": "Use a two-stage \u201cfast search\u201d workflow: (1) condition a generative model on the query\u2019s voxelised 3D pharmacophore-shape profile to sample ng diverse candidate molecules; (2) for each generated molecule, do a cheap 2D similarity search (Morgan fingerprint Tanimoto/Jaccard) over the database and keep only the na closest 2D analogs; (3) run the expensive 3D ROCS alignment/Tanimoto Combo only on this reduced set and take those exceeding a 3D threshold as hits. This shifts the 3D workload from O(database size) per query (brute force ROCS against the whole library) to O(ng \u00d7 na) 3D comparisons per query (up to a constant factor for conformers).",
      "source_document": "papers/2512.02031v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In pharmacophore/shape-conditioned de novo molecular generation, what representation of a molecule\u2019s 3D pharmacophore\u2013shape information enables a neural model to treat the problem like \u201ccaptioning,\u201d and what encoder\u2013decoder architecture can map that representation to a SMILES string? Be specific about the channels used in the 3D representation and the neural components in the encoder and decoder.",
      "answer": "Represent the input as a 3D voxel grid of Gaussian-like densities with one channel per pharmacophore type plus a separate shape channel: six pharmacophore channels (hydrogen-bond donor, hydrogen-bond acceptor, cation, anion, aromatic ring, hydrophobe) and one shape channel formed by densities centered at atom positions (all with the same radius). Feed this C=7-channel voxelised pharmacophore\u2013shape profile into an encoder\u2013decoder model where a 3D CNN compresses the voxel grid to a vector embedding, which is then used to initialise an LSTM decoder that autoregressively generates a SMILES character/token sequence (optionally sampled via ancestral sampling with temperature/top-k).",
      "source_document": "papers/2512.02031v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In unsupervised quality assessment for diffusion-based biomolecular structure predictors, how can you turn the sequence of per-layer latent embeddings into a scalar proxy for *topological frustration*, and what is the physical interpretation of that scalar in terms of folding-pathway difficulty (i.e., what do small vs large embedding-trajectory changes imply)?",
      "answer": "Extract the hidden representation from each layer of the diffusion transformer at the final denoising step, average residue embeddings within each layer to get a global layer embedding h_l = (1/N)\\sum_i z_i^l, and treat the ordered list {h_0,\u2026,h_L} as a \u201cchain\u201d (trajectory) through embedding space. Quantify topological frustration by measuring how much this global embedding changes from one layer to the next\u2014e.g., using the L2 norm of adjacent differences \\|h_{l+1}\u2212h_l\\| (and aggregating these changes along the chain to form a score). Physically, the trajectory magnitude reflects topological ruggedness/constraint of the folding landscape: smaller layer-to-layer changes correspond to smoother transitions through conformational space (lower topological frustration), while larger changes indicate more constrained, difficult-to-navigate pathways (higher topological frustration).",
      "source_document": "papers/2512.02033v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You want an unsupervised, per-sample reliability score for diffusion-based biomolecular structure predictions that accounts for both energetic plausibility and topology-driven folding difficulty. How can you construct such a combined score from (i) a confidence score computed from pLDDT and pTM and (ii) a topological-frustration proxy computed from diffusion-transformer embedding trajectories, and what biological/biophysical rationale determines how you choose the relative weight between the two components for a given target class?",
      "answer": "Construct a combined score (CONFIDE) by integrating an energetic-frustration confidence term with a topological-frustration term:\n\n- Energetic term: use the model\u2019s confidence score that aggregates local and global confidence, e.g., Confidence Score = 0.8\u00b7pLDDT + 0.2\u00b7pTM (or the analogous confidence output available in the predictor).\n- Topological term (CODE): compute the mean, range-normalized adjacent-layer embedding change along the diffusion transformer\u2019s hidden-state chain. Concretely, average residue embeddings per layer to get h_l, compute adjacent L2 changes ||h_{l+1}\u2212h_l||_2, sum/average them over layers, and normalize by the overall input\u2013output latent displacement ||h_L\u2212h_0||_2 to reduce sample-scale bias.\n- Combine them with a weighted sum where the weight is chosen/optimized for the task (e.g., select the combination coefficient that maximizes Spearman correlation with an external quality target when available; otherwise a simple sum is possible).\n\nRationale for weighting: different systems are limited by different frustration sources. Fast-folding proteins with simple topologies are often more constrained by energetic conflicts, whereas proteins/complexes with more complex topology or requiring chaperone assistance face substantial topological barriers; thus the energetic vs topological weights should be adjusted based on structural class, folding mechanism, and environmental context.",
      "source_document": "papers/2512.02033v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multimodal model that predicts a patient\u2019s somatic mutation profile from lesion imaging while conditioning on cancer type, what quantitative metrics would you use to (i) assess distributional goodness-of-fit for reconstructing the full mutation profile, (ii) evaluate count prediction error when using a negative binomial likelihood, and (iii) evaluate mutation-occurrence prediction when using a Bernoulli likelihood\u2014and what does each metric capture?",
      "answer": "(i) Use (log) perplexity computed from the reconstruction log-likelihood of the mutation profile (scaled by the total number of mutations per sample) to measure overall goodness-of-fit of reconstructed mutation profiles. (ii) For negative-binomial count models, use RMSE across genes to summarize average error in predicted mutation counts, and also evaluate tumor mutational load (TML), defined as the sum of all mutations in a sample, reporting the point-estimate error TML* \u2212 TML. (iii) For Bernoulli occurrence models, use F1-score and Positive Predictive Value (PPV): F1 compares mutation-occurrence prediction performance (e.g., across shared-latent-space experiments), while PPV measures precision/positive predictive accuracy, including for specific cancer types (with per-cancer held-out test samples).",
      "source_document": "papers/2512.02162v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a dual-VAE domain-mapping model that uses conditional normalizing-flow priors and a separate shared latent space to translate between lesion-derived features and high-dimensional somatic mutation profiles, how would you train the shared latent space so that it aligns the two domains: what divergence would you use instead of KL, and what practical advantages does this choice provide at test time?",
      "answer": "Train the shared latent space by minimizing Maximum Mean Discrepancy (MMD) between the shared-latent distribution and the flow-transformed encoder distribution (in both directions), rather than using a KL divergence. Using MMD provides (i) the ability to match the shared-latent distribution without assuming a parametric prior, and (ii) if convergence is achieved, the ability to sample from the shared latent space after the learned flow transformation\u2014removing the need to draw random samples from a fixed prior at test time (the only requirement is that the shared-latent dimensionalities match across domains).",
      "source_document": "papers/2512.02162v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a cross-domain latent-variable model to translate sparse, high-dimensional somatic mutation data from imaging-derived embeddings, why might a standard Negative Binomial (NB) likelihood systematically overestimate mutation counts on an imbalanced dataset, and what likelihood modification can better handle the excess zeros typical of mutation profiles?",
      "answer": "An NB likelihood can overestimate counts because, in an imbalanced dataset, frequently co-occurring mutations can induce a \u201ccross-excitation\u201d effect: the presence (or high predicted intensity) of one mutation tends to spuriously raise the predicted count of another that often co-occurs, even when it is actually absent. A practical modification is to use a Zero-Inflated Negative Binomial likelihood, which is better suited to data with many zeros (absent mutations) and can reduce this kind of over-counting.",
      "source_document": "papers/2512.02162v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You want to learn a single lesion-to-genotype model across heterogeneous imaging modalities (e.g., CT and MRI) and lesion shapes. What is the methodological rationale for converting each segmented lesion into an unordered 3D point cloud rather than using pixel/voxel intensities directly, and what preprocessing pipeline can be used to construct the lesion point cloud from clinical images?",
      "answer": "Rationale: representing a lesion as a point cloud makes the representation modality independent because it is an unordered set of (x,y,z) coordinates rather than modality-specific pixel intensities; it also better reflects lesions\u2019 irregular/heterogeneous geometry and reduces computational footprint by storing each lesion as a 2D matrix of points instead of a full 3D voxel volume.\n\nPipeline: (1) extract the lesion from individual image slices using available segmentation labels (or have a radiologist delineate the lesion when labels are missing); (2) convert lesion voxels from image index space to real-world coordinates using metadata stored in the DICOM files; (3) interpolate the resulting lesion volume to form a continuous point cloud representation; and (4) for each lesion sample, uniformly sample 3D points from the surface of the full lesion point cloud to obtain the final input point set.",
      "source_document": "papers/2512.02162v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a dual-VAE domain-mapping model that links lesion point-cloud embeddings to somatic-mutation profiles via a shared latent space, the shared-space flow is conditioned on a cancer-type label. If you ablate (remove) this cancer-type conditioning, what two concrete training/decoding failure modes should you expect, and why do they arise in terms of (i) the learnable conditional prior and (ii) the mutation decoder\u2019s use of lesion-specific features?",
      "answer": "Removing the cancer-type label causes a large performance drop because the label helps the lesion-domain distribution match the structure of the mutation-domain distribution. Without the label: (1) the learnable conditional prior over the mutation-domain latent (e.g., p(z_M0 | z*_M)) is driven by lesion-specific features, which increases the KL divergence in both domains; and (2) the mutation decoder effectively receives lesion-specific features that act as noise for reconstructing mutation profiles, so the reconstructed (predicted) mutation distribution becomes noisy.",
      "source_document": "papers/2512.02162v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a mean-field \u201ck-mer\u201d (sequence-motif) reactor model for prebiotic RNA pools, how can you write the ordinary differential equation for a motif concentration as the sum of an extension (templated ligation) term and a cutting/breakage term, and what concentrations and rate constants appear in each contribution?",
      "answer": "A motif concentration cp is modeled with a rate equation of the form c\u0307p = f_p^(ext) + f_p^(cut). The templated-ligation/extension contribution can be written as f_p^(ext) = k_ext(p,l,r,t) \u00b7 c_l \u00b7 c_r \u00b7 c_t, where l and r are the two reactants (left and right parts), t is the template, and k_ext is the extension rate constant (with the same form but negative sign for the reactant motifs). The cutting/breakage contribution takes the form f_p^(cut) = k_cut(p,b) \u00b7 c_b, where b is the breaking reactant (with products p = l,r and a negative rate for the reactant b).",
      "source_document": "papers/2512.02204v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You want to infer RNA-reactor reaction rate constants from data (e.g., templated ligation counts) using a Bayesian approach that relies on gradient-based optimization through an ODE simulator. What inference method and software choices enable this workflow, and what implementation detail makes the forward model differentiable for inference?",
      "answer": "A workable setup is to use Geometric Variational Inference implemented in nifty8.re (NIFTy) for Bayesian parameter inference, while integrating the reactor ODEs with diffrax and/or scipy.integrate.solve_ivp. The forward model is made differentiable by implementing the computations in JAX: user-level objects are converted to JAX arrays during computation, enabling fast, differentiable models needed by NIFTy\u2019s inference.",
      "source_document": "papers/2512.02204v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In RNA reactor simulations the number of possible sequences grows exponentially with strand length. Describe a practical modeling strategy to keep the state space manageable by explicitly simulating only short strands while still representing the influence of longer strands, and state a typical default cutoff length used for the explicitly tracked strands.",
      "answer": "A tractable strategy is to explicitly follow the reactions and concentrations of all RNA strands only up to a maximum length, and represent longer strands indirectly by tracking the concentrations of the sequence motifs (k-mers) of that maximum length that occur within them. A typical default cutoff for explicitly tracked strands is 4 nucleotides.",
      "source_document": "papers/2512.02204v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When implementing an RNA reactor simulation and inference framework that manipulates physical quantities (e.g., concentrations and rate constants), what is a practical way to enforce unit consistency in the library\u2019s core \u201cdomain\u201d objects beyond just tracking array shapes, and what problem does this prevent?",
      "answer": "A practical approach is to extend the core domain objects so they store the physical unit metadata in addition to the shape information. Keeping units attached to domains helps ensure consistency of quantities throughout simulation/inference (e.g., avoiding combining or comparing arrays with incompatible units).",
      "source_document": "papers/2512.02204v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a simulation-and-inference workflow for an RNA reactor model, you often want to repeatedly compute the same derived quantities (e.g., observables from parameters or data) without rerunning expensive computations. Describe a practical software design pattern to support this, including (i) what component computes observables, and (ii) how a \u201cgetter\u201d component uses a maintained result archive to either reuse previously computed results or trigger fresh generation.",
      "answer": "A practical pattern is to separate (i) an inference/analysis component that computes observables from parameters or data, and (ii) a \u201cget\u201d component that implements caching via a result archive. The inference module provides the functions that compute the observables. The get module then checks a maintained result archive: if a stored result object is found it reads/loads it, otherwise it triggers generation of the result via the inference module and can then store it for later reuse.",
      "source_document": "papers/2512.02204v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are training a sequence model that uses the previous 12 months of satellite+climate covariates to forecast next-month (t+1) species occurrence on a spatial grid. What evaluation/validation protocol can both (i) simulate operational deployment as new months arrive and (ii) prevent overly optimistic results due to spatial and temporal leakage? Describe how the time windows and folds are constructed.",
      "answer": "Use rolling-origin evaluation with warm-start initialization to mimic deployment: for each forecast window, train on all available historical data up to that point (then evaluate on the next time step/period). To avoid leakage and over-optimism, pair this with spatio-temporal block cross-validation in which folds are defined by spatial blocks and by holding out years (temporal blocks), so training and testing are separated in both space and time.",
      "source_document": "papers/2512.02260v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a spatio-temporal transformer that is deployed operationally and fine-tuned each month as new (t+1) observations arrive, what update strategy can be used to reduce catastrophic forgetting of previously learned ecological patterns? Describe (i) what data are replayed during the update, (ii) what parameter-constraint/regularization is applied, and (iii) how these terms are combined in the fine-tuning objective.",
      "answer": "Use a constrained continual-learning update that combines rehearsal (experience replay) with weight-regularization via Elastic Weight Consolidation (EWC). (i) Maintain a fixed-size replay buffer of past training examples and mix those replayed examples with the new month\u2019s data during the update. (ii) Apply an EWC penalty that discourages changing parameters that were estimated to be important for performance on prior data (i.e., penalize deviations of the updated parameters \u03b8\u2032 from the previous parameters \u03b8, weighted by parameter importance). (iii) Optimize a fine-tuning objective that adds the supervised loss on the new (and replay) data to a regularization term: minimize over \u03b8\u2032 the sum of the task loss L(f_{\u03b8\u2032}(x), y) plus \u03bb\u00b7\u03a9_EWC(\u03b8\u2032, \u03b8), where \u03a9_EWC is the EWC penalty and \u03bb controls the stability\u2013plasticity tradeoff.",
      "source_document": "papers/2512.02260v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a near-term species-occurrence forecasting model on presence-only citizen-science records, what data-processing and learning choices can you use to (i) construct negatives and (ii) reduce sampling-bias and class-imbalance effects? Describe the specific negative-label strategy, the bias-mitigation covariates/filters, and the batching/loss adjustments.",
      "answer": "Use pseudo-absences to create negatives by sampling them within the study area and within the same temporal window as the presence records. Reduce sampling bias by spatially thinning occurrences to the model\u2019s 0.1\u00b0 grid (to limit clustered reports) and by adding a sampling-effort covariate computed from monthly GBIF record density to account for uneven observer effort. Stabilize learning under rare presences via stratified mini-batches that balance presence vs pseudo-absence instances per species and a class-weighted (class-imbalance-robust) loss/weighting scheme for the positive class.",
      "source_document": "papers/2512.02260v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You deploy a spatio-temporal transformer to forecast next-month species occurrence from 12-month sequences of satellite and climate covariates. What interpretability workflow can you use to (i) identify which months/regions most influenced a forecast and (ii) sanity-check that these attributions reflect real ecological signal rather than artifacts (e.g., clouds)?",
      "answer": "Use transformer attention rollout over space\u2013time to highlight influential regions and months, and then validate the attribution maps with patch-wise permutation tests. As an additional diagnostic, apply a Grad-CAM-style attribution method adapted to Vision Transformers (ViTs), and flag cases where the attributions contradict ecological priors (for example, if the model focuses on clouds instead of vegetation change).",
      "source_document": "papers/2512.02260v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are choosing between (i) a month-independent tabular model (e.g., a random forest on per-month covariates) and (ii) a transformer that ingests a 12-month environmental sequence to forecast next-month bird occurrence. What key temporal ecological dependencies would you expect the transformer to capture that the tabular model will typically miss, and what parts of the transformer design enable capturing each dependency?",
      "answer": "A sequence transformer can capture temporal structure that a month-independent tabular model generally misses:\n\n1) Temporal autocorrelation in occurrence: presence in month t is often predictive of presence in month t+1 (especially for resident species). A transformer can condition its prediction on the whole recent history via self-attention over the 12-month sequence, whereas a tabular model treats each month as an independent sample.\n\n2) Lagged environmental responses: species may respond to environmental anomalies with multi-month delays (e.g., rainfall affecting food availability 2\u20134 months later). Self-attention over the input sequence lets the model learn these lagged relationships directly without manually engineering lag features.\n\n3) Seasonal periodicity/annual cycles (e.g., migratory phenology): the transformer can represent seasonality through positional embeddings over the 12 monthly steps, allowing it to encode annual patterns; a random forest typically requires hand-crafted seasonal indicators to represent such cycles.",
      "source_document": "papers/2512.02260v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Bayesian divergence-time (node-age) estimation that replaces the full phylogenetic likelihood with a pairwise composite likelihood, credible intervals can be miscalibrated because the composite likelihood is misspecified (it ignores higher-order dependence). How can you \u201cmagnitude-adjust\u201d the pairwise composite likelihood using asymptotic composite likelihood ratio theory to better match full-likelihood uncertainty? Define the key matrices involved and give the two moment-matching adjustment weights often used (APW1 and APW2), including how they are computed from eigenvalues.",
      "answer": "Composite-likelihood inference uses the composite score uc(\u03b8)=\u2207\u03b8\u2113c(\u03b8) and the sensitivity and variability matrices H(\u03b8)=\u2212E[\u2207\u03b8uc(\u03b8)] and J(\u03b8)=E[uc(\u03b8)\u2032uc(\u03b8)]. Because the likelihood is composite, generally J(\u03b80)\u2260H(\u03b80), so asymptotic theory uses the Godambe information G(\u03b80)=H(\u03b80)J(\u03b80)\u22121H(\u03b80). The composite likelihood ratio statistic \u039bc=\u22122(\u2113c(\u03b8\u0302c)\u2212\u2113c(\u03b80)) has an asymptotic weighted chi-square form with weights given by eigenvalues of H(\u03b80)\u22121J(\u03b80) (in the common case where \u03b8 itself is of interest).\n\nA magnitude-adjusted (adjusted pairwise) likelihood raises the composite likelihood to a constant power w>0: fac(\u03b8|X)=fc(\u03b8|X)^w (equivalently \u2113ac=w\u00b7ln fc). Two standard moment-matching choices based on the eigenvalues \u03bbi of H\u22121J (with p parameters) are:\n- APW1: w1 = p / (\u2211_{i=1}^p \u03bbi), chosen so that E[w1\u039bc] matches the mean of the usual \u03c7\u00b2 reference.\n- APW2: w2 = (\u2211_{i=1}^p \u03bbi) / (\u2211_{i=1}^p \u03bbi\u00b2), chosen to match the first two moments, yielding an asymptotic \u03c7\u00b2_\u03bd with \u03bd = (\u2211 \u03bbi)\u00b2/(\u2211 \u03bbi\u00b2).",
      "source_document": "papers/2512.02312v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You\u2019re designing a few-shot protein fitness predictor where the key objective is to prioritize (rank) promising variants rather than accurately regress absolute fitness values. How can you formulate a training objective that directly optimizes correct ranking from sparse labeled examples, and what is the specific pairwise preference-based loss (including its interpretation) used to reconstruct masked fitness labels in this approach?",
      "answer": "The approach trains the model to predict *relative* fitness by masking some fitness labels within a set of N variants from the same assay and asking the model to reconstruct them using a pairwise preference objective. During pre-training, either a fitness label or an amino-acid span is masked (labels are masked with 33% probability; otherwise spans are masked with 20%, placed to cover mutated regions). Amino-acid tokens are reconstructed with a standard cross-entropy masked-token loss. \n\nFor masked fitness labels, the model uses a preference-based ranking loss over the Q masked samples in the set (unmasked samples act only as context). The loss is\n\nL = \\sum_{i=1}^{Q} \\sum_{j=1}^{Q} -\\mathbb{I}(x_i^{\\mathrm{Fitness}} > x_j^{\\mathrm{Fitness}}) \\log \\sigma(\\hat y_i - \\hat y_j),\n\nwhere \\(\\mathbb{I}\\) is an indicator of which variant truly has higher fitness and \\(\\sigma\\) is the sigmoid. This is equivalent to performing \\(Q\\times Q\\) binary classifications on whether variant i should be ranked above variant j, directly optimizing correct ordering rather than absolute calibration.",
      "source_document": "papers/2512.02315v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a transformer that predicts protein variant fitness from a *set* of N related sequences, you want sequences to share information but also need to (i) support insertion/deletion (indel) variants with variable lengths/misaligned positions and (ii) avoid the O(N^2L^2) cost of full \u201csequence-of-sequences\u201d self-attention. What inter-sequence attention design achieves these goals, what are the main computation steps, and how does its asymptotic complexity compare to full attention?",
      "answer": "Use a lightweight pooled cross-sequence attention mechanism rather than column-wise or full sequence-of-sequences attention. First, run standard self-attention separately within each sequence hi: hi = MHA(hi, hi, hi) for i=1..N. Then pool each sequence into a fixed-size representation pi using attention pooling, pi = MHA(Mean(hi), hi, hi). Concatenate pooled vectors across the set p = Concat(p1,\u2026,pN), and let each sequence cross-attend to the pooled set: hi = MHA(hi, p, p). This exchanges information between sequences via the pooled representations, which is compatible with indels because it does not require aligned columns/positions (column attention becomes unsuitable when insertions/deletions cause misalignment and variable lengths). Complexity: per-sequence self-attention is O(NL^2); pooling is O(NPL); pooled cross-sequence attention is O(N^2PL). With pooled size P typically much smaller than L (NP \u226a L), the limiting scaling is O(NL^2), improving over full sequence-of-sequences attention O(N^2L^2).",
      "source_document": "papers/2512.02315v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a few-shot protein fitness predictor that uses in-context learning over a small set of labeled variants, you may encounter a distribution shift where simply conditioning on the context does not let the model effectively use the labels. Describe a test-time training (TTT) protocol that adapts the model to the specific assay at inference time without needing a held-out validation set: what data are used for the adaptation, what loss/objective is optimized during TTT, how the optimization is run (e.g., number of steps and what parameters are/aren\u2019t updated), and what happens to the adapted weights after making predictions.",
      "answer": "Use test-time training by fine-tuning the model\u2019s weights on the same few-shot context set that will be used for prediction. Rather than a purely self-supervised objective, optimize the model\u2019s existing hybrid objective that combines masked amino-acid token reconstruction with preference-based fitness label reconstruction, by repeatedly sampling masked sets from the few-shot context. Run a short, fixed-length gradient-descent adaptation (25 steps, using the same learning rate and loss as pre-training). Keep the underlying protein language model used for sequence embeddings frozen during TTT. After producing predictions on the test variants, discard the adapted weights (i.e., do not retain task-specific parameters).",
      "source_document": "papers/2512.02315v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When pre-training a model on many deep mutational scanning (DMS) assays and then evaluating it on \u201cnew\u201d assays for few-shot/zero-shot protein fitness prediction, what specific form of train\u2013test leakage can severely inflate apparent performance even if the held-out assay is a different experiment, and what splitting/holdout strategy (including how similarity is quantified) can be used to prevent this inflation?",
      "answer": "A major leakage mode is protein-level overlap: the train set can contain the same wild-type protein (or a very close homolog with near/100% sequence identity) as the test assay, often with thousands of training measurements of the same protein from other assays/experiments. This makes reported \u201czero-shot\u201d (and downstream few-shot) performance look artificially high because the model has effectively already seen that protein\u2019s sequence/landscape. To prevent this, construct a holdout split that controls overlap at the protein level by excluding test assays whose wild-type protein has a close homolog/identical sequence in the training set. Similarity between proteins is quantified as pairwise global sequence identity computed via Needleman\u2013Wunsch alignment of the wild-type sequences.",
      "source_document": "papers/2512.02315v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You train a transformer to predict *relative* protein fitness by reconstructing masked labels with a preference/ranking head (so its native output is not an absolute fitness value). At inference time, how can you turn this into a usable fitness score for (i) few-shot prediction with Q labeled context variants and (ii) the zero-shot setting with no experimental labels\u2014specifically, what masking procedure is used, what model output is taken as the prediction, and what \u201creference\u201d context must be provided in the zero-shot case (including what reference fitness value is used and why such a reference is needed)?",
      "answer": "For inference, the model is run in a masked-label reconstruction mode: you condition on Q unmasked (context) samples and predict each candidate/test variant by masking its fitness label and passing the set through the model. The model\u2019s **unnormalized preference score** for the masked sample is used as that sample\u2019s fitness prediction.\n\nBecause the model is designed to predict **relative fitness**, a pure zero-shot query still requires a reference context sample to define the comparison baseline. In the zero-shot setting, you therefore provide an **arbitrary sequence** with an **arbitrary fitness of 0.5 (the center of a min\u2013max scale)** as an unmasked context item when scoring. This plays the same conceptual role as providing a reference (often wild-type) sequence in masked-LM-style scoring.",
      "source_document": "papers/2512.02315v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In protein\u2013ligand docking algorithm selection using learned performance prediction from protein/ligand embeddings, what dataset-level diagnostics indicate that selection is likely to beat the single-best solver, and what characteristic failure mode should you expect when the docking workflow (e.g., relaxation/post-processing regime) changes between training and testing?",
      "answer": "Selection is most likely to improve over the single-best solver when (i) the oracle landscape has low-to-moderate diversity of winners (low VBS entropy / a \u201ccompact, low-entropy portfolio\u201d) and (ii) different docking algorithms are at least partially separable in embedding space (distinct/partially distinct regions; better silhouette/tighter within-algorithm clusters). In these regimes, even if the selector does not pick the exact oracle-best, choosing among the top few algorithms still yields gains because the solver hierarchy is stable and several solvers are competitive.\n\nWhen the workflow changes between training and testing, the solver hierarchy (rankings of algorithms) shifts; the learned selector has fit workflow-specific priors tied to the training protocol\u2019s pose-generation and scoring landscape. Under such protocol-induced hierarchy shifts, performance can collapse toward an SBS-like default policy (or worse), producing systematic negative transfer because changing the docking protocol effectively changes the algorithm-selection problem/oracle structure.",
      "source_document": "papers/2512.02328v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a supervised algorithm-selection model for protein\u2013ligand docking, how can you define a training target that (i) provides a smooth learning signal around the standard \u201cnear-native\u201d RMSD cutoff, and (ii) ensures geometrically close but chemically impossible poses are not rewarded? Describe the components of such a score and how they are combined.",
      "answer": "Define the per-pose target score as a smooth RMSD-based accuracy term gated by a physicochemical validity indicator.\n\n1) Geometric accuracy: use a sigmoid-based function of RMSD, centered at the common 2 \u00c5 near-native threshold so the gradient is largest near that boundary and differences among moderately poor poses still carry signal:\n\ns_RMSD(x; \u03bb) = (1 + e^{-2\u03bb}) / (1 + e^{\u03bb(x \u2212 2)}),  with \u03bb > 0 controlling sensitivity.\n\n2) Physicochemical plausibility: define a binary PoseBusters-validity gate, s_PB = 1 if the pose passes all PoseBusters checks (18 checks in the benchmark), else 0.\n\n3) Combine them by multiplication to prevent chemically invalid poses from receiving a nontrivial score:\n\ns = s_RMSD \u00b7 s_PB.",
      "source_document": "papers/2512.02328v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In docking algorithm selection using a model that predicts a per-algorithm score vector, how can the *predicted score margin* (difference between the top-1 and top-2 predicted scores) be interpreted as a deployment-time confidence signal, and why can increasing margin lead to *higher oracle agreement* but *lower probability of beating the single-best solver (SBS)*? Also state when this margin is expected to be a trustworthy confidence indicator versus when it is least informative.",
      "answer": "The score margin can be used as a (partially calibrated) confidence indicator about how strongly the selector prefers its top-ranked algorithm over the runner-up. Empirically, when the margin is small, the selector rarely matches the oracle best algorithm, but it can still often outperform the SBS because it explores alternative solvers that are, on average, better than the SBS in \u2018healthy\u2019 portfolios where several solvers have similar oracle scores. As the margin grows, the pattern reverses: oracle agreement (reliability) increases, but the probability of outperforming the SBS decreases because large margins primarily reflect confident *re-selection of the SBS* (i.e., the model defaults back to the dominant baseline rather than choosing a genuinely better alternative). The margin is most trustworthy as a confidence signal when oracle diversity is low or the algorithm portfolio is small; it is least informative on chemically richer benchmarks with higher oracle diversity/entanglement (e.g., the PoseX-type regimes), where high margins do not reliably indicate a meaningful advantage over SBS.",
      "source_document": "papers/2512.02328v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You want to train and evaluate an *instance-wise algorithm selection* model for protein\u2013ligand docking (i.e., pick one solver per complex). What per-complex data must a benchmark provide to (i) define the Single Best Solver (SBS) and Virtual Best Solver (VBS) baselines and (ii) create supervised training targets, and why are many standard docking datasets (e.g., those providing only one model\u2019s submissions, aggregated scores, or affinity labels) unusable for this purpose unless you rerun all docking methods under a controlled protocol?",
      "answer": "An algorithm-selection benchmark must provide, for each protein\u2013ligand complex, a full *performance profile across a solver portfolio*\u2014i.e., per-algorithm predicted poses (or otherwise per-algorithm outcomes) scored under the same evaluation/scoring protocol. These per-instance, per-algorithm results are required to compute SBS (the single solver with best mean performance over the benchmark), VBS (an oracle picking the highest-scoring solver per instance), and the supervised labels/targets (the vector of per-algorithm scores for that instance). Datasets that only offer single-model submissions, only aggregated scoring-function results, or only affinity labels do not expose per-algorithm instance-level performance profiles, so SBS/VBS and training labels cannot be constructed without recomputing all algorithms\u2019 predictions under a controlled protocol.",
      "source_document": "papers/2512.02328v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are designing a learned algorithm-selection model for protein\u2013ligand docking that takes pretrained protein and ligand embeddings as input. How can ablation results be used to distinguish a *data-limited* regime from an *architecture-limited* regime, and what specific patterns in model variants (e.g., adding GNN encoder stacks, increasing attention heads, increasing residual-decoder capacity, or swapping in ranking-aware objectives) would support the conclusion that additional labeled complexes are more valuable than further architectural scaling?",
      "answer": "Ablations can indicate a data-limited regime when substantial increases in model expressiveness do not yield consistent, transferable gains, and performance differences across variants remain within a narrow band.\n\nConcretely, the evidence supporting \u201cdata-limited, not architecture-limited\u201d includes these patterns:\n- Replacing the embedding-only encoder with more complex graph encoders (parallel GNN stacks such as GCN\u2013GAT\u2013GINE and an equivariant EGNN\u2013GAT\u2013GINE variant) fails to outperform the embedding-only setup, implying the pretrained residue-level protein embeddings already capture most of the usable structural signal at the available data scale.\n- Increasing the number of attention heads in the pooling/fusion stage produces only sub-percentage fluctuations with no consistent trend.\n- Expanding decoder capacity (deeper or wider residual stacks) yields at best marginal gains on one benchmark but harms another; even a single linear decoder can peak on one dataset (PoseX+Astex under the 2 \u00c5 & PoseBusters-valid criterion) while reducing performance on another (MOAD-curated by ~2%), indicating redundancy/overfitting rather than missing capacity.\n- Changing the learning objective to ranking-aware losses (pairwise logistic, NDCG@3, or combining them with BCE) produces only noise-level variation, with the overall spread across ablations staying within ~2%.\n\nTogether these patterns support the conclusion that the main constraint is limited and heterogeneous labeled training data, so exploiting larger architectures would require additional labeled complexes rather than further scaling depth/complexity.",
      "source_document": "papers/2512.02328v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking scRNA-seq clustering methods against known cell-type labels, what two cell-type annotation strategies can be used to map predicted clusters to reference types, and how is the marker-overlap mapping score computed (including how the marker lists are constructed)?",
      "answer": "Two complementary annotation strategies are:\n1) Best-mapping annotation: directly align predicted cluster labels to ground-truth cell-type labels by maximizing one-to-one correspondence with the Hungarian algorithm, without using gene expression.\n2) Marker-overlap annotation: build a gold-standard marker list by detecting DEGs per ground-truth cluster (using Scanpy\u2019s `rank_genes_groups` with default settings) and taking the top 100 DEGs per reference cluster; apply the same DEG procedure to each model\u2019s predicted clusters to obtain 100-marker lists per predicted cluster. For a predicted cluster p and gold-standard cluster g, compute the overlap score as\nscore(p,g) = |DEG_p \u2229 DEG_g| / 100.\nAssign p to the gold-standard cell type g with the highest overlap score.",
      "source_document": "papers/2512.02471v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In scRNA-seq clustering, how can you diagnose over-smoothing/representation collapse in a model\u2019s learned cell embeddings using an embedding-similarity analysis, and what qualitative pattern in the similarity distribution indicates collapse versus well-separated representations?",
      "answer": "One way to diagnose over-smoothing (representation collapse) is to compute pairwise cosine similarities between all cell (sample) embeddings within each dataset and then examine the resulting probability distribution of these similarities. If most embedding pairs concentrate at very high cosine similarity (near 1), the embeddings are largely indistinguishable\u2014consistent with representation collapse/over-smoothing. In contrast, a distribution with a substantial fraction of low-similarity pairs (values closer to 0) indicates many embeddings are dissimilar and therefore more easily distinguished, suggesting better-separated representations (less collapse).",
      "source_document": "papers/2512.02471v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In benchmarking scRNA-seq clustering algorithms for deployment in real analyses, how can you operationalize *algorithmic stability* with two complementary criteria, and what Reiducible thresholds (including number of runs/seeds and allowable variation) would you use to decide whether a method is stable enough for approval?",
      "answer": "Algorithmic stability can be assessed with (1) **seed robustness** and (2) **reproducibility across computational environments**. Seed robustness is defined by requiring the clustering metric variability to be small across repeated runs: standard deviation **< 5%** over **at least 5 independent runs** with different random seeds (to ensure results are not initialization-dependent). Reproducibility is defined by requiring essentially the same results when the code is run in different environments: results must remain within **2% variation** across different computational environments, including different hardware configurations and software versions.",
      "source_document": "papers/2512.02471v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are selecting an scRNA-seq clustering method for routine use in a core facility and want to include *computational performance* as an explicit approval gate (separate from clustering accuracy). How can you operationalize this gate using (i) a training-efficiency criterion and (ii) an inference-scalability criterion, including concrete thresholds/requirements and the additional practical considerations that should be checked beyond wall-clock time?",
      "answer": "Operationalize computational performance with two complementary criteria:\n\n(i) Training efficiency: require methods to finish \u201ctraining\u201d within a practical budget, using method-specific standards\u2014deep learning methods must converge within 200 training epochs, while traditional clustering methods must complete processing within 25 minutes on standard datasets. In addition to time/epochs, check memory scalability, whether GPU access is required/available, and sensitivity to hyperparameters.\n\n(ii) Inference scalability: require linear or sub-linear time complexity as the number of cells increases, plus the ability to handle large datasets via batch processing. Also require support for distributed computing and (where relevant for deployment) real-time processing capability.",
      "source_document": "papers/2512.02471v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are benchmarking multiple scRNA-seq clustering algorithms across many public datasets that may have been preprocessed differently. What standardized preprocessing *checks* and *transformations* would you apply to each dataset to make clustering results comparable, and what is the purpose of each step (normalization/size factors, log transformation, and scaling)?",
      "answer": "Use a uniform pipeline after loading each dataset (e.g., from .h5ad into an AnnData object) that first checks whether the data have already been (i) normalized, (ii) log1p-transformed, and (iii) scaled. For datasets lacking these steps: perform library-size normalization to reduce technical variability due to sequencing depth; compute cell-specific size factors to standardize expression levels across cells; apply a log1p transform to address the strong skewness of gene expression distributions; and finally apply z-score scaling to center and scale gene expression so features have comparable variance, yielding a standardized input suitable for fair cross-dataset evaluation of clustering methods.",
      "source_document": "papers/2512.02471v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a molecular communication system over a vessel network with multiple TX\u2013RX paths, how can you summarize the topology-induced temporal dispersion into (i) a single \u201cmean delay\u201d metric and (ii) a \u201cmulti-path spread\u201d metric using the peak arrival times of each path and weights proportional to the fraction of molecules transported on that path? Explain how these two metrics together predict the qualitative trend in received SNR, and why the spread metric is zero for a single straight channel.",
      "answer": "Temporal dispersion is summarized by treating the VN as a multi-path channel whose k-th path Pk has a peak (most-probable) arrival time t_peak,Pk and carries a fraction of molecules reflected by a nonnegative weight \u03b3_Pk (giving more weight to paths transporting more molecules).\n\n(i) The molecule delay between two nodes (e.g., TX node n_a and RX node n_b) is defined as the \u03b3-weighted mean of the path peak times:\n\nt^{n_b}_{n_a} = \u03a3_{Pk\u2208P(n_a,n_b)} \u03b3_Pk \u00b7 t_peak,Pk .\n\nThis parallels \u201cexcess delay\u201d in wireless multipath channels.\n\n(ii) The multi-path spread is defined as the \u03b3-weighted root-mean-square deviation of the path peak times around the mean delay:\n\n\u03c3^{n_b}_{n_a} = sqrt( \u03a3_{Pk\u2208P(n_a,n_b)} \u03b3_Pk \u00b7 (t_peak,Pk \u2212 t^{n_b}_{n_a})^2 ).\n\nThis parallels RMS delay spread.\n\nTogether, (t^{n_b}_{n_a}, \u03c3^{n_b}_{n_a}) define a 2-D \u201cdispersion space\u201d location that depends only on topology (via the set of paths, their flow/molecule fractions, and their peak times). Larger delay and/or larger spread imply molecules arrive more spread out in time, reducing instantaneous concentration at the receiver and therefore reducing received signal strength relative to noise; thus SNR decreases systematically as t^{n_b}_{n_a} and \u03c3^{n_b}_{n_a} increase.\n\nFor a single straight channel there is only one TX\u2013RX path, so the weighted variance term is zero (t_peak,Pk equals the mean), yielding \u03c3^{n_b}_{n_a}=0; nonzero \u03c3 requires at least two distinct paths with differing peak times (i.e., true multi-path propagation).",
      "source_document": "papers/2512.02811v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You want a *transparent* receiver model for molecular communication using superparamagnetic iron-oxide nanoparticles (SPIONs) and a planar inductive coil placed adjacent to a vessel (not wrapped around it). How can you map the 1-D longitudinal SPION concentration field in the receiver pipe, c(z,t), into a measurable resonance-frequency shift signal \u0394f_res(t) of an LC oscillator? In your answer, state (i) why a spatial weighting function is required and how it is defined from magnetic susceptibility and magnetic-field strength, (ii) how this yields a time-varying effective permeability/inductance, and (iii) the resulting expression for \u0394f_res(t) and the assumed noise/SNR model.",
      "answer": "(i) A spatial weighting function is needed because the magnetic field around a planar coil is non-homogeneous, so SPIONs at different longitudinal positions contribute unequally to the sensed signal. The model captures this via a unitless weighting function proportional to the magnetic field magnitude along the pipe and the reference susceptibility: w(z)=\u03b2 \u03c7_ref |\u03a5(z)|, where |\u03a5(z)| is the magnetic-field strength around the coil (obtained from simulation for the actual coil geometry) and \u03b2 is a proportionality constant. The time-varying volume magnetic susceptibility is then computed as \u03c7_v(t)=\u222b_{z\u2208dom(w)} w(z) c(z,t) dz.\n\n(ii) The increased susceptibility implies an increased relative permeability and thus inductance: L(t)=\u03bc_r(t) L_0=(1+\u03c7_v(t)) L_0, where L_0 is the baseline coil inductance when no SPIONs are nearby.\n\n(iii) The LC circuit resonance is f_res(t)=1/(2\u03c0\u221a(L(t)C)). With f_res,0 defined for L(t)=L_0, the deterministic received signal is the resonance-frequency shift r(t)=\u0394f_res(t)=f_res,0\u2212f_res(t), which can be written explicitly as \u0394f_res(t)=(\u221aL(t)\u2212\u221aL_0)/(2\u03c0\u221a(L_0 L(t) C)). Measurement noise is modeled as additive, signal-independent white Gaussian noise n_sensor~N(0,\u03c3^2), giving the stochastic observation \u0394F_res(t)=\u0394f_res(t)+n_sensor(t). A time-averaged SNR is defined over a chosen interval [t0,t1] as SNR=10 log10( (1/(t1\u2212t0)) \u222b_{t0}^{t1} \u0394f_res(t)^2 dt / \u03c3^2 ), with [t0,t1] selected to capture (virtually) all signal energy.",
      "source_document": "papers/2512.02811v1.pdf",
      "mode": "textual",
      "content_refs": [
        "Lines 1066-1179"
      ]
    },
    {
      "question": "You are building a reduced 1D channel model for molecular communication in a cylindrical vessel network where radial mixing is assumed to be fast at the injection site and at branching points (so you track only the cross-section\u2013averaged concentration along each pipe). How can you collapse (i) molecular diffusion, (ii) turbulence-induced mixing near injection/branches, and (iii) shear-induced (Aris\u2013Taylor) dispersion into a single longitudinal effective diffusion coefficient for each pipe? Give the expressions for the molecular diffusion coefficient, the eddy diffusion coefficient (including its dependence on pipe radius and mean flow velocity and the allowed range of its proportionality constant), the resulting \u201capparent\u201d diffusion coefficient, and the final Aris\u2013Taylor effective diffusion coefficient used in the 1D PDE.",
      "answer": "Per pipe p_i with radius r_i and mean cross-sectional velocity u_i, diffusion effects are combined as follows:\n\n\u2022 Molecular diffusion is modeled by the Stokes\u2013Einstein expression\nD = (k_B T)/(6\u03c0 \u03bc R),\nwhere k_B is Boltzmann\u2019s constant, T the fluid temperature, \u03bc the dynamic viscosity, and R the signaling-molecule radius.\n\n\u2022 Turbulence-induced mixing at injection and VN branching points is represented by an eddy diffusion coefficient\nK_i = \u03b1 u_i r_i,\nwith a unitless proportionality constant \u03b1 constrained to \u03b1 \u2208 [0, 2].\n\n\u2022 Molecular and eddy diffusion are treated as additive, yielding an apparent diffusion coefficient\nD_i = D + K_i.\n\n\u2022 Shear-induced dispersion from laminar flow together with the apparent diffusion is then collapsed into the Aris\u2013Taylor effective longitudinal diffusion coefficient\nD_eff,i = (r_i^2 u_i^2)/(48 D_i) + D_i.\nThis D_eff,i is the single longitudinal diffusion coefficient used in the 1D advection\u2013diffusion(-sorption) model for pipe i.",
      "source_document": "papers/2512.02811v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a 1D cross-section\u2013averaged molecular communication model for a directed vessel network (pipes connected by bifurcations and junctions), how can you construct the end-to-end channel impulse response (CIR) between two nodes when (i) molecules may reversibly sorb to vessel walls (fluid + solid phases) within each pipe and (ii) there are multiple distinct paths from TX to RX? In your answer, state (a) how flow partitioning at a junction is captured as a multiplicative factor on a traversed inflow pipe, (b) how the CIR contribution of a single path is formed from convolutions of per-pipe outlet fluxes, and (c) how the resulting path contribution is combined with the (fluid+solid) concentration response in the RX pipe and then summed across paths.",
      "answer": "(a) At a junction jm where multiple inflow pipes merge, the fraction of molecules contributed by an inflow pipe pi\u2208I(jm) is modeled by a mass-conservation partition factor\n\u03b3_i^m = Q_i / (\u2211_{p_v\u2208I(j_m)} Q_v), with 0<\u03b3_i^m\u22641,\nso junction traversal contributes a multiplicative weight (product over junctions on the path). (Bifurcations do not appear explicitly because the cross-section\u2013averaged concentration is taken as unchanged across a bifurcation.)\n\n(b) Under linear molecule transport, for any path P_k between nodes n_a and n_b, the path \u201ctransfer function\u201d up to the inlet of the RX pipe p_{i\u2032} is the time-convolution of the normalized outlet flux PDFs of all pipes on that path except the RX pipe, multiplied by the product of junction weights along the path:\n(\u229b_{i\u2208E_k: p_i\u2260p_{i\u2032}} J_i(l_i,t)) \u00b7 (\u220f_{(i,m)\u2208E_k\u00d7J_k: p_i\u2208I(j_m)} \u03b3_i^m),\nwhere \u229b denotes convolution over the set of pipes with respect to time.\n\n(c) The end-to-end CIR at position z in the RX pipe is obtained by convolving this upstream-path term with the combined (fluid + solid phase) concentration response in the RX pipe and summing over all distinct paths:\nh_{n_a,n_b}(z,t)= [\u2211_{P_k\u2208P(n_a,n_b)} ( (\u229b_{i\u2208E_k: p_i\u2260p_{i\u2032}} J_i(l_i,t))\u00b7\u220f\u03b3_i^m ) ] * (1/N)(c_{i\u2032}(z,t)+s_{i\u2032}(z,t)),\nwith * the time convolution. For an arbitrary injection \u03bb(t) at n_a, the end-to-end concentration is then c_{n_a,n_b}(z,t)=\u03bb(t)*h_{n_a,n_b}(z,t).",
      "source_document": "papers/2512.02811v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are validating a molecular-communication receiver based on a planar inductive coil that outputs a resonance-frequency shift signal \u0394F_res(t). You repeat the same impulse-response experiment many times for each channel setting. How can you (i) empirically identify an appropriate statistical noise model for the sensor output from these repeated measurements, and (ii) define a time-averaged SNR metric from the deterministic model prediction \u0394f_res(t) and the noise variance? State the resulting stochastic observation model and the SNR expression, including how the time interval used in the SNR should be chosen.",
      "answer": "(i) For each channel setting, compute the ensemble-averaged measured CIR over time, then collect the pointwise deviations of each individual measured CIR from this average and analyze their distribution. The deviations are well modeled as signal-independent additive white Gaussian noise with zero mean and constant variance across settings and times: n_sensor(t) ~ N(0, \u03c3^2) (in Hz units). \n\n(ii) With deterministic model output \u0394f_res(t), the stochastic observation is modeled as\n\u0394F_res(t) = \u0394f_res(t) + n_sensor(t),\nso \u0394F_res(t) is a random variable for each t. A time-averaged SNR over an interval [t0, t1] is defined as\nSNR = 10 log10( (1/(t1 \u2212 t0)) * \u222b_{t0}^{t1} \u0394f_res(t)^2 dt / \u03c3^2 ).\nThe interval [t0, t1] should be chosen to capture (virtually) the entire energy of \u0394f_res(t) (e.g., by taking the time range covering the full non-negligible signal duration).",
      "source_document": "papers/2512.02811v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a stochastic gene-regulation model where promoter activation requires binding of multiple transcription-factor molecules (e.g., a reaction of the form D0 + \u03a3_j n_j P_j \u2192 D1 with rate constant k(t)), how does a binomial capture/measurement process with per-protein capture probabilities p_j distort the *apparent* kinetic parameters inferred from observed single-cell protein counts? State the effective renormalized binding rate and describe when this rate-renormalization approximation is expected to hold.",
      "answer": "Under imperfect detection modeled as independent binomial capture with probability p_j for protein P_j, the promoter-binding rate constant is renormalized in the effective (observation-level) model as\n\nk(t) \u2192 k(t) / \u220f_j p_j^{n_j},\n\nso binding appears faster by a factor 1/\u220f_j p_j^{n_j}. More generally, technical noise also reduces apparent synthesis/burst sizes by a factor p (e.g., mean burst size scales as p times the true mean).\n\nThis binding-rate renormalization holds in the limit of protein abundance (where moment-closure/PDMP approximations apply) and is also supported without assuming high protein abundance when promoter switching is on a separated timescale: either slow switching (binding and unbinding much slower than other reactions) or a special fast-switching regime where binding is much faster than unbinding and both are much faster than the remaining reactions.",
      "source_document": "papers/2512.02908v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In stochastic promoter models for transcription where you add a binomial capture process (each mRNA detected independently with probability p), under what structural condition on the transcription reactions can the effect of imperfect detection be absorbed exactly into a simple renormalization of kinetic rates? Explain why this rate-renormalization generally fails when transcription also changes the promoter state, and state the special kinetic limit in which an approximate mapping becomes valid for a promoter model that includes a paused polymerase state.",
      "answer": "The capture process can be absorbed exactly into a parameter renormalization when transcription does **not** change the promoter state: for reactions of the form Di \u2192 Di + M (i = w in the notation Di \u2192 Dw + M), the observed generating-function dynamics are identical to the true dynamics after scaling the transcription rates by p (riw(t) \u2192 p\u00b7riw(t)).\n\nIf transcription **does** change promoter state (i \u2260 w), this equivalence generally breaks because the substitution z \u2192 1 \u2212 p(1 \u2212 z) does not preserve the algebraic form of the transcription terms. In a three-state model with a paused state where transcription corresponds to pause release (D3 \u2192 D2 + M), the generating-function term \u03c1(t)\u00b7z\u00b7G3(z,t) transforms into \u03c1(t)\u00b7(1 \u2212 p(1 \u2212 z))\u00b7G3(z,t), not into \u03c1(t)\u00b7p\u00b7z\u00b7G3(z,t); therefore no choice of renormalized rates can make the observed equations match the no-noise equations exactly.\n\nAn approximate mapping becomes possible in the limit of **large pause-release rate \u03c1** (rapid exit from the paused state), where the paused-state model converges to an effective telegraph-like model; physically, this corresponds to a short-lived polymerase pause prior to elongation.",
      "source_document": "papers/2512.02908v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a piecewise-deterministic (PDMP) model of bursty protein expression where the *measured* concentration x is a noisy downsample of the *true* concentration y with per-molecule capture probability p, a common approximation is to replace the discrete binomial observation model by a Gaussian kernel and then take a macroscopic (large-volume) limit. In that limit, what is the resulting mapping between the observed distribution P(x,t) and the true distribution Q(y,t)? State the implied relationships between the observed and true mean/variance, and give the condition (in terms of the true Fano factor and p) under which the variance relationship is a good approximation (and hence when the macroscopic mapping is expected to break down).",
      "answer": "Approximating binomial capture by a Gaussian observation kernel for abundant molecule numbers and applying Laplace\u2019s method as volume V\u2192\u221e gives\n\nP(x,t) \u2248 (1/p) Q(x/p, t).\n\nThis implies the observed mean scales as \u03bcx = p \u03bcy and, under the same macroscopic approximation, the observed variance scales as \u03c3x^2 \u2248 p^2 \u03c3y^2.\n\nComparing to the exact discrete binomial-capture moment relations, the mean scaling is correct, but the variance scaling is only approximately correct provided the true Fano factor satisfies\n\nFFy = \u03c3y^2/\u03bcy \u226b (1\u2212p)/p.\n\nSince typical FFy is O(1\u201310), this condition fails when p is small; thus the approximation (especially for the variance) is only useful when p is not too small.",
      "source_document": "papers/2512.02908v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When modeling whole-transcriptome scRNA-seq with a sequential state-space decoder (e.g., Mamba), fixed gene order can impose an artificial sequence and hurt biological fidelity. Describe a principled way to construct a *cell-specific* gene ordering from locally encoded gene embeddings, and how the resulting ordered sequence can be processed bidirectionally and fused to form final gene features.",
      "answer": "A cell-specific ordering can be obtained by first computing a global cell state vector from chunk summaries, then ranking genes by relevance to that global state. Concretely: (1) split the sampled genes into multiple chunks, encode each chunk with shared self-attention, and take each chunk\u2019s [CLS] embedding as its summary; average-pool the [CLS] vectors across chunks to form a global cell state h_global_cls. (2) Concatenate all gene embeddings from all chunks into a single matrix H_genes,full and compute a per-gene importance score s_i via dot product with the global state (s_i = h_i \u00b7 h_global_cls^T). (3) Sort genes in descending s_i to form the ordered feature sequence H_sorted, which is then fed to a multi-layer bidirectional Mamba: run one Mamba forward on H_sorted and one backward on the reversed sequence. (4) Fuse forward/backward outputs with a learned sigmoid gate per gene, \u03b3_i = \u03c3(Linear(h_sorted,i)), using elementwise mixing h_fused,i = \u03b3_i \u2299 h_fwd,i + (1\u2212\u03b3_i) \u2299 h_bwd,i, yielding the final globally contextualized gene features.",
      "source_document": "papers/2512.03111v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are curating a large pan-cancer scRNA-seq benchmark by aggregating many public studies. Before merging datasets, what minimum metadata and gene-identifier harmonization should you require, and after merging what specific QC filtering steps would you apply to ensure a high-quality unified cohort?",
      "answer": "Require that each candidate dataset includes essential metadata: cellular transcriptomic profiles plus cancer type, tissue of origin, and corresponding patient information. Harmonize gene identifiers by standardizing them to official HGNC gene symbols. After merging into a unified cohort, apply QC filtering by: (1) removing cells with a low number of expressed genes; (2) excluding likely doublets/multiplets identified by abnormally high gene or UMI counts; (3) filtering out cells with a high proportion of mitochondrial gene expression (low viability); and (4) removing genes with low expression across the dataset to reduce noise and improve computational efficiency.",
      "source_document": "papers/2512.03111v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You have a single-cell foundation model that represents genes as tokens and uses self-attention in its encoder. How can you extract an interpretable gene regulatory network (GRN) from such a model, and what is an example of a biologically focused gene module you could use to sanity-check whether the inferred edges reflect known regulatory biology?",
      "answer": "A GRN can be inferred by using the model\u2019s attention mechanism as a proxy for gene\u2013gene interaction strength: attention-derived weights between gene tokens are used to score/construct edges so that strong attention corresponds to strong inferred regulatory interactions. As a biological sanity check, one can focus on a well-characterized module such as genes associated with major histocompatibility complex (MHC) class II molecules (central in immune cell function) and examine whether the inferred network recovers confident interactions within this module (and can recover additional relevant genes/edges compared with a baseline).",
      "source_document": "papers/2512.03111v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a foundation model for scRNA-seq that needs to scale beyond the quadratic cost of full-transcriptome self-attention, how can a \u201cdivide-and-conquer\u201d local-context encoder be designed to (i) form token embeddings that incorporate both gene identity and expression magnitude, and (ii) reduce compute/memory while still learning informative per-gene features? Describe the concrete chunking scheme, the token/summary mechanism, and the parameterization choice used to process chunks.",
      "answer": "Use a local-context encoder that samples a manageable subset of genes per cell and processes them in parallel chunks with a lightweight, parameter-shared Transformer:\n\n\u2022 Chunking: in each training epoch, randomly sample 3072 genes from a cell and partition them into C = 4 non-overlapping chunks of fixed size M = 768 genes.\n\n\u2022 Summary token per chunk: prepend an independent learnable [CLS] token to the beginning of each chunk so each chunk has length M+1; the [CLS] output serves as a chunk-level summary while the remaining outputs are gene-specific embeddings.\n\n\u2022 Token embedding that fuses identity and expression: for each gene token, project (a) the discrete gene ID and (b) the binned expression value through separate embedding layers and add them element-wise to form the input embedding e = Emb_id(gene_id) + Emb_val(gene_value).\n\n\u2022 Efficient parameterization: process all chunks in parallel using a six-layer Transformer encoder with shared parameters across layers, which lowers computational and memory overhead versus applying standard full-length self-attention to the entire transcriptome while still learning context-rich gene embeddings within each chunk.",
      "source_document": "papers/2512.03111v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a model for scRNA-seq batch integration across multiple datasets, what two complementary quantitative metrics can be used to separately assess (i) how well batch effects are removed and (ii) how well true biological variation is preserved, and how should the direction of each metric be interpreted?",
      "answer": "Use two scores: **Avg batch**, which assesses batch-effect removal, and **Avg bio**, which measures preservation of biological variance/signal after integration. For both metrics, **higher scores indicate better integration performance** (better batch removal for Avg batch and better biological-signal retention for Avg bio).",
      "source_document": "papers/2512.03111v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a reference-free, label-free pipeline for clustering viral variants from k-mer\u2013tokenized wastewater reads, how can contrastive fine-tuning be set up so that it learns discriminative sequence embeddings without variant annotations? Describe (i) how the sequence-level embedding is constructed from the encoder outputs, (ii) how positive pairs are generated (what augmentations are used and at what probabilities), and (iii) what contrastive objective (including any normalization/temperature detail) is optimized.",
      "answer": "A VQ-VAE encoder can be contrastively fine-tuned by turning each read into a sequence-level embedding and then applying a SimCLR-style InfoNCE loss.\n\n(i) Embedding construction: take the encoder latent vectors over positions and mean-pool them across valid (non-padding) positions to get a single vector z_pool; pass z_pool through a 2-layer MLP projection head (latent dimension D=64 projected to D\u2032\u2208{64,128}), then L2-normalize the output so embeddings lie on the unit hypersphere.\n\n(ii) Positive pairs without labels: for each input sequence, create two stochastic \u201cviews\u201d by applying independent random augmentations\u2014random token masking with probability 15% (replace selected tokens with a MASK token) and token-dropout with probability 10% (zero token embeddings). The two augmented versions of the same sequence form a positive pair; other sequences in the batch provide negatives.\n\n(iii) Objective: optimize the InfoNCE contrastive loss, which maximizes similarity of each embedding to its paired view while minimizing similarity to other embeddings in the batch, using a temperature parameter \u03c4=0.5; cosine similarity is used implicitly via L2-normalized embeddings.",
      "source_document": "papers/2512.03158v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using a VQ-VAE to learn discrete latent \u201cgenomic motifs\u201d from k-mer\u2013tokenized viral reads, what training mechanisms can be used to (i) keep the encoder latents close to their assigned codebook vectors and (ii) prevent codebook collapse where only a small subset of codes are ever selected? Describe how the codebook itself is updated during training and write down the forms of the relevant loss terms (including how stop-gradient is used and how code usage frequencies enter the regularizer).",
      "answer": "A practical setup combines: (1) nearest-neighbor vector quantization into a finite codebook, with the codebook updated by exponential moving averages (EMA) for stability; (2) a commitment loss that applies gradients only to the encoder to pull encoder outputs toward the selected code; and (3) an entropy-based regularizer on code-selection frequencies to encourage diverse code usage and avoid collapse.\n\n\u2022 Quantization and EMA codebook updates: For each sequence position i, the encoder output z_e^(i) is assigned to its nearest codebook vector e_k by q_i = argmin_k ||z_e^(i) \u2212 e_k||_2^2 and z_q^(i) = e_{q_i}. Instead of learning e_k directly by backprop, maintain EMA statistics per code k: an exponentially weighted usage count N_k^(t) = \u03b3 N_k^(t\u22121) + (1\u2212\u03b3) n_k^(t), and an exponentially weighted sum m_k^(t) = \u03b3 m_k^(t\u22121) + (1\u2212\u03b3) \u03a3_{i:q_i=k} z_e^(i). Update the code as e_k^(t) = m_k^(t) / N_k^(t). This lets code vectors track the encoder-output distribution without unstable gradient updates.\n\n\u2022 Commitment loss to keep encoder outputs close to codes: add\nL_commit = (1/L) \u03a3_i || z_e^(i) \u2212 sg[z_q^(i)] ||_2^2,\nwhere sg[\u00b7] is stop-gradient on the quantized code. Because the code term is stop-gradiented, this loss updates only the encoder, encouraging it to \u201ccommit\u201d to the chosen codebook entry rather than drifting.\n\n\u2022 Entropy regularization to prevent codebook collapse: compute empirical code-selection frequencies p_k in the current batch (fraction of positions assigned to code k) and add an entropy term\nH[C] = \u2212 \u03a3_{k=1}^K p_k log p_k.\nMaximizing this entropy (implemented by subtracting it in the total loss with a small weight) encourages a more uniform distribution of code usage, preventing a few codes from dominating.\n\nThese terms are optimized alongside the token reconstruction loss (cross-entropy/negative log-likelihood over valid positions), so the model must both reconstruct sequences accurately and maintain a stable, well-utilized discrete latent vocabulary.",
      "source_document": "papers/2512.03158v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In supervised scRNA-seq cell-type annotation, how can you implement an abstention/novelty-detection mechanism that flags query cells belonging to cell types missing from the reference, and what specific decision rule (including the threshold) was used to assign an \u201cUnknown\u201d label in the described pancreas hold-out experiment?",
      "answer": "Use the model\u2019s predicted class-probability distribution and apply post hoc confidence-based rejection: for each query cell, compute the maximum predicted probability across known cell types; if this top probability is below a predefined threshold, abstain and label the cell as \u201cUnknown.\u201d In the pancreas simulation where all alpha cells were removed from training, the rule was max probability < 0.95 \u2192 \u201cUnknown\u201d (which led to 97% of alpha cells being rejected as Unknown).",
      "source_document": "papers/2512.03286v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a hybrid scRNA-seq annotation model that feeds pretrained cell embeddings into a spiking self-attention/Transformer block, what are two embedding \u201cexpansion\u201d operations you can apply before the spiking attention, and what is the intended modeling benefit and interpretation of each operation?",
      "answer": "Two pre-attention expansion operations are used:\n1) Per-cell token expansion: repeat each cell\u2019s k-dimensional embedding vector m times to form an n\u00d7m\u00d7k tensor. This creates a multi-token representation for each cell, increasing representational richness and helping capture subtle differences between closely related cell types; although the tokens start identical, later nonlinear spiking activations and attention can diversify them into parallel feature channels.\n2) Temporal/view expansion: repeat the whole expanded embedding tensor T times to form a T\u00d7n\u00d7m\u00d7k input. This T dimension is interpreted as multiple repeated exposures (multiple \u201cstochastic views\u201d) of the same data, encouraging learning of invariant features and improving robustness/generalization and training stability under sparse spiking dynamics.",
      "source_document": "papers/2512.03286v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a spiking-Transformer approach for scRNA-seq cell-type annotation, how can you modify standard multi-head self-attention to use spiking neuron dynamics, and what specific spiking neuron model and implementation settings are used to produce sparse, temporally encoded Q/K/V representations and head outputs?",
      "answer": "Replace the usual dense attention pipeline with spiking neuron layers applied to the projected attention tensors. For each head i, compute linear projections from the input embedding tensor to obtain Qi, Ki, Vi, then pass each through its own spiking neuron layer (SNQ, SNK, SNV) to produce spike-form representations Q\u2032i, K\u2032i, V\u2032i before computing attention. The per-head spiking self-attention output is then passed through another spiking neuron function (SN), and outputs from all heads are concatenated and fed to an MLP; the resulting features are globally averaged and classified with a softmax head.\nThe spiking neuron model used in all spiking layers is the Leaky Integrate-and-Fire (LIF) neuron implemented with the SpikingJelly framework, using time constant \u03c4 = 2.0 (membrane potential decay control) and step_mode = 'm' to run in multi-step temporal mode so the neuron integrates inputs across discrete time steps. Spikes are binary and temporally sparse, enabling sparse/energy-efficient activations and improved generalizability.",
      "source_document": "papers/2512.03286v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a single-cell annotation model that represents each cell as multiple \u201ctokens\u201d (e.g., repeated embedding vectors) and processes them with multi-head spiking self-attention, how can you collapse the per-head, per-token attention outputs into a single cell-level prediction? Specify the sequence of operations used to merge attention heads, reduce spatial dimensions, and produce the final cell-type probability vector.",
      "answer": "Merge the multi-head spiking self-attention outputs by concatenating the head outputs and transforming the concatenated representation with an MLP to obtain a refined feature matrix O. Then apply global averaging of O across the spatial/token dimensions to produce a compact cell-level feature vector. Finally, pass this vector through a fully connected classification head and apply softmax (softmax(Wc\u00b7O + bc)) to yield a probability distribution over cell types; the predicted label is the argmax of that distribution.",
      "source_document": "papers/2512.03286v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You\u2019re benchmarking a cell-type annotation model that takes scRNA-seq counts, produces pretrained scGPT cell embeddings, and then trains a downstream attention-based classifier. What concrete preprocessing and training/regularization protocol would you use to (i) put the expression matrix on a comparable scale across cells and (ii) train the classifier while controlling overfitting\u2014specify the normalization/log/scale steps, the train/test split strategy, the loss, optimizer and key hyperparameters, learning-rate schedule, and any dropout used?",
      "answer": "A suitable protocol is:\n\n\u2022 Preprocessing: use standard single-cell normalization and scaling\u2014normalize each cell\u2019s total counts to 10,000 (counts-per-cell normalization), apply a log transform, and then feature-scale the resulting values so they are appropriate for downstream scGPT embedding and for running other annotators under comparable input processing.\n\n\u2022 Train/test split: randomly split the reference dataset into 70% training and 30% testing.\n\n\u2022 Objective: optimize cross-entropy loss on cell-type labels.\n\n\u2022 Optimizer and hyperparameters: stochastic gradient descent (SGD) with initial learning rate 0.01, momentum 0.9, and weight decay 1e\u22124; train for 10 epochs with batch size 128.\n\n\u2022 Learning-rate schedule: use cosine learning-rate decay to reduce update magnitude later in training.\n\n\u2022 Regularization: apply dropout of 0.1 in the MLP layers to mitigate overfitting (with hyperparameters configurable).",
      "source_document": "papers/2512.03286v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You run an inference-time enhanced-sampling procedure for a biomolecular diffusion model that produces samples from a set of biased/conditional ensembles p(x\\mid y_j) defined by \u201ctwisting\u201d potentials y_j(x)=exp(\u2212U_j(x)). If your goal is to recover the model\u2019s *unbiased* conformational landscape (the unconditional p(x)) and compute free-energy differences, what reweighting problem do you need to solve, and how can MBAR be used to solve it? Explain what normalizing quantities must be estimated and why this approach avoids needing to evaluate the absolute probability of a structure under the diffusion model.",
      "answer": "To reconstruct the unbiased landscape from conditional samples, you must reweight the pooled samples drawn from multiple conditional distributions p(x\\mid y_j) back to the unconditional distribution p(x). Because each biased ensemble satisfies p(x\\mid y_j)=p(x) y_j(x) / p(y_j), combining samples from multiple biases requires estimating the normalizing constants/partition functions p(y_j) for each condition (equivalently the relative free energies of the biased ensembles). While twisted diffusion sampling provides an unbiased estimator of p(y_j), it can be noisy; MBAR instead jointly estimates {p(y_j)} (up to an overall constant) by exploiting overlap among the biased ensembles. In the resulting MBAR equations, p(x) cancels, so one can compute the needed weights and free-energy differences using only the bias factors (e.g., exp(\u2212U_j(x))) and the estimated partition functions, without evaluating the diffusion model\u2019s absolute likelihood for each sample. The unconditional target can be treated as an additional \u201censemble\u201d p(x\\mid y_0)=p(x) with zero bias, giving a solvable system for the partition functions.",
      "source_document": "papers/2512.03312v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You have a diffusion-based biomolecular structure predictor that, under its default sampling procedure, produces many nearly identical samples for a given sequence. Propose an inference-time strategy to *automatically* explore alternative conformations without prior knowledge of the relevant degrees of freedom. In your answer, specify (i) a concrete form of the conditioning/bias potential you would use to steer sampling away from the default prediction, (ii) a design choice that prevents the method from spending all its effort on flexible/disordered loops rather than large-scale motions, and (iii) post-generation checks used to filter out physically implausible structures.",
      "answer": "One automated strategy is to first draw a default prediction x_d and then run conditional (guided) diffusion sampling using a sequence of RMSD-based bias potentials that push samples away from x_d. A concrete choice is a quadratic RMSD potential, e.g. U_j(x) = (\u03b1/2)\u00b7(RMSD(x, x_d; mask) \u2212 r_j)^2 (equivalently using y_j(x)=exp(\u2212U_j(x))), where r_j spans a range of target RMSDs (e.g., exploring from near 0 up to large RMSD values to probe flexibility).\n\nTo avoid mainly sampling fast, low-information motions of disordered loops, compute the RMSD only over a \u201crigid-element\u201d subset: restrict the RMSD calculation to residues in secondary-structure elements (\u03b1-helices and \u03b2-sheets) while still allowing all atoms/residues to move during sampling.\n\nAfter generating conditioned samples, filter physically implausible structures by rejecting (a) samples where any 10-residue sliding window has an average pLDDT more than 20% below that of the default prediction (to detect local nonphysical distortions) and (b) structures with steric clashes.",
      "source_document": "papers/2512.03312v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You have a pretrained diffusion model for biomolecular structure generation/prediction and want to do *pure inference-time* conditional sampling where the constraint is naturally defined on the fully denoised structure (e.g., an RMSD-to-reference or a distance between atom groups), without retraining the model or changing its architecture. Why are common diffusion guidance approaches like time-dependent classifier guidance or classifier-free guidance a poor fit for this setting, and what two properties of a sequential Monte Carlo (SMC) / \u201ctwisted diffusion sampling\u201d approach make it well suited? Also state the asymptotic correctness guarantee SMC provides as the number of particles increases.",
      "answer": "Time-dependent classifier guidance and classifier-free guidance are not well suited here because they typically require either additional training (e.g., training a classifier or the unconditional/conditional branches needed for classifier-free guidance) or special architectural choices, which conflicts with the goal of a purely inference-time method on an already trained structure model.\n\nTwisted diffusion sampling cast as SMC is well suited because (1) it only requires a *time-independent* guidance function y(x)=exp(\u2212U(x)), which makes it easy to express conditions directly as a function of the (estimated) fully denoised state x\u03020 at any diffusion timestep, and (2) it operates as a particle filter with resampling, enabling conditional generation by maintaining a weighted particle population.\n\nIts asymptotic guarantee is that, in the limit of many particles, the sampling procedure approaches (i.e., converges to) the true target conditional distribution p(x | y, s).",
      "source_document": "papers/2512.03312v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are benchmarking an inference-time conformational sampling method on proteins known to undergo a dominant open\u2194closed domain motion, and you want to test whether the sampled ensemble follows a biologically meaningful transition rather than producing diffuse, off-path structural noise. Describe a concrete analysis pipeline that uses PCA to (i) characterize the major axes of variation in the sampled structures and (ii) quantify whether those axes align with the experimentally observed conformational change between two reference structures. In your answer, specify what structural representation PCA is performed on, how the experimental references are used in the PCA space, and what summary metrics indicate \u201cfocused, on-path\u201d sampling versus diffuse variation.",
      "answer": "Perform principal component analysis on pairwise structural distances computed from the sampled ensemble (the document describes PCA on internal/pairwise distances such as pairwise C\u03b1 distances / internal atomic distances, rather than raw coordinates). After computing principal components from the samples, project the experimentally determined reference structures into the same PC space. Then assess whether the major axes of sample variance correspond to the known transition by checking (1) how much of the sample variance is explained by the top principal components (e.g., PC1+PC2)\u2014high values indicate focused diversity with dominant directions\u2014and (2) how well the experimental conformational difference between the two references is aligned with those top PCs (high fraction of the reference-to-reference variance captured by PC1+PC2 indicates the sampled variation follows the experimentally observed motion). Diffuse/off-path sampling is indicated by low variance explained by the top PCs and/or poor alignment of the experimental difference with the top PCs, even if the ensemble contains diverse structures.",
      "source_document": "papers/2512.03312v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You have a pretrained diffusion model that generates biomolecular structures via a reverse denoising trajectory \\(x_T\\to \\cdots \\to x_0\\). You want to do *pure inference-time* conditional sampling under a constraint that is naturally defined on the fully denoised structure, expressed as a time-independent guidance function \\(y(x_0)=\\exp(-U(x_0))\\). In a twisted diffusion / SMC \u201cparticle filter\u201d approach, how is the guidance signal computed and applied at an intermediate timestep \\(t\\) (i.e., how do you obtain a gradient with respect to \\(x_t\\) when the potential is defined on \\(x_0\\))? In your answer, explain the role of an inexpensive estimate \\(\\hat x_0\\) of the denoised structure, how the chain rule/Jacobian is used to backpropagate \\(\\nabla_{\\hat x_0} y\\) (or \\(\\nabla_{\\hat x_0}\\log y\\)) to \\(\\nabla_{x_t} y\\), and why periodic resampling of multiple particles is needed to approximate samples from \\(p(x\\mid y,s)\\).",
      "answer": "At each reverse-diffusion step, twisted diffusion treats the evolving states \\(x_t\\) as particles in an SMC sampler targeting the conditional distribution induced by a time-independent guidance function \\(y(x_0)=\\exp(-U(x_0))\\).\n\n\u2022 Because \\(y\\) is naturally a function of the *fully denoised* structure, at timestep \\(t\\) the sampler first constructs an inexpensive estimate of the denoised state, \\(\\hat x_0\\), from the current noisy particle \\(x_t\\) (using the diffusion model\u2019s denoiser/predictor).\n\n\u2022 It evaluates the guidance on that estimate, i.e. uses \\(y(\\hat x_0)\\) as a proxy for the potential of the current particle, and computes the gradient with respect to the estimated denoised structure (e.g., \\(\\nabla_{\\hat x_0} y\\) or \\(\\nabla_{\\hat x_0}\\log y\\)).\n\n\u2022 To obtain a guidance signal in the space the diffusion update operates in, it maps this gradient back to \\(x_t\\) via the chain rule using the Jacobian of the denoiser: \n\\[\\nabla_{x_t} y \\;=\\; (J_{x_t\\,\\hat x_0})^\\top\\, \\nabla_{\\hat x_0} y\\,,\\]\nwhere \\(J_{x_t\\,\\hat x_0}\\) denotes the Jacobian of \\(\\hat x_0\\) with respect to \\(x_t\\). This gives a gradient-based signal that can be used to guide the particle\u2019s denoising step toward the conditional target \\(p(x\\mid y,s)\\).\n\n\u2022 Multiple particles are denoised in parallel and periodically *resampled* based on their (approximate) importance weights derived from the guidance evaluations (using \\(y(\\hat x_0)\\)). Resampling focuses computation on particles that better satisfy the condition and is part of maintaining correct SMC sampling; with many particles, the procedure approaches sampling from the true target conditional distribution.",
      "source_document": "papers/2512.03312v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a differentially private GWAS release mechanism that needs to account for genetic relatedness (so phenotypes are not conditionally independent), how can you make a quadratic-program\u2013based phenotype-randomization approach computationally tractable while still using correlated, genotype-informed priors? Describe the key approximation/engineering step and what quantity is used to group individuals.",
      "answer": "Use a clustered (MultiQP) formulation: instead of solving a separate QP for each individual, partition the cohort into k disjoint clusters of individuals with similar marginal phenotype priors, constructed by applying k-means to the mean vector X\u03b2 from the estimated personalized prior distribution. For each cluster, build a cluster-specific QP (with the same DP and simplex constraints as the original QP), then combine these into a single block-structured QP that captures cross-cluster dependencies while reducing the number of QPs to solve and keeping computation tractable.",
      "source_document": "papers/2512.04225v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a phenotypically differentially private GWAS pipeline that wants to use genotype-informed phenotype prediction (e.g., PRS) to build personalized phenotype priors, how can you structure the workflow so that training the predictor and then using it to personalize the randomization does not increase the total privacy loss beyond the chosen budget? Explain the key dataset split/training steps and name the differential-privacy composition principle that justifies the unchanged overall privacy cost.",
      "answer": "Use a held-out (disjoint) workflow: randomly split the cohort into two non-overlapping subsets. On the first subset, privately estimate a phenotype prior (e.g., via a private histogram), then apply the DP phenotype-randomization mechanism to produce privatized phenotypes for that subset; train the phenotype prediction/PRS model on genotypes paired with these privatized phenotypes. Next, apply the trained predictor to the second subset\u2019s genotypes to obtain individual-specific (or group-based) phenotype priors, and solve the per-individual (LP) or grouped (QP) optimization to randomize phenotypes in the second subset under the same overall privacy budget. Because each individual\u2019s true phenotype is used only once (in exactly one subset), the privacy cost does not increase; this is justified by the parallel composition property of differential privacy (with the mechanism\u2019s total budget accounted for as \u03b5 = \u03b51 + \u03b52 across the steps applied within a subset).",
      "source_document": "papers/2512.04225v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a GWAS where you want to release association statistics genome-wide under phenotypic differential privacy, why does privatizing the phenotype vector once (and then running a standard GWAS on the privatized phenotypes) avoid having the required noise/utility loss scale with the number of variants? In your explanation, name the differential-privacy principle that guarantees privacy is preserved for any downstream GWAS computations and allows the same privatized phenotypes to be reused for multiple downstream analyses without additional privacy loss.",
      "answer": "Because the DP mechanism is applied only to the phenotype vector (randomizing/privatizing y once), the privacy guarantee is paid at the phenotype-randomization step and does not require adding independent noise to each of the m per-variant statistics. All genome-wide association computations across variants are then performed on the already-DP-protected phenotypes, so the number of variants only affects post-processing computation/utility, not the privacy mechanism\u2019s noise calibration; thus noise is effectively decoupled from m. Privacy for any GWAS statistics computed from the privatized phenotypes (and reuse across multiple downstream analyses) follows from the post-processing property of differential privacy: any computation on DP-protected data that uses no additional private inputs remains differentially private and does not incur extra privacy loss.",
      "source_document": "papers/2512.04225v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When designing an optimization-based differentially private phenotype-randomization mechanism that directly minimizes expected squared error in GWAS correlation (or t-) statistics, you can end up with a quadratic objective under DP simplex constraints. Explain (i) why this quadratic program can be non-convex in the presence of cross-individual covariance/relatedness terms, and (ii) how a difference-of-convex (DC) reformulation plus the DC algorithm (DCA) turns it into a sequence of convex subproblems\u2014include how the quadratic matrix is split and how the shift parameter is chosen.",
      "answer": "(i) The QP is non-convex because the quadratic matrix A in the term w^T A w is not guaranteed to be positive semidefinite: cross-terms C(u,u\u2032) that encode covariance across individuals (e.g., due to relatedness) can induce negative eigenvalues in A.\n\n(ii) The fix is to rewrite A as a difference of two PSD matrices, A = C \u2212 E, by adding and subtracting a scaled identity: set C = A + \u03c1 I and E = \u03c1 I, choosing \u03c1 so that C is PSD (in practice \u03c1 = max(0, \u2212\u03bb_min(A)), where \u03bb_min(A) is A\u2019s smallest eigenvalue). This yields an objective of the form f(w)=g(w)\u2212h(w) with g(w)=w^T C w + b^T w and h(w)=w^T E w, where both g and h are convex. DCA then iteratively linearizes the concave part \u2212h(w) around the current iterate w^(k) (i.e., replaces h by its first-order Taylor expansion at w^(k)), and at each iteration solves the resulting convex QP over the same DP constraints to obtain w^(k+1).",
      "source_document": "papers/2512.04225v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When deploying an object detector to drive a physical sorting actuator in real time (e.g., sex-sorting insects on an edge device), what control logic can be added on top of per-frame predictions to reduce incorrect actuator triggers in the presence of multiple animals and occasional misclassifications? Describe the specific spatial and temporal gating strategy and its key parameter values.",
      "answer": "Add both spatial and temporal gating before commanding the actuator: (1) spatial gating\u2014only trigger the sorting arm when the detected individual is sufficiently close to the sorting mechanism, using a fixed proximity threshold and, if multiple crickets are detected, prioritizing the one closest to the sorting point; the activation threshold is implemented as a horizontal reference line corresponding to a minimum distance of 100 pixels. (2) temporal gating\u2014apply a sliding window over 10 consecutive frames (about one frame every ~300 ms, i.e., ~3 seconds total) and activate the arm only if the majority of those frames satisfy both the predicted class (male/female) and the proximity criterion, thereby filtering sporadic misclassifications and preventing unintended activations.",
      "source_document": "papers/2512.04311v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are building an edge-deployed object detector to sort male vs female house crickets on an overhead camera rig placed above a reflective surface. What dataset collection, labeling, and augmentation choices help the model generalize to real operating conditions (lighting/reflections, multiple animals, partial occlusion), and what simple input preprocessing can reduce CPU inference cost without \ud06c\uac8c degrading detection accuracy?",
      "answer": "Use a dataset collected under deployment-matched geometry: images captured with the same overhead-mounted Raspberry Pi AI Camera position used during operation, so training and inference see the same viewpoint. To improve robustness to reflections and illumination changes from an acrylic surface, collect images under multiple light sources placed at different angles. Increase scene complexity by including frames with one, two, or three crickets (including mixed sexes) crossing simultaneously so the detector learns multi-object cases and overlaps.\n\nLabel each cricket with a sex-specific bounding box (manual annotation), relying on visible sexual dimorphism (e.g., females generally larger and with a visible ovipositor). Expand variability via augmentation using random zoom to mimic partial occlusions/cropping and overlapping individuals. Keep the dataset balanced across sexes and reserve a held-out test set after splitting into training/validation/test.\n\nFor edge efficiency, convert images to grayscale so the network input tensor has a single channel, reducing computation and improving inference speed while maintaining detection performance.",
      "source_document": "papers/2512.04311v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a real-time sex-sorting pipeline that combines an object detector with a mechanical actuator, what evaluation metric should be used to quantify end-to-end performance of the whole device (not just the detector), and how is this metric computed? Explain what sources of error it captures that standard detection metrics (e.g., mAP) do not.",
      "answer": "Use a system-level \u201csorting accuracy\u201d metric defined over physical outcomes: it is the fraction of animals that are ultimately placed into the correct compartment by the actuator.\n\nSorting accuracy is computed as:\n\nSorting Acc. = (number of correctly sorted crickets) / (total number of crickets sorted).\n\nIt explicitly includes both (i) the vision-based sex classification/detection decisions and (ii) failures of the mechanical actuation (e.g., the detector may label correctly but the arm may not react in time), so it can decrease even when detector-only metrics like mAP/IoU-based accuracy remain high.",
      "source_document": "papers/2512.04311v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an end-to-end real-time insect sex-sorting device that couples a vision classifier to a mechanical actuator, how can you determine whether most sorting errors come from the detector or from actuator response limits? Describe a concrete way to estimate each insect\u2019s crossing speed from the video stream, summarize the key speed trends observed for (i) males vs females and (ii) correctly sorted vs mis-sorted individuals, and explain what those trends imply about the dominant failure mode.",
      "answer": "A practical diagnostic is to relate sorting outcomes to the animal\u2019s transit speed through the camera\u2019s \u201cdecision\u201d region: if mis-sorted individuals are disproportionately fast, errors are likely dominated by actuation timing/response rather than visual misclassification.\n\nSpeed can be estimated by measuring the elapsed time from when a cricket first appears in the detection zone of the camera\u2019s field of view until it completely crosses the bridge, then dividing that time by the length of the traversed section (distance/time).\n\nThe analysis showed two main trends: (1) males moved faster than females on average (median male speed about 39.5 mm/s vs median female speed about 18.8 mm/s), and (2) mis-sorted/misclassified individuals of either sex were much faster than correctly sorted ones (median about 75.0 mm/s for mis-sorted vs about 20.3 mm/s for correctly sorted). These trends indicate that higher movement speed strains the mechanical response of the sorting arm, making actuator latency a dominant contributor to end-to-end sorting errors even when the detector can correctly identify the cricket in individual frames.",
      "source_document": "papers/2512.04311v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are designing a real-time, vision-guided device that must *self-feed* insects through a narrow sorting region (e.g., a bridge) without continuous human handling. What behavioral \u201cflow-control\u201d strategies can be used to motivate animals to traverse the sorting zone, and what trade-offs would you expect between (i) stress/locomotion speed, (ii) mechanical actuation reliability, (iii) throughput (time to process a cohort), and (iv) whether all individuals will traverse? Illustrate your answer by describing multiple stimulus regimes (from high to low stress) and the corresponding qualitative performance outcomes, including at least one proposed way to automate stimulation without human intervention.",
      "answer": "A practical flow-control strategy is to create a strong environmental gradient that makes the start chamber unattractive and the destination chamber attractive, so animals move voluntarily through the sorting region. Here this is done by keeping the starting chamber brightly lit with no food/water (inhospitable) while keeping the target chamber dark with food and moisture (attractive), connected by a bridge where detection and actuation happen.\n\nOn top of this passive incentive, different stimulus regimes can be applied to regulate movement, which changes stress level and therefore walking speed:\n- High-stress / forced movement: manually forcing individuals to cross one-by-one produces fast, nervous motion. This increases throughput, but can reduce end-to-end sorting performance because even when the detector classifies correctly, the servo-actuated arm may not react fast enough at high speeds.\n- High-stress but automatable: shaking the starting chamber induces collective movement with similar nervousness/speed and similar performance characteristics, but removes the need for continuous human handling because agitation can be automated.\n- Lower-stress, intermittent stimulation: occasional mild agitation plus gentle air blows reduces stress so crickets cross more calmly. This tends to make sorting smoother (better system accuracy), but decreases throughput because the cohort takes much longer to traverse.\n- Lowest-stress / no stimulation: allowing crickets to move entirely at their own pace can yield the best classification/sorting metrics among the tested regimes, but at the cost of very poor time efficiency and incomplete traversal: some individuals delay or avoid crossing, with the crossing rate declining over time.\n\nA further practical consideration is that the actuator itself can alter behavior: the noise of the sorting arm can startle crickets mid-crossing and cause retreats back to the start chamber, contributing to delays; males were observed to be more skittish in the low-stress regimes.\n\nFor automation of stimulation without humans, suggested options include controllable heat pads under the starting chamber or small fans, which can provide consistent inducement to move while preserving autonomy.",
      "source_document": "papers/2512.04311v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In all-atom MD of an amphiphilic pheromone monolayer at an air\u2013water interface, how can you extract the collective (lateral-interaction) contribution to adsorption thermodynamics from simulations that only give you surface tension as a function of surface concentration, and how does a 2D liquid\u2013gas coexistence region modify this inference?",
      "answer": "Use interfacial thermodynamics to convert the simulated isotherm \u03b3(\u0393a) into the chemical potential of the adsorbed phase. With the Gibbs dividing surface chosen so the interfacial excess of solvent is zero, the Gibbs equation relates changes in adsorbed chemical potential and surface tension:\n\nN_a d\u03bc_a = \u2212S d\u03b3  \u21d2  d\u03bc_a = \u2212(1/\u0393_a) d\u03b3  (since \u0393_a = N_a/S).\n\nThus, by integrating \u2212d\u03b3/\u0393_a along the simulated \u03b3(\u0393a) curve (often aided by fitting an analytical equation of state), you obtain \u03bc_a(\u0393a). Comparing this to the ideal 2D-gas chemical potential \u03bc_a^ideal = \u03bc_a^0 + kBT ln(\u0393_a/\u0393^0) defines the activity coefficient \u03b3_a via \u03bc_a = \u03bc_a^0 + kBT ln(\u03b3_a \u0393_a/\u0393^0). The collective/lateral-interaction free-energy contribution is then kBT ln \u03b3_a = \u03bc_a \u2212 \u03bc_a^ideal.\n\nIf the fitted equation of state predicts a 2D liquid\u2013gas transition, the isotherm must include a coexistence plateau: for \u0393_a between the condensing concentration \u0393_C (where liquid first appears) and the vaporisation concentration \u0393_V (where gas disappears), the surface tension is constant (\u03b3 = \u03b3_sat) and the chemical potential is constant across coexistence. This coexistence is enforced via a Maxwell construction on the EOS. In this regime the activity coefficient (and thus the collective free-energy gain) is determined by \u0393_C and \u0393_V; at the end of coexistence one gets ln \u03b3_a = ln(\u0393_C/\u0393_V), corresponding here to a collective stabilization of order \u22122 kBT.",
      "source_document": "papers/2512.04340v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking polyploid haplotype assembly methods that may output multiple phased blocks and may leave some haplotypes unphased at certain SNPs, how can vector error rate (VER) and minimum error correction (MEC) be generalized so that (i) partially phased or completely unphased blocks are still scored and (ii) additional fragmentation into many blocks is penalized? Describe the key ideas used for VER across block types and the modification to MEC for reads spanning multiple blocks, including how unphased alleles are treated.",
      "answer": "A workable generalization is to explicitly define block types by how many haplotypes are phased per SNP within the block (constant within a block): (1) fully phased (K), (2) mostly phased (K\u22121, augmented with the missing haplotype implied by the genotype and then treated like fully phased), (3) partially phased (between 1 and K\u22122), and (4) completely unphased. \n\nFor VER, within a fully/mostly phased block, compute vector error in the usual way by finding a sequence of valid bijections \u03d5\u2113 between predicted and true haplotype rows that respect the genotype at each SNP and minimizing the total number of \u201cswitches\u201d |{k:\u03d5\u2113(k)\u2260\u03d5\u2113\u22121(k)}| across adjacent SNPs (a DP over adjacent-SNP matchings). For completely unphased blocks, define VER as an expectation: at each SNP position (after the first) consider all valid bijections uniformly and compute the expected number of non-fixed points relative to a fixed previous permutation, summing over positions. For partially phased blocks, combine the actual within-block vector error on the phased subset with an expected contribution for the unphased haplotypes (computed analogously to the completely unphased case). To penalize fragmentation, add an extra penalty for each additional block beyond the first, computed as the expected number of vector errors \u201cinto\u201d the first SNP of the block (using K \u2212 I{g[\u2113]>1} \u2212 I{K\u2212g[\u2113]>1}). The final VER is the sum of within-block errors plus these inter-block penalties.\n\nFor MEC, keep the standard per-read objective (for each read, take the minimum Hamming distance to any reconstructed haplotype restricted to the SNPs the read covers, normalized by total covered alleles), but treat an unphased allele in the reconstruction as a mismatch (Hamming distance 1) so partially phased blocks can be scored when a read lies entirely within a single block. Because a read may span multiple blocks, add a penalty based on the probability that the read\u2019s best-matching haplotype differs between adjacent blocks: for a read covering B blocks, add (B\u22121)\u00b7(1\u22121/K), i.e., the expected number of adjacent-block changes in the optimal haplotype assignment assuming a single optimal haplotype per block. This makes MEC increase with block fragmentation (a read spanning B blocks contributes O(B) rather than O(1)).",
      "source_document": "papers/2512.04393v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a probabilistic polyploid haplotype assembler that models a distribution over local SNP-pair phasings on a directed acyclic version of the SNP-pair graph, how can forward-filtering backward-sampling (FFBS) be used to (i) generate posterior samples of the phasing state at each SNP-pair node and (ii) convert those samples into an uncertainty quantification summary as a function of SNP\u2013SNP distance? Give the key message definitions/sampling steps and the uncertainty statistic that is computed from the samples.",
      "answer": "(i) FFBS posterior sampling on the directed SNP-pair graph proceeds by first imposing a topological order on SNP-pair nodes by genomic position and directing edges to respect this order. Each node ut has a discrete state space \u03a6t of possible phasings for its SNP pair. The forward pass computes \u201cforward messages\u201d\n\n\u03b1t(\u03d5t) = P(\u03d5t, R1:t),\n\nwhere R1:t denotes all reads covering SNP pairs in nodes u1,\u2026,ut. Base case:\n\n\u03b11(\u03d51) = P(\u03d51) \u00b7 \u220f_{r\u2208R_{\u21130\u21131}} L(r, \u03d51),\n\nwith a uniform prior P(\u03d51) and emission/likelihood term L(r,\u03d5) for reads covering that SNP pair. Recursion for t=2,\u2026,|U|:\n\n\u03b1t(\u03d5t) = (\u220f_{r\u2208Rt} L(r, \u03d5t)) \u00b7 \u2211_{\u03d5Pa(ut)} P(\u03d5t | \u03d5Pa(ut)) \u00b7 \u220f_{up\u2208Pa(ut)} \u03b1p(\u03d5p),\n\nwhere Pa(ut) are parents of ut and P(\u03d5t|\u03d5Pa(ut)) is a transition probability derived from edge potentials and normalized over three-SNP phasings consistent with the parent states. Each connected component (phasing block) is processed independently.\n\nBackward sampling then draws a full phasing sample \u03d51:|U| from the posterior. First sample the last node via its normalized forward message:\n\n\u03d5|U| \u223c \u03b1|U|(\u03d5|U|) / \u2211_{\u03d5\u2032} \u03b1|U|(\u03d5\u2032).\n\nThen sample nodes in reverse topological order using the conditional posterior given already-sampled children:\n\nP(\u03d5t | {\u03d5c : uc\u2208Ch(ut)}, R) \u221d \u03b1t(\u03d5t) \u00b7 \u220f_{uc\u2208Ch(ut)} P(\u03d5c | \u03d5t).\n\nCollecting multiple such FFBS samples approximates the exponentially large distribution over phasings; in this framework, the probability of a phasing can be approximated as proportional to how many FFBS samples are consistent with it.\n\n(ii) To summarize uncertainty versus SNP distance, align (optimally match) each sampled haplotype phase to the ground-truth haplotypes, then compute \u201cpair phase accuracy\u201d as the mean Hamming distance between each sampled phase and the ground truth for SNP pairs at a given separation. Hamming distance is extended to pairs of {0,1}^{K\u00d72} allele matrices as the number of entries that differ (0 means perfect agreement; larger values indicate more vector errors). Plotting mean Hamming distance against SNP\u2013SNP distance yields an uncertainty curve: increasing distance tends to increase Hamming distance (uncertainty) until it plateaus, partly because long distances span across separate phased blocks.",
      "source_document": "papers/2512.04393v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a probabilistic polyploid haplotype assembler designed for long reads that models each haplotype as a chain-structured CRF and treats read-to-haplotype assignments as latent variables, how are the per-haplotype (i) marginal/emission and (ii) transition potentials constructed from the currently assigned reads and a sequencing error rate, including the case where a read covers multiple disjoint SNP intervals? Then, describe how Gibbs sampling updates each read assignment z_j and how genotype constraints are enforced in the global CRF that couples the K haplotype chains.",
      "answer": "For haplotype k, the model builds a chain CRF h*_k over SNP positions with binary allele states and uses the reads currently assigned to that haplotype (those with z_j = k) to set potentials.\n\n(i) Marginal (\u201cemission\u201d) potentials at position \u2113 aggregate base-call evidence with an explicit sequencing-error model: for allele a\u2208{0,1},\n\u03c8^k_\u2113(a) \u221d \u220f_{r_j: z_j=k, r_j[\u2113] \u2260 \u2018\u2212\u2019} \u03b5_a(r_j[\u2113]),\nwhere \u03b5_a(r[\u2113]) = 1\u2212\u03b5 if the read reports allele a and = \u03b5 if it reports the opposite allele a\u2032\u2260a.\n\n(ii) Transition potentials between adjacent SNPs are empirical frequencies among assigned reads that cover both endpoints, with additive smoothing:\n\u03c8^k_{\u2113,\u2113+1}(a_1,a_2) \u221d |{r_j: z_j=k, r_j[\u2113]=a_1, r_j[\u2113+1]=a_2}| / (|{r_j: z_j=k, r_j[\u2113]\u2260\u2018\u2212\u2019, r_j[\u2113+1]\u2260\u2018\u2212\u2019}| + \u03b4),\nwhere \u03b4\u22650 is a hyperparameter to smooth transitions.\n\nFor a read r_j that may cover several disjoint SNP intervals (e.g., {\u21130\u2026\u21130+m0} \u222a {\u21131\u2026\u21131+m1} \u222a \u2026), its likelihood under haplotype k is computed by multiplying the appropriate marginal terms at observed SNPs (\u03c8^k_\u2113 combined with \u03b5(r_j[\u2113])) and the transition terms \u03c8^k_{\u2113,\u2113+1} across adjacent SNP indices between the read\u2019s start and end, even across gaps, and summarizing unobserved prefix/suffix regions via forward and backward messages into the read endpoints (\u03b1^k(\u21130) and \u03b2^k(\u2113d+md)).\n\nGibbs updates: holding current potentials fixed, each read\u2019s assignment is resampled from\nP(z_j = k | \u00b7) \u221d (1/K) \u00b7 P(r_j | h*_k).\nAfter sampling z_j, if the assignment changes, the potentials of the affected haplotype CRFs are updated accordingly.\n\nGenotype constraints are enforced by introducing a coupling chain CRF H* over joint variables W_\u2113 whose states represent the 2^K configurations of (w^1_\u2113,\u2026,w^K_\u2113). The marginal/transition potentials of H* are formed by taking K-ary products of the per-haplotype marginals/transitions, and then zeroing (nullifying) any joint states at W_\u2113 that do not agree with the observed genotype g_\u2113 (i.e., the total number of alternate alleles).",
      "source_document": "papers/2512.04393v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a polyploid haplotype assembly pipeline that (i) first infers a most likely phasing state for each SNP-pair node in a SNP line graph and then (ii) builds a global K\u00d7L haplotype matrix one SNP at a time, what heuristic is used to choose the next SNP to phase within the current connected component, and how is that SNP\u2019s phase decided?\n\nYour answer should specify: (a) how the neighbor set and the \u201cconnectivity\u201d score for an unphased SNP are defined and how disconnected components become separate phased blocks; (b) how candidate phasings for the chosen SNP are generated from adjacent SNP-pair nodes, including the required read support and consistency under haplotype permutation with already-phased positions; and (c) the three candidate-scoring terms (likelihood, MEC, inference-agreement), how they are normalized/combined (including the sign of each term), and what the selected candidate contributes to the global haplotype matrix and block labels.",
      "answer": "(a) Variant choice and blocks. Maintain sets of phased SNP-pair nodes U_phased and unphased nodes U_unphased in the SNP line graph Q, and phased/unphased SNP positions S_phased and S_unphased. At each step, define the neighbor set\nN = { u \u2208 U_unphased : \u2203 v \u2208 U_phased with (v,u) or (u,v) an edge in Q },\n(i.e., unphased SNP-pair nodes adjacent to the already phased node set). If N is empty, the algorithm has reached a disconnected component: it starts a new block (increment block_id) and seeds it by selecting the next SNP-pair node in topological order, assigning its inferred K\u00d72 phasing directly into the global matrix and labeling those two SNPs with the new block_id.\nIf N is nonempty, it scores each still-unphased SNP position \u2113 that appears in at least one node in N by\nconnectivity(\u2113) = |{ u \u2208 N : \u2113 \u2208 u }|,\nand chooses the next SNP to phase as \u2113* = argmax_\u2113 connectivity(\u2113).\n\n(b) Candidate generation for \u2113*. For each neighbor node u_{\u2113i\u2113j} \u2208 N that contains \u2113*, consider phasing states \u03d5_{\u2113i\u2113j} from that node\u2019s state space that satisfy: (1) non-zero node potential f(\u03d5_{\u2113i\u2113j}) > 0 (i.e., supported by at least one read), and (2) there exists a permutation of haplotype labels under which the phasing is consistent with the already-assigned haplotypes at any shared positions in S_phased \u2229 {\u2113i,\u2113j}. If multiple neighbor nodes contain \u2113*, their compatible phasings are combined to form candidate haplotype matrices C over a set of positions V that includes \u2113* and the already-phased positions used by the generating nodes, ensuring mutual consistency across shared positions under appropriate permutation alignments.\n\n(c) Candidate scoring and update. Each candidate C is scored using three terms over reads restricted to positions V:\n\u2022 Likelihood L(C): for each read r covering at least one position in V,\nL(C) = \u03a3_{r\u2208R(V)} log \u03a3_{k=1..K} \u03b5^{d(C(k), r[V])} (1\u2212\u03b5)^{|cov(r)| \u2212 d(C(k), r[V])},\nwhere d(\u00b7,\u00b7) is Hamming distance on covered positions and \u03b5 is the sequencing error rate.\n\u2022 MEC M(C):\nM(C) = \u03a3_{r\u2208R(V)} min_{k=1..K} d(C(k), r[V]).\n\u2022 Inference agreement F(C): the number of neighbor nodes in N whose inferred phasing \u03d5*_u (from Viterbi/FFBS) is matched by C under some haplotype permutation.\nWithin the current iteration\u2019s candidate set, each metric is min\u2013max normalized to [0,1] (denoted L\u0303, M\u0303, F\u0303) and combined as\nS(C) = w_L\u00b7L\u0303(C) \u2212 w_M\u00b7M\u0303(C) + w_F\u00b7F\u0303(C),\nwith weights summing to 1 (defaults w_L=1/12, w_M=10/12, w_F=1/12). The highest-scoring candidate is selected; its column assignment for position \u2113* is written into the global haplotype matrix H*, \u2113* is marked phased and assigned the current block_id, and any neighbor nodes that become fully utilized by phasing \u2113* are moved from U_unphased to U_phased.",
      "source_document": "papers/2512.04393v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You want to benchmark polyploid haplotype assembly methods on synthetic data in a way that reflects realistic ambiguity from highly similar haplotypes, and you also want the benchmarks to cover both autopolyploid and allopolyploid genome structures using both Illumina short reads and ONT long reads. Describe a concrete end-to-end simulation and preprocessing workflow that accomplishes this, including:\n\n(a) how haplotype genomes are generated from a reference haplotype (mutation model and what kinds of mutations are allowed);\n(b) how autopolyploid versus allopolyploid structure is encoded (ploidies/structures and the roles of inter-subgenome vs within-subgenome mutation rates);\n(c) how short reads and long reads are simulated (simulators, key parameter settings, and how \u201ccoverage per haplotype\u201d is defined/distributed);\n(d) how reads are aligned and converted into haplotype-informative fragments, including the rationale for using different minimum base-quality thresholds for short vs long reads.",
      "answer": "One concrete workflow is:\n\n(a) Genome/haplotype generation from a reference haplotype:\n- Start from a reference haplotype sequence (e.g., from an S. tuberosum haplotype set) and generate mutated haplotypes using a haplotype generator (an adapted Haplogenerator from HaploSim).\n- Use a Poisson mutation model to introduce random mutations.\n- Configure mutation rates as -s [\u00b5, 0, 0], meaning only single-nucleotide mutations occur at rate \u00b5, with no insertions or deletions.\n\n(b) Encoding autopolyploid vs allopolyploid structure:\n- Autopolyploid: generate K haplotypes from the same reference haplotype for ploidies K \u2208 {2, 3, 4, 6}, each haplotype mutated at rate \u00b5 \u2208 {0.001, 0.005, 0.01}.\n- Allopolyploid: first create divergent \u201cancestor\u201d subgenome haplotypes (A, B, C) by mutating the original haplotype at an inter-subgenome divergence rate \u00b5_sub \u2208 {0.0001, 0.0005}. Then, within each subgenome, create additional haplotypes by further mutating at within-subgenome rate \u00b5_within \u2208 {0.00005, 0.0001}.\n- Simulate explicit allopolyploid copy-number structures such as AAB (triploid, unbalanced), AABB (tetraploid, balanced), and AABBCC (hexaploid, balanced).\n\n(c) Read simulation (and \u201ccoverage per haplotype\u201d):\n- Short reads (Illumina): simulate paired-end reads with ART using the HiSeq 2500 error profile (-ss HS25). Use read length 125 bp (-l 125), mean insert size 350 bp (-m 350) with SD 50 (-s 50). Use per-haplotype coverage levels of 3\u00d7, 5\u00d7, 10\u00d7, 20\u00d7, 40\u00d7 for autopolyploid short-read data (and 5\u00d7, 10\u00d7, 20\u00d7, 40\u00d7 for other configurations).\n- Coverage is defined per haplotype, and total sample coverage is distributed equally across all haplotypes (e.g., tetraploid at 20\u00d7 total implies 5\u00d7 per haplotype).\n- Long reads (ONT): simulate with PBSIM3 using the QSHMM-ONT-HQ error model (\u2013qshmm QSHMM-ONT-HQ.model) under a whole-genome strategy (\u2013strategy wgs), at 5\u00d7, 10\u00d7, 20\u00d7, 40\u00d7 per haplotype. The model targets modern ONT R10.4-like read characteristics (mean base quality around Q13\u201314) with realistic indel-dominated errors (notably in homopolymers).\n\n(d) Alignment and fragment extraction:\n- Align Illumina short reads with BWA-MEM (default parameters) and ONT long reads with Minimap2 using the ONT preset (-x map-ont). Sort/index with SAMtools.\n- Extract haplotype-informative fragments from the alignments using extractHAIRS (Hap10).\n- Use different minimum base-quality thresholds: Q13 (\u2013mbq 13) for Illumina (high per-base accuracy; filters bases with >95% accuracy) and a more permissive Q4 (\u2013mbq 4) for ONT to accommodate lower per-base qualities typical of ONT reads.",
      "source_document": "papers/2512.04393v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are building a protein function predictor that must remain usable when the Gene Ontology adds new terms, and you also want to score proteins against GO terms that were never seen with protein\u2013term labels during training (zero-shot). What combination of (1) GO term embedding strategy and (2) decoder-side modeling choice would let the model (a) generate representations for novel GO terms directly from their text definitions while still encoding GO hierarchy, and (b) propagate information from general ancestor functions to specific descendants during prediction? Explain why this design reduces or eliminates the need to retrain when new GO terms appear.",
      "answer": "A suitable design is to (1) embed each GO term from its textual definition using a sentence encoder (SBERT-BioBERT with mean pooling) and then pass these frozen text embeddings through a trainable projection autoencoder that injects ontology structure via multi-task supervision: the latent vector is trained to jointly predict (i) the term\u2019s ancestor set, (ii) its subontology/aspect (MF/BP/CC), and (iii) its term identity (using weighted binary cross-entropy losses). This couples semantic similarity from definitions with hierarchical dependencies, and because embeddings are computed from definitions at inference, new/unseen GO terms can be embedded without retraining (unlike fixed, one-hot/anc2vec-style structural embeddings that require retraining when new terms are added).\n\nFor (2) the predictor uses an encoder\u2013decoder Transformer in which GO terms are arranged in a topological (breadth-first root-to-leaf) order and the decoder applies causal self-attention with a lower-triangular mask so each term attends only to its ancestors; cross-attention then links each GO term representation to the encoded protein residue embeddings (from a pretrained protein LM such as ProtT5). The topological ordering plus causal masking provides an inductive bias that propagates information from general to specific GO terms, improving hierarchical consistency and supporting zero-shot generalization.",
      "source_document": "papers/2512.05245v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When constructing a zero-shot benchmark for protein function prediction with Gene Ontology (a DAG) under the true path rule, what preprocessing steps are needed to ensure that \u201cheld-out\u201d GO terms are genuinely unseen during training without breaking hierarchical consistency, and why must the order of these steps matter?",
      "answer": "To make held-out GO terms truly unseen, you first remove *all* protein\u2013GO associations involving those terms from the training data, and only then apply the true path rule that propagates remaining annotations upward to ancestor terms. Doing the removal before propagation prevents label leakage (a held-out term would otherwise be implicitly reintroduced via hierarchy-based propagation) while still keeping the dataset hierarchically consistent because the true path rule is applied to the cleaned annotation set. At evaluation time, predictions (and labels) are likewise propagated up the GO hierarchy before computing metrics so that scoring respects DAG dependencies.",
      "source_document": "papers/2512.05245v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When extending average-case guarantees for seed-chain-extend from a substitution-only model to an indel channel, what changes are needed in the *definition of recoverability* so that (i) \u201cno-op\u201d indel histories (e.g., delete+insert that leave the observed strings unchanged) don\u2019t unfairly penalize an aligner, and (ii) anchors that are only partially on the true homologous path can still contribute? Give the resulting formal definition of recoverability in terms of (a) the set of non-recoverable points removed from the homologous path and (b) the union of alignments implied by a chain (anchors plus extension boxes).",
      "answer": "The recoverability definition must be generalized by (1) removing from the homologous path any points that are inherently ambiguous/unfindable because indel \u201cno-ops\u201d can create stretches where the strings match but the homologous path \u2018skips\u2019 the matching diagonal points, and (2) counting recovery via the *union of all alignments implied by the chain*, so that clipping anchors (anchors that touch the homologous path but are not fully on it) can still allow the gap-extension region to recover most homologous-path points.\n\nFormally, for each point (i,j) on the homologous path PH, define lengths of non-recoverable regions:\n- r(i,j) = max{ t\u22650 | S[i+1:i+t] = S\u2032[j+1:j+t] and (i+\u2113,j+\u2113) \u2209 PH for 1\u2264\u2113\u2264t } (or 0 if none),\n- l(i,j) = max{ t\u22650 | S[i\u22121:i\u2212t] = S\u2032[j\u22121:j\u2212t] and (i\u2212\u2113,j\u2212\u2113) \u2209 PH for 1\u2264\u2113\u2264t } (or 0 if none).\nThen define the right/left non-recoverable regions:\n- NR(i,j) = { (x,y) \u2208 PH | i < x \u2264 i+r(i,j) OR j < y \u2264 j+r(i,j) },\n- NL(i,j) = { (x,y) \u2208 PH | i \u2265 x > i\u2212l(i,j) OR j \u2265 y > j\u2212l(i,j) }.\nLet U = \u22c3(i,j)\u2208PH ( NR(i,j) \u222a NL(i,j) ) be the set of non-recoverable points to remove from PH.\n\nGiven a chain C = ((i1,j1),\u2026,(iu,ju)), define the union of all alignments it implies:\nAlign(C) = (\u22c3_{\u2113=1..u} {(i\u2113,j\u2113),\u2026,(i\u2113+k\u22121,j\u2113+k\u22121)}) \u222a (\u22c3_{\u2113=1..u\u22121} Ext(\u2113)),\nwhere Ext(\u2113) = {i\u2113+k\u22121,\u2026,i\u2113+1} \u00d7 {j\u2113+k\u22121,\u2026,j\u2113+1} (empty if the ranges are invalid).\n\nThe generalized recoverability is then\nR(C) = | Align(C) \u2229 (PH \\ U) | / | PH \\ U |,\ni.e., the fraction of *recoverable* homologous-path points that lie either on chain anchors or inside some extension box between consecutive anchors.",
      "source_document": "papers/2512.05247v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In average-case analyses of seed\u2013chain\u2013extend under an indel mutation model, concentration bounds often require treating anchor indicators as (approximately) independent. Using the \u201cmatch graph\u201d view with the correspondence map f from reference positions to their unique corresponding query positions (when not deleted), give a set of **sufficient conditions** on two k-mer anchors A(i,j) and A(h,\u2113) that guarantee the two anchor events are independent. Explain briefly (in graph terms) what failure mode these conditions are preventing.",
      "answer": "A sufficient condition is that both of the following hold:\n\n1) **Limited overlap:** the two anchors are separated by at least k on at least one axis, i.e.\n   |i\u2212h| \u2265 k  **or**  |j\u2212\u2113| \u2265 k.\n   (So the anchors\u2019 k-mer intervals cannot overlap on both S and S\u2032 simultaneously.)\n\n2) **No \u201ctwisting\u201d via correspondence edges:** at least one anchor\u2019s k-block in S has no reference positions that correspond (under f) to the other anchor\u2019s k-block in S\u2032, i.e.\n   [i : i+k\u22121] \u2229 f\u207b\u00b9([\u2113 : \u2113+k\u22121]) = \u2205  **or**  [h : h+k\u22121] \u2229 f\u207b\u00b9([j : j+k\u22121]) = \u2205.\n\nThese conditions ensure that, when you add the match-variable edges implied by both anchors on top of the underlying correspondence edges (i \u2194 f(i)) in the match graph, the induced subgraph contains **no cycles**. Since cycles are the source of dependence among match variables (and hence among anchor indicators), acyclicity implies the anchor events factorize: Pr(A(i,j) \u2227 A(h,\u2113)) = Pr(A(i,j))Pr(A(h,\u2113)).",
      "source_document": "papers/2512.05247v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In seed\u2013chain\u2013extend, extension between consecutive anchors is typically done by dynamic programming over an \u201cextension box,\u201d so runtime is proportional to the box area. Suppose two chains C and C\u2032 share the same first and last anchor, and C\u2032 is obtained from C by removing some internal anchors (so C\u2032 \u2282 C but endpoints match). (i) Express the extension runtime through a chain in terms of per-gap sizes on S and on S\u2032. (ii) Using this expression, prove that extension through the sparser chain cannot be faster\u2014i.e., that removing internal anchors (while keeping endpoints fixed) makes total extension runtime weakly larger: TExt(C\u2032) \u2265 TExt(C).",
      "answer": "(i) For a chain C=((i1,j1),\u2026,(iu,ju)), define the gap sizes between consecutive anchors on S and S\u2032 as\nG\u2113(S;C)=max(i\u2113+1\u2212i\u2113\u2212k+1,0) and G\u2113(S\u2032;C)=max(j\u2113+1\u2212j\u2113\u2212k+1,0), for 1\u2264\u2113\u2264u\u22121. The DP extension runtime through C decomposes over consecutive anchor pairs and is proportional to the sum of extension-box areas:\nTExt(C)=\u2211_{\u2113=1}^{u\u22121} G\u2113(S;C)\u00b7G\u2113(S\u2032;C).\n(ii) Let C\u2032 share the same first/last anchor as C and be a subchain (C\u2032\u2282C). Consider any consecutive pair of anchors in C\u2032; in C, that same interval is subdivided into several smaller consecutive anchor gaps. For one such interval, write the C-subdivision gaps as {(G_r(S;C),G_r(S\u2032;C))}_{r=1}^{p\u22121}. Then extension through C over that interval is\n\u2211_{r=1}^{p\u22121} G_r(S;C)G_r(S\u2032;C) \u2264 (\u2211_{r=1}^{p\u22121} G_r(S;C)) (\u2211_{r=1}^{p\u22121} G_r(S\u2032;C))\n(using non-negativity and that \u2211 a_r b_r \u2264 (\u2211 a_r)(\u2211 b_r)). The sums of gap lengths equal the single larger gap length for C\u2032 on S and on S\u2032, so the RHS is exactly the area (runtime) of extending directly between the two C\u2032 anchors. Applying this argument to every consecutive pair in C\u2032 and summing shows that the total runtime through C is \u2264 the total runtime through C\u2032, i.e. TExt(C\u2032) \u2265 TExt(C).",
      "source_document": "papers/2512.05247v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an average-case analysis of seed\u2013chain\u2013extend under an indel mutation channel, a central step is to argue that **spurious k-mer anchors** (exact k-mer matches that lie entirely off the true homologous path) do not appear with high probability.\n\nAssume the reference string has length n, the query has length m\u2272n, seeds have length k=C\\log_\\sigma n, and work under an \u201cexpansion\u2013contraction\u201d event EC that guarantees each k-mer region in the generative part of S corresponds to only O(k) possible positions in S\u2032 (so anchor dependencies are localized). Let NS be the number of spurious anchors.\n\nDerive a high-probability bound showing that, for C>3 and sufficiently large n, **NS=0 with probability at least 1\u2212O(1/n)**. Your derivation should:\n1) state the variance bound for NS under EC,\n2) apply a concentration inequality to bound NS above its mean,\n3) use k=C\\log_\\sigma n (so \\sigma^k=n^C) and m\u2272n to simplify the bound enough to conclude NS<1 (hence NS=0), and\n4) explain briefly what EC is buying you in terms of controlling dependence between potential anchors.",
      "answer": "1) **Variance bound under EC.** Under the EC event, the number of spurious anchors NS satisfies\n\\[\\operatorname{Var}(N_S)\\le T_0 k^2 \\frac{mn}{\\sigma^k},\\]\nfor a constant \\(T_0\\) (defined from the EC expansion constants), equivalently\n\\[\\mathbb{E}(N_S^2)\\le (\\mathbb{E}N_S)^2 + 2T_0 k^2\\frac{mn}{\\sigma^k}.\\]\n\n2) **Concentrate using Chebyshev (conditional on EC).** Since \\(N_S\\) is a sum of anchor indicators and EC makes dependencies sufficiently local to bound \\(\\operatorname{Var}(N_S)\\) as above, Chebyshev gives\n\\[\\Pr\\Big(N_S\\ge \\mathbb{E}(N_S\\mid EC) + \\sqrt{n\\,\\operatorname{Var}(N_S\\mid EC)}\\,\\Big\\vert\\, EC\\Big)\\le \\frac{1}{n}.\\]\nUsing the crude mean bound \\(\\mathbb{E}(N_S\\mid EC)\\le \\mathbb{E}(N_S)\\le mn/\\sigma^k\\), this yields the explicit high-probability upper bound (still conditional on EC):\n\\[\\Pr\\Big(N_S\\ge \\frac{mn}{\\sigma^k} + \\sqrt{T_0\\,n\\,k^2\\frac{mn}{\\sigma^k}}\\,\\Big\\vert\\, EC\\Big)\\le \\frac{1}{n}.\\]\n\n3) **Plug in \\(k=C\\log_\\sigma n\\), \\(m\\lesssim n\\), and simplify to get \\(N_S<1\\).** Since \\(\\sigma^k=n^C\\) and \\(m\\le n\\),\n\\[\\frac{mn}{\\sigma^k}\\le \\frac{n^2}{n^C}=n^{2-C}.\n\\]\nAlso,\n\\[\\sqrt{T_0\\,n\\,k^2\\frac{mn}{\\sigma^k}}\\le \\sqrt{T_0\\,n\\,k^2\\,n^{2-C}} = k\\sqrt{T_0}\\,n^{\\frac{3-C}{2}}.\n\\]\nWith \\(k=C\\log_\\sigma n\\), the upper bound becomes\n\\[N_S\\ \\le\\ n^{2-C} + \\sqrt{T_0 C\\log n}\\;n^{\\frac{3-C}{2}}\\quad \\text{with probability }\\ge 1-\\frac{1}{n}\\text{ (given EC)}.\n\\]\nIf \\(C>3\\), then both exponents \\(2-C\\) and \\((3-C)/2\\) are negative, so for sufficiently large \\(n\\) the right-hand side is <1. Because \\(N_S\\) is an integer, \\(N_S<1\\Rightarrow N_S=0\\). Thus,\n\\[\\Pr(N_S=0\\mid EC)\\ge 1-\\frac{1}{n}\\quad (\\text{for large }n,\\ C>3).\\]\nUnconditioning and using \\(\\Pr(EC)\\ge 1-2/n\\) yields\n\\[\\Pr(N_S=0)\\ge \\Pr(N_S=0\\mid EC)\\Pr(EC)\\ge (1-1/n)(1-2/n)\\ge 1-3/n.\\]\n\n4) **What EC buys you.** EC enforces bounded expansion/contraction so that a k-mer region in S can correspond to only O(k) candidate locations in S\u2032 (and vice versa). This localizes which anchor indicators can share underlying letters, enabling a usable bound on \\(\\operatorname{Var}(N_S)\\) despite indels creating long-range index shifts; without EC, a single S k-mer could correspond to many S\u2032 positions, producing too many dependent anchor events to control \\(\\operatorname{Var}(N_S)\\) tightly enough to conclude \\(N_S=0\\) w.h.p.",
      "source_document": "papers/2512.05247v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "Under an indel+substitution mutation channel, overlapping k-mers along the generative region are k-dependent, so standard Chernoff bounds do not directly apply.\n\nSuppose you define an indicator for each k-block in the generative region of S that equals 1 iff the entire k-block experiences **no mutations** (no insertion immediately before any position in the block, no deletion of any position in the block, and no substitution within the block). This implies the block is a homologous anchor.\n\n(a) Explain how to construct a \u201cproper cover\u201d of these k-block indicators into at most k groups of mutually independent variables, and why this implies the cover number \u03c7(A) is \u2264 k.\n\n(b) Using a dependent Chernoff\u2013Hoeffding bound with cover number \u03c7(A), derive an explicit upper bound (in terms of \u2113, k, and total mutation rate \u03b8T) on the probability that **a fixed interval of length \u2113** in the generative region contains **zero** homologous anchors.\n\n(c) Show how choosing a gap length \n\\[g(n)=\\frac{50k\\ln n}{8(1-\\theta_T)^k}\\]\nleads to a high-probability statement that no homologous gap of length \u2265 g(n) occurs anywhere in the generative region (probability at least 1\u22121/n).",
      "answer": "(a) Index k-block indicators by their start positions i. Overlaps only occur among starts within <k. Partition the starts by residue class modulo k: for each j\u2208{1,\u2026,k}, let Aj={E_{j+tk : j+(t+1)k\u22121} : t\u22650 and the block lies in the generative region}. Within each Aj the corresponding k-blocks are disjoint, hence their mutation events are independent. The family {Aj} covers all block indicators and has size k, so the minimum proper cover number satisfies \u03c7(A)\u2264k.\n\n(b) Let q be the probability a k-block has no mutation. Under the channel, this is q=((1\u2212\u03b8i)(1\u2212\u03b8d)(1\u2212\u03b8s))^k \u2265 (1\u2212\u03b8T)^k. Applying the dependent Chernoff\u2013Hoeffding bound with \u03c7(A)\u2264k to the count NH of homologous anchors in an interval of length \u2113 (using NH \u2265 NP, the number of mutation-free k-blocks), gives for t=\u2113q:\nPr(NH=0) = Pr(NH\u2264\u2113q\u2212t) \u2264 exp(\u22128t^2/(25\u00b7\u2113\u00b7k\u00b7q)) = exp(\u22128\u2113q/(25k)) \u2264 exp(\u22128\u2113(1\u2212\u03b8T)^k/(25k)).\nThus, for any fixed length-\u2113 interval, the probability it contains no homologous anchor is at most exp(\u22128\u2113(1\u2212\u03b8T)^k/(25k)).\n\n(c) For \u2113=g(n)=50k ln(n)/(8(1\u2212\u03b8T)^k), the bound in (b) becomes exp(\u22122 ln n)=1/n^2. There are at most m\u2032\u2212\u2113+1 \u2264 n possible starting positions for such a length-\u2113 interval in the generative region, so the expected number of length-\u2113 homologous gaps is \u2264 n\u00b7(1/n^2)=1/n. By Markov\u2019s inequality, the probability that at least one such gap exists is \u22641/n. Equivalently, with probability \u22651\u22121/n, no homologous gap has length \u2265g(n) (the \u201cdense homologous anchors\u201d/F2 event).",
      "source_document": "papers/2512.05247v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an imaging-based digital assay where each candidate particle count n defines a generative image model with unknown background and particle parameters, how can you formulate a multiple-hypothesis decision rule that (i) compares different n while discouraging overfitting, and (ii) chooses the maximum hypothesis order n_max from an expected mean particle density so that \u201ctoo many particles\u201d is a rare event? State the scoring function and the criterion used to pick n_max.",
      "answer": "A practical rule is to fit each hypothesis H_n (n=0,1,\u2026,n_max) by maximum-likelihood estimation of the background and particle parameters using a penalized log-likelihood \u2113_p, then score each hypothesis with\n\n\u03be_n = \u2113_p \u2212 (1/2) log det(M),\n\nwhere M is the Fisher information matrix for that fitted model. The (1/2) log det(M) term acts as an information-criterion/MDL-style complexity correction (asymptotically equivalent to BIC) to guard against overfitting as n (and hence degrees of freedom) increases. The estimated particle count is the n that maximizes \u03be_n.\n\nTo set n_max from an estimated mean density N\u0304 assuming a Poisson point process, choose n_max so the Poisson tail probability is below a small tolerance \u03b5:\n\np(N > n_max) = 1 \u2212 \u03a3_{k=0}^{n_max} (N\u0304^k/k!) e^{\u2212N\u0304} < \u03b5.",
      "source_document": "papers/2512.05346v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an imaging-based digital assay where you count bright spots per sub-image, the empirical count histogram often violates Poisson assumptions (variance \u2260 mean). How can you (i) model such over- or under-dispersed spot counts with a two-parameter distribution, (ii) interpret the sign of its dispersion parameter in terms of plausible physical assay/imaging mechanisms, and (iii) explain why estimating the mean spot density from the fraction of empty sub-images (\u201cnull count method\u201d) remains valid even when dispersion is present?",
      "answer": "(i) Use the generalized Poisson distribution (GPD) with PMF\np_N(N; \\bar N, \\psi) = [\\bar N(\\bar N + N\\psi)^{N-1}/N!]\\,\\exp[-\\bar N - N\\psi],\nwhere \\bar N>0 is a rate parameter (note the GPD mean is \\bar N/(1-\\psi)) and \\psi is a dispersion parameter.\n\n(ii) \\psi>0 gives over-dispersion (variance greater than mean), which can arise from effects like image distortion or surface heterogeneity/preferential binding; \\psi<0 gives under-dispersion (variance less than mean), which can be caused by coincidence loss suppressing high-count events when particles overlap strongly.\n\n(iii) The null-count estimator uses the observed frequency of N=0, \\nu_N(0), via \\bar N = -\\log \\nu_N(0). This remains robust to dispersion because the GPD\u2019s zero-count probability reduces to p_N(0;\\bar N,\\psi)=\\exp(-\\bar N), independent of \\psi, just as for a Poisson process.",
      "source_document": "papers/2512.05346v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an imaging-based digital assay where particle images are modeled as sums of Gaussian PSFs with Poisson noise, how can you operationally define a \u201ctwo-particle resolution threshold\u201d using outcomes from a multi-hypothesis particle-count estimator, and what qualitative under- and over-counting behaviors should you expect as the true inter-particle separation crosses that threshold? Also explain why increasing the PSF width can shift the threshold to smaller separations when expressed in units of \u03c3.",
      "answer": "A practical resolution threshold can be defined from repeated trials on images containing exactly two particles at varying separations by looking at the estimator\u2019s selected count: define the transition point (resolution threshold) as the separation where the fraction of outcomes classified as two particles (i.e., selecting the two-particle hypothesis) drops to 50% of all trials.\n\nAs separation decreases past this threshold, the dominant failure mode is conservative under-counting: the estimator increasingly selects the one-particle hypothesis (two particles reported as one) because the two-particle image becomes hard to distinguish from a single broadened spot under noise. Over-counting is not dominant at very small separations; it appears mainly in the transition region near the threshold, where noise can create spurious maxima that get misclassified as additional particles (occasionally yielding 3+ particle estimates).\n\nWhen the PSF is broader, the normalized threshold separation (in multiples of \u03c3) can move to smaller values because each particle\u2019s information is spread across more pixels; in the Poisson-noise regime this provides more pixels with useful SNR, allowing smaller intensity differences between one- vs two-particle images to be detected. (In absolute pixels, the threshold still grows with \u03c3, but relative to \u03c3 it can decrease.)",
      "source_document": "papers/2512.05346v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an imaging-based digital assay where each nanoparticle produces a Gaussian-like PSF but background can come from (i) optical/sample stray light and (ii) detector/electronic noise, how should you expect particle-counting performance to change as microscope magnification increases, and what acquisition guideline follows for how many pixels the PSF should span? In your answer, distinguish the \u201ctype I\u201d case where background scales with magnification like the signal from the \u201ctype II\u201d case where background is magnification-independent, and describe the dominant failure modes at too-low vs too-high magnification.",
      "answer": "As magnification increases, the PSF spreads over more pixels while conserving total signal per particle, so peak per-pixel signal decreases. Two distinct background scalings lead to different trends:\n\n\u2022 Type I (optical/sample background): the background dilutes with magnification in the same way as the signal (\u221d 1/magnification\u00b2). Under this condition, performance is roughly constant over a moderate magnification range, but degrades at both extremes. At too low magnification the PSF is undersampled (narrow), producing mainly false positives (true N=0 misclassified as N\u0302=1; over-counting). At too high magnification the PSF spans many pixels; larger Poisson fluctuations across more pixels produce spurious maxima (over-counting), and broadened PSFs overlap more so close pairs may be merged (under-counting from limited resolvability).\n\n\u2022 Type II (detector/electronic background): the background is independent of magnification. Performance stays high up to about ~2\u00d7 baseline magnification, then drops when magnification becomes large enough that the fixed background becomes significant relative to the diluted per-pixel signal (a readout-noise\u2013dominated regime).\n\nCombining these, a practical magnification window is expected: the low end is limited by type I undersampling, while the high end is limited by type I spurious maxima/resolvability loss and by type II fixed-noise floor depending on whether SBR or SNR is limiting. An empirical acquisition guideline is to choose magnification so the PSF spans roughly 2\u20134 pixels, which gives best overall counting performance.",
      "source_document": "papers/2512.05346v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a physics-based particle-counting method that models pixel intensities with independent Poisson noise and scores each candidate particle-count hypothesis using a fitted image-formation model, suppose you multiply *both* the particle signal and the uniform background level by a common scale factor s (i.e., you collect proportionally more photons without changing contrast). (i) How do the signal-to-background ratio (SBR) and the signal-to-noise ratio (SNR) change with s under this Poisson model? (ii) Why can pushing s to very large values paradoxically increase false-positive particle detections (over-counting), even though the SNR is higher?",
      "answer": "(i) With Poisson noise, scaling both the particle intensity Ik and background \u03bbBG by the same factor s leaves the SBR unchanged because SBR = Ik fmax / \u03bbBG, so the common factor cancels. The SNR, defined as SNR = Ik fmax / sqrt(Ik fmax + \u03bbBG), increases as \u221as when Ik and \u03bbBG are both multiplied by s (numerator scales \u221d s, denominator scales \u221d \u221as).\n\n(ii) At very large s, although relative noise decreases, the *absolute* magnitude of Poisson fluctuations grows with the photon count. Those larger absolute fluctuations can create spurious local maxima that resemble weak/narrow PSFs in the likelihood surface, causing background-only images (true N=0) to be misclassified as containing a particle (often N\u0302=1). This yields an increased over-count (false-positive) rate at extremely high SNR.",
      "source_document": "papers/2512.05346v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When designing a linkage disequilibrium (LD) metric for the highly polymorphic HLA using phased haplotype genomes, what specific algorithmic components can be combined to (i) explicitly use phasing information, (ii) avoid over-reliance on marginal allele frequencies, and (iii) retain information from low-frequency/rare variants\u2014and how does each component achieve its role?",
      "answer": "A haplotype-aware LD metric can be built by combining:\n\n1) A genetic compatibility/allelic-concordance term that is computed on phased alleles (not unphased genotypes). In this document, this term is \u03a6(X,Y), defined across the 2K alleles from K individuals by mapping each allele to REF vs ALT (g(t)=0 for REF, 1 for ALT) and then using XOR to count how often the paired alleles at loci X and Y are in the same state (XOR=0). \u03a6 is the fraction of allele pairs that match (both REF or both ALT), so it directly incorporates phasing/haplotype pairing and distinguishes different joint allele arrangements even when marginal allele frequencies are the same.\n\n2) An informativeness term based on conditional probabilities to regulate skewed allele frequencies and preserve rare variants. Here, the Informativeness Factor IF(i|j) is computed from the conditional probability P(i|j) and uses an arctan-based nonlinear transformation (IF(i|j) = (2/\u03c0)\u00b7arctan(1 \u2212 log2(P(i|j)))). Using conditional information content reduces dependence on marginal allele frequencies and retains contributions from low-frequency/rare variants rather than filtering them out.\n\nThese pieces are combined multiplicatively (and aggregated by taking the maximum across allele-pair choices), yielding CICC = max( \u03a6(X,Y) \u00b7 IF(i|j) \u00b7 IF(j|i) ), which addresses the common shortcomings of r\u00b2 and D\u2032: lack of phasing, sensitivity to marginal frequencies, and exclusion/instability of rare variants.",
      "source_document": "papers/2512.05573v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You develop a new linkage disequilibrium (LD) metric intended to work well in the HLA. How can experimentally validated recombination hotspots be used as \u201cground truth\u201d to validate whether your metric and downstream haplotype-block calling are working, and which statistical comparisons of correlation-value distributions can be used to show the metric separates hotspot (weak-LD) regions from high-linkage regions?",
      "answer": "Validated recombination hotspots can serve as ground truth because recombination breaks down linkage: SNPs within hotspot regions are expected to show very weak LD. A practical validation is to (i) compute pairwise LD with the candidate metric, (ii) run a haplotype-block partitioning method (e.g., CI, solid spine, or four-gamete test) to delineate blocks, and then (iii) assess whether inferred block boundaries/low-correlation gaps overlap the known hotspot intervals (e.g., by direct overlap analysis) and whether highly correlated blocks flank rather than span the hotspots. In addition to overlap, the metric should distinguish hotspots from high-linkage regions by their correlation-value distributions: apply a Kolmogorov\u2013Smirnov (KS) test to compare the distributions and use Kernel Density Estimation (KDE) to visualize/quantify differences. A better metric shows clearer distributional separation between hotspot regions (weak LD) and regions of high linkage, and stronger concordance between called blocks and known hotspot locations.",
      "source_document": "papers/2512.05573v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are benchmarking multiple LD measures paired with haplotype-block callers on simulated data where the \u201ctrue\u201d block structure is known. What two complementary metrics can you use to evaluate block-calling quality\u2014one focused on whether the right SNPs are assigned to a block and one focused on boundary alignment\u2014and how are they combined into a single summary score?",
      "answer": "Use (1) an **accuracy level**, defined as the ratio of SNPs assigned to a called block that are genuine members of the true (synthetic) block\u2014i.e., it measures precision of SNP allocation to blocks; and (2) a **consistency level**, which quantifies how the identified SNPs are distributed relative to the true synthetic-block boundaries and penalizes deviations outside the boundary (boundary misalignment/fragmentation). Combine them with an F1-style harmonic mean: **F1 = 2 \u00d7 accuracy \u00d7 consistency / (accuracy + consistency)**.",
      "source_document": "papers/2512.05573v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a case\u2013control study where you suspect disease risk is driven by sets of loci that are strongly coordinated (high LD) in cases but \u201cdecoupled\u201d (weak LD) in controls, how can you operationalize this idea to select candidate disease-related SNPs using pairwise LD values and network filtering? In your answer, specify (i) the inequality constraints you would impose on pairwise LD in case vs control cohorts, (ii) how you would choose/adapt the case-threshold when the LD metric has a very different empirical distribution than standard r\u00b2, and (iii) a secondary network-based stability criterion that can prune spurious high-LD pairs even after thresholding.",
      "answer": "(i) Define a SNP set S={s1,\u2026,sn} and keep SNP pairs (si,sj) whose pairwise correlation under an LD metric M satisfies two simultaneous constraints: the correlation in the case cohort is above a lower bound, M_case(si,sj) \u2265 l_case, while the correlation in the control cohort is below an upper bound, M_control(si,sj) \u2264 l_control. This encodes \u201cstrong coordination in cases but disrupted coordination in controls.\u201d\n\n(ii) For r\u00b2, a commonly used lenient GWAS-style threshold can be used for l_case (e.g., 0.6) because r\u00b2 values are typically low overall. For an alternative metric with different scale/distributional behavior (e.g., CICC), l_case should be set by inspecting the metric\u2019s empirical distribution and the topology of the resulting SNP\u2013SNP network: look for distributional breakpoints/inflection points (e.g., sharp shifts in the correlation-value histogram) and adjust the threshold accordingly so that the retained edges correspond to coherent high-interdependence structure rather than the bulk of weak correlations.\n\n(iii) After correlation thresholding, further filter by a network stability measure based on node/pair co-occurrence frequency: examine the distribution of how often SNP nodes (or SNP pairs) co-occur in the thresholded network across partitions/segments, identify an inflection point (\u201csharp drop\u201d), and retain only those edges/nodes whose co-occurrence frequency exceeds that point. This additional co-occurrence filter removes many low-connectivity loci that pass the correlation cutoff but do not form stable high-correlation components.",
      "source_document": "papers/2512.05573v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are defining a haplotype-aware LD score from phased diploid genomes. Suppose that, for a SNP pair, you can compute an allele-level concordance term (e.g., a REF/ALT compatibility score) and conditional-information terms for each direction of conditioning, yielding four possible allele-combination scores across the two SNPs. What is the rationale for aggregating these four candidate scores by taking their maximum (rather than averaging), and what biological/LD-architecture assumption does this choice encode about haplotype blocks in the presence of heterozygosity, mutation, and variable recombination rates?",
      "answer": "The max aggregation is used because two SNPs in diploid genomes generate four possible allele-combination (haplotype) cases (maternal vs paternal alleles), and strong LD may exist on one chromosome even if the other chromosome carries different alleles due to heterozygosity/mutation or local recombination differences. By taking the maximum over the four conditional-information\u2013weighted compatibility scores, the method captures the strongest haplotype-specific linkage signal instead of diluting it across mixed haplotypes.\n\nThis choice encodes the assumption that when two sites form a haplotype block due to high correlation within one chromosome, \u201ctheir counterparts in another chromosome also constitute a haplotype block by nature,\u201d so the relevant LD strength should be represented by the strongest consistent allele pairing. It is also motivated by the idea that \u201cchromosomal base correlations remain strong despite gene mutations or regions with low recombination rates,\u201d so the LD metric should reflect the most linked allele configuration rather than an average that could be reduced by heterozygotes or discordant pairings.",
      "source_document": "papers/2512.05573v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are designing an R/Bioconductor container to manage results from many differential-expression and downstream enrichment contrasts (e.g., multiple conditions and cell types). What data-organization strategy lets you (i) avoid duplicating large per-contrast model/result objects while (ii) still enabling fast access to contrast-specific gene-level statistics and (iii) keeping enough provenance (tool/version, thresholds) to support reproducibility? Describe where you would store the full original DEA objects versus per-gene summaries, and how contrasts should be referenced/annotated; also note how this differs for a multi-contrast limma fit versus muscat pseudobulk outputs.",
      "answer": "Use a contrast-centric design where DEA/FEA results are stored as a set of uniquely named contrasts, and each contrast carries basic metadata (e.g., package and version used; for DEA also alpha level and log-fold-change threshold when available; for FEA also which DEA contrast it derives from and the enrichment tool/version). For efficient storage, keep each full original DEA result object only once in the container\u2019s global metadata and have each contrast entry keep an internal pointer/reference to its associated original object so it can be retrieved when needed (for limma this means linking the underlying MArrayLM object rather than duplicating it across contrasts). In parallel, embed the feature-level DEA summaries needed for quick queries\u2014log2 fold change/logFC, p-value, and adjusted p-value\u2014directly into the feature annotation (rowData) for each contrast. For muscat pseudobulk workflows, which generate separate result tables per contrast, store those per-contrast tables individually, but still organize them under the same named-contrast and metadata infrastructure.",
      "source_document": "papers/2512.05731v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using sparse autoencoders (SAEs) to causally control (steer) an autoregressive antibody language model toward a desired germline heavy J-gene (e.g., IGHJ4), how is a single latent feature used as an intervention on the model\u2019s hidden state during generation, and what does the document show about why a latent with high concept-predictive performance (e.g., high F1 for IGHJ4) may still fail to reliably change the IGHJ distribution of generated sequences? Also, which SAE variant provides more reliable steerable features for this task and what interpretability trade-off does it introduce?",
      "answer": "Steering is done by taking the latent\u2019s decoder direction d(i)=W_dec[i,:] and adding a scaled version of it to the model hidden state at the intervention layer: h_l* = h_l + \u03b1\u00b7d(i), where \u03b1 is the steering factor, h_l is the pre-intervention hidden state and h_l* is the post-intervention hidden state.\n\nDespite some TopK-SAE latents being strong predictors of germline identity (high predictive performance for IGHJ classes), steering along those TopK latents was unpredictable and did not consistently increase the target gene\u2019s proportion in generated libraries. The document attributes this to the broader point that correlation/predictive power is not equivalent to causal influence on generation: some gene-correlated latents may simply mark conserved positions or other non-causal, context-specific signals, and TopK SAEs can suffer from feature splitting (higher-level concepts broken into contextual/residue-level fragments), making individually \u201cinterpretable\u201d latents insufficient for reliable generative control.\n\nOrdered SAEs provide more reliably steerable features in this setting because they impose a strict hierarchy over latents, yielding more abstract, higher-level features that better influence downstream generation. The trade-off is that these ordered/hierarchical latents have more complex, less intuitively interpretable activation patterns (they can activate broadly across a range of residues rather than cleanly localizing to a specific residue or motif).",
      "source_document": "papers/2512.05794v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You\u2019ve identified SAE latents that correlate with an antibody heavy-chain J-gene label (e.g., IGHJ4), but you want to check whether these latents reflect a genuine gene-level concept rather than an artifact of sequence length or a single conserved residue. What analysis can you do to test whether the latent\u2019s activations are anchored to the aligned J-region, and what does this analysis indicate about the specific residue triggers for the top IGHJ4-correlated TopK latents?",
      "answer": "Align antibody sequences to a common antibody numbering scheme (ANARCI with IMGT numbering) and then compare where the latent fires when positions are indexed by absolute sequence position versus by aligned IMGT position. If activations are broadly distributed near the sequence end in absolute coordinates but become concentrated at consistent IMGT sites, this supports that the latent is tied to the aligned J-region rather than merely sequence length.\n\nUsing this analysis for IGHJ4-correlated TopK latents, the strongest activations localize to specific IMGT positions: two top latents activate at IMGT positions 120 (Q) and 119 (G), which are conserved across human IGHJ genes, while another activates at IMGT position 117 (Y), which is characteristic of IGHJ4. This pattern suggests that some highly correlated TopK latents may be contextual residue markers (often involving preceding-motif context) rather than clean, abstract gene-identity features.",
      "source_document": "papers/2512.05794v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a \u201cmining minima\u201d (VM2) binding free-energy workflow, how is the intractable configurational integral for the partition function made computationally manageable, what criterion is used to decide that the free-energy estimate has converged, and what are the two main search moves used to discover relevant minima (including how conformational entropy is treated)?",
      "answer": "VM2 avoids exhaustive MD-style sampling of the full conformational space by approximating the multidimensional configurational integral as a sum of local configuration integrals over a manageable set of low-energy conformational minima, focusing computation on the thermodynamically dominant states. The chemical potential/free-energy accumulation is considered converged when iterative searches fail to find any new lower-energy minima and the cumulative free energy stops changing significantly. To locate minima, VM2 uses (1) rigid-body translation/rotation searches that sample ligand translations/rotations relative to the protein with relaxation via energy minimization steps, and (2) mode distort\u2013minimize searches that apply ligand-focused or random-pair mode distortions followed by minimization, iterating from previously found low-energy minima. Entropy is included explicitly via a harmonic approximation with a mode-scanning approach, and bulk solvation is handled with continuum models (optionally adding explicit waters when needed).",
      "source_document": "papers/2512.06141v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a binding free-energy workflow that augments a classical mining-minima/VM2 estimate with a quantum correction, how is the QM/MM+VQE correction constructed and added to the free energy? Describe (i) what is placed in the QM region versus the MM region and how the MM environment is coupled to the QM Hamiltonian, (ii) the definition of the electronic binding-energy metric used for the correction and how a \u201cfrozen protein\u201d approximation simplifies it, and (iii) how this correction enters the final \u0394G expression (including the role of the scaling constant).",
      "answer": "(i) The ligand is treated as the QM region, while the surrounding protein environment is treated classically: only protein residues with at least one atom within 8.0 \u00c5 of the ligand are included as the MM region and represented as fixed point charges (protein charges generated with tleap/ff99SB). The coupling uses electrostatic embedding, i.e., the MM charge distribution is included explicitly in the one-electron part of the QM Hamiltonian.\n\n(ii) The correction is based on an electronic binding-energy metric (U_bind) defined as the QM/MM energy of the complex minus the energies of the separated components: \u0394U_bind = U_complex \u2212 E_protein \u2212 E_ligand. Using a \u201cfrozen protein\u201d approximation where the protein is treated as distributed point charges, protein\u2013protein terms cancel, simplifying the correction to a single energy difference between ligand-in-protein-field and the isolated ligand: \u0394U_QM = U_ligand_in_protein_field \u2212 U_ligand.\n\n(iii) The final binding free energy adds this quantum correction to the standard VM2 decomposition: \u0394G = \u0394U_MM + \u0394W \u2212 T\u0394S + \u03b1\u00b7\u0394U_QM, where \u03b1 is a fixed scaling constant (given as 1.59\u00d710\u207b3).",
      "source_document": "papers/2512.06141v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a QM-enhanced binding free-energy workflow that seeks to improve electrostatics without switching to fully polarizable force fields, how can ligand atomic partial charges be re-parameterized from a quantum-mechanical electrostatic potential (ESP) using a RESP-style fit? Specify (i) the QM electronic-structure level and implicit-solvent model used to compute the ESP, (ii) how the ESP grid points are placed relative to the ligand, and (iii) the key RESP fitting settings (probe radius, restraint form/parameters, convergence/iteration limits, and any atoms excluded from restraints).",
      "answer": "One protocol is to replace the force-field-assigned ligand charges with RESP-fitted charges derived from a QM electrostatic potential computed on the free ligand:\n\n(i) QM level/solvent for the ESP: compute the ligand ESP with PySCF at the HF/ma-def2-SVP level using ddCOSMO implicit solvent (water).\n\n(ii) ESP grid placement: evaluate the QM ESP on a predefined grid of points located in the solvent-accessible region around the ligand, with grid points positioned outside the molecule\u2019s van der Waals radius.\n\n(iii) RESP fitting settings: use a probe radius of 0.7 \u00c5; apply RESP restraints with alpha = 0.001 au and beta = 0.1 au; run up to 25 fitting iterations with a convergence tolerance of 1.0 \u00d7 10\u207b4 electrons; exclude hydrogen atoms from the restraint procedure.",
      "source_document": "papers/2512.06141v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a protein\u2013ligand binding free-energy predictor against experimental \u0394G values, how can you justify using a linear regression-style comparison (predicted vs. experimental) and claim that the method is suitable for lead-optimization ranking? Describe (i) a quantitative rank-order criterion based on Kendall\u2019s \u03c4, and (ii) the specific residual-plot diagnostics and the error-behavior assumptions they are used to verify.",
      "answer": "(i) A Kendall rank correlation threshold of \u03c4 \u2265 0.5 can be used as a criterion for \u201cgood\u201d rank-ordering performance in pharmaceutical-style benchmarks, supporting use for ligand prioritization.\n\n(ii) Residual-plot diagnostics include: residuals versus predictors (or versus the independent variable), a histogram of residuals, residuals versus fitted values, and a normal probability (Q\u2013Q) plot. Together these are used to verify key linear-model assumptions\u2014normality of errors, constant variance (homoscedasticity), and independence of errors. A patternless/random scatter of residuals around zero supports that a linear regression model is appropriate; a roughly symmetric bell-shaped residual histogram supports normality; and an approximately linear normal-probability plot supports normally distributed error terms.",
      "source_document": "papers/2512.06141v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a protein\u2013ligand binding free-energy prediction workflow across many targets, why is it useful to stratify targets by binding-site/functional class, and what four mechanistic classes can be used for such stratification (including the defining binding-site characteristics for each class)?",
      "answer": "Stratifying targets by binding-site/functional class helps interpret performance by separating systems with qualitatively different physical challenges (e.g., deep catalytic pockets vs flexible induced-fit sites vs highly dynamic membrane receptors), so that failures/successes can be attributed to specific modeling limitations such as conformational flexibility, solvent exposure, or receptor dynamics.\n\nOne four-class scheme is:\n1) Proteases: deep, well-defined catalytic pockets often used as scoring benchmarks.\n2) Protein kinases and signaling enzymes: conformationally flexible ATP-binding clefts.\n3) Shallow or induced-fit pockets: surface-exposed or inducible pockets where ligand binding requires local structural rearrangement.\n4) G protein\u2013coupled receptors (GPCRs): membrane-embedded, highly dynamic systems that pose distinct challenges for binding free-energy prediction (including receptor dynamics and environment).",
      "source_document": "papers/2512.06141v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When designing a neural-augmented importance sampling estimator for expected outputs in a stochastic reaction network, what conditions and theoretical arguments ensure (i) the estimator remains unbiased for E[f(X(T))], and (ii) it can achieve zero variance in the idealized limit? State the key requirement on the output function f and explain the role of the learned approximation \\(\\hat U\\) in both guarantees.",
      "answer": "(i) Unbiasedness is obtained by simulating an auxiliary SRN with modified (generally time-dependent) propensities \\(\\lambda_k^{IS}(x,f,t)=\\Pi_{\\zeta_k}\\hat U(x,f,t)\\,\\lambda_k(x)\\) and weighting samples by the corresponding likelihood ratio process \\(Z^{IS}\\). For positive output functions f, the Girsanov theorem implies that the weighted terminal functional \\(E^{IS}(f,\\tilde f,T,T)=f(X^{IS}(\\tilde f,T,T))\\,Z^{IS}(\\tilde f,T,T)\\) has the same expectation as \\(f(X(T))\\), so Monte Carlo averages of this quantity are unbiased for \\(\\mathbb E[f(X(T))]\\). (ii) In the idealized case where the learned approximation exactly equals the true expected output (\\(\\hat U=U\\)), using \\(\\tilde f=f\\) yields \\(E^{IS}(f,f,T,T)=\\mathbb E[f(X(T))]\\) almost surely, so the estimator has zero variance; this is the classical zero-variance property of Doob\u2019s h-transform. Thus positivity of f is required to make the change of measure/propensities well-defined, and \\(\\hat U\\) defines both the change of measure (variance reduction) while the likelihood-ratio construction preserves unbiasedness.",
      "source_document": "papers/2512.06294v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For an ergodic stochastic reaction network modeled as a CTMC with generator A, how can you build a deep control-variate estimator of the steady-state mean U(f)=E_\u03c0[f(X)] from a single trajectory X(s) over a finite horizon [0,t]? Give the estimator\u2019s form, explain why the added \u201cshadow function\u201d term has zero contribution in the long-time limit, and state the condition under which the estimator becomes exact with zero variance for any finite t.",
      "answer": "A variance-reduced ergodic (steady-state) estimator can be constructed by augmenting the standard time average with a control variate built from an approximate solution \\(\\hat F\\) of the Poisson equation. The estimator is\n\\[\nE^{\\mathrm{CV}}_\\eta(f,t)=\\frac{1}{t}\\int_0^t [\\,f + A\\hat F_\\eta\\,](X(s))\\,ds,\n\\]\nwhere \\(\\hat F_\\eta\\) is represented by a Poisson-version of the spectral network (P-SDnet) and \\(A\\hat F_\\eta\\) is computed by applying the CTMC generator to \\(\\hat F_\\eta\\).\n\nIn the long-time limit, ergodicity gives \\(\\frac{1}{t}\\int_0^t f(X(s))ds\\to U(f)\\). At stationarity, the control-variate term averages to zero because \\(U(A\\hat F_\\eta)=\\langle A\\hat F_\\eta,\\pi\\rangle=\\langle \\hat F_\\eta, A^*\\pi\\rangle\\) and the stationary distribution satisfies the stationarity condition (equivalently \\(Q\\pi=0\\)), implying \\(U(A\\hat F_\\eta)=0\\). Thus \\(E^{\\mathrm{CV}}_\\eta(f,t)\\) is an exact estimator of \\(U(f)\\) in the large-time limit.\n\nIf \\(\\hat F_\\eta\\) is an exact solution of the Poisson equation, then the integrand \\(f + A\\hat F_\\eta\\) is constant and equals \\(U(f)\\), so \\(E^{\\mathrm{CV}}_\\eta(f,t)=U(f)\\) for any finite \\(t\\), yielding zero variance.",
      "source_document": "papers/2512.06294v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You want an interpretable neural surrogate for the expected output of an exponentially ergodic stochastic reaction network (CTMC) that generalizes across initial states x, times t, and multiple output functions f. Under the spectral-decomposition assumptions (distinct complex eigenvalues and f lying in the eigenfunction span), how can you construct the Spectral Decomposition-based network (SDnet) approximation \\(\\widehat U_\\eta(x,f,t)\\)? In your answer: (i) write the truncated spectral form used by SDnet and identify which quantities correspond to decay modes, eigenfunctions, and function coordinates; (ii) state which parts are treated as trainable parameters versus fixed/estimated quantities (including how the steady-state term \\(U(f)\\) is handled); and (iii) explain two consequences of this parameterization: why the model enforces physically consistent steady-state behavior as \\(t\\to\\infty\\), and how it scales/transfer-learns across many or new output functions f without retraining the whole network.",
      "answer": "(i) Using the Koopman/spectral expansion for expected outputs,\n\\[\nU(x,f,t)=U(f)+\\sum_{\\ell\\ge 1} e^{-\\sigma_\\ell t}\\,\\gamma_\\ell(f)\\,\\phi_\\ell(x),\n\\]\nSDnet truncates to the first r modes and parameterizes each complex factor explicitly. Writing \\(\\sigma_\\ell=a_\\ell+i b_\\ell\\) (decay mode), \\(\\phi_\\ell(x)=c_\\ell(x)+i d_\\ell(x)\\) (eigenfunction), and \\(\\gamma_\\ell(f)=g_\\ell(f)+i h_\\ell(f)\\) (function coordinate), SDnet uses\n\\[\n\\widehat U_\\eta(x,f,t)=\\widehat U(f)+\\sum_{\\ell=1}^r e^{-(\\widehat a_\\ell+i\\widehat b_\\ell)t}\\,(\\widehat g_\\ell(f)+i\\widehat h_\\ell(f))\\,(\\widehat c_\\ell(x)+i\\widehat d_\\ell(x)).\n\\]\n(ii) The steady-state mean term \\(\\widehat U(f)\\) is not learned from scratch; it is estimated separately (e.g., by a long-time ergodic average \\(E(f,t)=\\frac1t\\int_0^t f(X(s))ds\\) using the ergodic theorem so that \\(E(f)=U(f)\\)) and then kept fixed during training (or replaced by an exact analytic \\(U(f)\\) when available). The truncated-mode parameters \\((\\widehat a_\\ell,\\widehat b_\\ell)\\) and the per-function coordinates \\((\\widehat g_\\ell(f),\\widehat h_\\ell(f))\\) are trainable scalar variables. The eigenfunctions are represented by a feedforward neural network mapping the state to \\((\\widehat c_\\ell(x),\\widehat d_\\ell(x))_{\\ell=1}^r\\). (In the complex version, the output can be complex during training and one can penalize nonzero imaginary parts; at inference one can take the real part, while a matched/paired variant yields real outputs by design.)\n(iii) Steady-state consistency follows because all truncated terms are exponentially decaying in t, so\n\\(\\lim_{t\\to\\infty}\\widehat U_\\eta(x,f,t)=\\widehat U(f)\\),\nforcing the large-time limit to match the supplied steady-state mean rather than drifting arbitrarily as in a generic black-box network. Scalability/transfer across output functions follows because the decay modes and eigenfunctions are shared across all f, while each new function only requires learning its r complex coordinates \\(\\gamma_\\ell(f)\\) (i.e., 2r real scalars) without changing the eigenfunction network; once \\(\\{\\widehat\\sigma_\\ell,\\widehat\\phi_\\ell\\}\\) are learned, approximating U for a new f can be done by learning only these coordinates (or by regression against the learned eigenfunctions) rather than retraining the full model.",
      "source_document": "papers/2512.06294v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You have trained an SDnet-style model for a stochastic reaction network and it outputs (for each mode \\(\\ell\\)) estimated complex function coordinates \\(\\hat\\gamma_\\ell(f)=\\hat g_\\ell(f)+i\\hat h_\\ell(f)\\), decay modes \\(\\hat\\sigma_\\ell=\\hat a_\\ell+i\\hat b_\\ell\\), and eigenfunctions \\(\\hat\\phi_\\ell(x)=\\hat c_\\ell(x)+i\\hat d_\\ell(x)\\). Describe two concrete, simulation-based diagnostics to test whether these learned quantities behave like a valid Koopman/spectral decomposition: (i) a check based on reconstructing the output function \\(f(x)\\) from \\(\\hat\\gamma_\\ell(f)\\) and \\(\\hat\\phi_\\ell(x)\\); and (ii) a check based on the eigenfunction relation \\(K_t\\phi_\\ell(x)=\\mathbb E_x[\\phi_\\ell(X(t))]=e^{-\\sigma_\\ell t}\\phi_\\ell(x)\\). For (ii), write the real-valued identities (in terms of \\(\\hat a_\\ell,\\hat b_\\ell,\\hat c_\\ell,\\hat d_\\ell\\)) that you would compare against Monte Carlo estimates of \\(\\mathbb E_x[\\hat c_\\ell(X(t))]\\) and \\(\\mathbb E_x[\\hat d_\\ell(X(t))]\\).",
      "answer": "(i) **Function reconstruction check (coordinates/eigenfunctions):** Use the spectral expansion of the output function,\n\\[\n f(x)=\\gamma_0(f)+\\sum_{\\ell=1}^{\\infty}\\gamma_\\ell(f)\\,\\phi_\\ell(x),\n\\]\nand approximate it with SDnet\u2019s learned quantities by truncating and substituting \\(\\hat\\gamma_\\ell(f)\\) and \\(\\hat\\phi_\\ell(x)\\). The discrepancy between the true \\(f(x)\\) and the reconstructed right-hand side quantifies how well the learned coordinates and eigenfunctions satisfy the defining decomposition property.\n\n(ii) **Koopman eigenfunction check (decay modes/eigenfunctions):** For each mode \\(\\ell\\), the Koopman/eigenfunction identity is\n\\[\nK_t\\phi_\\ell(x)=\\mathbb E_x[\\phi_\\ell(X(t))]=e^{-\\sigma_\\ell t}\\phi_\\ell(x).\n\\]\nWriting \\(\\sigma_\\ell=a_\\ell+i b_\\ell\\) and \\(\\phi_\\ell=c_\\ell+i d_\\ell\\), this implies the following real-valued relations:\n\\[\n\\mathbb E_x[c_\\ell(X(t))]=e^{-a_\\ell t}\\big(c_\\ell(x)\\cos(b_\\ell t)+d_\\ell(x)\\sin(b_\\ell t)\\big),\n\\]\n\\[\n\\mathbb E_x[d_\\ell(X(t))]=e^{-a_\\ell t}\\big(d_\\ell(x)\\cos(b_\\ell t)-c_\\ell(x)\\sin(b_\\ell t)\\big).\n\\]\nIn practice, you estimate the left-hand sides with SSA/Monte Carlo for various \\((x,t)\\), compute the right-hand sides directly from SDnet using \\((\\hat a_\\ell,\\hat b_\\ell,\\hat c_\\ell,\\hat d_\\ell)\\), and compare them; the residuals measure how well the learned \\(\\hat\\sigma_\\ell\\) and \\(\\hat\\phi_\\ell\\) behave like true decay modes and eigenfunctions.",
      "source_document": "papers/2512.06294v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are estimating an expected output \\(\\mathbb E_x[f(X(T))]\\) of a stochastic reaction network using a deep control-variate built from a learned surrogate \\(\\hat U_\\eta\\) along the same SSA trajectories. \n\n(a) Write the variance of the generic control-variate estimator \\(f(X(T)) - \\rho Z_\\eta^{\\mathrm{CV}}(f,T,T)\\) as a function of \\(\\rho\\), and derive the variance-minimizing coefficient \\(\\rho^*\\).\n\n(b) Explain how \\(\\rho^*\\) can be estimated in practice and why this yields a \u201csafety\u201d guarantee: even if using \\(\\rho=1\\) would increase variance, you can still ensure the DeepCV estimator has variance no larger than the crude SSA estimator without extra simulation cost.",
      "answer": "(a) For the SSA-with-DeepCV estimator defined as\n\\[E^{\\mathrm{CV}}_\\eta(f,\\tilde f,T,T)= f(X(T)) - \\rho\\, Z^{\\mathrm{CV}}_\\eta(\\tilde f,T,T),\\]\nwith \\(Z^{\\mathrm{CV}}_\\eta\\) a (typically) zero-mean martingale control variate, the variance when \\(\\tilde f=f\\) is\n\\[\n\\mathrm{Var}_x\\big(E^{\\mathrm{CV}}_\\eta(f,f,T,T)\\big)=\\mathrm{Var}_x(f(X(T)))+\\rho^2\\,\\mathrm{Var}_x\\big(Z^{\\mathrm{CV}}_\\eta(f,T,T)\\big)-2\\rho\\,\\mathrm{Cov}_x\\big(f(X(T)),Z^{\\mathrm{CV}}_\\eta(f,T,T)\\big).\n\\]\nThe coefficient minimizing this quadratic in \\(\\rho\\) is\n\\[\n\\rho^* = \\frac{\\mathrm{Cov}_x\\big(f(X(T)),Z^{\\mathrm{CV}}_\\eta(f,T,T)\\big)}{\\mathrm{Var}_x\\big(Z^{\\mathrm{CV}}_\\eta(f,T,T)\\big)}.\n\\]\n\n(b) In practice, \\(\\rho^*\\) can be estimated by replacing the covariance and variance in the formula above by their empirical estimates computed from the sampled SSA trajectories (using the paired samples \\(f(X(T))\\) and \\(Z^{\\mathrm{CV}}_\\eta(f,T,T)\\) generated from those same trajectories).\n\nSafety guarantee: because SSA-with-DeepCV and crude SSA are computed from the same sampled trajectories, you can directly compare their sample variances. If the default choice \\(\\rho=1\\) happens to increase variance, you can revert to the SSA estimator at no extra simulation cost; moreover, estimating \\(\\rho^*\\) and using \\(\\rho\\neq 1\\) can be used to ensure the control-variate variance is reduced relative to crude SSA (asymptotically), thereby guaranteeing variance reduction even in unfavorable correlation scenarios.",
      "source_document": "papers/2512.06294v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building and validating a model that predicts per-residue energetic effects of *double* amino-acid insertions/deletions, how can you structure your dataset splits so that you separately test (i) generalization to completely unseen insertion-position pairs and (ii) generalization to completely unseen inserted amino-acid pairs, while still retaining a conventional held-out test set?",
      "answer": "Create two dedicated holdout test sets *before* doing the usual random train/validation/test split: one test set (TestPos) containing insertion-position pairs that do not appear anywhere in training/validation (to assess generalization to novel position combinations), and a second test set (TestAA) containing amino-acid pair combinations that do not appear in training/validation (to assess generalization to novel inserted residue identities). After removing these two sets from the full mutation dataset, split the remaining examples randomly into training, validation, and a standard held-out test set (TestRand), where TestRand is unseen during training but may include position-pair and amino-acid-pair combinations that were present in training/validation. Together, these three test sets probe generalization to unseen data, unseen position pairs, and unseen amino-acid pairs.",
      "source_document": "papers/2512.06496v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When a model predicts *per-residue* Rosetta energy terms for double amino-acid insertions/deletions using primarily sequence-local information (e.g., a convolutional/residual network operating over residues), which classes of Rosetta score terms should be expected to achieve the highest predictive correlations versus the lowest\u2014and what biological/computational reason explains this difference?",
      "answer": "Local interaction terms tend to be predicted best: energy components tied to mostly local residue environments (e.g., fa_atr, fa_sol, fa_intra_sol) show consistently high correlations across proteins. In contrast, terms reflecting more nonlocal or structurally complex effects (e.g., pro_close and the overall total score) have lower and more variable performance. The key reason is that long-range interactions/cooperativity are harder to infer accurately from individual-residue, primarily local features, whereas local solvation and near-neighbor interaction signals are more directly captured at residue resolution.",
      "source_document": "papers/2512.06496v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You have a per-residue deep learning model that predicts Rosetta energy terms for double amino-acid insertions/deletions from sequence. To interpret *where* along the protein the model struggles, you aggregate prediction error by insertion site (e.g., average RMSE over all mutants that place an insertion at each residue index). What qualitative pattern in error versus position would you expect with respect to (i) N/C termini, (ii) structured secondary-structure elements like central \u03b1-helices and \u03b2-sheets, and (iii) \u03b2-sheets specifically\u2014and what higher-level property does the document suggest these effects correlate with?",
      "answer": "When error is aggregated by insertion position, insertions at terminal regions (N- and C-termini) tend to have *lower* RMSE and are therefore more predictable. Insertions within structured regions (central \u03b1-helices and \u03b2-sheets) tend to have *higher* RMSE, reflecting greater sensitivity of structured cores to energetic disruption and making these positions harder to predict. \u03b2-sheet insertions are highlighted as particularly unpredictable across proteins (high error). The document further suggests that the impact of inserting into secondary structure correlates with the *size of the secondary-structure element* and with the *overall size of the protein*.",
      "source_document": "papers/2512.06496v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You train a per-residue model to predict Rosetta energy terms for many double InDel mutants and make a predicted-vs-true scatter plot for the per-residue **ref** term. The plot shows pronounced **vertical striping** (many points aligned at discrete x-values). What is the most likely explanation for this pattern, and what does it imply about the relationship between your ground-truth labels and the model\u2019s outputs?",
      "answer": "Vertical striping indicates that the ground-truth ref labels take on discretized (quantized) values, so many examples share the same true value, while the model\u2019s predictions remain continuous. The stripes therefore reflect label discretization rather than model collapse; despite the striping, the overall distribution can still show strong correlation and retained signal between predicted and true ref scores.",
      "source_document": "papers/2512.06496v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are training a per-residue deep model to predict Rosetta energy terms for many double InDel mutants. During label inspection you find (i) some Rosetta terms are nearly constant across all mutants of a given protein, and (ii) backbone/sidechain hydrogen-bond terms are extremely sparse (often exactly zero) at the per-residue level because the Rosetta run did not decompose backbone H-bond energies into pairwise contributions. Which specific score terms should you exclude from training/evaluation in these cases, and what is the methodological rationale for excluding each group?",
      "answer": "Exclude dslf_fa13 and yhh_planarity because they remain effectively constant across all mutations for each protein, so they do not meaningfully differentiate mutational effects; including them would provide negligible learning signal and could bias training toward irrelevant patterns. Also exclude the hydrogen-bond terms hbond_sr_bb, hbond_lr_bb, hbond_bb_sc, and hbond_sc because they are inherently pairwise/geometry-dependent interaction energies rather than properties localized to a single residue; when Rosetta is run without the decompose_bb_hb_into_pair_energies flag, these terms are frequently output as zeros for most mutations, yielding sparse signals that are poorly suited to a per-residue, position-independent framework and would likely add noise.",
      "source_document": "papers/2512.06496v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You\u2019re training a protein\u2013protein binding affinity regressor on a heterogeneous benchmark where affinities come from many different experimental studies, and you include a within-mini-batch pairwise ranking loss in addition to a pointwise regression loss. What batching strategy can you use to mitigate study-wise batch effects in the ranking comparisons, and what is the key idea behind why it helps?",
      "answer": "Construct each mini-batch using examples from a single study/source (grouped by PubMed ID), so that the ranking loss only compares predicted affinities among interactions measured within the same experimental study. This reduces confounding from between-study shifts (study-wise batch effects) because relative rankings are enforced only within a consistent measurement context.",
      "source_document": "papers/2512.06592v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are building a protein\u2013protein affinity predictor on a small set of multi-chain complexes (e.g., TCR\u2013pMHC) and want a train/test split that prevents closely related complexes from being separated across splits (to reduce sequence-similarity leakage). Describe a practical splitting procedure that (i) defines a distance between two complexes using Levenshtein distances between their component sequences, (ii) turns this into a graph, and (iii) uses connected components to form a \u201chard\u201d split with a target test-set fraction.",
      "answer": "One procedure is:\n1) Define a complex\u2013complex distance D(Ai,Aj) based on sequence similarity across chains: for each chain sequence SAi,k in complex Ai, compute its minimum Levenshtein distance to any chain sequence in Aj, then average these minima over chains in Ai (i.e., D(Ai,Aj) = (1/NAi) * sum_{k=1..NAi} min_\u2113 Levenshtein(SAi,k, SAj,\u2113)).\n2) Build an undirected graph G whose nodes are complexes; add an edge (Ai,Aj) whenever D(Ai,Aj) is below a chosen threshold \u03c4.\n3) Compute connected components of G and assign entire components (not individual complexes) to splits, so that no two complexes closer than \u03c4 can land in different splits.\n4) To hit a desired test fraction r, iteratively add remaining components to the test set V until |V| is about r|C| (using a cap such as |V|+|Ck| \u2264 1.2 r|C|); place the rest in the training set T.\nThis keeps clusters of similar complexes together and discourages near-duplicates from being split across train/test.",
      "source_document": "papers/2512.06592v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are training a structure-based protein\u2013protein binding affinity regressor that takes predicted complex structures as input, and you suspect that inaccurate structure prediction is the main reason affinity performance is poor. What controlled ablation can you run to test whether structure quality is actually the bottleneck, and what conclusion do you draw if the ablation does not improve affinity prediction?",
      "answer": "Run a matched retraining/evaluation where you keep the affinity model and training protocol the same but replace the predicted complex coordinates with experimentally determined (true) structures for the same complexes. If training and testing on true structures does not outperform training on predicted structures, then errors in the predicted structures are unlikely to be the primary performance bottleneck; instead the limitation is more likely in the learned structural representation/affinity head or the supervision/data regime.",
      "source_document": "papers/2512.06592v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You want to adapt an existing structure-based protein\u2013ligand affinity head to predict protein\u2013protein binding affinity from a complex. The original head was designed with an asymmetric \u201cprotein vs ligand\u201d view and uses some inter-chain and intra-chain pair representations but omits others. What change should you make to the pairwise representation inputs so the model respects the symmetry of protein\u2013protein interactions (and what specific categories of representations should be included)?",
      "answer": "Modify the affinity head inputs so it no longer treats one chain as a special \u201cligand\u201d: instead of using only the inter-chain protein\u2013ligand pairs plus intra-chain ligand\u2013ligand pairs, include all pair representations\u2014both inter-chain (between the two proteins) and intra-chain (within each protein chain). This makes the input symmetric with respect to swapping the two proteins.",
      "source_document": "papers/2512.06592v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You have two affinity predictors for protein\u2013protein complexes: (i) a structure-based model that produces a learned representation from its affinity head (e.g., an MLP embedding), and (ii) a sequence-based protein language model that outputs a final embedding for each complex. If you want a minimal, controlled way to test whether the structure representation provides complementary signal beyond the sequence model, what simple fusion model can you build from these two embeddings, and what trainable mapping do you use to produce the affinity prediction?",
      "answer": "A simple controlled fusion is to concatenate the structure-derived embedding (e.g., the MLP representation from the structure model\u2019s affinity module) with the sequence model\u2019s final embedding, then fit a linear projection (i.e., a single linear layer/regressor on the concatenated vector) to predict affinity from this combined representation.",
      "source_document": "papers/2512.06592v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In masking-based self-supervised pretraining on molecular graphs, how can you use an information-theoretic criterion to decide whether a proposed non-uniform masking distribution is worth the added complexity compared with uniform masking, and what conclusion follows if this criterion shows no improvement?",
      "answer": "Treat the pretraining signal as the masked label variable X and the downstream task label as Y, and use mutual information I(X;Y) as a model-agnostic measure of how informative the masking-induced pretraining signal is for the downstream property. A non-uniform masking distribution is only justified if it demonstrably increases I(X;Y) relative to uniform sampling; otherwise, the extra computational overhead is unlikely to translate into downstream gains, and effort is better spent on designing richer prediction targets (e.g., semantically meaningful motif-level labels) rather than more sophisticated masking distributions.",
      "source_document": "papers/2512.07064v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In masking-based self-supervised pretraining on molecular graphs, why can switching from a message-passing GNN encoder (e.g., GIN) to a graph Transformer encoder yield large downstream gains only for certain pretraining tasks? Explain what kind of prediction target is required to exploit a Transformer\u2019s global attention, and why motif-aware masking without a motif-level prediction target typically fails to realize the same benefit.",
      "answer": "Large gains from replacing a local message-passing encoder (GIN) with a graph Transformer (GraphGPS) depend on encoder\u2013target compatibility: the pretraining task must require modeling non-local, semantically rich structure. Atom-level reconstruction targets are fundamentally local\u2014nearby topology and atoms provide enough context\u2014so even if the input corruption is motif-aware, keeping atom-level supervision does not force the Transformer to use its global receptive field, leading to only marginal improvements. In contrast, motif-level prediction targets (chemically meaningful substructures) are semantically richer and non-local; they benefit from global attention and long-range dependency modeling, allowing expressive Transformer encoders to unlock substantial downstream performance gains that local MPNNs struggle to capitalize on due to their strong local inductive bias.",
      "source_document": "papers/2512.07064v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In masking-based self-supervised learning on molecular graphs, you want to compare atom-level versus motif-level prediction targets for a downstream *graph-level* binary property. How can you use a Jensen\u2013Shannon divergence analysis focused on low-frequency local labels to test whether rare substructures are truly more discriminative than common atoms, and what empirical pattern (including a control) would support the conclusion that rare motifs carry the key signal?",
      "answer": "Compute the Jensen\u2013Shannon divergence (JSD) between the conditional local-label distributions for the two graph classes, restricting attention to rare labels:\n- Define the rare-label subset as S_\\tau = {x \\in X : P(x) < \\tau}.\n- Estimate P(X\\mid Y=1, S_\\tau) and P(X\\mid Y=0, S_\\tau), then compute JSD(P(X\\mid Y=1, S_\\tau), P(X\\mid Y=0, S_\\tau)).\n\nA result consistent with \u201crare motifs are discriminative\u201d is that as \\tau decreases (i.e., you focus on increasingly rare labels), motif labels show substantially higher JSD and a sharp increase at low \\tau (e.g., \\tau \\le 0.1), while node-level targets (atom type, learned node codes) remain relatively flat. To rule out a vocabulary-size/cardinality confound, a shuffle-control that randomly permutes motif labels across molecules (keeping Y fixed) should cause motif MI/JSD to collapse back to the atom-level baseline, indicating the gain comes from semantic informativeness of motifs rather than simply having more classes.",
      "source_document": "papers/2512.07064v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You want to adapt a unidirectional state-space sequence model (like Mamba) to represent genomic DNA where meaningful dependencies can occur in both directions along the sequence. How can you compute a bidirectional representation in such a model, including (i) how the reverse-direction pass is implemented on the input sequence and (ii) how the forward and reverse outputs are fused, and what is the main motivation for the chosen fusion method?",
      "answer": "A bidirectional variant runs two state-space passes: one in the forward direction on X=[x1,\u2026,xT] and one in the reverse direction on the same sequence presented in reverse order (implemented by flipping the input along the sequence-length dimension, X\u2190 = flip(X, dim=1), feeding it to a reverse Mamba module, then flipping that output back to restore the original temporal order). The forward outputs y\u2192t and the restored reverse outputs y\u2190t are fused by element-wise addition: ht = y\u2192t + y\u2190t. Element-wise addition is used because it is simple and computationally efficient, retains bidirectional contextual information effectively, and introduces no additional parameters (minimal overhead compared with more complex fusions such as gating or concatenation).",
      "source_document": "papers/2512.07113v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a large-scale pretraining corpus from reference plant genomes for a DNA language model, what specific preprocessing and splitting steps can you use to (i) generate fixed-length training examples while providing implicit data augmentation, (ii) reduce noise from ambiguous bases, (iii) enforce strand-invariance, and (iv) avoid train\u2013test leakage so that generalization assessment remains valid across chromosomes/species?",
      "answer": "One workable pipeline is:\n\n(i) Cut each reference genome into fixed-length segments of 32,768 bp, using an overlap strategy for implicit augmentation: retain 64\u2013128 bp of overlap between neighboring segments, with the sliding step randomly sampled within that range.\n\n(ii) Normalize base symbols by replacing any non-standard bases (anything other than A/T/C/G/N) with N, and then filter out fragments with more than 2% N to form the initial dataset.\n\n(iii) Enforce strand-invariance via reverse-complement augmentation: randomly select 30% of the fragments and add their reverse complements to the dataset.\n\n(iv) Prevent leakage and keep validation meaningful by splitting by chromosomes (rather than random fragments), using ~5% as a held-out test set; this helps keep training/validation relatively independent in terms of species and chromosomes and improves the validity of generalization assessment.",
      "source_document": "papers/2512.07113v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are benchmarking a plant-genome foundation model on a suite of heterogeneous downstream tasks that includes (i) regression tasks (e.g., promoter/terminator strength) and (ii) multiple classification tasks drawn from different existing plant-genome benchmarks. What evaluation metrics should you use for the regression tasks and for each group of classification tasks, and what is the rationale for preferring Matthews correlation coefficient (MCC) over F1 for some of the classification problems?",
      "answer": "Use R\u00b2 (coefficient of determination) uniformly for regression tasks. For the classification tasks coming from the AgroNT Plants Genome Benchmark (polyadenylation, splice sites, lncRNA, chromatin accessibility, enhancer region), evaluate with AUC to match the benchmark\u2019s standard. For the classification tasks drawn from the PDLLMs benchmark (histone modification, core promoter, conservation, open chromatin), evaluate with MCC (instead of F1) because MCC provides a more comprehensive assessment of classification performance.",
      "source_document": "papers/2512.07113v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a sparse mixture-of-experts (SparseMoE) block used inside a long-context DNA language model, how can the router compute per-token expert assignments starting from a 3D hidden-state tensor, and why does a Top-k routing policy improve both computational efficiency and generalization? Describe the key tensor reshaping, the scoring/normalization step, and the intended effect of activating only a subset of experts per token.",
      "answer": "A common SparseMoE routing procedure starts from the hidden states H \u2208 R^{B\u00d7L\u00d7d} (batch size B, sequence length L, hidden dimension d) and first flattens them across tokens to a 2D matrix for routing: \u0124 = reshape(H) \u2208 R^{(B\u00b7L)\u00d7d}. The router then produces a distribution of expert scores for each token via a learned linear projection followed by softmax: R = softmax(\u0124 W_r) \u2208 R^{(B\u00b7L)\u00d7N}, where W_r \u2208 R^{d\u00d7N} are routing parameters and N is the number of experts. For each token, the router selects the experts with the Top-k scores and assigns corresponding weights, so only a small subset of experts is activated per token. This Top-k routing reduces computation because it performs conditional computation on a sparse set of experts rather than all N. It also improves generalization by encouraging specialization (each expert focuses on a subset of the input space), promoting diversity among experts, and reducing interference between unrelated inputs.",
      "source_document": "papers/2512.07113v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are building a unified benchmark to compare plant-genome language models, but some baseline models have not reported results on all tasks. What evaluation protocol can you use to make the comparison as fair as possible\u2014specifically, how do you handle (i) tasks where a baseline already reports results, (ii) tasks missing for a large Transformer baseline, and (iii) tasks missing for a lightweight Mamba baseline that has multiple tokenization variants?",
      "answer": "A fair protocol is to (i) reuse previously reported results for tasks where a baseline already provides them, rather than re-running them under potentially different settings; (ii) for tasks not covered by the large Transformer baseline (AgroNT), run a parameter\u2011efficient fine\u2011tuning procedure using IA3, following the established methodology for that model; and (iii) for tasks not reported by the lightweight Mamba baseline family (PDLLMs/PlantDNAMamba), fine\u2011tune the PlantDNAMamba series under full\u2011parameter fine\u2011tuning and then standardize on a single variant for all tasks\u2014specifically the 6\u2011mer tokenization variant\u2014because it had the highest average performance in the PDLLMs evaluations and the single\u2011base and BPE variants underperformed on some classification and regression tasks.",
      "source_document": "papers/2512.07113v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In E(3)-equivariant machine-learning coarse-grained force fields that build interaction neighborhoods purely from geometry (no explicit bond graph), what mapping-dependent condition can cause unphysical \u201cbond partner\u201d switching (bond permutations) during a simulation, and what is the mechanistic reason this happens?",
      "answer": "Bond permutations can occur when the coarse-grained mapping makes bonded and nonbonded interactions occur at overlapping length scales (e.g., in very low-resolution mappings like two-site hexane or C\u03b1-only polyalanine). Because these equivariant MLPs do not include explicit topological information, they cannot reliably distinguish bonded neighbors from nonbonded neighbors; if both lie at similar distances, the model treats nearby beads interchangeably and can swap which particles are effectively \u201cbonded,\u201d leading to unphysical bond partner switching and, in some cases, instability.",
      "source_document": "papers/2512.07692v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a stochastic multi-step biochemical process modeled as a linear cascade of n reversible transitions where each elementary step follows Arrhenius temperature dependence and the cascade is forward-biased at a reference temperature, what qualitative temperature\u2013rate behavior is predicted across (i) physiological temperatures near the reference point and (ii) extreme low and high temperatures? Explain the mechanistic reason for the regime changes and state how the effective activation energy behaves in each extreme regime.",
      "answer": "The model predicts an asymmetric triphasic temperature response for the overall rate (inverse mean first-passage time). Near the reference temperature (the physiologically relevant range), many forward transitions collectively dominate the mean first-passage time; by the law of large numbers the summed activation energies become approximately normally distributed, yielding a generic quadratic-exponential (curved) dependence of log-rate on inverse temperature. At sufficiently low temperatures, backward reactions become comparably fast so the process spends much of its time in forward\u2013backward cycles; below a critical temperature a single \u2018critical cycle\u2019 dominates the mean first-passage time and the overall rate reverts to Arrhenius-like behavior with a large positive effective activation energy (set by sums/differences of activation energies in that cycle). At sufficiently high temperatures, a similar breakdown occurs: above a second critical temperature one cycle again dominates the mean first-passage time and the rate becomes Arrhenius-like, but with a negative effective activation energy in this high-temperature regime.",
      "source_document": "papers/2512.08074v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "Many biochemical oscillators are built from transcription\u2013translation negative feedback loops whose elementary reaction rates speed up with temperature. Describe a mechanistic design principle by which such an oscillator can nevertheless maintain an approximately constant period across temperatures without fine-tuning Arrhenius parameters. In your answer, explain (i) what additional state structure the molecular components must have, (ii) how temperature shifts those states to buffer the effective feedback strength, and (iii) what kind of experimental perturbation would be expected to break temperature compensation compared to changing synthesis rates alone.",
      "answer": "A robust way to obtain temperature compensation is \u201cadaptive buffering\u201d via multistate clock components rather than relying on perfectly matched temperature sensitivities of individual reactions. (i) The oscillator\u2019s molecular species (e.g., mRNAs/proteins) must exist in multiple interconverting functional states, interpretable as distinct mRNA isoforms or post\u2011translationally modified protein forms. The interconversion reactions occur on timescales comparable to the temperature\u2011sensitive synthesis and degradation steps. (ii) As temperature changes, the occupancy balance among these states shifts; because the different states contribute differently to transcriptional repression (feedback), this redistribution adjusts the *effective* negative\u2011feedback strength so that it counteracts the direct temperature dependence of the underlying biochemical rates, stabilizing the period without parameter fine\u2011tuning. (iii) Because compensation relies on the modification/interconversion cycle, perturbing or disrupting that cycle should abolish temperature compensation, whereas manipulating synthesis rates alone should have much less effect on compensation (consistent with experimental tests in fly and mammalian systems).",
      "source_document": "papers/2512.08074v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a stochastic biochemical reaction network modeled as a continuous-time Markov process where each elementary transition rate follows Arrhenius temperature dependence, how can the overall rate to reach a target state show systematic curvature in an Arrhenius plot (log rate vs 1/T) rather than a straight line? In your answer, explain (i) how mean first-passage times connect the temperature dependence to graph-theoretic objects in the network, (ii) why the log-rate can be expressed as a series in inverse temperature whose coefficients are cumulants of activation-energy distributions, and (iii) under what large-network conditions this reduces to a quadratic dependence on 1/T (a \u201cquadratic exponential\u201d rate law) and what biological meaning is assigned to the reference temperature used in the expansion.",
      "answer": "For a stochastic network, the overall timescale to reach a target state is captured by the mean first-passage time (MFPT); the overall rate is r = 1/MFPT. Using a graph-theoretic representation of the MFPT, the temperature dependence of ln r(T) can be expanded around a reference inverse temperature \u03b2* = 1/(kB T*) as a Taylor series in \u0394\u03b2 = 1/(kB T) \u2212 1/(kB T*). The coefficients of this series are differences between cumulants of the distributions of total activation energies summed along spanning trees (ET) and spanning forests with two trees (EF) of the network:\n\nln r(T) = \u03a3_{n\u22651} [(-1)^n/n!]*(\u03ba_n(ET) \u2212 \u03ba_n(EF))*(\u0394\u03b2)^n + const.\n\nThus, deviations from a linear Arrhenius law arise because higher-order cumulants (n\u22652) generally do not vanish; these higher cumulants generate systematic curvature in the Arrhenius plot. In the large-network limit, if individual activation energies can be treated as independent random variables, then ET and EF are sums of many contributions and become approximately Gaussian (central limit theorem), so cumulants of order n\u22653 vanish. The result is a quadratic form in \u0394\u03b2:\n\nln r(T) = (\u27e8E\u27e9F \u2212 \u27e8E\u27e9T)\u0394\u03b2 + (\u03c3_T^2 \u2212 \u03c3_F^2)\u0394\u03b2^2/2 + const,\n\ni.e., a quadratic dependence of ln r on inverse temperature (equivalently, a quadratic-exponential rate law). This approximation requires a sufficiently complex network with a macroscopic number of spanning trees and forests, and it assumes a reference temperature T* at which Arrhenius activation energies and prefactors can be treated as statistically independent; biologically, T* is interpreted as the temperature to which the organism has evolutionarily adapted (often requiring a bias in activation energies toward the target state so tree/forest energy statistics remain distinct).",
      "source_document": "papers/2512.08074v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "Enzyme-catalyzed processes often show an optimum temperature and declining rates at high (and sometimes low) temperatures rather than a single Arrhenius line. Using a transition-state (Eyring) perspective with a temperature-dependent *active enzyme fraction* P(En), explain mechanistically how (i) the Johnson\u2013Lewin model generates a high-temperature downturn, (ii) the Sharpe\u2013Schoolfield extension can generate both cold and hot inactivation in one framework, and (iii) what additional thermodynamic ingredient the Ratkowsky\u2013Ross approach introduces to control the temperature dependence of enzyme denaturation.",
      "answer": "In modified Eyring/transition-state descriptions, the rate is written as an Eyring factor multiplied by an active-enzyme fraction, r(T)=P(En)\u00b7(kB/h)\u00b7exp(\u2212\u0394G\u2021/RT), so non-Arrhenius behavior comes from how P(En) varies with temperature.\n\n(i) In the Johnson\u2013Lewin model, the downturn at high temperature (\u201chot inactivation\u201d) is attributed to reversible high-temperature denaturation of a master enzyme: a transition between active and denatured states with a temperature-dependent free-energy change \u0394G makes P(En) decrease at high T, yielding a double-exponential form that behaves like exp(\u2212\u0394G\u2021/RT) at low T but crosses over to exp(\u2212(\u0394G\u2021\u2212\u0394G)/RT) at high T.\n\n(ii) The Sharpe\u2013DeMichele/Sharpe\u2013Schoolfield framework extends this by allowing one active and two inactive enzyme states\u2014one favored at low temperature and one favored at high temperature\u2014so the same model can produce three-phase scaling: cold inactivation at low T, an approximately Arrhenius-like \u2018normal physiological\u2019 middle range, and hot inactivation at high T.\n\n(iii) The Ratkowsky\u2013Ross approach further models the thermodynamics of denaturation by explicitly incorporating the heat capacity of protein unfolding (\u0394Cp) as a key factor shaping large positive changes in the free energy of denaturation, thereby influencing both high- and low-temperature inactivation behavior.",
      "source_document": "papers/2512.08074v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a minimal relaxation-oscillator model of the early embryonic cell cycle with Cyclin B synthesis, Cdk1-dependent Cyclin degradation, and a bistable Cdk1 activation switch, each kinetic rate can be given Arrhenius temperature dependence. Explain mechanistically how *unequal activation energies* for Cyclin synthesis versus degradation can (i) produce a curved (non-Arrhenius) period\u2013temperature relationship within the physiological range and (ii) impose high- and low-temperature \u201cthermal limits\u201d where oscillations disappear. Frame your explanation in terms of how temperature shifts the phase-plane nullclines and what dynamical transition occurs at extreme temperatures.",
      "answer": "Assigning Arrhenius scaling to Cyclin synthesis (ks) and degradation (kd) means their ratio ks/kd changes with temperature if they have different activation energies. This temperature-dependent change in ks/kd shifts the Cyclin nullcline in the Cyclin\u2013Cdk1 phase plane relative to the S-shaped Cdk1 activation nullcline. Within the viable (physiological) range, these shifts alter where the trajectory spends time on the slow Cyclin-accumulation and Cyclin-degradation branches, so different segments of the cycle scale differently with temperature (e.g., synthesis being more temperature-sensitive than degradation), yielding a nonlinear/curved period\u2013temperature relation rather than a single Arrhenius line.\n\nAt sufficiently high temperature, if ks increases more steeply with temperature than kd, the Cyclin nullcline moves upward enough that the nullclines no longer intersect on the middle branch of the Cdk1 nullcline: the unstable steady state needed for the limit cycle disappears and the oscillation collapses into a high-Cdk1 fixed point (persistent M-phase arrest). At sufficiently low temperature, the opposite shift traps the system in a low-Cdk1, interphase-like fixed point. These temperature-driven nullcline shifts therefore generate both non-Arrhenius period scaling and thermal boundaries where the limit cycle is lost via bifurcation to fixed points.",
      "source_document": "papers/2512.08074v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In subcellular spatial proteomics, you often want an uncertainty estimate for an entire protein abundance profile across all fractions (not just pointwise at each fraction). How can you construct a *simultaneous* (across fractions) cluster-conditional prediction band for a protein group using a model that assigns proteins probabilistically to groups, and how do you set the per-fraction error rate so the whole-profile band attains a target overall coverage (e.g., 95%)?",
      "answer": "Construct the band in two stages. First, for each group k, draw (or otherwise select) a set of protein curves deemed to belong to k using the fitted mixture-model membership information (a multinomial membership draw based on the fitted responsibilities), yielding curves {y*_{ik\u00b7}}. Second, for each fraction j, estimate the marginal distribution of abundance in that group nonparametrically using a kernel density estimator f\u0302_{kj}(\u00b7) (with an RBF kernel and bandwidth h_{kj} chosen by a normal-reference rule). Let F\u0302_{kj}(\u00b7) be the corresponding estimated CDF. The pointwise (per-fraction) prediction interval bounds are the quantiles C^{lower}_{kj}=F\u0302^{-1}_{kj}(\u03b1*/2) and C^{upper}_{kj}=F\u0302^{-1}_{kj}(1\u2212\u03b1*/2). To make the band simultaneous across all d fractions with overall error rate \u03b1 (overall coverage 1\u2212\u03b1), set the per-fraction error rate using a Bonferroni correction, \u03b1* = \u03b1/d, so that the probability a new protein\u2019s *entire* signature is contained within its group\u2019s band is at least 1\u2212\u03b1.",
      "source_document": "papers/2512.08087v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When fitting a functional mixture model for subcellular spatial proteomics where each protein has an abundance profile across fractions, you want to tune the kernel bandwidth that controls how strongly the estimated cluster mean profiles are smoothed across fractions. Describe a computationally feasible cross-validation strategy that holds out data at the level of entire fractions (rather than individual protein\u2013fraction observations): what is held out, what is refit, how are \u201ctarget\u201d mean values at the held-out fraction constructed using posterior membership probabilities, how is the held-out mean predicted from the refit model, and what loss is minimized to choose the bandwidth?",
      "answer": "Use leave-one-fraction-out cross-validation (LOFO-CV). For each fraction j, treat all measurements at that fraction as held-out data Y^(j), and fit the mixture model on the remaining fractions Y^(\u2212j) (so the model is fit only d times rather than n\u00d7d times as in leave-one-out over protein\u2013fraction observations).\n\nFrom the model trained on Y^(\u2212j), compute posterior membership probabilities (responsibilities) \\(\\hat\\gamma^{(\u2212j)}_{ik}\\) for each protein i and cluster k, and use them as a proxy for the true membership probabilities when evaluating the held-out fraction. Construct a \u201ctarget\u201d (pseudo-truth) cluster mean at the held-out fraction as the responsibility-weighted average of the held-out measurements:\n\\(\\tilde\\mu_{kj} = \\sum_{i=1}^n \\hat\\gamma^{(\u2212j)}_{ik} y_{ij}\\).\nAlso compute a corresponding noise level at the held-out fraction:\n\\(\\tilde\\sigma^2_{kj}=\\sum_{i=1}^n \\hat\\gamma^{(\u2212j)}_{ik}(y_{ij}-\\tilde\\mu_{kj})^2\\), with an effective cluster size \\(n_k=\\sum_i \\hat\\gamma^{(\u2212j)}_{ik}\\).\n\nPredict the held-out mean using the refit model\u2019s estimated means at the other fractions and kernel smoothing across fractions:\n\\(\\hat\\mu^{(\u2212j)}_{kj} = \\frac{\\sum_{j'\\neq j} K_h(j',j)\\hat\\mu^{(\u2212j)}_{kj'}}{\\sum_{j'\\neq j} K_h(j',j)}\\).\n\nMeasure prediction error for each (k,j) by the squared deviation between \\(\\tilde\\mu_{kj}\\) and \\(\\hat\\mu^{(\u2212j)}_{kj}\\), normalized by the estimated variance scaled by cluster size (\\(\\tilde\\sigma^2_{kj}/n_k\\)), and average over all fractions and clusters using the fitted cluster weights \\(\\hat\\pi^{(\u2212j)}_k\\):\n\\(\\mathrm{CV}(h)=\\frac{1}{d}\\sum_{j=1}^d\\sum_{k=1}^K \\hat\\pi^{(\u2212j)}_k \\frac{(\\tilde\\mu_{kj}-\\hat\\mu^{(\u2212j)}_{kj})^2}{\\tilde\\sigma^2_{kj}/n_k}\\).\nChoose the bandwidth h by grid search to minimize \\(\\mathrm{CV}(h)\\).",
      "source_document": "papers/2512.08087v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are using a semi-supervised mixture model for subcellular spatial proteomics that can optionally discover previously unannotated protein groups. Propose a principled evaluation protocol (on a dataset where all groups are actually known) to quantify the model\u2019s ability to discover entirely unseen groups. Your protocol should specify (i) how to create \u201cunknown\u201d groups via label masking, (ii) how to choose the number of new groups to allow, (iii) how to align discovered clusters with the withheld ground-truth groups despite arbitrary cluster labels, and (iv) the final accuracy metric computed from this alignment.",
      "answer": "Create \u201cunknown\u201d groups by selecting two known protein groups and masking all labels for proteins in those groups, then fit the model using labels only from the remaining known groups. Choose how many new groups to allow by selecting the model\u2019s novelty parameter/number of additional clusters (K0) via AIC. After fitting, cross-tabulate predicted cluster labels for the masked proteins against their withheld true group labels; because cluster labels are arbitrary, apply the Hungarian algorithm (maximum-weight bipartite matching) to match the discovered clusters to the withheld true groups. Finally, compute the proportion of proteins in the withheld groups whose memberships are correctly assigned after this optimal matching (i.e., correct assignments divided by the total number of proteins in the withheld groups).",
      "source_document": "papers/2512.08087v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In mixture-model\u2013based analysis of subcellular spatial proteomics profiles (each protein measured across d fractions), you need to choose a within-cluster noise model (covariance across fractions). Explain the rationale for modeling each cluster\u2019s covariance as diagonal but heteroskedastic across fractions (i.e., independent fractions with cluster- and fraction-specific variances \u03c3^2_{kj}), rather than (i) a full d\u00d7d covariance per cluster or (ii) a homoskedastic variance shared across fractions. In your answer, quantify how the number of variance/covariance parameters scales under these alternatives and state why the diagonal-heteroskedastic choice is statistically/computationally advantageous for low signal-to-noise datasets.",
      "answer": "Model each cluster\u2019s covariance as diagonal with fraction-specific variances \u03c3^2_{kj} (heteroskedastic and independent across fractions) because it captures the empirically heterogeneous noise level by fraction while keeping the parameter count manageable. A full covariance matrix \u03a3_k would require up to about K\u00b7d^2/2 covariance parameters overall (per-cluster d\u00d7d covariance), which is often infeasible with limited data. A homoskedastic model imposing \u03c3^2_{k1}=\u2026=\u03c3^2_{kd} is described as inaccurate and tends to work only when the signal-to-noise ratio is strong. The diagonal-heteroskedastic model instead uses only K\u00b7d variance parameters (one per cluster per fraction), providing a more statistically efficient fit in low S/N settings while avoiding the over-parameterization of full covariance estimation; remaining across-fraction structure is intended to be captured by the smooth mean trajectories rather than by correlated noise.",
      "source_document": "papers/2512.08087v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a semi-supervised Gaussian-mixture model for subcellular spatial proteomics (proteins measured across d fractions), you have posterior membership probabilities for a new protein, \\(\\hat\\gamma_k(y_{new})=P(z_{new}=k\\mid y_{new})\\). Describe two principled ways to turn these probabilities into a single cluster label for that protein (a \u201chard\u201d vs \u201csoft\u201d assignment). What is a key drawback of the hard assignment in protein-signature space, and why might you prefer the soft assignment in this application?",
      "answer": "Two ways are: (1) a hard assignment \\(\\hat k_{hard}(y_{new})=\\arg\\max_k \\hat\\gamma_k(y_{new})\\); and (2) a soft assignment obtained by a single multinomial draw \\(\\hat k_{soft}(y_{new})\\sim \\mathrm{Multinom}(n=1,\\{\\hat\\gamma_k(y_{new})\\}_k)\\). The hard argmax rule has the drawback that it \u201cunnaturally partitions\u201d the d-dimensional protein-signature space into crisp regions even when probabilities are close, discarding uncertainty. The soft multinomial assignment preserves and propagates uncertainty encoded in \\(\\hat\\gamma\\) and is therefore preferred here for labeling new proteins from the fitted FSPmix model.",
      "source_document": "papers/2512.08087v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When annotating structural variants called from long-read sequencing, breakpoint/sequence differences can cause the same underlying allele to be represented by slightly different SV calls across individuals. What strategy can be used to compute reliable population (and ancestry-stratified) allele frequencies for a query SV set in spite of this variability, and what quality-control test can be applied to flag suspicious genotype distributions?",
      "answer": "A practical strategy is to first merge/match each query SV against a pre-merged population SV callset using an SV comparison/merging tool with user-tunable matching criteria to accommodate long-read variability\u2014specifically thresholds for sequence similarity, size similarity tolerance, and reference-distance tolerance. After the query SV is assigned to a merged population group, allele frequencies are computed from genotype counts extracted from that merged population VCF, reporting both overall frequency and ancestry-specific frequencies across the five 1KGP superpopulations (African, American, East Asian, European, South Asian). As QC, variants can be screened with Hardy\u2013Weinberg equilibrium testing using chi-square statistics on observed heterozygote and homozygote counts to flag unexpected genotype distributions.",
      "source_document": "papers/2512.08175v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a pipeline that infers directed gene triplets (X, Y, Z) from spatially projected expression fields, what concrete statistical criteria can be used to decide whether the triplet represents a mediated chain (X\u2192Y\u2192Z) versus a convergent-input motif (X\u2192Z and Y\u2192Z), when pseudotime is a potential confounder? Describe both the generalized additive model (GAM) tests and the conditional \u03a6-mixing checks, including the decision thresholds.",
      "answer": "Use pseudotime-adjusted GAMs plus conditional \u03a6-mixing to classify each consensus triplet (X, Y, Z).\n\nGAM-based conditional-effect test (controlling for pseudotime \u03c4): fit\n\u2022 Model1: Z ~ s(\u03c4) + s(X)\n\u2022 Model2: Z ~ s(\u03c4) + s(Y)\n\u2022 Model3: Z ~ s(\u03c4) + s(X) + s(Y)\n\nCall the structure mediated if adding Y (the putative mediator) in Model3 substantially attenuates the partial effect of X on Z (p_X > 0.05 or \u0394AIC_X > 0), while Y contributes significantly (p_Y < 0.05). Call it convergent if both s(X) and s(Y) remain independently significant in Model3 and dropping either term markedly worsens fit (\u0394AIC > 2 or \u0394GCV > 0.01). Repeat the GAM fitting with block-bootstrap replicates that preserve pseudotime ordering to obtain bootstrap support probabilities for mediation (p_med) and convergence (p_con).\n\nConditional \u03a6-mixing check: recompute directed dependence after conditioning on the other upstream gene, e.g. \u03a6(Z|X;Y) and \u03a6(Z|Y;X). Label as mediated if |\u03a6(Z|X;Y)| < 0.5\u00b7|\u03a6(Z|X)| and p_med > 0.6. Label as convergent if both conditional coefficients remain large relative to their unconditioned counterparts (|\u03a6(Z|X;Y)| and |\u03a6(Z|Y;X)| > 0.5 times the respective unconditioned magnitudes, i.e., they do not collapse when conditioning) and p_con > 0.6; otherwise mark ambiguous.",
      "source_document": "papers/2512.08202v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a \u03c6-mixing\u2013based gene regulatory inference pipeline where gene expression is first discretized, how can you assign a promotive vs inhibitory *sign* to an inferred directed dependency X\u2192Y using only the discrete conditional distributions (not just correlation)? Describe (i) the total-variation criterion used to pick the conditioning state of X that is most informative, (ii) how conditional vs global means of Y are compared to label promote/inhibit/neutral, and (iii) how this sign is combined with the \u03c6-mixing magnitude to form a signed edge weight.",
      "answer": "After discretizing X and Y into bins, compute for each bin f of X the total-variation deviation between the conditional and marginal distributions of Y:\n\nTV(Y|X=f) = (1/2) * \u03a3_e |P(Y=e|X=f) \u2212 P(Y=e)|.\n\nSelect the \u201cmost perturbed\u201d conditioning bin f* = argmax_f TV(Y|X=f); this is the state of X that induces the largest distributional shift in Y and is therefore used to determine polarity. Let \u03bc(Y|X=f*) be the conditional mean of Y under this maximally perturbed bin and \u03bc(Y) the global mean. Label the direction X\u2192Y as:\n- promotive if \u03bc(Y|X=f*) > \u03bc(Y)\n- inhibitory if \u03bc(Y|X=f*) < \u03bc(Y)\n- neutral otherwise.\n\nConvert this label to a sign s_{X\u2192Y} \u2208 {+1, \u22121, 0} and form the signed coefficient as \u03c6_signed(Y|X) = s_{X\u2192Y} \u00b7 \u03c6(Y|X), combining the asymmetric \u03c6-mixing magnitude with the inferred promotive/inhibitory polarity.",
      "source_document": "papers/2512.08202v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You infer directed 3-gene regulatory triplets from spatially projected expression fields and want to test whether the resulting \u201cspatial flow\u201d visual patterns are biologically meaningful rather than artifacts of generic tissue geometry. How can you set up a real\u2013null benchmarking procedure based on a path drawing rate (PDR) to validate spatial coherence? In your answer, specify (i) how the null triplets are sampled so they are comparable to the real set, (ii) what constitutes a triplet being \u201csuccessfully drawn,\u201d (iii) how PDR is defined for real vs null triplets, and (iv) what statistical test is used to compare real and null across patches and what effect size is reported.",
      "answer": "A real\u2013null benchmark can be built by comparing how often inferred triplets yield drawable, spatially coherent paths versus randomly assembled triplets under the same spatial/expression landscape:\n\n(i) Null sampling (matched control): On each eligible patch, first define a gene pool from the patch-restricted mapped single-cell expression matrix by keeping genes with the required prefix, nonzero expression in at least a minimal fraction of mapped cells, and non-negligible variance (exclude absent/constant genes). Collect the \u201creal\u201d set as the consensus triplets that have non-ambiguous GAM support (mediation or convergence). Then sample, without replacement, the same number of null triplets by drawing three distinct genes uniformly from the gene pool, explicitly excluding any triplet already in the real set. This matches cardinality and marginal expression characteristics (same mapped cells, same gene pool, identical preprocessing) while removing \u03a6-mixing/GAM structure.\n\n(ii) Successful drawing criterion: For every real and null triplet, generate spatial bundles/streamlines using identical kNN graphs and identical KDE/visualization parameters. Count a triplet as successfully drawn (per mode: mediated or converged) if the bundle overlay produces a non-empty trajectory that can be rendered without geometric degeneracy and exceeds a minimal admissible length (empirically > 20 \u00b5m) with sufficient transition continuity along the kNN graph.\n\n(iii) PDR definition: For each patch and mode m \u2208 {med, conv}, define\nPDR(m)_real = N(m)_{real,drawn} / N(m)_{real,total} and\nPDR(m)_null = N(m)_{null,drawn} / N(m)_{null,total}.\n\n(iv) Statistical comparison: Compare PDR_real vs PDR_null using patch-level 2\u00d72 contingency tables and a one-sided Fisher\u2019s exact test; report odds ratios (with exact p-values) as the effect size for enrichment of drawable/coherent paths in real triplets over null triplets.",
      "source_document": "papers/2512.08202v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When inferring a spatially resolved directed GRN using an expensive asymmetric dependency measure (e.g., \u03c6-mixing) on spatially diffused gene-expression fields, how can you design a neighborhood-based *pairwise pre-screening* step to reduce the combinatorial number of candidate gene pairs while still capturing both activating and repressive relationships? Describe the two complementary screening modules, the statistics/criteria each uses (including the role of kNN neighborhoods and conditional means), and how their outputs are combined before running the full directed dependency computation.",
      "answer": "A practical pre-screen is to work on each tissue patch\u2019s mapped, spatially diffused gene fields and build, for every \u201canchor\u201d gene A, a small candidate neighbor set using two complementary modules:\n\n1) **Promotive (positively co-varying) neighbor detection**: compute *sparse local correlations* between A and candidate genes across **k-nearest-neighbor (kNN) neighborhoods** on the tissue graph, and keep the top correlated neighbors. This targets pairs that co-vary in the local spatial context.\n\n2) **Repressive (inhibitory) neighbor detection**: test for candidates B such that **expression of A reduces B** in spatial locations where **B is normally expressed**, using (i) **negative correlation** and (ii) a **decreased conditional mean of B when A is \u201con\u201d** (i.e., comparing B\u2019s mean under the condition A is present/high versus its typical level).\n\nThe two neighbor lists are then **merged** into a combined set of promotive and repressive candidate directed pairs (A,B), retaining their correlation- and suppression-based scores. This combined list is used as a biologically informed candidate set entering the subsequent \u03c6-mixing computation, with the expectation that promotive pairs should yield positive signed dependencies and repressive pairs negative signed dependencies, while discarding pairs with little evidence of local spatial interaction.",
      "source_document": "papers/2512.08202v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You have computed a signed, asymmetric dependency matrix between discretized spatial gene fields, where each directed edge weight is a signed \u03c6-mixing coefficient (positive = promotive, negative = inhibitory, zero = neutral). Given an unordered 3-gene set {A,B,C}, how can you algorithmically choose a *canonical* directed triplet X\u2192Y\u2192Z that (i) is maximally supported by the pairwise signed dependencies and (ii) enforces a coherent regulatory \u201cmode\u201d (promotive\u2013promotive or inhibitory\u2013inhibitory) across the two-step path? Specify the permutation search, the path-strength/consistency scoring rule, and what quantities are recorded for the selected triplet.",
      "answer": "Treat the signed \u03c6-mixing matrix \u03a6_signed as a weighted directed graph and evaluate all 6 orderings (E,F,G) \u2208 Perm(A,B,C) as candidate chains E\u2192F\u2192G. For each ordering, compute the two edge signs s_{F|E}=sign(\u03c6_signed(F|E)) and s_{G|F}=sign(\u03c6_signed(G|F)). Define a chain-strength term as the bottleneck magnitude along the path,\n\nS(E,F,G) = min(|\u03c6_signed(F|E)|, |\u03c6_signed(G|F)|),\n\nand a sign-coherence indicator that requires the two consecutive edges to have the same polarity,\n\nI(E,F,G) = 1{ s_{F|E} = s_{G|F} }.\n\nScore the ordering by multiplying these terms:\n\nF_score(E,F,G) = S(E,F,G) \u00b7 I(E,F,G).\n\nSelect the canonical triplet ordering (X,Y,Z) = argmax_{(E,F,G)} F_score(E,F,G). The triplet\u2019s overall polarity (promotive or inhibitory) is inherited from the common edge sign (since s_{Y|X}=s_{Z|Y} when I=1). Record the selected ordering along with the two signed edge coefficients \u03c6_signed(Y|X) and \u03c6_signed(Z|Y) (and the maximizing score); triplets whose best score falls below a predefined path-score threshold are discarded.",
      "source_document": "papers/2512.08202v1.pdf",
      "mode": "textual",
      "content_refs": [
        "Lines 557-609"
      ]
    },
    {
      "question": "In an end-to-end neoantigen vaccine design workflow that starts from tumor DNA/RNA and matched normal DNA sequencing, what is a practical two-stage immunogenomics review strategy for selecting final neoantigen candidates, and what specific checks/criteria are applied in each stage (including the role of an IGV-based manual review)?",
      "answer": "A practical strategy is a two-stage review combining computational prioritization with manual validation. Stage 1 is an initial case/QC and prioritization pass: reviewers evaluate case characteristics and any cancer driver variants, perform sequencing QC (e.g., read depth, duplication, sample relatedness, contamination, and RNA alignment characteristics such as transcript end bias/strandedness), and compare clinical vs in-silico HLA calls between tumor and normal to detect mix-ups. Candidates are then loaded into an interactive prioritization tool to score/filter against predefined criteria including DNA variant allele frequency, transcript support, binding affinity/presentation predictions, agretopicity, and RNA expression; driver-gene candidates may have slightly relaxed thresholds, and discordant binding algorithms or complex variants can be flagged for extra scrutiny. Stage 2 is a manual IGV-based genomics review of shortlisted candidates: each candidate is inspected to assess somatic variant quality, check for proximal variants that could change the predicted peptide sequence, verify RNA expression of the variant allele, and evaluate transcript isoform structure/expression, culminating in a finalized review report and an order-form-ready set of candidates.",
      "source_document": "papers/2512.08226v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a tumor/normal sequencing\u2013based neoantigen pipeline, suppose the tumor sample has low purity and you are worried about missing true somatic variants. What specific variant-calling sensitivity parameters can you relax (name them), and what is the key trade-off/risk introduced by doing so?",
      "answer": "To increase sensitivity in low-tumor-purity cases, the pipeline allows reducing the YAML thresholds for somatic calling\u2014specifically the parameters `varscan_min_var_freq`, `fp_min_var_freq`, and `filter_somatic_llr_threshold`. Lowering these cutoffs can recover more low-VAF variants, but it increases the risk of including false-positive somatic calls, so it should be done cautiously.",
      "source_document": "papers/2512.08226v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When prioritizing predicted neoantigens for a personalized cancer vaccine, why is it important to screen whether a mutant peptide sequence closely matches any wild-type peptide sequence elsewhere in the human proteome, and how is this kind of \u201creference match\u201d information used during candidate selection?",
      "answer": "A mutant peptide can inadvertently be identical or highly similar to a wild-type peptide from a different human protein; if so, the immune system is more likely to be tolerant to that sequence, reducing the chance the candidate will elicit a tumor-specific response and raising safety/target-specificity concerns. The workflow addresses this by performing a reference-match analysis as part of pVACseq and visualizing it in pVACview; candidates with significant matches to wild-type human protein sequences are flagged during review and are less likely to be selected/accepted for the final vaccine set.",
      "source_document": "papers/2512.08226v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In neoantigen prediction from tumor/normal sequencing, why can a nearby (proximal) germline variant invalidate the peptide sequence predicted from a somatic mutation, and what concrete pipeline steps/inputs are used to correctly generate the mutant peptide in that situation?",
      "answer": "A proximal germline missense variant that is in-phase with the somatic variant can change the amino-acid context of the translated protein, so predicting peptides from the somatic change alone can yield the wrong mutant peptide sequence. To handle this, the workflow calls germline variants from the matched-normal DNA, phases variants to identify in-phase proximal germline/somatic combinations (producing a phased VCF), and then supplies both the phased VCF and the somatic-calling VCF to the neoantigen predictor (pVACseq within pVACtools) so peptide generation incorporates the correct haplotype sequence.",
      "source_document": "papers/2512.08226v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are running a neoantigen vaccine bioinformatics pipeline across multiple patients in a clinical trial, but you also want to keep improving the pipeline over time. What concrete version-control and regression-testing strategy can you use to (i) keep results consistent within an ongoing trial while (ii) validating that pipeline updates behave as expected? Name the specific mechanisms/tools used to \u201cpin\u201d versions and the tool used to compare results between pipeline versions.",
      "answer": "Use strict versioning of both the workflow code and the containerized toolchain, then regression-test updates before adopting them. Concretely: manage the pipeline repositories with Git and run analyses from a checked-out, tagged release to pin the exact workflow/scripts used; use Docker containers so underlying tools are fixed via versioned image tags; before releasing an updated pipeline, re-run it on multiple datasets (including a well-characterized tumor/normal dataset such as HCC1395 plus real patient data) and compare the new outputs against the prior pipeline version using pVACcompare (available in pvactools v6.0+). This allows keeping a stable pinned version for an ongoing trial while evaluating and deploying newer versions for future trials/experiments once comparisons are acceptable.",
      "source_document": "papers/2512.08226v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an RNA duplex unwinding simulation where you use a learned model to predict an entropy-like order parameter from coarse-grained coordinates, which classes of RNA interactions can be disentangled into separate entropy contributions, and what thermodynamic ranking of their importance emerges for dsRNA conformational reorganization during DDX3X-mediated unwinding (ATP-independent)?",
      "answer": "The entropy contributions can be decomposed into three interaction classes encoded as separate \u201centropy fingerprints\u201d: (i) hydrogen-bonding (base pairing), (ii) base-stacking, and (iii) sugar\u2013phosphate/backbone electrostatic interactions. The resulting thermodynamic hierarchy for driving the entropy landscape (and thus conformational reorganization during unwinding) is: hydrogen bonding (primary contribution) > base stacking (secondary) > backbone electrostatics (tertiary), with cooperative hydrogen-bonding and base-stacking effects dominating overall.",
      "source_document": "papers/2512.08367v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In coarse-grained Langevin simulations of an RNA helicase that can unwind dsRNA without ATP, what analysis would let you conclude that the *onset* of unwinding is a stochastic (Poisson-like) event, and how would you identify the main kinetic bottleneck intermediate from the observed state-to-state transitions?",
      "answer": "Treat dsRNA unwinding as transitions among discrete end states (stable bound SB, partially unwound PU, fully unwound FU, and partially re-closed PR) and measure, over many independent replicas, (i) the distribution of the pre-unwinding duration (time from simulation start to the first conformational transition/unwinding onset) and (ii) the probabilities of each stepwise transition among states. A Poisson-like pre-unwinding-duration distribution supports that initiation is stochastic. The kinetic bottleneck is the intermediate whose onward transition is least probable; here PU is the bottleneck because the PU\u2192FU transition has the lowest probability among the step transitions, making completion of unwinding rare and leaving PU as the most populated end state.",
      "source_document": "papers/2512.08367v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are building a deep-learning surrogate that predicts a coordination-resolved entropy (CRE) order parameter from coarse-grained dsRNA\u2013helicase trajectories, and you want the model to (i) generalize across the SB/PU/PR/FU conformational states and (ii) decompose the predicted entropy into contributions from hydrogen bonding, base stacking, and backbone electrostatics. What enhanced-sampling strategy and collective variables would you use to generate a training dataset that spans the relevant conformations; how would you represent the three interaction types as model inputs to enable decoupling; and what interpretability analysis can you apply to verify the network is using physically meaningful features\u2014including the specific failure mode it exposes for backbone (sugar\u2013phosphate bead) features?",
      "answer": "Use metadynamics as the primary sampling strategy, with predefined collective variables chosen to span the unwinding conformational space\u2014specifically the system radius of gyration (Rg_system) and the center-of-mass distance between the two dsRNA strands (Dist)\u2014to generate large sets of conformational samples that cover the key states. Represent inputs as physically informed \u201centropy fingerprints\u201d that separately encode Hydrogen Bonding (HB), Base Stacking (BS), and Sugar\u2011Phosphate Bead (SPB/backbone electrostatics) information via explicit spatial adjacency modeling between bead types, allowing CRE prediction and interaction-specific CRE decomposition. To verify the model leverages meaningful physics, apply Grad\u2011CAM to an encoder layer (penultimate encoder layer): this shows SPB features have higher semantic sparsity than HB and BS, and that incomplete SPB representations correlate with prediction errors, producing over/underestimation of entropy especially in PU and PR states; overall the network primarily learns HB/BS/backbone electrostatics while filtering non-essential background features.",
      "source_document": "papers/2512.08367v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In coarse-grained simulations of ATP-independent dsRNA unwinding by a DEAD-box helicase, partially unwound (PU) intermediates can transition to a fully unwound (FU) state through more than one microscopic route. Describe two distinct PU\u2192FU pathways that can occur, and explain what feature of the RNA dynamics determines (stochastically) which pathway is followed.",
      "answer": "Two PU\u2192FU routes are observed: (i) starting from a stable-bound duplex, the duplex undergoes sequential opening of ~3\u20134 base pairs to reach the PU state, and then proceeds by continued base-pair disruption (maintaining roughly 5\u20138 opened pairs) until complete unwinding; (ii) an existing PU conformation undergoes a base-rearrangement into another PU-like structure and then ruptures terminal base pairs, culminating in full unwinding. Which pathway occurs is stochastic and depends on whether transient nucleobase fluctuations at either dsRNA terminus are captured in real time (i.e., pathway selection is governed by terminal base fluctuation events rather than being fixed by a specific DDX3X binding site).",
      "source_document": "papers/2512.08367v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You have coarse-grained MD trajectories of a dsRNA\u2013protein complex and want a scalar order parameter that reports the entropy gain associated with strand separation, using only bead coordinates. How would you compute a coordination-resolved entropy (CRE) from the trajectories (including the intermediate quantities you compute and how they are turned into a Shannon entropy), and what is the key structural signal in the trajectories that CRE is designed to track during unwinding?",
      "answer": "Compute CRE by converting local coordination changes around each bead into a Shannon entropy.\n\n1) For each bead i, compute a \u201cmollified\u201d radial distribution function over its neighbors j:\n- Use distances r_ij from the trajectory and Gaussian broadening with width \u03c3 to obtain g_i^m(r) (a smoothed RDF).\n\n2) Convert the smoothed RDF into a coordination number by integrating out to a cutoff (here r = 2.1 nm) with the local bead density \u03c1:\n- n_i(r) = 4\u03c0\u03c1 \u222b_0^r g_i^m(r) r^2 dr.\n\n3) Normalize coordination numbers into a probability distribution over beads:\n- P(i) = n_i(r) / \u03a3_i n_i(r).\n\n4) Compute Shannon entropy (and treat it as the coordination-resolved entropy):\n- CRE = \u2212 \u03a3_i P(i) log2 P(i) (with normalization as described).\n\nCRE is designed to track subtle conformational/coordination-number alterations in ultrafast dissociation dynamics\u2014i.e., changes in local packing/contacts among dsRNA beads (reflecting disrupted base pairing/stacking and increased conformational freedom) as the strands separate under thermal fluctuations.",
      "source_document": "papers/2512.08367v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You\u2019re building a CLIP-style dense retrieval model to screen enzymes for candidate reactions. How can you construct a training objective that (i) aligns enzyme and reaction embeddings using known catalytic pairs, (ii) preserves intra-domain structure/hierarchy (e.g., enzymes/reactions sharing the same EC number), and (iii) further enforces domain-level structural alignment via a Gromov\u2013Wasserstein (GW)-motivated regularizer? In your answer, specify the loss components and explain the roles of the weighting coefficients \u03b1 (for GW vs. contrastive terms) and \u03bb (for EC supervision), including what failure modes occur when \u03b1 or \u03bb are set too large or too small.",
      "answer": "A suitable objective combines three ideas:\n\n1) **Inter-domain alignment (reaction\u2194enzyme) with InfoNCE.** Build a binary label matrix over a minibatch indicating which reaction\u2013enzyme pairs are catalytically valid. For each reaction i (and symmetrically each enzyme i), use an InfoNCE-style contrastive loss that pulls together positive reaction\u2013enzyme pairs and pushes away all other batch members using cosine similarity and a temperature \u03c4:\n- L_reaction-enzyme contrasts reaction embeddings r_i against enzyme embeddings e_j (and vice versa).\n\n2) **Intra-domain alignment using shared EC numbers.** Define, for each item i, the set I_i of batch samples sharing the same EC number with i. Apply an internal contrastive loss\n- L_internal = \u2212\u2211_i \u2211_{j\u2208I_i} log exp(sim(x_i,x_j)/\u03c4) / \u2211_k exp(sim(x_i,x_k)/\u03c4),\nwhere x denotes either reaction or enzyme embeddings. This yields separate intra-domain terms (reaction\u2013reaction and enzyme\u2013enzyme), encouraging embeddings to respect EC-based hierarchy/structure.\n\n3) **GW-motivated structural regularization.** Introduce an additional regularization term L_GW designed to minimize a GW-style discrepancy between the *intra-domain* relational structures of the reaction space and enzyme space while using a soft cross-domain alignment \u0393_\u03b8 derived from reaction\u2013enzyme similarities. The intra-domain structure matrices (e.g., \u0393^{\u03c81}_d for reactions and \u0393^{\u03c82}_d for enzymes) are computed from intra-domain similarities and are **detached** (no gradient) to prevent this regularizer from backpropagating into and interfering with optimization of the other contrastive objectives; \u0393_\u03b8 provides the learnable soft alignment.\n\nThese are combined into the overall training objective:\n- **L_FGW = (1\u2212\u03b1)(L_reaction-enzyme + L_reaction + L_enzyme) \u2212 2\u03b1 L_GW + \u03bb L_EC**, \nwhere L_EC is a multi-head cross-entropy loss predicting the EC number at each of the four EC hierarchy levels.\n\n**Role of \u03b1:** it controls the trade-off between the standard contrastive alignment terms and the GW structural regularizer. If \u03b1 is too small, the GW term has little effect and alignment is weaker; if \u03b1 is too large, GW optimization is overemphasized and can disrupt intra-domain alignment.\n\n**Role of \u03bb:** it weights EC-number supervision. Because the EC prediction loss can have large raw magnitude, setting \u03bb too high can dominate the total objective and suppress the other loss components, degrading performance; therefore \u03bb should be kept small to incorporate EC supervision without overwhelming contrastive/GW objectives.",
      "source_document": "papers/2512.08508v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a gait-analysis pipeline where each gait cycle is represented as a time-by-feature matrix \\(X\\) of joint coordinates/angles, how can you construct a machine-learning-ready feature vector that (i) captures correlations between joints while respecting the non-Euclidean geometry of covariance matrices, and (ii) is robust to outlier gait cycles within a recording? Describe the key mathematical steps and the final aggregation strategy.",
      "answer": "Use the covariance of each centered gait-cycle segment as the descriptor: compute the sample covariance \\(C\\) from \\(X\\). Because \\(C\\) is symmetric positive definite (SPD), it lies on the SPD Riemannian manifold (not a Euclidean vector space), so direct Euclidean operations are geometrically invalid. Map \\(C\\) to a Euclidean tangent space using the Log-Euclidean framework by taking the matrix logarithm via eigendecomposition \\(C=U\\Sigma U^T\\) and \\(L=\\logm(C)=U\\,\\log(\\Sigma)\\,U^T\\). Convert this symmetric log-covariance matrix \\(L\\) into a unique vector embedding by extracting the upper-triangular (nonredundant) entries, i.e., \\(v=\\mathrm{vech}(L)\\). Compute \\(v\\) separately for each detected step (gait cycle), then report the subject/speed-level diagnostic value as the median of the per-step values to reduce sensitivity to measurement outliers and detection noise.",
      "source_document": "papers/2512.09158v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When quantifying gait variability across increasing speed conditions (e.g., slow \u2192 medium \u2192 fast), how would you expect a simple Euclidean variability metric like total variance Tr(Cov(X)) to behave compared with a variability metric computed on the SPD covariance manifold using a Log-Euclidean mapping, and what motor-control interpretation can be drawn if the manifold-based metric shows an \u201cinverted-U\u201d trend (peaking at medium speed and stabilizing/decreasing at sprint speed)?",
      "answer": "A Euclidean baseline such as total variance Tr(Cov(X)) is expected to increase roughly linearly with speed (Slow < Medium < Fast), effectively treating sprinting as just a higher-magnitude version of slower gait and tending to overestimate variability because it does not capture posture-dependent, non-linear dependencies.\n\nIn contrast, a Log-Euclidean SPD-manifold variability metric can show a non-linear \u201cinverted-U\u201d relationship: variability increases from slow to medium but then stabilizes or decreases at fast (sprinting). This pattern is interpreted as evidence for geodesic optimization/biomechanical efficiency: at high performance demands the motor system minimizes \u201ctask-irrelevant\u201d noise and adheres more closely to coordinated, geodetically efficient (minimum-effort) trajectories, consistent with Optimal Feedback Control.",
      "source_document": "papers/2512.09158v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a manifold-based gait analysis pipeline that builds per-step SPD covariance descriptors from joint kinematics, what key clinical gait property cannot be quantified if the analysis is restricted to unilateral (one-sided) lower-extremity joints, and what data/modeling extension is needed to enable that quantification?",
      "answer": "Restricting the analysis to unilateral kinematics (e.g., only the right hip/knee/ankle) yields geometric features that describe only the intrinsic dynamics of the right hemibody, so it cannot quantify gait asymmetry between left and right sides. To quantify asymmetry, the pipeline must be extended to bilateral data (including both limbs/joint sets) so left\u2013right differences can be measured.",
      "source_document": "papers/2512.09158v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "Suppose you encode each detected gait cycle as a high-dimensional geometric feature vector by (i) building an SPD covariance descriptor from the kinematic time-series and (ii) mapping it to a Euclidean tangent space (e.g., via a matrix logarithm) and vectorizing it. How can you then visualize and qualitatively assess whether gait cycles from different speed conditions (slow/medium/fast) form separable groups, and what key geometric property should the chosen dimensionality-reduction method aim to preserve?",
      "answer": "Project the vectorized Riemannian features into a low-dimensional (2D) embedding using a manifold-learning dimensionality-reduction method such as UMAP. This enables intuitive visual inspection of the high-dimensional manifold structure and a qualitative assessment of separability between speed conditions. The projection should aim to preserve the global/topological structure of the underlying data manifold rather than treating the features as living in flat Euclidean space.",
      "source_document": "papers/2512.09158v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are given a segmented gait cycle as a time-by-feature matrix \\(X\\in\\mathbb{R}^{T\\times N}\\) of raw joint coordinates/angles (Euclidean representation). Describe how to compute three baseline Euclidean-space summary metrics for that cycle\u2014(i) a smoothness metric, (ii) a velocity metric, and (iii) a variability metric\u2014and state the physical interpretation of each (what aspect of the motion it is intended to quantify).",
      "answer": "Baseline Euclidean metrics computed directly from the raw coordinate data matrix \\(X\\) are:\n\n1) **Euclidean smoothness (\\(E_{\\text{smooth}}\\))**: compute frame-to-frame velocity vectors (differences between consecutive frames) and take the **cumulative sum of their Frobenius norms**. This represents the **total path length of the motion in flat (Euclidean) space**.\n\n2) **Euclidean velocity (\\(E_v\\))**: compute the Center-of-Mass (root joint) velocity vectors across the gait cycle and report the **mean magnitude** of those velocity vectors. This summarizes the **average movement speed of the Center of Mass over the cycle**.\n\n3) **Euclidean variance (\\(E_{\\text{var}}\\))**: compute the covariance matrix of the raw data and take its **trace** \\(\\mathrm{Tr}(\\mathrm{Cov}(X))\\). This is the **total variance (sum of per-feature variances across all joints)**, ignoring geometric dependencies/correlations beyond what is captured by this scalar trace.",
      "source_document": "papers/2512.09158v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a statistical model for single-cell batch correction where cells come from K biological clusters and each batch induces a cluster-specific shift (so a cell from cluster k in batch b has mean \u00b5_k + \u03b2_{bk}), how does the minimax lower bound on the expected batch-correction loss scale in the high-separation regime (SNR \u2192 \u221e), and what are the two distinct sources of error that yield the two terms in this bound?",
      "answer": "When SNR \u2192 \u221e (with B,K,d and eigenvalues of \u03a3_k bounded), the minimax expected batch-correction loss is bounded below by a two-term rate:\n\nexp(\u2212(1+o(1))\u00b7SNR^2/8) + exp(\u2212(1+o(1))\u00b7log n).\n\nThe first term comes from clustering error (mis-estimating the latent cluster labels a*), and the second term comes from estimation error in the batch-effect parameters \u03b2* (even if labels were known).",
      "source_document": "papers/2512.09259v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a Gaussian-mixture model for single-cell batch correction where each batch b and cluster k has mean \\(\\mu^*_{bk}=\\mu^*_k+\\beta^*_{bk}\\) and cluster-specific covariance \\(\\Sigma^*_k\\), how can the \u201csignal-to-noise ratio\u201d (SNR) governing the difficulty of batch correction be defined by reducing the problem to a hardest within-batch quadratic-discriminant (QDA) hypothesis test between two clusters? In your answer, specify (i) the key whitening change of variables, (ii) how the QDA likelihood-ratio rejection region induces a set \\(A_{b,k,k'}\\subset\\mathbb{R}^d\\), and (iii) how SNR is obtained from \\(A_{b,k,k'}\\) and why it takes a minimum over batches and cluster pairs.",
      "answer": "(i) For distinguishing two clusters k vs. k\u2032 within a fixed batch b, one tests \\(H_0: y\\sim N(\\mu^*_{bk},\\Sigma^*_k)\\) vs. \\(H_1: y\\sim N(\\mu^*_{bk'},\\Sigma^*_{k'})\\). A whitening change of variables is applied under \\(H_0\\): \\(x=(\\Sigma^*_k)^{-1/2}(y-\\mu^*_{bk})\\), which makes the null \\(H_0: x\\sim N(0,I_d)\\) and the alternative \\(H_1: x\\sim N((\\Sigma^*_k)^{-1/2}(\\mu^*_{bk'}-\\mu^*_{bk}),\\,(\\Sigma^*_k)^{-1/2}\\Sigma^*_{k'}(\\Sigma^*_k)^{-1/2})\\).\n\n(ii) The likelihood-ratio test for this transformed problem is equivalent to checking whether x lies in a (generally quadratic) rejection region \\(A_{b,k,k'}\\) defined by a quadratic inequality involving \\((\\Sigma^*_k)^{1/2}(\\Sigma^*_{k'})^{-1}\\), the mean difference \\(\\mu^*_{bk}-\\mu^*_{bk'}\\), and the log-determinant term \\(\\tfrac12\\log|\\Sigma^*_k| - \\tfrac12\\log|\\Sigma^*_{k'}|\\). Thus \\(\\varphi=1\\) iff \\(x\\in A_{b,k,k'}\\).\n\n(iii) A local separation measure is then \\(X_{b,k,k'} := 2\\min_{x\\in A_{b,k,k'}}\\|x\\|\\), i.e., twice the minimum Euclidean norm of any point in the rejection region. The overall SNR is defined as \\(\\mathrm{SNR} := \\min_{b\\in[B]}\\min_{k\\neq k'} X_{b,k,k'}\\). The minimization captures that batch correction is limited by the least favorable (hardest-to-separate) pair of clusters in the hardest batch; this hardest within-batch distinction governs the effective difficulty of correctly assigning cluster labels and estimating batch shifts.",
      "source_document": "papers/2512.09259v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a Gaussian-mixture model for single-cell batch correction where cells in cluster k and batch b follow \\(X_{bi}\\sim\\mathcal N(\\mu_k+\\beta_{bk},\\Sigma_k)\\), the decomposition \\(\\mu_k\\) (global biological center) + \\(\\beta_{bk}\\) (batch-specific shift) is not identifiable without a constraint. What identifiability constraint can be imposed, and\u2014conditional on a fixed set of cluster assignments \\(a_{bi}\\)\u2014what are the resulting closed-form maximum-likelihood (EM M-step) updates for \\(\\mu_k\\), \\(\\beta_{bk}\\), and \\(\\Sigma_k\\)?",
      "answer": "An identifiability constraint is to require that, within each cluster k, the batch effects average to zero when weighted by the number of cells from that cluster in each batch, i.e.\n\\[\\sum_{b=1}^B n_{bk}\\,\\beta_{bk}=0\\quad(\\text{equivalently }\\sum_{b,i}\\beta_{bk}\\,\\mathbf 1\\{a_{bi}=k\\}=0),\\]\nso that the global center \\(\\mu_k\\) cannot be shifted arbitrarily while compensating by shifting all \\(\\beta_{bk}\\) in the opposite direction.\n\nGiven current hard assignments \\(a_{bi}\\), the constrained MLEs used in MoDaH\u2019s M-step have closed form:\n\\[\n\\mu_k\\leftarrow \\frac{\\sum_{b,i} X_{bi}\\,\\mathbf 1\\{a_{bi}=k\\}}{\\sum_{b,i}\\mathbf 1\\{a_{bi}=k\\}},\n\\]\n\\[\n\\beta_{bk}\\leftarrow \\frac{\\sum_i X_{bi}\\,\\mathbf 1\\{a_{bi}=k\\}}{\\sum_i\\mathbf 1\\{a_{bi}=k\\}}-\\mu_k,\n\\]\n\\[\n\\Sigma_k\\leftarrow \\frac{\\sum_{b,i}\\mathbf 1\\{a_{bi}=k\\}(X_{bi}-\\mu_k-\\beta_{bk})(X_{bi}-\\mu_k-\\beta_{bk})^\\top}{\\sum_{b,i}\\mathbf 1\\{a_{bi}=k\\}}.\n\\]\nWith \\(\\mu_k\\) defined as the overall mean across all batches for cluster k, the update \\(\\beta_{bk}=\\bar X_{bk}-\\mu_k\\) automatically satisfies \\(\\sum_b n_{bk}\\beta_{bk}=0\\).",
      "source_document": "papers/2512.09259v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In benchmarking single-cell batch correction methods, how can you operationalize the tradeoff between (i) removing technical batch variation and (ii) preserving true biological heterogeneity using a standardized metric suite? Describe how the nine scib-metrics used here are partitioned into these two goals, how their directions are normalized for comparability, and how they can be combined into a single overall score without implicitly giving one metric more weight than another.",
      "answer": "A standardized way to capture the over-correction vs under-correction tradeoff is to evaluate methods with nine scib-metrics that are grouped into two categories.\n\n\u2022 Bio-conservation (penalizes over-correction; checks preservation of biological structure): Isolated labels, Leiden NMI, Leiden ARI, Silhouette label, and cLISI.\n\n\u2022 Batch-correction (penalizes under-correction; checks removal of technical variation / batch mixing): Silhouette batch, iLISI, KBET, and Graph connectivity.\n\nFor ease of comparison across heterogeneous metrics, each metric is rescaled to a common [0,1] range where 1 indicates best performance and 0 worst. Category scores are computed by averaging the metrics within each category. To obtain a single overall score while keeping equal weight across the original nine metrics, take a weighted average of the two category scores with weights proportional to the number of metrics in each category: (5/9)\u00b7(bio-conservation average) + (4/9)\u00b7(batch-correction average).",
      "source_document": "papers/2512.09259v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In mixture-model-based batch correction methods that require specifying the number of shared biological clusters K, K is often unknown in practice. What practical procedure can be used to estimate K from the data using graph-based Leiden clustering, and what specific neighborhood-graph and resolution settings are used by default for (i) scRNA-seq data and (ii) spatial proteomics data? Additionally, what qualitative robustness behavior is observed when the batch-correction algorithm is run with a misspecified K that is larger than the true value?",
      "answer": "A practical approach is to estimate K by running Leiden clustering on a kNN neighborhood graph constructed from the preprocessed data and taking the number of Leiden communities as \\(\\hat K\\). For scRNA-seq datasets, the neighborhood graph is constructed with 20 neighbors and Leiden is run with resolution 0.25. For spatial proteomics datasets, the Leiden parameters are modified to reflect weaker cluster separation: the neighborhood graph uses 5 neighbors and Leiden is run with resolution 1. Under cluster-number misspecification, performance (batch-correction loss and bio-conservation/batch-removal metrics) is best when K is correctly specified, but remains stable or only degrades slowly over a sizable range of misspecified inputs, especially when the supplied K is larger than the true K (i.e., overestimating K is relatively well tolerated).",
      "source_document": "papers/2512.09259v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building an LLM-assisted RNA-seq downstream analysis workflow that can justify interpretations with prior literature, what concrete retrieval-augmented generation (RAG) pipeline can be used to contextualize differentially expressed genes and enriched pathways, from how the literature query is formed through how documents are indexed for retrieval?",
      "answer": "Use a literature-aware RAG pipeline in which PubMed is queried via Entrez using search terms derived from the dysregulated genes and enriched pathways; the returned abstracts are embedded with sentence-transformer models and stored in a FAISS vector database, enabling retrieval and summarization of relevant literature to support the LLM\u2019s interpretations.",
      "source_document": "papers/2512.09964v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are designing an automated, novice-friendly RNA-seq downstream analysis workflow that takes a gene-expression matrix plus clinical metadata. How can the system automatically prioritize which clinical variables are most relevant to a user\u2019s hypothesis, and what statistical tests and summary fields should it compute to support that choice?",
      "answer": "Compute a clinical \u201csignificance table\u201d by testing associations between the hypothesis-related variable(s) and each other clinical column: use Pearson correlation for continuous\u2013continuous relationships and t-tests for group comparisons as appropriate. For each clinical variable, summarize at least the variable\u2019s data type, which test was used, the test statistic, and the resulting p-value so the user can select target variables for downstream comparisons.",
      "source_document": "papers/2512.09964v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You\u2019re building a novice-friendly, agentic assistant for bulk RNA-seq downstream analysis that goes beyond DEGs and enrichment by proposing *advanced* follow-up analyses (e.g., survival modeling or classifiers). How can the system present these advanced options so they are (i) tailored to the dataset\u2019s earlier results, (ii) directly executable by the user, and (iii) still keep the user in control while the agent orchestrates the workflow? Describe the concrete interaction pattern and what the agent should output at this stage.",
      "answer": "After interpreting the core results (DEG/clustering/pathway outputs, optionally enriched with literature retrieval), the agent should *recommend a small set of advanced methods* tailored to the dataset and findings\u2014specifically, three candidate follow-up analyses\u2014each packaged with: (1) a short description of what it does, (2) the rationale for why it fits the observed results/hypothesis, and (3) executable Python code. In the UI, the user chooses one recommended method via a selection widget (e.g., a Streamlit selectbox) and explicitly triggers execution with a \u201crun\u201d action (button). The agent then executes the chosen method, summarizes the advanced analysis outputs in beginner-friendly language, and produces a downloadable report. Organizing the workflow into explicit, button-driven stages preserves user control while the agent autonomously plans, executes, interprets, and recommends subsequent steps.",
      "source_document": "papers/2512.09964v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You\u2019re building an automated bulk RNA-seq downstream analysis tool for non-experts that should (i) uncover unsupervised structure among samples and (ii) help users interpret whether that structure aligns with a chosen clinical variable. What unsupervised clustering approach and dimensionality-reduction visualization would you implement, how should the expression matrix be oriented for clustering, and what should the PCA visualization encode to make the clinical association interpretable?",
      "answer": "Use k-means clustering to find unsupervised sample groups, applying it to the **transposed expression matrix** (so samples are clustered based on their gene-expression profiles). Visualize the resulting structure with **principal component analysis (PCA)**, plotting samples in PCA space and **coloring points by the user-selected clinical variable** (with an accompanying legend) so users can visually assess separation/association between clusters and the clinical grouping.",
      "source_document": "papers/2512.09964v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an automated RNA-seq downstream analysis tool that accepts user-uploaded expression matrices of unknown formatting and dynamic range, what two preprocessing steps can you apply to (i) make gene identifiers consistent across all analysis modules and (ii) stabilize expression variance before downstream tests/clustering, and what are the intended benefits of each step?",
      "answer": "(i) Extract/standardize the gene identifiers up front so they are handled consistently across all components of the pipeline. (ii) Normalize the expression matrix with a log2 transformation to stabilize variance and reduce the impact of extreme values before downstream analyses (e.g., t-test DEG and k-means/PCA).",
      "source_document": "papers/2512.09964v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building an alignment-free k-mer embedding for supervised SARS\u2011CoV\u20112 spike lineage classification using hashing, what computational issues does hashing address compared with an explicit k-mer spectrum, how is the collision rate controlled/tuned via the hash table size m, and why can classification performance remain relatively robust even when a higher fraction of k-mers collide?",
      "answer": "Hashing is used to avoid (1) the potentially time\u2011consuming \u201cbin searching\u201d needed to place each k\u2011mer into its position in an explicit, ordered k\u2011mer spectrum vector and (2) the very high dimensionality of that spectrum (curse of dimensionality). In the hashed embedding, each k\u2011mer is assigned a Murmur hash value corresponding to a position in a hash table of size m, and the k\u2011mer\u2019s frequency is aggregated into that bin; once m is fixed, placing counts is O(1) per unique k\u2011mer and scales without a bin\u2011search overhead.\n\nThe collision rate is controlled by the hash table size m: increasing m decreases the percentage of k\u2011mers that collide (and decreasing m increases collisions), with m treated as a tunable hyperparameter whose optimal value is selected using a validation\u2011set approach. The collision percentage can be computed by hashing the set of unique k\u2011mers and comparing how many unique hash values are obtained versus the number of unique k\u2011mers.\n\nPerformance can remain robust under higher collision rates because spike mutations are disproportionate but relatively few and are captured by lineage\u2011informative k\u2011mers; moreover, mutations within a lineage tend to be consistent across its spike sequences, so even if some mutational k\u2011mers are missed due to collisions in some embeddings, the downstream classifiers are not strongly confused and can still generalize well.",
      "source_document": "papers/2512.10147v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In supervised SARS\u2011CoV\u20112 spike lineage classification, why might embeddings from a large pre\u2011trained protein language model (e.g., trained on broad UniProt proteins) fail to outperform a simple k\u2011mer count\u2013based representation (even after fine\u2011tuning), and what property of spike sequences drives this domain shift?",
      "answer": "Because the language model is pre\u2011trained on diverse protein sequences that are not directly related to coronavirus, its learned representation may not transfer well to SARS\u2011CoV\u20112 spike proteins; the spike sequences have distinct properties not present in other proteins, making generalization from UniProt pretraining difficult, so even fine\u2011tuning does not necessarily close the gap to a task\u2011specific spike k\u2011mer representation.",
      "source_document": "papers/2512.10147v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When comparing two sequence-embedding pipelines for supervised SARS\u2011CoV\u20112 spike lineage classification (e.g., a new embedding vs. existing baselines), how can you assess whether the observed differences in classification performance are statistically significant across repeated runs, and what significance criterion is used?",
      "answer": "Run the classifiers multiple times and summarize variability with standard deviations; then apply a Student t-test using the averages and STDs across runs to compute p-values. Statistical significance is concluded when the p-value is below 0.05 (reported as p-values < 0.05).",
      "source_document": "papers/2512.10147v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You\u2019re benchmarking multiple embedding methods for supervised SARS\u2011CoV\u20112 spike lineage classification and need an evaluation protocol that (i) tunes classifier hyperparameters without leaking information from the held\u2011out test set and (ii) reduces variance in the reported performance. What train/validation/test procedure achieves this, and where is cross\u2011validation applied relative to the held\u2011out test set?",
      "answer": "Use a held\u2011out test split and perform hyperparameter tuning only inside the training portion: split the data into a 70% training set and a 30% held\u2011out test set; within the training set, run 5\u2011fold cross\u2011validation to tune model hyperparameters. To stabilize estimates, repeat the train/test split multiple times (five repeats) and report the average results across runs.",
      "source_document": "papers/2512.10147v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an alignment-free k-mer\u2013hashing embedding pipeline for viral spike proteins, what characteristics of a hash function make it suitable for mapping k-mers into a fixed-size feature vector with low collision bias, and what empirical behaviors/tests indicate that the chosen hash has these properties?",
      "answer": "A suitable hash should (i) mix input bits strongly so outputs are well-distributed across the hash space (reducing collision rates and collision bias) and (ii) behave deterministically and efficiently for large-scale embedding generation. In this document\u2019s approach, Murmur hash is used because its multiply\u2013rotate\u2013multiply\u2013rotate + XOR mixing produces uniform, well-distributed hash values with low collision rates. Empirically, it is described as having excellent hash-value distribution (passing chi-squared tests over many key sets and bucket sizes), desirable avalanche behavior (maximum bias about 0.5%), and robust collision resistance (passing Bob Jenkins\u2019 frog.c torture test), and it is stated to have fewer collisions/more uniform output than FNV.",
      "source_document": "papers/2512.10147v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "Atom-level diffusion models typically require a fixed graph size/topology during denoising, but amino-acid side chains (especially when including non-canonical amino acids) vary in atom count. What strategy can be used to enable end-to-end diffusion-based generation of variable-sized side chains in a single model, and how does it work during training and sampling?",
      "answer": "Use a virtual-node padding scheme with an upper bound Nmax on side-chain atoms. Add a random number of disconnected virtual nodes during training; these nodes have a special atom type (e.g., NOATOM) and edges labeled as no-bond (e.g., NOBOND), and are initialized at the side-chain center of mass. The model is trained to denoise graphs that include both real atoms and these virtual placeholders. At sampling time, denoise a graph with Nmax nodes; after denoising, remove any nodes predicted/denoised to the virtual type, yielding a variable-sized side chain while keeping the diffusion process defined on a fixed-size graph.",
      "source_document": "papers/2512.10515v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are given an atom-level generative model that samples complete amino-acid side chains conditioned on a residue\u2019s local 3D environment. How can you turn this generator into a zero-shot variant-effect predictor for single-residue substitutions and validate it against deep mutational scanning (DMS) data? Describe (i) how probabilities/likelihoods for wild-type vs mutant residues are estimated from samples, (ii) the mutation score computed from those probabilities, and (iii) the statistic used to compare predictions to experimental measurements.",
      "answer": "(i) For each assayed site, repeatedly sample side chains conditioned on the fixed local environment (e.g., 1000 sampling iterations per site) and identify each sampled side chain\u2019s residue identity by matching the generated atom-level graph to known amino acids via graph isomorphism. Use the sample frequencies as estimated probabilities P(residue) for each candidate (with a small floor for residues not observed in sampling).\n\n(ii) Compute a differential log-likelihood score between mutant and wild-type:\n\u0394log L = \u2212log P(mutant) + log P(wild-type),\nwhere P(mutant) and P(wild-type) are the estimated probabilities from sampling.\n\n(iii) Validate by correlating \u0394log L with the experimental mutational-effect readout (e.g., DMS-measured \u0394\u0394G) using Spearman rank correlation (ProteinGym-style reporting). For NCAA DMS sets, restrict evaluation to substitutions where both mutant and wild-type appear in the sampled data.",
      "source_document": "papers/2512.10515v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating an atom-wise side-chain generator as a zero-shot mutation-effect predictor on a deep mutational scanning (DMS) benchmark, what diagnostic metric can you use to check whether the sampler is even capable of reconstructing the native sequence, and how is that metric computed at the assay level? Explain why this diagnostic is particularly important for atom-wise (rather than token-vocabulary) residue samplers.",
      "answer": "Use the wild-type coverage rate. For an assay, mask each position and sample candidate residues; the wild-type coverage rate is the frequency (fraction) with which the wild-type residue is successfully sampled across all positions in that assay (often aggregated/averaged over positions after repeated sampling per site). This diagnostic matters for atom-wise samplers because they must assemble residues from lower-level representations; if they rarely recover the wild-type residue, their sampled frequencies/likelihood proxies for mutants vs wild-type are unreliable and downstream correlations with experimental effects will be poor regardless of the scoring scheme.",
      "source_document": "papers/2512.10515v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You want to train an atom-level diffusion model to propose both canonical and non-canonical amino-acid (NCAA) substitutions, and you decide to augment standard protein-residue training examples with (i) standalone NCAA molecules and (ii) protein\u2013ligand complexes to expose the model to broader chemistry. How should the diffusion \u201cfixed vs noised\u201d atoms/graph construction differ between these two augmentation sources, and what does an ablation study indicate about which augmentation source is more critical for downstream zero-shot mutational-effect performance?",
      "answer": "For standalone/independent NCAA examples, the residue\u2019s four backbone atoms (C, C, O, N) are kept fixed and only the side-chain atoms are diffused/perturbed. For protein\u2013ligand (PDBBind) complexes, the protein pocket atoms are fixed while noise is applied to all ligand atoms (as in structure-based drug design diffusion setups). In ablations, removing PDBBind has the smallest impact on performance, whereas removing the independent NCAA data causes a substantial drop (and removing both reduces performance further), indicating the independent NCAA augmentation is more critical for mutational-effect prediction signal.",
      "source_document": "papers/2512.10515v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an atom-level diffusion model for protein side-chain generation, you need to model both (i) continuous 3D coordinates and (ii) discrete atom-wise chemical features (e.g., atom type, formal charge, hybridization, aromaticity, ring membership, degree). How can you define the forward noising process for these two modalities, and what prediction parameterization and loss would you use to train the reverse model so it can denoise both coordinates and categorical features?",
      "answer": "Use a heterogeneous diffusion process with independent noising for the continuous and discrete modalities:\n\n\u2022 Continuous coordinates X: apply a Gaussian forward process such that at timestep t,\n  q(x_t | x_0) = N(x_t | sqrt(\\bar\\alpha_t) x_0, (1-\\bar\\alpha_t) I),\n  where \\bar\\alpha_t = \\prod_{k=1}^t (1-\\beta_k) is the cumulative noise schedule.\n\n\u2022 Discrete atom-wise features (categorical labels) c (collectively H): apply a categorical forward process\n  q(c_t | c_0) = C(c_t | \\bar\\alpha_t c_0 + (1-\\bar\\alpha_t) \\tilde c),\n  where \\tilde c is a categorical prior (e.g., uniform or empirical). A practical choice is a mask-diffusion scheme that gradually converts labels into an absorbing \u201cmask\u201d state during the forward process.\n\nFor training, use the \\hat{x}_0 parameterization: the network predicts the clean data at each timestep (clean coordinates and clean categorical labels/logits). Optimize with mean squared error for denoising coordinates and cross-entropy for denoising the categorical features.",
      "source_document": "papers/2512.10515v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When conditioning a 3D molecular diffusion generator on representations from a large pretrained 1D molecular foundation model, why is it beneficial to keep the 1D foundation model frozen (rather than fine-tuning it on the smaller 3D dataset), and what measurable degradation is expected if (i) the foundation model is removed entirely or (ii) it is fine-tuned during 3D training?",
      "answer": "Keeping the 1D foundation model frozen preserves its broad, generalizable chemical priors and avoids distribution sharpening and catastrophic forgetting that can occur when adapting it to a smaller downstream dataset; instead, a learnable projector transfers the latent chemical knowledge into the diffusion model\u2019s conditioning space. Empirically, removing the foundation model degrades 3D generation quality (e.g., FCD3D rises to 0.998 and molecular stability drops relative to the full model), and fine-tuning the foundation model in the 3D stage is also worse than freezing it (FCD3D 0.969 vs 0.952 when frozen).",
      "source_document": "papers/2512.10991v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a 1D-to-3D cross-modal diffusion generator that uses a small set of learnable \u201cquery\u201d tokens to extract chemical priors from a frozen 1D foundation model, how should you expect the number of query tokens to affect 3D generation quality, and what practical tradeoff motivates choosing an intermediate token count as the default?",
      "answer": "As the number of learnable query tokens increases from a small value, generation quality improves because more queries can capture richer chemical motifs from the foundation model (e.g., increasing from 16 to 64 tokens reduces FCD3D from 0.972 to an optimum of 0.952). However, making the query set too large degrades performance (e.g., at 128\u2013256 tokens FCD3D rises to about 0.964), likely because redundant/noisy information makes optimizing the diffusion process harder. This motivates selecting an intermediate default (NQ = 64) that balances information sufficiency with a compact representation.",
      "source_document": "papers/2512.10991v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a property-conditional 3D molecular diffusion pipeline that leverages a pretrained 1D SELFIES language model for cross-modal conditioning, what is a principled way to inject the target property into (i) the 1D model\u2019s inputs and (ii) the 3D diffusion model\u2019s denoising network so that the condition influences generation throughout the diffusion steps? Describe the two conditioning pathways and where in the diffusion network the property signal is fused.",
      "answer": "Use two parallel conditioning pathways. (i) For the 1D foundation model, encode the target property with a small \u201ccondition MLP\u201d into a soft-prompt embedding and prepend this soft prompt to the SELFIES token embeddings before feeding the sequence into the frozen 1D model, so the extracted cross-modal queries reflect the desired property. (ii) For the 3D diffusion model, encode the property value with an MLP and then linearly project it to the same dimensionality as the diffusion time embedding; add this processed condition to the time embedding (and fuse with other conditions via element-wise addition) so the property guides the denoising dynamics at every diffusion step.",
      "source_document": "papers/2512.10991v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a de novo 3D molecule generator, how can you structure the evaluation to separately test (i) 2D chemical/graph validity and diversity and (ii) 3D geometric realism? Name the specific metrics in each group and state what the 3D distributional metrics are comparing.",
      "answer": "A comprehensive evaluation can be split into two groups. \n\n(1) 2D (chemical/graph) metrics: atom stability; validity & completeness (V&C); validity & uniqueness (V&U); validity & uniqueness & novelty (V&U&N); similarity to nearest neighbor (SNN); fragment similarity (Frag); scaffold similarity (Scaf); and Fr\u00e9chet ChemNet Distance (FCD).\n\n(2) 3D (geometric) metrics: atom stability; FCD3D; and maximum mean discrepancy (MMD) computed between the generated set and real data for the distributions of bond length, bond angle, and dihedral angle. FCD3D is used as a distributional distance indicating how closely the generated molecules\u2019 3D feature distribution matches that of the real training data, while MMD quantifies mismatch in the key geometric-parameter distributions (bond lengths/angles/dihedrals).",
      "source_document": "papers/2512.10991v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a 1D-to-3D molecular generation framework where a frozen sequence foundation model provides chemical information to a 3D diffusion model, what is the role of a transformer-based \u201cprojector\u201d between the two modalities, and how would you expect generation fidelity to change as you increase the projector depth from no transformer layers to a deeper stack? Explain why a shallow/no-projector mapping fails and why gains eventually saturate at higher depths.",
      "answer": "The projector is the cross-modal alignment module that maps the foundation model\u2019s extracted 1D chemical latent information (from learnable query interactions with the SELFIES tokens) into the conditioning space used by the 3D diffusion denoiser. A simple mapping without transformer layers (effectively just an FFN) performs poorly because it cannot bridge the mismatch between the 1D semantic space (sequence-level chemical syntax/priors) and the 3D geometric conditioning space required by the diffusion model.\n\nAs projector depth increases, 3D generation fidelity improves substantially at first (e.g., FCD3D drops from a very poor value around 1.233 with 0 layers to ~0.977 with 6 layers and ~0.954 with 12 layers), indicating that deeper cross-attentive/transformer processing is needed to translate and integrate the chemical priors into a usable geometric condition. Beyond this, gains saturate: going from 12 to 24 layers yields only a marginal improvement (FCD3D ~0.952), suggesting diminishing returns once the semantic-to-geometric mapping is sufficiently expressive. A deep projector is therefore chosen to maximize fidelity, but the trend indicates that after moderate depth the alignment capacity is no longer the primary bottleneck.",
      "source_document": "papers/2512.10991v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-task molecular toxicity model that uses a shared transformer backbone and task-specific soft attention masks to gate the shared token representations, how can you define a training objective that (i) handles missing labels across tasks and (ii) encourages each task to select only a small set of salient tokens? Give the loss form and explain the role of the indicator variable and the regularization hyperparameter.",
      "answer": "Define, for each task k, a total loss as a sum of a prediction loss and an attention-mask sparsity penalty: L_k = L_pred,k + \u03bb R_a,k. The prediction term is binary cross-entropy computed only over samples with labels for that task by using an indicator \u03b4_{n,k} (1 if sample n has a valid label for task k, else 0) and normalizing by the number of labeled samples: L_pred,k = (1 / \u03a3_n \u03b4_{n,k}) \u00b7 \u03a3_n \u03b4_{n,k} \u00b7 BCE(\u03c3(\u0177_{n,k}), y_{n,k}). The sparsity term is an L1 penalty on the task\u2019s attention masks, also averaged over labeled samples: R_a,k = (1 / \u03a3_n \u03b4_{n,k}) \u00b7 \u03a3_n \u03b4_{n,k} ||M_{n,k}||_1, where M_{n,k} is the task-specific mask for sample n. The overall objective is the mean across tasks: L = (1/K) \u03a3_k (L_pred,k + \u03bb R_a,k). Here \u03b4_{n,k} ensures unlabeled entries do not contribute to the loss, and \u03bb controls the strength of the sparsity regularization (larger \u03bb enforces sparser masks).",
      "source_document": "papers/2512.11412v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In hard-parameter-sharing multi-task toxicity prediction with a shared transformer encoder, how can adding task-specific soft feature gating reduce negative transfer between endpoints? Describe (i) how the gate/mask is computed and applied to the shared token representations and (ii) why this filtering helps when tasks have conflicting relevant chemistry.",
      "answer": "Add a small task-specific attention/gating module on top of the shared transformer outputs H (a sequence of token embeddings). For task k, a lightweight two-layer FFNN (with LayerNorm and GELU) maps H to per-token mask scores, then a sigmoid bounds them to [0,1], giving a soft mask Mk = \u03c3(Fk(H)). The mask is applied element-wise to the shared features to form task-specific features \u0124k = Mk \u2299 H, and these re-weighted features are aggregated with weighted pooling (sum then normalize by the accumulated mask weights) into a fixed-size context vector used by the task\u2019s linear prediction head. This gating lets each task suppress irrelevant tokens/features coming from the shared backbone and keep only salient fragments for that endpoint, thereby filtering out conflicting or task-irrelevant information; by reducing the amount of shared representation each head consumes, it mitigates interference from conflicting gradients and stabilizes learning compared with passing the entire shared representation to every head (a typical source of negative transfer in HPS).",
      "source_document": "papers/2512.11412v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are benchmarking a multi-task SMILES-transformer toxicity predictor on a multi-label dataset where different endpoints have different class imbalance and many tasks. What evaluation protocol would you use to (i) create train/test splits that preserve the per-task label distribution, (ii) tune hyperparameters without leaking test information, and (iii) report a single performance number across tasks?",
      "answer": "Use RDKit to validate/canonicalize SMILES, then split the dataset into 80% train and 20% test using iterative stratified sampling to preserve the multi-label (per-task) label distribution. Perform hyperparameter optimization on the training set with 5-fold cross-validation, using Bayesian search (Optuna). After selecting hyperparameters, retrain the final model on the full 80% training set and evaluate once on the held-out 20% test set. Report performance as macro-averaged ROC-AUC across all tasks.",
      "source_document": "papers/2512.11412v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-task SMILES-transformer toxicity model that learns task-specific sparse attention masks over token (atom-level) representations, what qualitative evidence from mask visualizations would support the claim that the model is both (i) endpoint-specific and (ii) learning generalizable toxicophore-like features rather than memorizing scaffolds? Describe the two behaviors you would look for and what each implies.",
      "answer": "Two qualitative behaviors support this:\n\n1) Task-specific differentiation within the same molecule: for different toxicity endpoints, the attention mask should shift to different molecular features in the same compound (e.g., one task heavily weights a specific atom/fragment while another task focuses on a different functional group). This indicates the task-specific gating is filtering the shared backbone representation so each head uses features relevant to its biological endpoint.\n\n2) Cross-scaffold consistency for the same endpoint: across structurally diverse molecules that are positive for the same toxicity task, the mask should consistently highlight the same type of motif (e.g., consistently emphasizing nitrogen atoms such as an amide nitrogen in one scaffold and a piperidine nitrogen in another). This suggests the sparsity constraint is encouraging selection of common, chemically meaningful motifs (toxicophore-like patterns) and supports generalization beyond specific scaffolds rather than rote memorization.",
      "source_document": "papers/2512.11412v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a transformer-based multi-task toxicity predictor that learns a task-specific soft mask over SMILES token embeddings, how can you aggregate the masked per-token features into a fixed-size representation for a task-specific classifier, and why is it useful to normalize the pooled vector by the sum of the attention/mask weights (instead of using an unnormalized sum)?",
      "answer": "After computing a task-specific mask Mk\u2208[0,1]^L and applying it element-wise to the shared token representations H to obtain masked features H\u0302k= Mk \u2299 H, the model forms a single fixed-size context vector ck for task k via weighted pooling: it sums the masked token features and then normalizes by the accumulated (summed) attention/mask scores. This normalization keeps the representation scale stable (i.e., comparable across molecules and across different sparsity/attention patterns) so the downstream linear classifier does not see logits that vary simply because more tokens received nonzero weight.",
      "source_document": "papers/2512.11412v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a diffusion-model-based peptide generator trained on a small set of known BBB-penetrating peptides, how can a feedback loop with an external similarity analyzer be used to reduce underfitting and improve the quality of augmented positive samples? Describe the operational steps of the feedback mechanism and how pseudo-peptides are selected and incorporated during training.",
      "answer": "A feedback loop can be added by coupling the diffusion generator to an independent similarity analyzer (here, a BLAST-based analyzer) that evaluates how close generated peptides are to the distribution of real BBB-penetrating peptides. The workflow is: (1) pretrain the diffusion model on the real BBBP set so it can generate valid candidate sequences; (2) during each subsequent training epoch, sample a batch of sequences from the generator and pass them to the analyzer; (3) the analyzer assigns a similarity score to each generated sequence, and only sequences whose scores exceed a predefined threshold are kept as pseudo-BBBPs; (4) add these high-scoring pseudo-BBBPs to the real BBBP set and retrain the diffusion model on this expanded set; (5) as training proceeds, periodically replace the oldest pseudo-BBBPs with newly generated high-scoring ones, progressively refining the training data toward higher-quality samples. This feedback-guided selection and continual refresh helps pull the reverse diffusion distribution closer to the true BBBP distribution, mitigating underfitting in the small-sample regime and producing higher-quality augmented positives.",
      "source_document": "papers/2512.11511v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a classifier to predict blood\u2013brain barrier penetrating peptides (BBBPs) from scarce, imbalanced data, how can a dual-stream Transformer architecture be designed to jointly exploit (i) residue-level sequence/motif information and (ii) global physicochemical properties, and how are the two streams fused to produce the final BBBP/non-BBBP prediction?",
      "answer": "Use two parallel Transformer encoders that learn complementary representations from different input modalities: one encoder takes the peptide sequence (with positional encoding) so multi-head self-attention can model residue-to-residue dependencies, motifs, and structural patterns; a second encoder processes a vector of physicochemical descriptors (e.g., hydrophobic surface area, molecular charge, number of rotatable bonds, polarizability, etc.) to capture global molecular properties. After encoding, take the final outputs from the two encoders (sequence representation and physicochemical representation), concatenate them, and feed the concatenated vector to an MLP classifier to output the BBBP/non-BBBP label. This fusion leverages complementary biases of sequence-only vs physicochemical-only models and improves overall discrimination.",
      "source_document": "papers/2512.11511v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a BBB-penetrating peptide (BBBP) prediction task where the positive class is severely underrepresented and you use a diffusion generator to create pseudo-BBBPs, how would you design an evaluation protocol that (i) avoids performance inflation from synthetic/test leakage, (ii) lets you attribute gains specifically to augmentation rather than class-ratio changes, and (iii) checks whether generator training is actually producing increasingly \u201crealistic\u201d positives over time? Describe the key dataset construction choices and the performance trend you would expect to see if augmentation is helping.",
      "answer": "A suitable protocol is:\n\n1) Prevent synthetic/test leakage: keep an independent test set made only of real (experimentally verified) peptides\u2014no pseudo-BBBPs are ever included in the test set. Use a balanced test set with equal numbers of BBBPs and non-BBBPs so that sensitivity/specificity and threshold-dependent metrics are not dominated by class prevalence.\n\n2) Isolate augmentation from class-ratio effects: inject pseudo-BBBPs only into the training set\u2019s positive class, and when comparing \u201caugmentation intensity\u201d (different numbers of pseudo-BBBPs), keep the overall class balance fixed\u2014i.e., choose the number of real+synthetic BBBPs so it matches the number of non-BBBPs used for training\u2014so any performance change is attributable to the added positive diversity/quality rather than a shifting positive/negative ratio.\n\n3) Verify generator improvement over training: as the diffusion model trains, periodically sample pseudo-BBBPs from the current generator, add the screened/retained sequences to the training positives, retrain the classifier, and evaluate on the fixed real-only test set. If augmentation is working and generated samples are becoming closer to the true BBBP distribution, test-set AUC/accuracy should increase as generator training progresses and then plateau once generation quality saturates; similarly, using more high-quality pseudo-BBBPs should generally improve performance compared to using fewer or none.",
      "source_document": "papers/2512.11511v1.pdf",
      "mode": "textual",
      "content_refs": [
        "lines 110-114",
        "lines 179-189",
        "lines 276-287"
      ]
    },
    {
      "question": "You have a dual-stream BBB-penetrating peptide (BBBP) classifier with (i) a Transformer encoder over the amino-acid sequence and (ii) a Transformer encoder over a vector of physicochemical properties. How would you design an ablation study to isolate the contribution of each stream, and what qualitative pattern in prediction bias and overall metrics would demonstrate that the two streams provide complementary information rather than redundant features?",
      "answer": "Train and compare three model variants on the same (augmented) training set: (1) the full dual-stream model using both sequence and physicochemical branches, (2) a sequence-only model made by disabling the physicochemical-feature branch, and (3) a physicochemical-only model made by disabling the sequence branch. The sequence-only and physicochemical-only variants show slightly different biases toward predicting BBBPs vs non-BBBPs; when both streams are combined in the dual-stream model, performance improves further (higher AUC/ACC/MCC), indicating the two feature types contribute complementary information.",
      "source_document": "papers/2512.11511v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a BBB-penetrating peptide (BBBP) classifier trained on a small dataset, how can you combine k-fold cross-validation with early stopping to mitigate overfitting, and how do you turn the k models into a single final predictor at inference time? Specify the key operational steps (how many folds/models are produced and how their outputs are combined).",
      "answer": "Train the classifier using 10-fold cross-validation with an early-stopping criterion: split the training data into 10 folds, train a separate model for each fold (with that fold serving as validation) for up to 200 epochs but stop early when validation performance no longer improves. This yields ten independently trained models. At inference time, obtain the final prediction by averaging the predicted probabilities (or outputs) from all 10 models.",
      "source_document": "papers/2512.11511v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You need to enumerate all **maximal** paths in a pangenome graph whose node segments contain a given ordered chain of query sequences (allowing only a bounded amount of \u201cgap\u201d traversal through non-matching vertices), and you only want to report paths that match at least m query segments. What traversal strategy and state representation can be used to do this efficiently, how are queue items prioritized and pruned when multiple partial paths reach the same vertex, and what asymptotic time complexity is achievable with appropriate data structures?",
      "answer": "Use a customized multi-source BFS seeded from all vertices whose segment contains the first query sequence S1. Maintain a priority queue of 7-tuples (v, e, P, P\u2032, i, j, k) where v is the current vertex, e is the incoming edge (null for seeds), P is the committed path so far, P\u2032 is the current \u201cgap\u201d suffix since the last match (kept separate to avoid reporting paths with trailing unnecessary gaps), i is the index of the last matched query segment Si, j marks how much of S(v) has been consumed, and k is the running total gap length. When processing v, search S(v) starting after j for the next query segment Si+1; if found, append any gap path P\u2032, then e and v to P, increment i, reset the gap suffix, and (if i\u2265m) update the result set; if not found, extend only the gap suffix and increment k, pruning if k>l.\n\nPrioritize queue items by lower total gap k first; break ties by higher matched-prefix length i (so paths with fewer gaps and longer matches are expanded earlier). Prune redundant traversals using an explored set T of pairs (v,i): once a vertex v has been reached matching the first i query segments, there is no need to continue from another partial path that reaches the same (v,i), because it cannot match a longer prefix than the earlier one (and the algorithm also opts to continue only with the minimum-gap way to reach a given state). With a hash map for T, the expected running time is O(|V| + |E| + |R|), linear in the graph size plus the total length of all reported paths R.",
      "source_document": "papers/2512.12052v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an attention-based multi-omics model for predicting sensitivity to a targeted kinase inhibitor, why can integrating *more* omics layers (e.g., 3\u20135 modalities) fail to improve\u2014and sometimes underperform\u2014the best 2-modality model, and what biologically motivated 2-modality pairing provides the strongest signal for response to a BRAF inhibitor like PLX-4720?",
      "answer": "Adding more modalities can yield diminishing returns because extra layers may contribute redundant information rather than independent predictive signal, while also increasing noise and model complexity; in practice this is exacerbated by cross-modality sample-size imbalance (not all cell lines have all omics layers), which makes training less stable. Consequently, careful modality selection is more important than maximizing the number of modalities. For PLX-4720 (a BRAF inhibitor), the strongest 2-modality pairing is genomics + transcriptomics: genomics captures upstream driver status (e.g., BRAF/RAS mutations) and transcriptomics captures downstream pathway activation, so together they represent MAPK pathway state more comprehensively than either layer alone.",
      "source_document": "papers/2512.12113v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an attention-based multi-omics drug response predictor, you want to include protein\u2013protein interaction (PPI) information alongside standard omics matrices. How can you convert a PPI graph plus sample-specific protein abundance into a fixed-length, per\u2013cell line embedding that can be fused with other modalities, and what is the role of using a Graph Convolutional Network (GCN) in this step?",
      "answer": "Construct a PPI network (e.g., from STRING) where nodes are proteins and edges are high-confidence interactions (optionally weighted by interaction strength). Initialize each node\u2019s features using protein expression/abundance values, then pass these node features through a two-layer GCN so that each protein\u2019s representation is updated by aggregating information from its interaction neighborhood\u2014capturing pathway context and protein modules. Finally, pool the resulting node embeddings across the graph to obtain a single vector per cell line (e.g., using top\u2011k pooling or expression\u2011weighted averaging). This pooled vector serves as the sample-specific PPI modality embedding that can be fused with other omics embeddings via attention.",
      "source_document": "papers/2512.12113v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an attention-based multi-omics drug response predictor, you already have one latent embedding vector per modality (genomics, transcriptomics, etc.) for each cell line. How can an attention-based fusion layer combine these modality embeddings into a single integrated representation, and what is the key interpretability benefit of using softmax-normalized modality weights in this setting?",
      "answer": "First project each modality embedding z^(m) into a shared latent space (same dimension) using learned linear layers. Then compute a modality-specific importance score for each sample and normalize these scores across modalities with a softmax to obtain weights \u03b1_m that sum to 1. The fused representation is formed as a weighted sum of the modality embeddings, h = \u03a3_m \u03b1_m z^(m). Because the weights are softmax-normalized and computed per sample, they can be interpreted as the relative contribution/importance of each omics modality for that particular cell line\u2019s prediction, providing a direct modality-level interpretability signal beyond simple concatenation.",
      "source_document": "papers/2512.12113v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are training an attention-based multi-omics model that fuses modality embeddings to regress a continuous drug response value (e.g., AUC or ln(IC50)) in a cancer cell-line panel. What training/validation scheme and optimization choices would you use (data split strategy, loss/optimizer/batch setup), and what specific mechanisms would you apply to control overfitting and select the final model for test evaluation?",
      "answer": "Use a three-way split into training/validation/test with an approximate 60\u201320\u201320 ratio while preserving the distribution of drug-response values. Train the regression model with Mean Squared Error (MSE) loss using the Adam optimizer (initial learning rate \u22481\u00d710\u207b\u00b3) and a batch size of 128. Control overfitting with regularization (dropout and weight decay) and apply early stopping based on validation performance with a patience of several epochs. Save the best-performing parameters (e.g., lowest validation MSE/best generalization) and use that checkpoint for the final evaluation on the held-out test set.",
      "source_document": "papers/2512.12113v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You\u2019re building an attention-based multi-omics model to predict a continuous drug response across cancer cell lines, but each omics matrix has different scales, outliers, missing values, and potentially redundant features. What preprocessing steps would you apply *before* learning modality embeddings to make training stable and enable correct cross-modality alignment, and what is the purpose of each step?",
      "answer": "A stable preprocessing pipeline includes:\n\n1) Handle missing values: impute missing entries using the per-feature median (or set to zero after normalization when the distribution allows), so models can train on complete numeric tensors without biasing strongly toward extreme values.\n\n2) Remove constant / near-zero-variance and duplicate features: drop features with near-zero variance (e.g., <1e\u22128) and duplicates to reduce redundancy and prevent training instability from uninformative dimensions.\n\n3) Scale normalization: normalize continuous features using a robust scaler (to reduce the influence of outliers) or column-wise z-score normalization to harmonize feature scales across omics layers.\n\n4) Sample ID harmonization: standardize cell-line identifiers (e.g., consistent casing and trimming) so that samples can be correctly mapped and matched across modalities and to the drug-response labels.",
      "source_document": "papers/2512.12113v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In multi-omics models for predicting targeted drug sensitivity, why can adding more omics layers (e.g., \u22653 modalities) sometimes *reduce* generalization performance, and how does an attention-based fusion mechanism address this issue? In the same setting, what role do GCN-derived protein\u2013protein interaction (PPI) network embeddings typically play when used alone versus when combined with expression- or protein-level modalities?",
      "answer": "Integrating many modalities can hurt performance because unselective fusion introduces redundancy and overlap across modalities, high inter-feature correlation/multicollinearity, and additional noise from heterogeneous data with unequal biological relevance; this can make model performance unstable rather than additive. An attention-based fusion mechanism mitigates this by learning softmax-normalized, adaptive weights per modality so the fused representation emphasizes the most informative molecular layers and down-weights less predictive ones.\n\nGCN-derived PPI embeddings primarily provide structural/functional context: when used alone they show only modest predictive power, but when combined with proteomics or transcriptomics they add complementary network-context information (capturing pathway dependencies/co-activation/functional relationships) that can improve overall prediction and interpretability compared with using expression/protein profiles alone.",
      "source_document": "papers/2512.12134v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a drug-response prediction model that incorporates protein\u2013protein interaction (PPI) topology via a graph convolutional network (GCN), how would you construct the PPI-derived embedding for each cancer cell line\u2014specifically, what are the graph\u2019s nodes/edges, what is used as node features, and what step converts node-level representations into a single sample-level vector?",
      "answer": "Use a PPI graph built from STRING where nodes are proteins and edges represent physical/functional interactions. Define node features by mapping gene-expression\u2013derived values onto the corresponding protein nodes. Apply GCN layers to propagate information and learn topology-aware node representations, then use a pooling operation to aggregate the node-level outputs into one sample-level PPI embedding vector per cell line.",
      "source_document": "papers/2512.12134v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-omics drug-response prediction pipeline, what modality-specific preprocessing steps are used to make heterogeneous omics layers comparable and robust to artifacts, and what normalization/scaling approach is applied specifically to reduce sensitivity to outliers and distribution imbalance?",
      "answer": "Each omics modality is preprocessed independently by handling missing values, removing constant features, and then scaling/normalizing the features. To reduce sensitivity to outliers and distribution imbalance, the pipeline applies robust scaling using a RobustScaler together with median normalization (i.e., median-based scaling rather than mean/variance scaling).",
      "source_document": "papers/2512.12134v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a deep-learning multi\u2011omics model for predicting targeted drug response, what two types of prediction heads can be trained from the fused modality embedding, and what training/validation strategy and evaluation metrics would you use to assess generalization on held\u2011out cell lines?",
      "answer": "Two prediction heads can be trained from the fused embedding: (1) a regression head to predict continuous drug response values such as IC50/AUC, and (2) a classification head to predict binary drug\u2011sensitivity labels. Generalization is assessed using explicit train/validation/test splits (with regularization and early stopping during training) and evaluation on the held\u2011out test set. Report regression metrics including MSE, RMSE, and R\u00b2, and classification performance using accuracy.",
      "source_document": "papers/2512.12134v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In pharmacogenomic prediction of response to a MAPK-pathway inhibitor like dabrafenib, why can mutation-level genomics alone be a poor predictor of sensitivity/resistance, and what complementary drug-response determinants are captured specifically by transcriptomic and proteomic modalities that makes their combination synergistic?",
      "answer": "Genomics (static DNA-level alterations) can be insufficient because it largely captures only mutational potential; for dabrafenib, mutations such as BRAF V600E often do not fully explain sensitivity or resistance without the cellular context of downstream signaling and adaptive rewiring. Drug response is strongly influenced by post-transcriptional regulation and pathway rewiring, with protein expression/kinase activity and downstream signaling states being more tightly coupled to pharmacologic phenotype than DNA. Transcriptomics contributes RNA-level regulatory state information, while proteomics reflects actual protein abundance and activity, including post-translational effects (e.g., degradation and phosphorylation). For MAPK inhibitors, phosphorylation dynamics and activation states within ERK/BRAF pathways are critical determinants of resistance and are often not inferable from DNA alone; combining transcriptomic and proteomic profiles therefore captures complementary transcriptional regulation and protein-level signaling/rewiring that improves prediction.",
      "source_document": "papers/2512.12134v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using classifier-free guidance for discrete molecular attributes (atom types, charges, bond orders) modeled with a continuous-time Markov chain, guidance can be applied either (i) directly to the CTMC rate matrices or (ii) to the model\u2019s denoising probability distribution before constructing the rate matrix, and in either case combined via linear or logarithmic interpolation. Which of these discrete-guidance formulations is practically most stable and effective for property alignment, and what failure mode limits the theoretically motivated logarithmic guidance on rate matrices?",
      "answer": "Applying guidance in probability space is more practically stable and achieves better property alignment than applying logarithmic guidance directly to CTMC rate matrices. In particular, logarithmic interpolation of denoising probabilities yields strong property control while remaining numerically stable across a broad range of guidance strengths (with its best MAE occurring at intermediate guidance). By contrast, logarithmic guidance on rate matrices becomes unstable: increasing the discrete guidance weight beyond a narrow range (around w2\u22481.2) causes a severe drop in molecular structural validity, which fundamentally limits its usefulness despite its theoretical grounding.",
      "source_document": "papers/2512.12198v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-modal 3D molecular generative model that represents a molecule with continuous atomic coordinates and discrete attributes (atom types, formal charges, bond orders), training often upweights coordinate reconstruction because geometry is chemically foundational. Yet during property-guided sampling with guidance weights applied separately to continuous vs. discrete modalities, what counterintuitive outcome can occur when comparing continuous-only guidance vs. discrete-only guidance for property alignment, and what mechanistic explanation justifies this outcome and informs how you should prioritize guidance across modalities at inference time?",
      "answer": "Even though coordinates receive the dominant training loss weight, discrete-only guidance (guiding atom types/charges/bonds while leaving coordinates unguided) can substantially outperform continuous-only guidance (guiding coordinates while leaving discrete variables unguided) in aligning generated molecules to target properties; hybrid guidance is best overall. The explanation is that training and guided sampling pose different optimization problems: during training, coordinates must be learned accurately to establish valid 3D geometry that supports learning the other modalities, but during sampling\u2014once geometry is robust\u2014discrete chemical features provide more direct, higher-leverage control knobs for hitting specific property values. Therefore, for optimal property alignment at inference, guidance should prioritize the discrete modalities (and combine with coordinate guidance when possible) rather than mirroring the training loss hierarchy.",
      "source_document": "papers/2512.12198v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When designing an equivariant graph neural network for 3D molecular generation, why might you choose an SE(3)-equivariant architecture rather than an E(3)-equivariant one, and what chirality-related failure can occur if reflection equivariance is enforced?",
      "answer": "Using SE(3) equivariance (rotations + translations, but not reflections) avoids forcing reflection symmetry. If reflection symmetry is enforced as in E(3)-equivariant frameworks, the model cannot represent molecular chirality correctly: mirror images can be treated as equivalent, even though they correspond to different chiral molecules, so enforcing reflection equivariance can prevent learning/generating distinct enantiomers.",
      "source_document": "papers/2512.12198v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating property-guided 3D molecular generation using a learned property regressor, what evaluation bias can occur if the regressor shares the same architecture/inductive biases as the generator, and what validation protocol can you use to verify true property alignment with quantum chemistry\u2014including any special handling needed for properties whose computation is sensitive to geometric artifacts?",
      "answer": "If the property predictor is architecturally similar to the generative model, it can share inductive biases that make it systematically \u201cagree\u201d with molecules produced by that generator, potentially overstating property alignment when you score samples only with the learned regressor. To validate alignment independently, you can run density functional theory (DFT) calculations on a subset of generated molecules (e.g., select molecules from a larger generated set and compute DFT properties at the same level of theory used for the training data). Single-point DFT on the directly generated geometries can be used for most properties, but for heat capacity (Cv) the protocol should instead use DFT properties computed on geometry-relaxed molecules because vibrational-frequency issues can make Cv unreliable on unrelaxed generated structures. In this setup, DFT confirmation can also reveal cases where the learned regressor underestimates true DFT error for some electronic properties (e.g., gaps and frontier orbital energies).",
      "source_document": "papers/2512.12198v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are choosing a guidance strategy for property-conditioned 3D molecular generation with a flow-matching model that jointly generates continuous atomic coordinates and discrete atom/charge/bond variables. How do classifier-free guidance (CFG), autoguidance (AG), and model guidance (MG) differ in the practical trade-offs among (i) property alignment, (ii) structural validity/stability, (iii) diversity/uniqueness, and (iv) sampling-time compute\u2014and what mechanistic reasons explain why CFG can degrade validity/uniqueness, why AG can improve validity relative to an unguided conditional model, and why MG often ends up close to the vanilla conditional baseline in this multi-modal setting?",
      "answer": "CFG provides the strongest property alignment among the compared methods, but tends to trade off against structural validity/stability and SMILES-level uniqueness (diversity), and it also increases inference cost because guided sampling requires two forward passes (conditional and unconditional). A key mechanism behind the validity drop is that stronger guidance\u2014especially larger weights on discrete modalities\u2014pushes samples toward regions that satisfy the property condition but are less chemically well modeled, which correlates with reduced molecule stability and uniqueness.\n\nAG improves property alignment versus a vanilla conditional model while often improving (rather than hurting) structural validity/stability. It still requires two forward passes at inference (main model + guide), but can be faster than CFG because the guide network is deliberately weaker (e.g., undertrained and/or lower-capacity), so its extra pass is cheaper. Mechanistically, AG can increase stability because it amplifies the discrepancy between the main and inferior guide in a way that steers trajectories away from poorly learned regions and toward regions the model represents more reliably.\n\nMG is the most sampling-efficient at inference (single forward pass, similar to vanilla conditional generation) and can improve stability somewhat, but in this multi-modal molecular setting it often remains close to the vanilla conditional baseline in overall performance. The limiting mechanism is that MG has difficulty leveraging different guidance scales across molecular modalities: it uses the same guidance weight for coordinates and discrete variables, and jointly learning the property embedding together with a guidance-weight embedding can be challenging, which weakens MG\u2019s ability to exploit guidance-weight effects for strong property control.",
      "source_document": "papers/2512.12198v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are designing a de novo peptide sequencing model that outputs a fixed-length token-probability matrix (CTC-style) over amino acids, PTM tokens, and blanks. What decoding strategy can you use to guarantee that the final predicted peptide (i) matches the measured precursor mass within tolerance and (ii) avoids chemically impossible PTM\u2013residue combinations? Describe the optimization formulation, the dynamic-programming state/axes, the allowed transition types, and give examples of hard biochemical compatibility rules that would be enforced.",
      "answer": "Use a precise mass-and-modification\u2013constrained decoding procedure that turns sequence generation into a mass-constrained knapsack optimization. The goal is to choose a collapsed token sequence y (amino acids and PTM tokens) that maximizes the cumulative log-probability from the model while satisfying a precursor-mass constraint: the total mass of residues/modifications in the collapsed sequence must lie within [m_pre \u2212 \u03b4, m_pre + \u03b4], where m_pre is the observed precursor mass and \u03b4 is the instrument tolerance. Solve this with a custom dynamic program in which columns are discretized mass bins of width \u03b5 (typically ~1e\u22124 Da) and rows are decoding time steps \u03c4; each DP cell d_{\u03c4,l} stores the top-B partial sequences whose total mass falls in that bin. Updates follow CTC-derived transitions: (1) add a blank token (mass unchanged), (2) repeat the previous non-blank token (merged under CTC collapse, mass unchanged), or (3) add a new amino-acid/PTM token (mass increases and the candidate is routed to the appropriate next-step mass bin). During the \u201cnew token addition\u201d transition, enforce hard biochemical constraints by checking PTM compatibility with the preceding residue and pruning invalid PTM\u2013residue pairs immediately (e.g., phosphorylation allowed only on Ser/Thr/Tyr; methylation/acetylation/ubiquitination on Lys/Arg; oxidation on Met; N-terminal modifications like acetylation allowed at sequence start).",
      "source_document": "papers/2512.12272v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In de novo peptide sequencing, you don\u2019t naturally have a target\u2013decoy database to estimate false discovery rates. Describe a database-search\u2013assisted, semi-supervised FDR calibration strategy that can be used instead. Your answer should specify (i) how to construct the calibration (\u201canchor\u201d) set, (ii) how to label a de novo prediction as a proxy false discovery using a spectrum\u2013peptide match score, (iii) how the cumulative FDR curve and the 1% confidence threshold are computed from the ranked anchor list, and (iv) one way to stress-test whether the resulting FDR control remains valid when the effective search space is greatly expanded.",
      "answer": "A semi-supervised calibration can be done by using high-confidence database-search identifications as anchors to map de novo confidence scores to an empirical FDR.\n\n(i) Calibration/anchor set: For a given MS project, run a standard database search (e.g., MaxQuant) and filter to 1% FDR (at PSM and peptide levels). Run the de novo model on all MS/MS spectra to produce a predicted peptide and a model confidence score S_conf. Intersect the de novo predictions with the high-confidence database-search PSMs to form the calibration set.\n\n(ii) Proxy false discovery labeling: For each anchor spectrum, compute hyperscores (spectrum\u2013theoretical match scores) for both the de novo peptide (H_pred) and the database peptide (H_db). Define a binary proxy false-discovery indicator I_FD(i)=1 if H_pred(i) < H_db(i), and 0 otherwise\u2014i.e., if the database assignment yields a significantly better spectral match than the de novo prediction, the de novo result is treated as a proxy false discovery.\n\n(iii) Cumulative FDR and 1% threshold: Rank anchor PSMs by the de novo confidence score S_conf (descending). At each rank k, compute cumulative FDR_k = (sum_{i=1..k} I_FD(i)) / k. Choose the 1% threshold \u03c4_1% as the S_conf score of the PSM immediately preceding the first rank k where FDR_k exceeds 0.01. Apply \u03c4_1% to filter the full de novo output (including spectra not identified by the database search).\n\n(iv) Stress test under expanded search space: Use paired entrapment-based FDP estimation (one-to-one pairing, r=1) by adding entrapment (mimic) sequences and comparing target vs entrapment scoring; then increase complexity dramatically (e.g., r=100 entrapment sequences per target) to test whether estimated false discovery proportion stays controlled at strict thresholds such as 1% FDR even when the effective search space is expanded by up to 100\u00d7.",
      "source_document": "papers/2512.12272v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In PTM-aware de novo peptide sequencing, you may correctly predict the peptide backbone and the multiset of PTM types present, but the exact residue positions of those PTMs can be ambiguous. Describe a post-inference PTM site-localization procedure that (i) constructs the candidate set of positional isomers under biochemical compatibility constraints, (ii) scores each isomer using the model\u2019s sequence-level confidence, and (iii) converts these isomer scores into a per-residue localization probability for a specific PTM type. Your answer should include the explicit marginalization formula for P(site_{i,m}) and explain how the procedure keeps localization probabilities for different PTM types conceptually separate even when multiple PTM types could occur at the same residue.",
      "answer": "A post-inference localization strategy can be done by enumerating and rescoring positional isomers.\n\n1) Candidate isomer construction under constraints: Start from the initially predicted sequence and extract the peptide backbone by removing all PTM tokens while recording the types and counts of PTMs. Then generate the full set A of theoretically plausible positional isomers by permuting (re-assigning) the recorded PTMs across all residues on the backbone that satisfy user-defined biochemical constraints (i.e., only place each PTM type on residues that are chemically allowed for that PTM).\n\n2) Isomer scoring: For each generated isomer k, compute a sequence-level confidence score S_k using the model\u2019s path-marginalized sequence scoring function (the same confidence scoring used for de novo predictions).\n\n3) Per-site probability by marginalization: For residue i and PTM type m, let M_{i,m} \\subset A be the subset of isomers in which residue i carries modification m. The localization probability is\n\nP(site_{i,m}) = (\\sum_{k \\in M_{i,m}} S_k) / (\\sum_{j \\in A} S_j).\n\nThis yields a probability distribution over backbone positions for each PTM type.\n\nDecoupling across PTM types: Localization probabilities are computed independently for distinct PTM types; even if a residue could accept multiple PTM types, the numerator for PTM type m sums only isomers where residue i is modified specifically by m, so confidence mass for other PTM types at the same residue does not contribute to P(site_{i,m}).",
      "source_document": "papers/2512.12272v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a non-autoregressive, CTC-trained de novo peptide sequencing model that outputs a fixed-length (T) probability matrix over amino-acid tokens, PTM tokens, and a blank, how can you define and compute a *sequence-level confidence score* for a single greedy-decoded prediction so that it accounts for alignment uncertainty? Your answer should (i) give the confidence definition as a marginalization over all CTC alignment paths that collapse to the predicted peptide, (ii) explain how the forward-algorithm dynamic program is used to compute this sum efficiently, and (iii) state what collapsing operations the CTC map performs (what happens to blanks and repeated tokens).",
      "answer": "A sequence-level confidence for a predicted peptide Y_pred is defined as the *total probability mass* assigned by the model to all length-T latent token sequences (alignments) that reduce to Y_pred under CTC collapsing. Formally, if the decoder outputs per-time-step probabilities P(y_t=\u00b7|S) over the vocabulary (amino acids, PTM tokens, and blank \u03b5), then\n\nConfidence(Y_pred) = P(Y_pred|S) =  \\sum_{\\pi \\in \\Gamma^{-1}(Y_{pred})} \\prod_{t=1}^{T} P(y_t = \\pi_t\\mid S),\n\nwhere \u0393 is the CTC collapse map and \u0393^{-1}(Y_pred) is the set of all alignments that collapse to Y_pred.\n\nThis sum is computed efficiently with the CTC forward algorithm (a dynamic program over time steps and positions in the target/collapsed sequence), analogous to the DP used to compute the CTC training likelihood: instead of enumerating exponentially many alignments, the DP recursively accumulates the total probability of reaching each prefix/state.\n\nThe CTC collapse map \u0393 merges consecutive identical *non-blank* tokens (repeats become a single token) and removes all blank tokens \u03b5; thus multiple raw alignments with inserted blanks and/or repeated tokens can correspond to the same final peptide sequence.",
      "source_document": "papers/2512.12272v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are designing a Transformer-based de novo peptide sequencing model that directly maps an MS/MS spectrum to a peptide sequence (including PTM tokens) using a non-autoregressive decoder. How can the spectrum be represented and encoded so that it is compatible with self-attention (peak selection and feature/positional encoding), and how is this encoded spectrum then consumed by the decoder (self-attention vs cross-attention), including one example of extra precursor information that can be injected during training to stabilize non-autoregressive decoding?",
      "answer": "A practical design is to treat the MS/MS spectrum as a sequence of peaks and feed it to a Transformer encoder. The input spectrum is first reduced to a fixed-size peak list by keeping the top 180 most intense peaks (to balance resolution and compute). Each peak\u2019s m/z value is mapped into an embedding using sinusoidal encoding; the peak intensity is embedded with the same sinusoidal mapping and then added (summed) with the m/z embedding to form the initial per-peak representation E0. This peak sequence E0 is then refined by stacked Transformer encoder self-attention layers, producing a final spectral embedding E(m) that summarizes the spectrum.\n\nA non-autoregressive peptide decoder then predicts all token positions in parallel. Decoder token embeddings are updated through (i) decoder self-attention (to model dependencies among output positions) and (ii) cross-attention that attends to the encoder features E(m), letting each output position condition on the spectrum. In addition to positional encodings, training can inject partial precursor-side information for \u201cglancing\u201d, specifically the precursor m/z and charge, to help stabilize non-autoregressive learning.",
      "source_document": "papers/2512.12272v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In knowledge-graph drug repurposing, simply removing known drug\u2013disease edges for a held-out disease can still leak label information through therapeutically related diseases. What split construction strategy can be used to mitigate this, and what concrete criteria can be used to identify and exclude \u201crelated diseases\u201d from the training graph?",
      "answer": "Use disease-centric splits: for each evaluation disease, remove all drug\u2013disease edges for that disease before training, and also remove drug-related information for diseases judged related enough to potentially share treatment strategies. Related diseases can be identified with a two-stage procedure: (1) nominate candidate related diseases using any of three lightweight similarity metrics\u2014cosine similarity of disease-name embeddings from Clinical BioBERT (nominate if >0.98), Levenshtein token set ratio on disease names (nominate if >80), or one-hop neighborhood overlap in the KG quantified by Jaccard similarity (nominate if >0.1); (2) have an LLM act as a semantic rater scoring each nominated disease from 1\u20135 for clinical/biological/therapeutic similarity to the held-out disease, retaining those with rating \u22653 and excluding the rest before training/evaluation.",
      "source_document": "papers/2512.13724v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a message-passing GNN for binary link prediction on an undirected biomedical knowledge graph where each relation is represented by both a forward edge (u,r,v) and a corresponding reverse edge (v,r\u207b\u00b9,u), what information-leakage failure mode can occur during neighborhood (mini-batch) sampling, and what two concrete countermeasures can be used to prevent the model from trivially recovering the label of a queried edge?",
      "answer": "Leakage can occur because, if the sampled message-passing subgraph includes the reverse edge corresponding to a target edge in the batch, the model can receive direct information about the very relationship it is trying to predict (effectively \u201cseeing\u201d the answer) and memorize direct connections rather than learn structural patterns.\n\nTwo concrete countermeasures are:\n1) During construction of the message-flowing graph (the L-hop neighborhood subgraph used for a mini-batch), exclude the reverse edge for every edge in the batch so the model cannot pass messages over the direct u\u2194v connection it is being asked to score.\n2) When splitting edges into train/validation/test, assign each forward edge and its corresponding reverse edge to the same partition, so one direction cannot appear in training while the other direction is evaluated, which would also leak the label.",
      "source_document": "papers/2512.13724v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building an SO(3)-equivariant kernel that maps type-k spherical tensor features to type-l features in a geometric GNN, why can the learnable part be implemented as a radial feed-forward network that depends only on the inter-node distance, and what set of weights must it output to parameterize the kernel for multi-channel features?",
      "answer": "Because the angular dependence required for equivariance is already captured by fixed basis kernels built from spherical harmonics/Clebsch\u2013Gordan structure, the learnable component can modulate only the radial distance without breaking equivariance. The key reason is that the scalar distance \u2225x_ij\u2225 is SO(3)-invariant (rotating the graph does not change vector lengths), so a function \u03c6(\u2225x_ij\u2225) is invariant to rotations: \u03c6(\u2225R_g x_ij\u2225)=\u03c6(\u2225x_ij\u2225). To parameterize the type-k\u2192type-l kernel, the radial FFN must output a separate weight for each basis kernel indexed by J in the allowed coupling range J = |k\u2212l|,\u2026,k+l; and for multi-channel features it must produce those J-weights for every input\u2013output channel pair, i.e., a (num bases)\u00d7(input channels m_i)\u00d7(output channels m_o)-dimensional set of weights (equivalently \u03c6^{lk}(\u2225x_ij\u2225) \u2208 R^{(2 min(l,k)+1) m_i m_o}).",
      "source_document": "papers/2512.13927v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an SE(3)-equivariant Transformer that builds edge features with spherical harmonics, what makes per-edge spherical-harmonic evaluation a computational bottleneck, and what precomputation strategy (including the maximum degree J that must be covered) can be used to reduce this cost? Briefly describe how using recursive associated Legendre polynomial (ALP) relations enables the precomputation.",
      "answer": "Evaluating spherical harmonics on every edge is expensive because, for each required degree J, the spherical harmonic must be computed for all 2J+1 orders m; and these computations are needed for all J values required to map between every pair of input and output feature degrees on every edge, so the number of evaluations grows rapidly with graph size and feature degree. To reduce this cost, the model precomputes the spherical-harmonic projections of the angular unit displacement vector for every edge for all J up to twice the maximum feature degree (since the largest degree that can appear in coupling is J_max = k + l). This precomputation is made efficient by computing ALPs recursively: compute boundary polynomials at m=J directly, derive m=J\u22121 from those, then obtain the remaining m\u22650 ALPs from a recurrence using the previous two degrees, and finally get ALPs for negative m from their positive counterparts via a simple coefficient/factorial relationship; intermediate values are stored and reused across edges.",
      "source_document": "papers/2512.13927v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an SO(3)-equivariant self-attention layer on a geometric graph where node features are grouped into type\u2011l spherical-tensor channels, how can you compute attention weights that are (i) invariant to rotations, yet (ii) unique for each node and each output channel, and then use them to mix the node\u2019s own features with incoming value messages without breaking equivariance? Describe the sequence of operations from raw scores to the final weighted sum, and explain why the raw scores are guaranteed to be scalars (type\u20110) even when l>0.",
      "answer": "To keep value updates equivariant while allowing attention-style, node-specific channel mixing, the attention *weights* must be rotationally invariant scalars, while the *values* being mixed remain type\u2011l tensors.\n\n1) **Concatenate features to be mixed (same degree only).** For each degree l, form a concatenated type\u2011l feature matrix f_cat,i^l for node i by concatenating the node\u2019s saved input type\u2011l channels with the type\u2011l value-message channels from neighbors. This gives an array of shape (m_cat, 2l+1) per node, where m_cat is the number of concatenated type\u2011l channels.\n\n2) **Form raw attention scores via dot products of type\u2011l channels.** Compute all pairwise dot products between the m_cat channels:\n   S_i^l[a,b] = f_cat,i^l[a] \u00b7 f_cat,i^l[b], producing an m_cat\u00d7m_cat score matrix.\n   **Why invariant/type\u20110:** each f_cat,i^l[a] transforms by the same Wigner\u2011D^l(g) under rotation g, and Wigner\u2011D^l is an orthogonal/unitary representation, so (D^l f_a)\u00b7(D^l f_b)=f_a\u00b7f_b. Thus the dot product is unchanged by rotation and is a scalar (type\u20110), even when l>0.\n\n3) **Convert scores into output-channel\u2013specific weights with a per-degree FFN.** Flatten S_i^l to a vector of length m_cat^2 (optionally clamping magnitude to avoid vanishing gradients), then pass it through a learned feed-forward network specific to degree l that includes LayerNorm, an element-wise nonlinearity (e.g., leaky ReLU) applied to these scalar scores, and a linear map that outputs a vector of length (m_cat\u00b7m_o), where m_o is the number of output channels for degree l.\n\n4) **Reshape and normalize with softmax.** Reshape the FFN output into a matrix W_i^l of shape (m_o, m_cat). Apply softmax across the m_cat dimension *row-wise* so each output channel\u2019s weights sum to 1.\n\n5) **Compute attentive self-interaction (channel mixing) as a weighted sum.** Multiply W_i^l (m_o\u00d7m_cat) by f_cat,i^l (m_cat\u00d7(2l+1)) to obtain the output type\u2011l tensor for node i with shape (m_o, 2l+1). Because the weights are scalars and the combination is a weighted sum within the same degree, the result remains a valid type\u2011l spherical tensor, preserving SO(3) equivariance.",
      "source_document": "papers/2512.13927v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an SE(3)/SO(3)-equivariant GNN that carries type\u2011l spherical-tensor features (with 2l+1 components per channel), why does applying a standard element-wise ReLU to those components generally break equivariance, and what \u201cnorm nonlinearity\u201d construction can be used instead to introduce ReLU-like nonlinearity while preserving equivariance? Describe the sequence of computations performed per type\u2011l channel group and explain which quantities are guaranteed to be rotation-invariant.",
      "answer": "A component-wise ReLU is not rotation-equivariant for l>0 because a rotation mixes the 2l+1 components within a type\u2011l feature; applying a nonlinear function independently to each component does not commute with that mixing, so rotating then ReLU-ing differs from ReLU-ing then rotating.\n\nA norm nonlinearity preserves equivariance by (1) only applying learned nonlinearities to rotation-invariant scalars (norms) and (2) re-scaling the original direction (\u201cphase\u201d) of each type\u2011l vector without changing how it transforms under rotations:\n\nFor each node and each feature type l (handled separately):\n1) Compute an L2 norm per type\u2011l channel cl across its 2l+1 components:  ||f^l_cl||_2 = sqrt( sum_{m=-l}^l (f^{l m})^2 ).  Collect these norms into an mo-dimensional vector (mo = number of type\u2011l channels). These norms are rotation-invariant because rotations are represented by orthogonal/unitary Wigner-D matrices that preserve Euclidean length.\n2) Form a unit-length \u201cphase\u201d (direction) for each channel by dividing the channel\u2019s 2l+1-vector by its norm (with an epsilon clamp to avoid division by zero):  phase = f / ||f||.\n3) Pass the mo-dimensional norm vector through an FFN acting only on these scalars: apply normalization across channels, apply a ReLU nonlinearity to the normalized norms, and optionally apply a linear layer (mo\u2192mo). This produces one learned scalar weight per type\u2011l channel.\n4) Multiply (broadcast) these per-channel scalar weights back onto the corresponding unit vectors:  output_cl = weight_cl * phase_cl.\n\nBecause all nonlinear processing happens on the norms (type\u20110 invariants) and the output is obtained by scaling the original type\u2011l vectors by scalars, the resulting output features remain SE(3)/SO(3)-equivariant.",
      "source_document": "papers/2512.13927v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an SE(3)-equivariant Transformer operating on geometric graphs with spherical-tensor feature types (degrees l) and multiple channels per type, how can multi-head self-attention be implemented without introducing separate query/key/value kernels per head, and what is the rationale for why simply splitting same-type channels into H heads still provides multiple \u201crepresentation subspaces\u201d? Describe (i) how queries/keys/values are partitioned and used to compute H attention weights per edge, and (ii) how the per-head value messages are recombined to form the final message.",
      "answer": "Multi-head attention can be done by reusing the same query/key/value kernel constructions as in single-head attention and then *splitting the channels of features of the same type* into H subsets (heads), computing attention separately within each subset.\n\nRationale: even with one set of kernels, different *channels* of the query/key/value embeddings already correspond to different learned transformations because (for each edge) each key-embedding channel is produced using key matrices parameterized by a unique set of learnable radial weights trained for that channel; the same holds across query and value channels. Thus, multiple channels already capture multiple representation subspaces, and multi-head attention just computes multiple attention weight sets to provide multiple interaction \u201cpaths\u201d without requiring separate kernels.\n\nMechanism:\n(i) For each type l, divide (split/reshape) the query and key embedding channels into H heads so that there are H query vectors and H key vectors, each of dimension d/H (where d is the full key/query embedding dimension). This produces tensors shaped like (H, d/H) per node for queries and per edge for keys. For each node i and head h, take the dot product between q_i^h and each incoming edge key k_{ij}^h to get a scalar score per edge per head, yielding H attention weights per edge (after the usual scaling/softmax).\n(ii) Split the value embedding channels the same way. For each edge and head h, use the head-specific attention weight to scale the corresponding subset of value channels (the value message subset) from that edge. Then concatenate the per-head value subsets back together to reconstruct the full value embedding per edge, and finally aggregate across edges (sum over neighbors) to get the updated feature tensor for the node.",
      "source_document": "papers/2512.13927v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Bayesian optimization for molecular discovery with a very large discrete candidate pool, you may pre-cluster molecules to reduce the cost of evaluating an acquisition function (AF) on every candidate each round. Describe a statistically principled cluster-selection procedure that uses only the observed BO data to decide which clusters to keep for AF evaluation, and explain which statistical tests are used (and why they are appropriate) before excluding any clusters.",
      "answer": "Precompute molecular representations and assign each molecule to a cluster; then, at each BO round, use the currently observed data Dt to statistically filter clusters and evaluate AFs only within the retained clusters (intersected with the selected tree partition). Specifically, run Welch\u2019s ANOVA on the observed property values grouped by cluster to test whether cluster means differ; Welch\u2019s ANOVA is appropriate because it is robust to heterogeneous variances across clusters. If the ANOVA indicates significant differences at a chosen p-value, apply the Games\u2013Howell post-hoc test (also robust to unequal variances) to identify and exclude outlier clusters with significantly poorer average property values using the same p-value. The BO initialization D0 is constructed by uniformly sampling candidates from each cluster to support these tests.",
      "source_document": "papers/2512.13935v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a likelihood-free Bayesian optimization (BO) approach that avoids learning an explicit surrogate p(y\\mid x, D_t), how can you convert acquisition-function estimation into (i) estimating a density ratio between \u201cpromising\u201d and \u201cnon-promising\u201d regions and (ii) training a binary classifier? Define the two densities, the \u03b3-relative density ratio, how the threshold \u03c4 is chosen via \u03b3, and how the resulting classifier output is transformed into an acquisition value used to rank candidates.",
      "answer": "You can estimate an acquisition function by reframing BO as distinguishing \u201cpromising\u201d observations (those with property y above a threshold \u03c4) from \u201cnon-promising\u201d ones and then using density-ratio / classification outputs as the acquisition score.\n\n1) Split the data distribution using a threshold \u03c4:\n- Define l(x) = p(x \\mid y \\le \u03c4, D_t) for the non-promising region.\n- Define g(x) = p(x \\mid y > \u03c4, D_t) for the promising region.\n\n2) Use the \u03b3-relative density ratio as a PI-equivalent acquisition surrogate:\n- Define\n  r_\u03b3(x) = g(x) / (\u03b3 g(x) + (1\u2212\u03b3) l(x)).\n- Choose \u03c4 as the \u03b3-th quantile of the observed y values so that\n  \u03b3 = p(y > \u03c4 \\mid D_t).\n  With this choice, r_\u03b3(x) corresponds to a Probability-of-Improvement (PI)-type objective.\n\n3) Estimate r_\u03b3(x) via binary classification instead of explicitly estimating l and g:\n- Introduce a class label c = 1(y > \u03c4).\n- Train a classifier \u03c0_\u03b8(x) \u2248 p(c=1 \\mid x, D_t). Under the likelihood-free formulation, this classifier output is proportional to the density-ratio-based quantity (\u03c0_\u03b8(x) \u2248 \u03b3 r_\u03b3(x)).\n- More generally, the classifier is trained by minimizing a weighted binary cross-entropy objective that uses the utility u(y;\u03c4) as a weight:\n  L_{D_t,\u03c4}(\u03b8) = E_{(x,y)\u223cD_t}[ \u2212u(y;\u03c4) log \u03c0_\u03b8(x) \u2212 log(1\u2212\u03c0_\u03b8(x)) ].\n\n4) Convert the trained classifier into an acquisition value:\n- After optimizing \u03b8*, the likelihood-free acquisition function is the odds of the classifier:\n  \u03b1(x; D_t, \u03c4) = \u03c0_{\u03b8*}(x) / (1 \u2212 \u03c0_{\u03b8*}(x)).\nThis \u03b1(x) can then be maximized over candidates to pick the next molecule for evaluation.",
      "source_document": "papers/2512.13935v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a discrete molecular Bayesian optimization method that recursively partitions the candidate set into a search tree (with a local model/acquisition estimate per node), how can you select which branch/partition to descend into while balancing exploitation of high-value regions and exploration of under-sampled regions? Also, what algorithmic safeguard can you use when the most promising leaf partition contains no valid candidates (a failure mode that can occur in discrete spaces)?",
      "answer": "Use a tree-policy score (UCT-style) to choose child partitions from root to leaf. For a node k, maintain the visit count n_k = |D_t \u2229 \u03a9_k| and node value v_k = (1/n_k)\u2211_{i=1}^{n_k} y_i (average observed property in that partition). Then compute an exploration\u2013exploitation score such as\nS_k^UCT = v_k + 2\u03bb\u00b7sqrt(2 log(n_p)/n_k),\nwhere n_p is the parent\u2019s visit count (and \u03bb controls exploration). An alternative is a variance-aware score S_k^Var = v_k + 2\u03bb\u00b7sqrt(var_k) with var_k = (1/n_k)\u2211(y_i\u2212v_k)^2. Descend by repeatedly selecting the child with the highest score until a leaf is reached.\nIn discrete candidate spaces, a selected leaf may contain no valid/unseen candidates; to handle this, use backtracking: if no candidates reach the chosen leaf, fall back to its parent node (and continue backing up as needed) to select a partition that does contain valid candidates; in the worst case, revert to the non-partitioned baseline search strategy.",
      "source_document": "papers/2512.13935v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a tree-based, likelihood-free Bayesian optimization scheme where each tree node learns a binary classifier on top of foundation-model molecular embeddings (used both to split the candidate set and to define a local acquisition function), how can meta-learning be used to stabilize the node classifiers in a low-data regime? Describe (i) what parameters are treated as the shared \u201cmeta\u201d initialization versus node-specific parameters, (ii) the inner-loop update performed at each node, and (iii) the outer-loop/meta update rule (including how the meta model is updated from node-adapted weights). Also explain how the procedure changes when you additionally do parameter-efficient fine-tuning (LoRA/PEFT) of the feature extractor: which parameters are held fixed while training node classifiers, and how the PEFT parameters are updated across nodes.",
      "answer": "The approach maintains one shared/meta classification head (meta-parameters \u03b8) and, for each splittable node k, a node-adapted copy \u03b8k. At node k, the method fixes the feature extractor/PEFT parameters w and runs an inner-loop optimization: starting from the shared initialization \u03b8, it performs K SGD steps on the node\u2019s classification loss L^{\u03c4k}_{Dt\u2229\u03a9k}(\u03b8,w) (where labels are obtained by thresholding y using the node\u2019s quantile threshold \u03c4k), producing \u03b8k. After adapting to node k, it performs a Reptile-style outer update of the shared initialization by moving \u03b8 toward the node-adapted weights: \u03b8 \u2190 \u03b8 + \u03b7(\u03b8k \u2212 \u03b8), and repeats this sequentially over nodes in the tree (storing the set of nodes N and their \u03b8k). \n\nWith PEFT/LoRA enabled, the procedure alternates roles: during the node-wise inner-loop classifier training, w is held fixed while \u03b8k are adapted and \u03b8 is meta-updated as above; then, after processing the nodes, \u03b8 is held fixed and the PEFT parameters w are updated by optimizing the average of the node losses across the visited/splittable nodes, i.e., updating w using (1/|N|)\u2211_{k\u2208N} L^{\u03c4k}_{Dt\u2229\u03a9k}(\u03b8,w).",
      "source_document": "papers/2512.13935v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking Bayesian optimization methods for molecular discovery across multiple datasets with different property scales, what two evaluation metrics can you use to (i) compare normalized progress toward the global optimum over BO iterations and (ii) measure the overall quality of the set of molecules found so far (not just the single best)? Define both metrics mathematically (including what y*, y0, y*_t, and y_i denote) and explain why the second metric is useful in molecular discovery.",
      "answer": "Two metrics are used:\n\n1) **GAP (normalized progress of the best-so-far value):**\n\\[\\mathrm{GAP}_t := \\frac{y_t^{*}-y_0}{y^{*}-y_0},\\]\nwhere \\(y^{*}\\) is the dataset\u2019s global optimum, \\(y_0\\) is the initial best value (best among the initial points), and \\(y_t^{*}\\) is the best observed property value up to BO iteration \\(t\\). This normalizes improvement so trajectories are comparable across datasets with different scales.\n\n2) **Average regret (quality of the discovered set):**\n\\[\\mathrm{Regret}_t := \\frac{1}{t}\\sum_{i=1}^{t}(y^{*}-y_i),\\]\nwhere \\(y_i\\) is the property value of the \\(i\\)-th evaluated molecule and \\(y^{*}\\) is the global optimum. This reflects the overall quality of all molecules discovered so far, which is useful because in molecular discovery the goal is often to return a set of high-performing candidates for further downstream analysis/evaluation rather than only a single best molecule. When aggregating across datasets, these average regrets are normalized for comparability.",
      "source_document": "papers/2512.13935v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In antigen-level MHC-II epitope discovery, peptide-level and epitope-level metrics can miss whether a model efficiently localizes immunologically relevant regions along a full antigen sequence. How can you evaluate this localization performance by defining (i) region-level coverage and (ii) redundancy of predicted regions, and how is a single summary score (CR-AUC) constructed and interpreted from these quantities?",
      "answer": "Define ground-truth antigen regions as contiguous non-overlapping residue segments where the per-residue epitope label is nonzero (overlapping epitopes are merged into one region). Define predicted regions as contiguous residue segments whose per-residue prediction exceeds a threshold (for peptide-based models, per-residue scores can be approximated by aggregating peptide scores over a fixed-length sliding window of length 9, the conventional binding-core size).\n\n(i) Region-level coverage measures how well predicted regions cover the ground-truth regions, computed as a weighted sum over ground-truth regions of the fraction of residues in each ground-truth region that are covered by any predicted region (with weights derived from log-scaled per-residue labels within each ground-truth region and normalized across regions).\n\n(ii) Redundancy measures how much of the antigen is proposed as positive: the total number of residues included in all predicted regions, normalized by the antigen length. It is the opposite of sparsity.\n\nTo summarize the tradeoff, vary the prediction threshold from 0 to 1 to obtain a coverage\u2013redundancy curve and report the area under this curve (CR-AUC). Higher CR-AUC means a more favorable tradeoff\u2014achieving high coverage while keeping redundancy low. A steep initial rise indicates that high-confidence predictions already localize the ground-truth regions; a shallow/flattened curve indicates that adding more predicted regions does not substantially improve coverage. CR-AUC is normalized to lie in [0,1] and is comparable across models and antigens.",
      "source_document": "papers/2512.14011v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are constructing train/validation/test splits for MHC-II peptide binding affinity (BA) and eluted-ligand presentation (EL) prediction, and you also want an antigen-level EL task where models output residue-level scores along a full antigen sequence. What concrete splitting constraints and procedures can you apply to (i) prevent information leakage (including between BA and EL when doing joint training), (ii) make the peptide test set strictly out-of-distribution with respect to binding motifs, and (iii) build an antigen-level validation set that is also out-of-distribution while still allowing a realistic use case for antigens?",
      "answer": "A strict split construction can combine: (i) leakage prevention across tasks by first selecting candidate BA and EL test samples via a fixed time cutoff (e.g., post-2020 records held out), then reassigning any peptide that appears in the other task\u2019s training set back into training (to avoid leakage during joint training); additionally moving peptides that lack antigen annotation into training so antigen-level modeling/evaluation is feasible. (ii) A binding-motif/sequence OOD constraint by enforcing that no 9-mer subsequence occurring in any test peptide appears anywhere in the training peptides; this can be implemented by iteratively checking for 9-mer overlaps and moving violating peptides from test to training until no overlaps remain. (iii) For antigen-level validation, choosing initial validation samples by sampling peptide clusters (e.g., from CD-HIT clustering) rather than using a year cutoff, then moving peptides with missing antigen information and any peptides with 9-mers seen in training into training; this makes observed peptides OOD for antigen-level tasks. The same antigen sequence can still be allowed to appear across splits to reflect the practical scenario of exploring alternative peptides within an antigen even when a known epitope exists. Validation for peptide-level tasks can be expanded with peptides lacking antigen information and selected with stratified sampling to match MHC-II distribution while controlling peptide overlap with training.",
      "source_document": "papers/2512.14011v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building an MHC-II binding/presentation predictor that uses AlphaFold-style predicted MHC-II structures, how can you quantify the model\u2019s robustness to structural prediction errors using a controlled perturbation experiment? Describe (i) how to inject realistic coordinate noise using per-atom confidence (pLDDT), (ii) how to summarize sensitivity in the model outputs, and (iii) what qualitative robustness difference you would expect between a multimodal sequence+structure model and a structure-only model.",
      "answer": "One robustness check is to measure how much the model\u2019s predictions change when the input structure is perturbed in a way that mimics AlphaFold3 uncertainty.\n\n(i) Noise injection: add zero-mean Gaussian noise to atomic coordinates at several base scales (e.g., \u03c3 \u2208 {0.1, 0.3, 0.5}), but down/up-weight the noise per atom using the atom\u2019s pLDDT confidence so that low-confidence atoms are perturbed more. Concretely, sample coordinate noise from N(0, \u03c3(1 \u2212 pLDDT/100)) for each atom.\n\n(ii) Sensitivity summary: for each MHC-II structure and \u03c3, generate multiple perturbed replicas (e.g., 5), convert each perturbed structure into the model\u2019s structure tokens/features, run the model, and compute the variance of the predicted outputs across the perturbed replicas; report this output variance averaged over all peptide\u2013MHC-II test pairs (for both BA and EL tasks).\n\n(iii) Expected outcome: the sequence+structure model should have highly stable outputs (low variance) under structural perturbations because the sequence modality provides a robust alternative signal, whereas a structure-only model should show much larger output variance as \u03c3 increases, indicating higher sensitivity to structural noise despite potentially similar nominal accuracy.",
      "source_document": "papers/2512.14011v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In peptide eluted-ligand (EL) presentation prediction for MHC-II, datasets can be globally balanced but highly imbalanced per allele (e.g., many alleles have only positive peptides), making randomly sampled proteome \u201cdecoys\u201d too easy and potentially non-biological. Describe a concrete strategy to (i) generate harder, antigen-aware negative peptides while avoiding false negatives due to overlap with known binders, (ii) reduce remaining per-allele imbalance during training via sampling, and (iii) add an auxiliary objective that still provides learning signal for alleles with only-positive EL labels. Include the key algorithmic details needed to implement these three components.",
      "answer": "(i) Generate antigen-aware hard negatives by extracting neighboring peptides from the same source antigen as each experimentally verified positive peptide, so negatives share similar biological context and processing. Use estimated binding cores (from motif deconvolution with MoDec) as guidance so that negative peptides are allowed to overlap positive peptides as long as they do not violate/overlap the binding core; create four negative augmentations per positive peptide. Additionally, increase robustness by randomly extending the peptide at both ends and randomly shifting the peptide window by 1 residue based on the source antigen.\n\n(ii) Mitigate residual imbalance with balanced sampling during training: at each training step for a positive-labeled peptide, sample an augmented peptide with 0.5 probability from the positive augmentation set or from the negative augmentation set (augmentations exist only for experimentally verified positives). A similar \u201cvalid subsequence\u201d sampling is used for the antigen-level task.\n\n(iii) Add an auxiliary binding-core prediction task using pseudo-label binding cores from MoDec to provide additional supervision even when an allele\u2019s EL labels are all positive. Implement it by taking the peptide\u2013MHC-II cross-attention map, encoding it with a 2D convolution, then predicting the binding core with a sliding-window (1D convolution) head using window size 9 (the conventional core length). Train this auxiliary task with binary cross-entropy and weight its loss by 0.1 (as a regularizer because labels are estimated). This auxiliary objective helps the model localize meaningful interaction patterns from peptide\u2013MHC-II interactions even in cases with no negative EL labels for an allele.",
      "source_document": "papers/2512.14011v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a residue-level (antigen-based) MHC-II eluted-ligand presentation model, full antigen sequences can be too long for GPU memory, but naive truncation can destroy the biological supervision signal. Describe a concrete truncation-and-sampling scheme that (i) makes training feasible, (ii) avoids cutting through known epitopes, and (iii) controls the resulting label imbalance. Also explain the trade-off of choosing the truncation window length k and one strategy to reduce sensitivity to the choice of a single fixed k.",
      "answer": "Use windowed training on antigens: truncate each antigen to subsequences of at most length k to avoid CUDA out-of-memory issues, but do not sample arbitrary length-k crops. Instead, sample only from \u201cvalid\u201d regions where no known epitope is cut through, so the subsequence preserves intact biologically meaningful epitope regions. For balanced optimization, at each training step group valid truncated subsequences into positive vs negative groups, where a subsequence is positive if it contains at least one known epitope, and then sample from either group to maintain balanced training. The window length k trades off context vs learnability: decreasing k (e.g., toward peptide-scale) progressively reduces antigen modeling to peptide modeling and hurts performance, while increasing k provides richer context but makes training harder because residue-level labels become more imbalanced. To avoid hand-picking a single k, use randomized window sizing by sampling k each iteration from a predefined set (e.g., {64, 128, 256, 512, 1024}) rather than fixing it.",
      "source_document": "papers/2512.14011v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When aligning multiple clinical modalities (e.g., WSI, RNA-seq, SNP, CNV, methylation) in a shared embedding space, what is the methodological problem with a standard CLIP-style softmax contrastive loss, and how does a sigmoid/BCE (SigLIP-style) formulation define positives/negatives while handling missing-modality samples?",
      "answer": "A CLIP-style softmax loss assumes one correct positive per row of the similarity matrix; with multiple modalities per patient there are multiple valid positives, so the softmax normalization forces them to compete, which is undesirable. A SigLIP-style sigmoid/BCE treats every similarity entry as an independent binary classification. For each modality pair (i,j), logits are computed between sample n\u2019s modality i (projected into j\u2019s space) and sample k\u2019s modality j (projected into i\u2019s space). The binary target is 1 only when n and k are the same patient/sample AND both modalities i and j are present for that patient (a validity mask); otherwise the target is 0. Negatives are all other cross-sample pairs in the batch, and missing-modality cases are excluded from being positives via the ValidPair mask.",
      "source_document": "papers/2512.14019v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In transformer-based aggregation of whole-slide image (WSI) patch tokens, how can you incorporate spatial positional information without creating spurious interactions between physically disconnected tissue fragments on the same slide, and what role does alternating \u201cpositional\u201d and \u201cposition-agnostic\u201d attention play in this design?",
      "answer": "A practical approach is to combine rotary positional embeddings with a fragment-aware attention mask built from tissue-fragment segmentation. Each patch token carries (i) its pixel coordinates, converted to continuous patch indices (normalized by patch size) for RoPE, and (ii) a fragment index indicating which tissue piece it belongs to. A pairwise boolean attention mask is then defined by equality of fragment indices (with CLS/register tokens assigned to a separate shared fragment), so RoPE-based attention is restricted to within-fragment interactions and suppresses positional attention across disconnected fragments.\n\nTo retain global evidence while avoiding positional bias, the aggregator interleaves RoPE-attention blocks (RoPE + fragment-aware mask) with NoPE-attention blocks that are position-agnostic (using only padding masks). RoPE blocks learn localized spatial geometry and fragment-level topology, while NoPE blocks allow global slide-level aggregation across tokens without forcing a positional coupling between separate tissue fragments.",
      "source_document": "papers/2512.14019v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You want to benchmark pathology foundation models that produce outputs at different granularities: some return only patch (tile) embeddings, while others return a single slide-level embedding. Under a unified evaluation protocol for (i) slide-level classification and (ii) time-to-event (survival) prediction, how should you construct slide representations and what type of probe/model should you train for each model type so the comparison is fair?",
      "answer": "Use frozen representations and a simple downstream probe, but construct slide features differently depending on output granularity.\n\n\u2022 Classification: For models that already output a slide embedding, train a linear classifier on top of the released (frozen) slide-level embeddings. For patch-level encoders without slide embeddings, first aggregate patch embeddings into a slide representation (e.g., via a MIL-style CLAM aggregator for the internal clinical benchmarks, or by simple mean pooling for Patho-Bench classification), then train a linear classifier on the resulting slide features.\n\n\u2022 Survival (time-to-event): Use a Cox proportional hazards model as the probe. Feed released slide embeddings directly for slide-level models; for patch-level models, first mean-pool patch embeddings to obtain slide features and then fit the Cox model on those slide representations (with fixed hyperparameters as in the benchmark scripts).",
      "source_document": "papers/2512.14019v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are building a multimodal pathology model that aligns WSI features with several bulk omics modalities (RNA-seq, SNP, CNV, DNA methylation), which differ strongly in sparsity and representational complexity. How should you design the molecular encoders so each modality is embedded into a shared latent space in a way that respects these differences, and what role do pretrained modality-specific foundation models play during training?",
      "answer": "Use separate encoders per molecular modality rather than a single shared encoder: assign four distinct gene encoders for RNA-seq, SNP, CNV, and DNA methylation to accommodate different sparsity/expressivity. RNA expression profiles are first embedded using an internally pretrained RNA-seq foundation model, while SNP/CNV/methylation measurements are treated as fixed-length vector representations. Each molecular encoder is implemented as a feed-forward stack consisting of an initial linear projection followed by multiple pre-norm SwiGLU FFN residual blocks to produce the final modality embedding. Pretrained foundation models for WSI patches and RNA-seq are used to provide stable, biologically meaningful embeddings and are kept frozen during the multimodal contrastive alignment training.",
      "source_document": "papers/2512.14019v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In all-pairwise multimodal contrastive alignment (e.g., WSI, RNA-seq, SNP, CNV, methylation), what is the methodological motivation for using a dedicated projection (adapter) for each ordered modality pair (i\u2192j) rather than a single shared projection per modality, and how are the projected embeddings and similarity logits computed?",
      "answer": "A separate projection for each ordered modality pair lets the model learn modality-pair-specific interaction spaces: the same source modality can be mapped differently depending on the target modality so it can capture different semantic/biological aspects relevant to that target, encouraging complementary information rather than collapsing toward a single \u201ceasy\u201d modality. Concretely, for every pair (i,j) a weight matrix and bias (W_{i,j}, b_{i,j}) define a projection from features x_{n,i} to the target space of modality j: h_{n,i\u2192j} = Normalize(W_{i,j} x_{n,i} + b_{i,j}). Cross-sample similarities are then computed with modality-pair-specific scaling and bias: \u2113_{n,i,k,j} = \u03b1_{i,j} \u27e8h_{n,i\u2192j}, h_{k,j\u2192i}\u27e9 + \u03b2_{i,j}, where \u03b1_{i,j} and \u03b2_{i,j} are learnable and the dot product compares the projected embeddings.",
      "source_document": "papers/2512.14019v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a whole-slide-image (WSI) classifier for a spatially heterogeneous tumor subtype, what is a principled way to aggregate multiple sampled region-level subtype probabilities into a single slide-level probability while down-weighting uncertain regions, and how is the region \u201cconfidence\u201d computed?",
      "answer": "A robust approach is confidence-weighted soft voting over sampled regions. For each sampled region r, the model outputs a subtype probability p_r. A confidence weight c_r is computed from the binary entropy of that prediction, so that uncertain (high-entropy) regions contribute less: c_r = 1 \u2212 H(p_r)/log 2, where H(p) = \u2212p log p \u2212 (1\u2212p) log(1\u2212p). The slide-level probability is then p_slide = (\\sum_r c_r p_r) / (\\sum_r c_r). This automatically down-weights regions with high predictive uncertainty and improves slide-level robustness under heterogeneous sampling.",
      "source_document": "papers/2512.14750v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a setting where only some patients have RNA-seq (molecular subtype) labels but a larger cohort has only preoperative CT, how can you construct a cross-modal training pipeline to predict a molecular ccRCC subtype from CT anyway, and what validation steps are needed to justify that the CT model is actually learning the molecular subtype signal rather than just overfitting to the intermediate labels?",
      "answer": "Use a two-stage, cross-modal supervision scheme. First, define a molecular \u201cgold standard\u201d subtype from RNA-seq using a template-based classifier (nearest template prediction, NTP) and train a WSI histopathology model (PathoDCCD) using only these molecular labels as supervision. Next, apply the trained pathology model to WSIs in a separate WSI\u2013CT cohort to generate predicted subtype assignments (pseudo-labels; pDCCD/pNonDCCD). Train the CT radiomics model (RadioDCCD/RadiopDCCD) on that larger CT cohort using the pathology-derived pseudo-labels as targets.\n\nTo validate that the CT model reflects the underlying molecular subtype rather than merely reproducing pathology-model artifacts, perform cross-modal \u201cgold-standard\u201d validation in independent subsets that have RNA-seq, WSI, and CT available: compare CT predictions directly against the RNA-seq/NTP molecular labels (and similarly check concordance of pathology predictions). An additional independent RNA\u2013WSI\u2013CT gene cohort can further support molecular validation. Finally, in CT-only external cohorts where molecular labels are unavailable, assess whether predicted subtypes show expected clinical associations (e.g., prognostic stratification/outcome differences), providing indirect clinical validation.",
      "source_document": "papers/2512.14750v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are building a CT radiomics classifier to predict a binary, molecularly defined tumor subtype. What feature-reduction/selection workflow can you use to obtain a compact, non-redundant radiomics signature from a very high-dimensional PyRadiomics feature set, and what complementary evaluation steps should you report beyond ROC-AUC to assess calibration, clinical utility, and interpretability?",
      "answer": "A workable pipeline is:\n\n**Radiomics feature extraction**: extract first-order, shape, and texture features from original images and filtered variants (e.g., wavelet and Laplacian-of-Gaussian).\n\n**Multi-step feature reduction/selection**:\n1) Remove low-variance features and features with high missingness.\n2) Remove redundant features by correlation filtering (e.g., drop one of any pair with Pearson r > 0.90).\n3) Perform univariate filtering using a two-sample statistical test (Student\u2019s t-test for parametric features or Mann\u2013Whitney U for nonparametric).\n4) Run LASSO logistic regression with cross-validation (ten-fold CV) to select a compact subset of predictive, non-redundant features.\n\n**Model evaluation beyond AUC**:\n- Use repeated five-fold cross-validation and report discrimination metrics (accuracy, sensitivity, specificity, F1) in addition to AUC.\n- Assess **calibration** with calibration curves and the Hosmer\u2013Lemeshow test.\n- Assess **clinical utility** with decision curve analysis (net benefit vs threshold).\n- Provide **interpretability** using SHAP for global and per-patient feature contributions.",
      "source_document": "papers/2512.14750v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You need to predict a molecularly defined ccRCC subtype from H&E whole-slide images despite strong intratumoral heterogeneity. What hierarchical sampling scheme and patch/region representation design can explicitly preserve (i) cellular composition, (ii) patch-level semantics/texture, and (iii) spatial context within a region\u2014and how is the region-level prediction aggregated to a slide-level subtype probability?",
      "answer": "A suitable design is a hierarchical, multi-branch WSI model with four-tier sampling and explicit spatial aggregation:\n\n\u2022 Hierarchical sampling: perform WSI-level tissue detection, then sample region windows of 3000\u00d73000 \u00b5m; within each region extract patch tiles of 1000\u00d71000 \u00b5m arranged as a 3\u00d73 grid (9 patches); and within each patch extract micro-units of 50\u00d750 \u00b5m forming a 20\u00d720 grid. This preserves cellular composition, mesoscopic morphology, and local spatial context.\n\n\u2022 Patch representation with three complementary branches: (1) a unit-level tissue encoder that classifies 7 tissue components (tumor, stroma, blood vessels, immune cells, hemorrhage, necrosis, lipid droplets) and compresses the resulting 20\u00d720\u00d77 tensor to a 32-d vector; (2) a ResNet34 branch producing 512-d semantic features from a 224\u00d7224 patch; and (3) a lightweight CNN producing a 16-d color/texture vector. Concatenating these yields a 560-d patch descriptor.\n\n\u2022 Spatial context within each region: build a patch adjacency graph over the 9 patches (8-neighborhood) and refine patch descriptors with a two-layer GNN; gated attention pooling then produces a region-level representation, which a shallow MLP maps to a region DCCD probability.\n\n\u2022 Slide-level aggregation: sample 8\u201310 regions per slide and compute the slide probability by confidence-weighted soft voting: p_slide = (\u03a3_r c_r p_r)/(\u03a3_r c_r), where c_r = 1 \u2212 H(p_r)/log 2 and H is the binary entropy, down-weighting uncertain regions.",
      "source_document": "papers/2512.14750v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You have two independently trained subtype predictors for ccRCC\u2014one from histology (WSI) and one from preoperative CT\u2014that each output a binary DCCD vs NonDCCD call for the same patient. How can you use agreement vs disagreement between the two modalities to create a clinically meaningful risk stratification for survival analysis, and which concordance/discordance subgroup corresponds to the highest-risk patients (relative to the other subgroups)?",
      "answer": "Risk stratification can be refined by partitioning patients into four groups based on prediction concordance between the pathology (PathoDCCD) and radiology (RadioDCCD) models: (1) concordant-DCCD: both modalities predict DCCD; (2) concordant-nonDCCD: both predict NonDCCD; (3) discordant-PathoDCCD: PathoDCCD predicts DCCD while RadioDCCD predicts NonDCCD; and (4) discordant-RadioDCCD: PathoDCCD predicts NonDCCD while RadioDCCD predicts DCCD. Survival (Kaplan\u2013Meier) analysis shows the concordant-DCCD group has the poorest outcomes (highest event rate and shortest median survival), whereas concordant-nonDCCD has more favorable outcomes and the discordant groups fall in between (discordant-PathoDCCD relatively favorable; discordant-RadioDCCD intermediate).",
      "source_document": "papers/2512.14750v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In RNA\u2013small molecule binding/affinity prediction for virtual screening, what evaluation procedure can you use to check whether a model has learned *RNA-target-specific* interaction rules rather than merely recognizing \u201cRNA-binder-like\u201d ligands (e.g., due to biased decoy choice), and how should the results be interpreted?",
      "answer": "Use a \u201ctarget swapping\u201d (RNA permutation) ablation: at evaluation time, keep each ligand and its activity label the same but swap/permute the associated RNA targets across test examples, then recompute the same metrics. If performance drops substantially after swapping, the model is using RNA features and capturing target-specific binding. If performance remains nearly unchanged, the model is likely ignoring the RNA and instead exploiting ligand/decoy biases (e.g., learning whether a molecule falls in an RNA-binding region of chemical space because decoys are chemically distinct), producing artificially inflated metrics. A symmetric control is to permute ligands instead of RNAs; an even stronger performance drop indicates the model cannot predict scores from RNA features alone and highlights the importance/expressiveness of ligand encodings and dataset label distribution (e.g., stable active proportion per RNA in the test set).",
      "source_document": "papers/2512.15645v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When developing an RNA\u2013small molecule binding affinity prediction model, how should you choose dataset splitting strategies to make the test set reflect real-world deployment in (i) early virtual screening, (ii) hit-to-lead/lead optimization on a known RNA target, and (iii) drug repurposing to a new RNA target? In your answer, explain what \u201cstrong\u201d vs \u201cweak\u201d generalization means for RNAs vs ligands, and name concrete splitting approaches that enforce each (e.g., sequence/structure similarity, scaffold, temporal).",
      "answer": "Splitting should be chosen to match the expected domain shift between training data and the intended application data, because structural biology/chemistry datasets violate the IID assumption: different RNAs or ligands can be highly similar (e.g., same RNA solved in different conditions; ligands sharing a chemical scaffold). \u201cWeak generalization\u201d means only preventing identical examples from appearing across train/val/test (random split), whereas \u201cstrong generalization\u201d enforces dissimilarity between train and test for the entities that must generalize.\n\n(i) Early virtual screening: typically little prior data exists for the RNA and the ligands of interest, so strong generalization is needed for both RNAs and ligands. This motivates similarity-aware splits for RNAs (e.g., clustering by RNA sequence similarity such as CD-Hit, or by structural similarity such as TM-score/RMscore) and similarity-aware splits for ligands (e.g., clustering by chemical similarity like Tanimoto or by scaffold splitting using Murcko scaffolds).\n\n(ii) Hit-to-lead / lead optimization on a target already screened: data on the target RNA has been collected, so weak generalization can be sufficient for the RNA (RNA random splitting can be acceptable), but strong generalization is still needed for ligands (e.g., ligand similarity-based clustering or scaffold splitting) because new chemical matter is being proposed/optimized.\n\n(iii) Drug repurposing to a distinct target: the model should generalize to new targets but not necessarily to new ligands. In this case, strong generalization should be preferred for targets (RNA similarity-based splitting), while weak generalization may be sufficient for ligands.\n\nAs an alternative to similarity-based splits (which can be too strict in some settings), temporal (time) splits can be used by splitting structures by release date (e.g., PDB release date) to mimic how data accumulates and reduce leakage from near-duplicate structures released together. A recommended practice is to explicitly justify the split in terms of intended use and/or report performance across multiple splitting strategies to cover different real-case situations.",
      "source_document": "papers/2512.15645v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating an RNA-target virtual screening model, why is it important to measure the *diversity* of the top-ranked predicted hits in addition to standard retrieval/enrichment metrics, and what concrete diversity measurements can you report to quantify this hit diversity?",
      "answer": "In RNA virtual screening, retrieving many actives is not sufficient because drug designers also want diverse hits: diverse chemotypes provide more opportunities for subsequent hit/lead optimization and reduce the risk of systematic failure later in the pipeline. To quantify this diversity, you can (i) measure chemical variety directly, e.g., using optimal transport\u2013based comparisons of the distribution of retrieved compounds (and/or visual checks of chemical space), and (ii) report scaffold-level diversity such as the number of unique scaffolds among retrieved active compounds; an example of an explicit scaffold-diversity metric is SD100 (which measures scaffold diversity among retrieved hits).",
      "source_document": "papers/2512.15645v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In RNA\u2013small-molecule drug design, full 3D RNA representations (e.g., voxel grids, all-atom 3D graphs with E(3)-equivariant GNNs, or surface meshes) seem attractive for geometric tasks like docking/pose scoring. What are the main practical challenges of using such full 3D representations for RNA-targeting ML models, and why is coarse-grained (residue-level) graph modeling proposed as an alternative?",
      "answer": "Full 3D representations are well suited to geometric tasks (docking/pose scoring), but they face two key practical issues for RNAs: (1) they are high-dimensional and therefore require large training sets and substantial compute, and (2) they depend on 3D structural data, which is much less abundant for RNAs than for proteins, leading to too little training data relative to model complexity. Using predicted 2D/3D structures can partially address data scarcity, but it risks distribution shift versus experimentally determined structures, and RNA 3D structure prediction still lags behind state-of-the-art protein prediction, which can limit downstream model performance.\n\nCoarse-grained modeling (typically residue-level graphs based on secondary structure or simplified 3D node coordinates) is proposed because it abstracts away fine atomic detail, imposing a sparsity prior that reduces computational complexity and helps cope with limited structural data while still capturing key relational/structural information (backbone/base-pairing and optionally non-canonical/3D proximity edges).",
      "source_document": "papers/2512.15645v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are designing a multimodal model to predict RNA\u2013small-molecule binding affinity. What kinds of RNA input modalities can reasonably be combined in one architecture, and what are three distinct strategies for fusing the resulting modality-specific embeddings into a single prediction model?",
      "answer": "A multimodal RNA model can combine multiple RNA representations/modalities such as sequence-derived features (including language-model embeddings), secondary-structure 2D graphs, more detailed 2.5D graphs that include non-canonical interactions/edge types, and/or geometric 3D graph or surface-based representations; some approaches also add expert-designed features.\n\nThree fusion strategies for the modality-specific embeddings are:\n1) Simple aggregation (e.g., concatenation or averaging of modality embeddings) before the predictor.\n2) Cross-attention mechanisms to let one modality attend to another during integration.\n3) Injecting sequence language-model embeddings into a structural graph by using them as node features within the graph representation, then running a graph encoder over that augmented graph.",
      "source_document": "papers/2512.15645v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a scalable agentic workflow that screens many designed binders with molecular dynamics, what binding free-energy estimation approach is used as a speed\u2013accuracy trade-off, and what summary quantities are returned to support robust ranking of candidates?",
      "answer": "The workflow uses MM-PBSA as the free-energy approximation, chosen because it balances rigor with speed for high-throughput use. The Free Energy analysis agent handles the needed trajectory pre/post-processing and returns the mean and standard deviation of the binding free energy between the specified atom groups, which the reasoning component can use to rank binders.",
      "source_document": "papers/2512.15930v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You\u2019re building a multi-agent pipeline to design biologic binders against intrinsically disordered proteins, and you want to use retrieval-augmented generation (RAG) to keep the LLM\u2019s hypotheses grounded in the biomedical literature. What end-to-end RAG workflow can you use\u2014from collecting and processing papers through retrieval\u2014and how should the retrieved evidence be represented and uncertainty handled so downstream agents can use it reliably?",
      "answer": "A workable workflow is: (1) automatically assemble a target-specific full-text corpus from scholarly aggregators (with de-duplication by DOI/PMCID and open-access\u2013prioritized PDF recovery); (2) parse PDFs into clean text and segment them into semantically coherent chunks using similarity/embedding-distance discontinuities (with a short sentence buffer around boundaries); (3) embed chunks, L2-normalize the embeddings, and index them in a FAISS vector store; (4) at inference time, embed the user/agent query and retrieve the most relevant chunks to provide as context to an arbitrary LLM. The retrieved text should then be distilled into structured assertions\u2014e.g., binding interfaces, affinities, post-translational modifications, and disease/cancer-relevant mutations\u2014and assembled into a hypothesis-specific knowledge graph shared across agents. Conflicting findings and gaps in the literature should be explicitly surfaced as uncertainties in that graph to reduce hallucinations and improve scientific grounding.",
      "source_document": "papers/2512.15930v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a high-throughput, autonomous biologics-design workflow that runs large numbers of MD simulations and structure-processing steps, what kinds of \u201csoft exception\u201d failures should the system anticipate, and what concrete quality-assurance checks can be used to diagnose (and sometimes salvage) these failures before deciding whether to restart a task or move on?",
      "answer": "The workflow should anticipate \u201csoft exceptions\u201d where tools fail or crash without informative errors (e.g., MD crashes with messages like \u201cParticle coordinates NaN\u201d or an opaque \u201cSegmentation fault\u201d), which can stem from poor starting configurations, bad atomic parameters, insufficient minimization, upstream system-building failures, or engine instability.\n\nTo diagnose and potentially salvage these failures, quality-assurance agents can run concrete checks such as:\n- For MD: verify the input structure has non-overlapping coordinates and non-zero coordinates, and probe pairwise forces between atom pairs (expensive but can reveal whether a run is recoverable rather than discarded).\n- For other tool outputs (e.g., structure prediction/binder design): inspect structure files for severe atomic clashes, improperly formatted outputs, and missing/incomplete scoring data.\n- Specifically watch for silent corruption during CIF\u2192PDB conversion (often required because many biophysics tools cannot ingest CIF); conversion can appear successful but write corrupt lines that later cause downstream crashes.\n\nThe reasoning/controller should gate these resource-intensive diagnostics, invoking them only when clearly indicated, and then use the annotations to decide whether to restart the task or shift attention to other steps/agents.",
      "source_document": "papers/2512.15930v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are running an autonomous, multi-agent binder-design campaign against multiple related intrinsically disordered protein (IDP) targets, generating many competing hypotheses in parallel. How can the system learn generalizable design rules across the entire hypothesis population (without retraining any model), and what memory organization and context-management steps let an LLM-driven controller preserve strategic continuity while keeping prompts within context limits?",
      "answer": "The controller can perform cross-hypothesis learning by comparing outcomes across many hypotheses that address related protein families or interaction motifs, identifying shared success/failure modes, and then promoting recurring patterns into *soft design constraints* used in future hypothesis generation (e.g., if hydrophobic hotspots repeatedly outperform charged residues for a given target class, that preference becomes a constraint).\n\nTo support this without model retraining, it uses a structured memory system with:\n- **Long-term memory** for persistent items the reasoner should not discard (e.g., key hotspots, top binders at each stage, experimental data).\n- **Short-term memory** for the evolving sequence of workflow decisions and task outcomes.\n\nTo manage context length while maintaining continuity, it **periodically trims the short-term memory** but **retains the long-term memory in its entirety**, updating long-term memory as new information arrives. This lets the system accumulate target-class-specific design intuition across extended campaigns without explicit retraining.",
      "source_document": "papers/2512.15930v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using an autonomous workflow to design binders against intrinsically disordered proteins, how can short molecular-dynamics (MD) runs be incorporated into a structural-validation stage to decide which proposed interfaces to advance, and how should stable trajectories be summarized to reveal distinct binding sites and feed back into hypothesis revision?",
      "answer": "Run a rapid MD equilibration on each predicted/experimental complex (on the order of ~10\u201350 ns depending on complex size) and use stability signals to triage candidates: interfaces that dissociate during equilibration or show high RMSF at proposed hotspot residues are deprioritized, while stable interfaces are prioritized. For the stable set, cluster conformations using all-to-all contact maps and characterize each cluster by interaction energies measured from the simulation to summarize distinct binding sites/binding modes. Candidates that fail validation are not thrown away; they are annotated with their failure modes and returned to the reasoning component for hypothesis revision (e.g., shift from a flexible loop to a neighboring structured region or reconsider the partner), and additional rounds of MD can be triggered if sampling is judged insufficient.",
      "source_document": "papers/2512.15930v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a continuous (ODE-semantics) biological Petri net, two reactions often \u201cshare a metabolite\u201d or \u201cshare an enzyme.\u201d Under what precise structural condition can such place-sharing transitions still be scheduled in parallel without changing the model\u2019s dynamics, and how does this criterion distinguish (i) true resource conflict from (ii) convergent production and (iii) regulatory coupling? Briefly justify why the latter two cases are dynamically equivalent to any sequential interleaving.",
      "answer": "Parallel scheduling is permitted under **weak independence**, which forbids **input competition** but allows certain kinds of place-sharing that are biologically non-conflicting. Two transitions t1 and t2 are weakly independent iff they do **not** share any input places (\u2022t1 \u2229 \u2022t2 = \u2205) and they share places only through either (a) **output convergence** (t1\u2022 \u2229 t2\u2022 \u2260 \u2205) or (b) **regulatory coupling** via non-consumptive regulatory arcs (\u03a3(t1) \u2229 \u03a3(t2) \u2260 \u2205).\n\nThis yields three coupling modes:\n1) **Competitive (conflict)**: \u2022t1 \u2229 \u2022t2 \u2260 \u2205. They compete for the same substrate/input resource, so sequential execution is required.\n2) **Convergent (weakly independent)**: t1\u2022 \u2229 t2\u2022 \u2260 \u2205 and \u2022t1 \u2229 \u2022t2 = \u2205. Both transitions produce into the same place without competing for inputs; under ODE semantics, their rate contributions **superpose** (for a shared product place p, dM(p)/dt includes r1 + r2), so parallel firing matches any sequential ordering.\n3) **Regulatory (weakly independent)**: \u03a3(t1) \u2229 \u03a3(t2) \u2260 \u2205 and \u2022t1 \u2229 \u2022t2 = \u2205. The shared place is read-only (e.g., a catalyst/inhibitor): firing either transition does not change the marking of that regulator place, so there is no conflict and parallel execution is equivalent to any interleaving.\n\nTherefore, convergent and regulatory coupling preserve dynamics because ODE updates add linearly for shared products (convergent) and shared regulators are not modified (regulatory), whereas shared inputs constitute genuine resource contention (competitive).",
      "source_document": "papers/2512.17106v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using Petri nets to model biochemical reaction networks, generic structural checks (e.g., boundedness/liveness or P-invariants) can flag biologically valid models as problematic. Describe a set of domain-specific validation tests that address this, including (i) the atom-conservation (mass-balance) condition for each reaction/transition using chemical formulas, (ii) a steady-state flux-feasibility test that can identify blocked reactions, and (iii) how to infer non-consumptive regulatory participants from a kinetic rate law by constructing a regulatory-arc set \u03a3(t). State the defining equations/relations for these three checks.",
      "answer": "A domain-specific validation suite can include:\n\n(i) **Mass balance / atom conservation per transition t** using a formula map for each species/place (Atoms(p) derived from \u03c1(p)): for each element (C, H, O, N, P, S), require equality of total atoms on inputs vs outputs:\n\\[\\sum_{p\\in {}^{\\bullet}t} W(p,t)\\,\\text{Atoms}(p) \\;=\\; \\sum_{p\\in t^{\\bullet}} W(t,p)\\,\\text{Atoms}(p).\\]\n\n(ii) **Flux-balance feasibility** at steady state using the stoichiometric matrix N and a flux vector v: check whether\n\\[N\\cdot v = 0\\]\nhas a feasible solution (used to identify blocked reactions).\n\n(iii) **Regulatory-structure inference from kinetics** by parsing the rate law \u03a6(t) to find variables/species V that appear in \u03a6(t) but are not consumed or produced by t, and defining the regulatory set as\n\\[\\Sigma(t) = V \\setminus ({}^{\\bullet}t \\cup t^{\\bullet}),\\]\nthen classifying those members as catalyst/activator/inhibitor based on how they appear in \u03a6(t).",
      "source_document": "papers/2512.17106v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In biochemical Petri-net modeling, classical nets do not explicitly represent (i) non-consumptive regulation/catalysis, (ii) boundary exchange with the environment, (iii) heterogeneous transition semantics (continuous vs stochastic vs timed), or (iv) biochemical formula metadata for semantic validation. How can a biological Petri net be extended to encode these capabilities as a single formal object? Give the extended tuple and, for each added component (\u03a3, \u0398, \u0394, \u03c4, \u03c1), state what it represents and its formal type/codomain.",
      "answer": "One formalization is an Extended Biological Petri Net defined as the 12\u2011tuple\nBioPN = (P, T, F, W, M0, K, \u03a6, \u03a3, \u0398, \u0394, \u03c4, \u03c1).\nBeyond the usual places/transitions/flow/weights/initial marking (P,T,F,W,M0) plus capacities and kinetics (K, \u03a6), it adds:\n\u2022 \u03a3 \u2286 (P \u00d7 T) \\ F: a regulatory-arc relation, used for non-consumptive participation such as catalysis (test/read arcs) and inhibition.\n\u2022 \u0398 : T \u2192 {Internal, Source, Sink, Exchange}: an environmental-exchange classifier that labels each transition\u2019s boundary role (open-system interaction).\n\u2022 \u0394 : T \u00d7 T \u2192 {Independent, Competitive, Convergent, Regulatory}: a dependency taxonomy that classifies how pairs of transitions couple (e.g., true resource conflict vs safe coupling).\n\u2022 \u03c4 : T \u2192 {Continuous, Stochastic, Timed, Immediate}: a transition-type map specifying heterogeneous execution semantics.\n\u2022 \u03c1 : P \u2192 Formula: a mapping from each place/species to a chemical formula (e.g., ATP \u21a6 C10H16N5O13P3), enabling biochemical/atom-balance validation.",
      "source_document": "papers/2512.17106v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You want a DNA language model tokenizer that explicitly captures transcription factor (TF) binding motifs while remaining robust to small frame shifts that can otherwise cascade through greedy segmentation. Describe a motif-aware tokenization scheme that (i) converts TF motif PWMs into discrete tokens for a fixed vocabulary, (ii) defines what else is included in the vocabulary besides motifs, and (iii) tokenizes an input sequence so that motif matches are preferred but small 1\u20132 bp shifts don\u2019t completely change the tokenization; also state what the tokenizer does when no motif match exists.",
      "answer": "A motif-aware scheme can be built by first turning TF motif PWMs into fixed sequence tokens: take a PWM, threshold nucleotide probabilities at 0.5 to mark low-probability positions as wildcards, trim wildcard positions at both motif ends, and then encode the remaining positions by choosing the nucleotide with highest probability at each position. Use a motif library (e.g., vertebrate TF motifs) and exclude motifs longer than 12 bp; include both each motif sequence and its reverse complement as vocabulary items. The final vocabulary can then combine: (1) special tokens ([PAD], [UNK], [CLS], [SEP], [MASK]), (2) motif tokens (motif sequences plus reverse complements), and (3) fallback tokens consisting of all 3-mers plus single-nucleotide 1-mers (A,T,C,G,N). For tokenization, scan the DNA sequence left-to-right with a greedy, non-overlapping procedure using a sliding window that varies from 4 to 12 bp. To reduce sensitivity to a single-base shift, allow local flexibility by evaluating offsets of 0, 1, or 2 bp to the right and choosing the option that yields the best (longest) match; at each position, if the subsequence matches one or more motifs in the vocabulary, select one of the matched motifs (the default behavior can randomly choose among candidates). If no motif match is found, tokenize using the fallback 3-mer representation (or 1-mer near the end when fewer than 3 bases remain).",
      "source_document": "papers/2512.17126v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are training a BPE tokenizer for a DNA masked-language model and can choose the tokenizer training corpus (e.g., whole genome vs biologically enriched subsets). How can you use motif-enriched regions and ENCODE cCRE regions as \u201cdomain-knowledge\u201d corpora for BPE training, what analysis would you run to verify that these corpora actually change what the tokenizer learns, and what empirical conclusion does this support about training BPE tokenizers on smaller curated genomic subsets versus the full reference genome?",
      "answer": "A practical way to inject domain knowledge into BPE training is to train separate BPE tokenizers on (i) the full human reference genome, (ii) genomic regions predicted as TF motif regions, and (iii) ENCODE candidate cis-regulatory element (cCRE) regions (motif regions cover a much larger fraction of the genome than cCREs, while cCREs are a smaller, function-enriched subset).\n\nTo verify that the choice of corpus changes what the tokenizer learns, compare the resulting learned vocabularies quantitatively via pairwise Jaccard similarity / overlap analysis (and optionally visualize overlaps with Venn diagrams), and examine how the learned tokens behave when applied to a common target corpus (e.g., the full genome), such as token length and token-frequency distributions. In this study, the motif-trained BPE produced the most distinct vocabulary relative to genome-trained and cCRE-trained BPEs and showed a higher proportion of low-frequency tokens when applied to the full genome.\n\nEmpirically, downstream benchmarking shows that models using BPE tokenizers trained on motif regions or cCRE regions can achieve comparable performance to models using BPE trained on the whole genome (across vocabulary sizes), supporting the conclusion that BPE tokenizers can be trained more efficiently on smaller curated, domain-knowledge\u2013enriched genomic subsets without sacrificing downstream performance.",
      "source_document": "papers/2512.17126v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are benchmarking multiple DNA tokenization strategies (overlapping/non-overlapping k-mers, BPE, and a motif-based tokenizer) and want to isolate the effect of tokenization from confounders like model capacity and compute. What concrete controls would you impose in the pretraining and fine-tuning pipeline to make the comparison fair, including (i) how you equalize compute across tokenizers, (ii) how you construct pretraining examples from a reference genome and handle ambiguous/low-quality sequence, and (iii) what you keep fixed vs allow to vary during fine-tuning and evaluation (including the choice of evaluation metrics across datasets)?",
      "answer": "A fair tokenizer comparison is done by holding the model architecture and training budget constant and changing only the tokenizer-dependent components.\n\n(i) Equalize compute: use the same BERT-style masked language model architecture for all tokenizers, and keep pretraining compute comparable by controlling training such that the models\u2019 FLOPs are consistent across experiments.\n\n(ii) Construct pretraining examples uniformly from the genome: apply each tokenizer to the same human reference genome (chromosomes 1\u201322, X, Y, and M), then take the resulting token stream and split it sequentially into fixed-length segments of 512 tokens to form MLM inputs. To avoid low-information/ambiguous regions, discard segments that contain more than 50% \u2018N\u2019 tokens.\n\n(iii) Fine-tuning/evaluation controls: fine-tune each pretrained model on the same set of benchmark datasets using mostly the same fine-tuning hyperparameters across models; the main tokenizer-specific adjustment is the maximum input length, which is set per tokenizer. Report performance with Matthews Correlation Coefficient (MCC) on four of the benchmark datasets and Accuracy (ACC) on the DART-Eval dataset.",
      "source_document": "papers/2512.17126v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You fine-tune a DNA masked-language model for multiclass classification of cell-type-specific ATAC-seq peaks and want to test whether a motif-aware tokenizer yields biologically interpretable features. Describe an attribution-based validation protocol at the *token* level (including how to compute and aggregate attributions, how to compare motif tokens vs generic k-mer tokens, and how to assess whether the most influential tokens are cell-type-shared vs cell-type-specific). What qualitative outcome would support the claim that motif tokens improve interpretability, and what kind of external biological evidence can you use to corroborate the cell-type specificity of the attributed motif tokens?",
      "answer": "A suitable protocol is to run token attribution analysis with Integrated Gradients on a held-out test set: for each input sequence, compute each token\u2019s attribution toward the true class label, then aggregate by averaging attribution scores per token across examples and rank tokens by magnitude (farther from zero indicates stronger influence). Compare the attribution-score distribution for motif tokens versus k-mer tokens; interpretability is supported if highly influential tokens are predominantly motif tokens while k-mer tokens have attribution scores closer to zero. To assess shared vs specific usage across cell types, take the top-ranked (e.g., top 200) contributive motif tokens per cell type and compute overlaps (e.g., via a Venn/overlap analysis), revealing both shared motif tokens and cell-type-specific motif tokens used for prediction. Finally, corroborate attributed motif-token specificity by comparing the highly attributed motifs to independent motif-enrichment results for the corresponding cell types (e.g., showing that motif families enriched in each cell type are also among the top-attributed motif tokens).",
      "source_document": "papers/2512.17126v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "A motif-aware genomic tokenizer scans a DNA sequence left-to-right, tries to match motif tokens of length 4 up to a maximum length Maxlen using a Trie, allows a local right-shift (offset) of 0\u20132 bp to reduce frame-shift cascades, and falls back to 3-mer or 1-mer tokens when no motif matches. Derive an upper bound on (i) the time complexity as a function of sequence length n, Maxlen, and the average token length, and (ii) the space complexity in terms of n, vocabulary size V, and average token length L. State the key algorithmic operations that drive each bound.",
      "answer": "Time: The tokenizer advances roughly one token per step, so for sequence length n and average token length \u22488.3 nt, it performs about O(n/8.3) tokenization steps. At each step it queries a Trie for motif matches of lengths 4\u2026Maxlen; the worst-case Trie lookup cost is O(Maxlen^2). Allowing 1\u20132 nt offsets adds a constant-factor overhead, giving an upper bound of O((n/8.3)\u00b7Maxlen^2) total time (with Maxlen set to 12 in the implementation).\n\nSpace: Storing motif tokens in a Trie costs O(V\u00b7L) space, and the motif\u2192token-ID lookup table costs O(V). The output token sequence occupies O(n/8.3) space. Total space is O(n/8.3 + V\u00b7L) (with V=901, L\u22488.3, and 827 motif tokens stored in the Trie in the implementation).",
      "source_document": "papers/2512.17126v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an embedding-space (soft-prompt) targeted attack on a Siamese protein language model variant-effect predictor that scores mutations via the pseudo-log-likelihood ratio (PLLR), how can the attacker optimize the system to misclassify benign variants as pathogenic while largely preserving predictions on known pathogenic variants? Specify the attack loss and which subset of examples receives gradient updates.",
      "answer": "Use a one-class targeted objective defined only on benign variants (y=0): set the attack loss to L_benign = \u2212log(\u03c3\u0302(\u03bb)) (where \u03bb is the PLLR and \u03c3\u0302(\u03bb)=2\u00b7\u03c3(\u03bb)\u22121 is the calibrated sigmoid output). This pushes PLLR (and thus the pathogenicity probability) upward for benign inputs. During attack training, gradient updates are applied only using benign examples, while pathogenic examples are held fixed (and the perturbation is introduced via the soft-prompt embeddings), which increases the false-positive rate without substantially disturbing performance on pathogenic variants.",
      "source_document": "papers/2512.17146v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are building an automated, interpretable \u201cmonitor-and-report\u201d auditor to test how a genomic foundation model\u2013based variant effect predictor responds to embedding-space (soft-prompt) perturbations over training checkpoints. What sequence of functional stages should such an agent execute, what does each stage do, and what simple metric-threshold rule can be used to flag a checkpoint as high risk?",
      "answer": "A suitable agentic auditing loop is a five-stage pipeline:\n1) OBSERVE: load wild-type/mutant protein sequence pairs, embed them with a chosen GFM checkpoint, and define the adversarial probe space via (e.g., randomly initialized) soft prompts.\n2) INTERVENE: inject the soft prompts and schedule when perturbations are applied/evaluated across training checkpoints (e.g., evaluating at regular intervals across steps).\n3) EVALUATE: compute quantitative robustness/performance metrics under perturbation, including AUROC, AUPR, and PLLR.\n4) REASON: interpret the metric values/trends to assign a risk level and generate a natural-language explanation (via an LLM). A concrete threshold rule used is: if AUROC < 0.6, label the checkpoint as \u201cHIGH\u201d risk.\n5) REPORT: compile the time-stamped results into a structured report (markdown/HTML) including metric trends and the narrative explanations per checkpoint.",
      "source_document": "papers/2512.17146v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an ESM-style masked language model variant-effect predictor that scores a mutation using a pseudo-log-likelihood ratio (PLLR) between a wild-type sequence and its mutant, how do you (i) compute the pseudo-log-likelihood (PLL) from token-level MLM outputs, (ii) form the PLLR score, and (iii) convert that score into a calibrated probability suitable for a binary cross-entropy objective? Explain how this objective pushes PLLR values for pathogenic versus benign variants.",
      "answer": "(i) For a sequence s=(s1,\u2026,sL), compute its pseudo-log-likelihood as a sum of per-position log probabilities assigned by the MLM to the observed amino acid at that position: PLL(s)=\u2211_{i=1}^L log P(x_i=s_i | s). (ii) Score a variant by the PLLR between wild-type and mutant sequences: \u03bb = PLL(sWT) \u2212 PLL(smut), which measures how much the mutation decreases the model\u2019s probabilistic compatibility with the sequence (wild-type typically has higher PLL). (iii) Because \u03bb is treated as nonnegative, applying a sigmoid \u03c3(\u03bb) maps \u03bb\u2208[0,\u221e) to [0.5,1), so the predictor rescales to the full [0,1] range via \n\\hat{\u03c3}(\u03bb)=2\u00b7\u03c3(\u03bb)\u22121,\nthen uses binary cross-entropy: LBCE = y\u00b7log(\\hat{\u03c3}(\u03bb)) + (1\u2212y)\u00b7log(1\u2212\\hat{\u03c3}(\u03bb)), with y\u2208{0,1}. This loss encourages larger PLLR (higher \\hat{\u03c3}(\u03bb)) when the label is pathogenic (y=1) and smaller PLLR (lower \\hat{\u03c3}(\u03bb)) when the label is benign (y=0).",
      "source_document": "papers/2512.17146v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating targeted embedding-space soft-prompt attacks against multiple ESM-family backbones used for variant effect prediction, what overall robustness pattern would you expect across model scales, and what non-size factors can explain why some large models may still show notable degradation on certain datasets/metrics?",
      "answer": "Across all evaluated GFM backbones and disease datasets, targeted soft-prompt attacks cause clear performance degradation (both AUROC/AUPR), indicating universal vulnerability. The degradation is generally more severe for smaller models (e.g., ESM2-150M), consistent with their internal representations being easier to disrupt, while larger pretrained models (e.g., ESM1b, ESM1v, larger ESM2) tend to be more resilient and maintain relatively higher AUROC/AUPR under attack. However, model size alone does not determine adversarial resilience: architectural design/depth, the pretraining corpus, and fine-tuning dynamics are highlighted as additional determinants. Even among the more resilient models, metric- and dataset-specific drops can still be notable (e.g., an especially large AUPR drop for ESM1v on the arrhythmia dataset), plausibly reflecting differences such as broader output diversity across variants rather than just parameter count.",
      "source_document": "papers/2512.17146v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When auditing a protein language model\u2013based variant effect predictor for adversarial vulnerabilities, why might you choose an embedding-space \u201csoft prompt\u201d attack that prepends trainable embeddings to the wild-type and mutant sequences instead of directly perturbing amino-acid tokens, and what optimization/evaluation protocol ensures that any drop in AUROC/AUPR can be attributed to the soft prompt rather than changes to the biological sequences or retraining the backbone model?",
      "answer": "Embedding-space soft prompts let an attacker manipulate the model\u2019s internal representations while keeping the original biological sequences intact (no token-level edits), so the attack \u201cpreserves biological input integrity\u201d yet can still degrade the model\u2019s decision boundary. To attribute degradation specifically to the prompt, the protocol holds the wild-type/mutant sequences and the backbone model weights fixed and updates only the soft-prompt parameters via gradient descent during the adversarial training phase; performance is then measured on a held-out test set using AUROC/AUPR (and related scores such as PLLR) to quantify the impact of the embedding-space perturbation alone.",
      "source_document": "papers/2512.17146v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When performing recursive spectral clustering on Hi-C contact maps to build a hierarchy of preferentially self-interacting chromatin clusters, what concrete criterion can be used to automatically choose the Hi-C bin size (resolution) at each split so that you balance capturing fine-scale structure with having sufficient interaction coverage, and what additional constraint should be enforced on how the chosen resolution can change down the recursion?",
      "answer": "A practical rule is to pick, at each clustering iteration, the finest (highest) Hi-C resolution for which the contact/interaction matrix is sufficiently populated\u2014specifically, the finest bin size where 99% of the interaction matrix is non-empty\u2014so that interaction coverage is maximized given sequencing depth. In addition, the chosen bin size should be constrained to remain constant or decrease (i.e., move to coarser or equal resolution, not finer) at each subsequent iteration of the recursive bipartitioning.",
      "source_document": "papers/2512.17512v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In the absence of a gold-standard set of functional TADs, how can you quantify how well a hierarchical Hi-C clustering method recovers TADs called by other algorithms, and how should the Jaccard-based comparison be constructed (including what is treated as the reference and how matches are assigned)?",
      "answer": "Use cross-method agreement rather than an absolute truth set by computing Jaccard indices between TAD intervals and reporting them as a distribution. Construct the comparison by alternating which TAD caller\u2019s set is treated as the reference; for each reference TAD, assign its score using the single best-matching domain/cluster produced by the method being evaluated (the match that maximizes the Jaccard index). Repeating this for each choice of reference produces a collection of Jaccard-index distributions summarizing recovery/agreement across callers when no gold standard exists.",
      "source_document": "papers/2512.17512v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using recursive spectral clustering on Hi-C contact maps to infer hierarchical chromatin organization, why is it beneficial to vary the Hi-C binning resolution across different levels of the hierarchy, and what kinds of chromatin interaction patterns/structures are primarily captured at high versus low resolution in such a multi-resolution framework? Also, what is the natural hierarchical data structure used to represent the resulting nested clusters?",
      "answer": "Varying the Hi-C resolution across recursive clustering levels helps ensure that both short- and long-distance interaction signals are represented in the hierarchy: high-resolution Hi-C emphasizes detailed short-range interaction patterns (e.g., loops and sub-TADs), while lower-resolution Hi-C more comprehensively captures long-range interaction patterns (e.g., compartments and higher-order aggregates). The output is represented as a binary tree of nested preferentially self-interacting clusters (a hierarchical bipartitioning of the genome).",
      "source_document": "papers/2512.17512v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "Many Hi-C analysis methods are tuned to detect specific interaction motifs (e.g., diagonal blocks for TADs or plaid patterns for compartments). Methodologically, what is the main advantage of instead using a pattern-agnostic, hierarchical clustering approach that searches for preferentially self-interacting regions across multiple scales, and give one example of a non-canonical Hi-C interaction pattern and the type of mechanistic insight it can support?",
      "answer": "A pattern-agnostic hierarchical clustering of preferentially self-interacting regions can capture a wider variety of chromatin interaction patterns than motif-specific callers, yielding a more adaptive and comprehensive multi-scale description of chromosome architecture (not limited to diagonal blocks or plaid). One example of an additional pattern it can incorporate is \u201cstrips\u201d, which can help infer mechanistic insights such as different forms of loop extrusion.",
      "source_document": "papers/2512.17512v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "Hi-C \u201cTAD callers\u201d often disagree because they target different interaction patterns and implicit TAD definitions. Methodologically, how can a hierarchical, multi-resolution spectral-clustering approach help reconcile these differing TAD definitions, and what type of downstream biological/structural interpretation does this enable beyond simply outputting one consensus TAD set?",
      "answer": "A hierarchical multi-resolution spectral-clustering approach can recover (i.e., match) TADs called by multiple, pattern-specific methods while still organizing the genome into a single, hierarchically nested set of preferentially self-interacting clusters. Because the result is a nested multi-scale decomposition rather than one flat TAD list, it enables studying how TADs (under different definitions) relate structurally to other kinds of chromatin structures in the same hierarchy and to characterize the broader architectural context in which TADs sit within chromosome organization.",
      "source_document": "papers/2512.17512v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You generate a large set of candidate antibody variants (e.g., CDR-focused point mutants) from a structure-conditioned inverse-folding model. Describe a two-stage computational screening strategy to prioritize high-affinity, structurally credible candidates, including (i) the fast prescreen metrics, (ii) the higher-confidence structure-based metrics, and (iii) how final candidates are selected when multiple metrics trade off.",
      "answer": "A practical two-stage screen is:\n\n1) Stage 1 (rapid prescreen on all sampled sequences): compute two fold-agnostic scores for each candidate\u2014(i) an antibody language-model log-likelihood (AntiBERTy log-likelihood) to measure evolutionary/plausibility of the sequence, and (ii) a physics-based estimate of binding improvement using FoldX binding free-energy change, \u0394\u0394G = \u0394G_mut \u2212 \u0394G_wt, where \u0394G is the FoldX interaction free energy of the complex; \u0394\u0394G < 0 implies a more favorable interaction (stronger predicted binding, consistent with \u0394G \u2248 RT ln Kd). Advance only variants that rank in the top 20% for both metrics.\n\n2) Stage 2 (high-confidence ranking on the reduced subset): refold each shortlisted sequence as a full antibody\u2013antigen complex with AlphaFold 3, then evaluate (i) complex-level pLDDT as a proxy for structural integrity, (ii) change in epitope solvent-accessible surface area (\u0394SASA) to gauge interface burial/stability (lower SASA/burial of hydrophobics generally stabilizes; lower \u0394SASA than wild type suggests greater stability), and (iii) inverse-folding log-likelihood under ProteinMPNN to assess backbone compatibility/foldability of the designed sequence on the predicted structure. PTM and interface-specific ipLDDT can also be used for additional confidence analysis.\n\n3) Final selection: treat the Stage-1 and Stage-2 quantities as a multi-objective set of criteria and select the Pareto-optimal designs (a Pareto frontier across the five primary metrics), yielding candidates that balance predicted affinity, plausibility, and structural reliability rather than optimizing a single score.",
      "source_document": "papers/2512.17815v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In structure-conditioned antibody design pipelines that rank candidate variants by a model\u2019s average token-wise log-likelihood, what is the key train\u2013test mismatch introduced by Direct Preference Optimization (DPO), and how does a reference-free objective like Simple Preference Optimization (SimPO) change the reward/loss so that training is aligned with inference while also reducing computational overhead?",
      "answer": "DPO defines its implicit reward as a log-ratio between the current policy and a frozen reference model (r_DPO(x,y)=\u03b2[log \u03c0_\u03b8(y|x)\u2212log \u03c0_ref(y|x)]). If inference ranks candidates by the policy model\u2019s absolute average token-wise log-likelihood, this creates a train\u2013test mismatch: a sequence can be preferred during training mainly because the reference assigns it even lower likelihood, even when the policy\u2019s standalone likelihood remains low, so the desired likelihood ordering is not reliably preserved (noted to hold for only about half of triples in practice). DPO also requires an additional forward pass through the reference model every step, which roughly doubles memory use and increases wall-clock time.\n\nSimPO removes the reference model entirely and defines the reward directly as the quantity used at test time: the length-normalized log-likelihood under the current policy (r_SimPO(x,y)=\u03b2/|y|\u00b7\u2211_i log \u03c0_\u03b8(y_i|x,y_<i)). It plugs this reward into the same Bradley\u2013Terry / logistic ranking formulation with a fixed margin \u03b3 between preferred and disfavored sequences (L_SimPO=\u2212log \u03c3(r_SimPO(x,y_w)\u2212r_SimPO(x,y_\u2113)\u2212\u03b3)). Because updates directly increase the policy likelihood of preferred sequences and there is no reference pass, SimPO aligns training with log-likelihood-based ranking at inference and reduces peak GPU memory and training time per epoch.",
      "source_document": "papers/2512.17815v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You want to repurpose a structure-conditioned inverse-folding model (that produces residue embeddings from an antibody\u2013antigen complex structure) to predict which antibody residues form the paratope. What is a minimal, parameter-efficient model adaptation and training setup to do this, and why would you emphasize precision\u2013recall evaluation rather than ROC-AUC in this setting?",
      "answer": "A minimal adaptation is to keep (freeze) all parameters of the pretrained inverse-folding model and train only a lightweight per-residue classification head on top of its residue-level representations. The head can be implemented as a small two-layer feedforward network that maps each residue embedding to a paratope probability, trained using residue-level paratope labels (with antibody sequences aligned to their structural chains so the frozen base model can use 3D context when producing token-level features).\n\nPrecision\u2013recall evaluation is emphasized because paratope prediction is highly class-imbalanced (few binding residues vs many non-binding residues). In such imbalanced settings PR performance is more informative about how well the model identifies true paratope residues with high precision, whereas ROC-AUC can change only modestly even when the practical precision of predicted binders improves substantially.",
      "source_document": "papers/2512.17815v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When applying an affinity-optimized, structure-conditioned antibody generator to influenza hemagglutinin (HA), what binding-site bias might the designed antibodies show, what is the intended benefit of this bias for neutralization, and what downside does it create for breadth against viral evolution? Propose one computational modification that could mitigate this downside.",
      "answer": "Designed variants can show a head-centric binding bias: they more consistently engage the HA globular head, particularly the receptor-binding site (RBS), rather than other regions such as conserved head\u2013stalk interface epitopes. The benefit is that head/RBS targeting matches the major immunodominant recognition site and can yield potent neutralization by blocking viral attachment. The downside is reduced breadth, because mutations in the RBS accumulate rapidly, so a head-centric preference can restrict cross-strain coverage. A proposed mitigation is to add an on-the-fly epitope-diversification module (and more broadly move toward objectives beyond single-objective affinity) to expand coverage while preserving design speed.",
      "source_document": "papers/2512.17815v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a structure-conditioned antibody generator that ranks mutant sequences by their average token-wise log-likelihood, what two complementary metrics can you use to evaluate (i) how well the ranking agrees with experimental binding affinities and (ii) how effectively the model surfaces large affinity improvements among the very top candidates? Define each metric precisely, including how a \u201c\u226510-fold affinity improvement\u201d is computed from pKd (\u2212log10 Kd) values and how this feeds into precision@10.",
      "answer": "Use (1) Spearman correlation between the model\u2019s predicted log-likelihood scores and the experimentally measured binding affinity values: a higher Spearman \u03c1 indicates the model assigns higher likelihood to stronger binders, i.e., its ranking better matches the experimental affinity ordering.\n\nUse (2) 10-fold precision@10: rank variants by model score, take the top 10, and compute the proportion whose experimental affinity shows at least a 10\u00d7 improvement over wild type. The fold-change is computed from pKd values as 10^(pKd_mutant \u2212 pKd_wild-type) (equivalently the ratio of dissociation constants Kd_wild-type / Kd_mutant). A variant counts as \u201c\u226510-fold improved\u201d if this value is \u226510; precision@10 is the fraction of the top-10 variants satisfying that criterion.",
      "source_document": "papers/2512.17815v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When supervised fine-tuning a pretrained protein encoder on substructure-type classification (using pooled residue embeddings as substructure representations), what regularization strategy can be used to reduce catastrophic forgetting of the encoder\u2019s original pretraining objective, and how is the regularization term computed in practice?",
      "answer": "Use elastic weight consolidation (EWC): add a quadratic penalty that pulls parameters back toward the original pretrained weights, weighted by each parameter\u2019s importance for the original task. The fine-tuning loss is\nL = L_c(\u03b8) + \u03a3_i (\u03bb/2) F_i (\u03b8_i \u2212 \u03b8_{0,i})^2,\nwhere L_c is the substructure classification cross-entropy loss, \u03b80 are the original pretrained parameters, and F is the diagonal Fisher information matrix. In practice, F is estimated once at the start of fine-tuning by making a single pass over the substructure-tuning training set, computing the original pretraining loss (e.g., masked amino-acid prediction, or masked prediction with structure tokens for sequence\u2013structure models), running backward passes, and averaging the squared gradients (per-parameter) over minibatches; these averaged squared gradients approximate the Fisher diagonal used as F_i.",
      "source_document": "papers/2512.18114v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When designing a supervised substructure-type classification task for protein encoders, substructure labels are typically long-tailed. What concrete filtering rule can be used to define a \u201cfrequent\u201d substructure label set for training, and why does this rule still preserve most substructure occurrences despite discarding many label types?",
      "answer": "Use a minimum-occurrence cutoff on substructure types in the training corpus (e.g., keep only substructures that appear at least 75 times in the SwissProt proteins). This corresponds to retaining only roughly the top 10% most frequently occurring domain types. Even though many rare substructure types are removed, the vast majority of substructure *occurrences* are preserved because most substructure types have very few instances while a small number of common types account for most annotations.",
      "source_document": "papers/2512.18114v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When a pretrained protein encoder is fine-tuned to classify annotated substructures by pooling residue embeddings, downstream performance can improve for protein-level function tasks but degrade for some residue-level tasks. How can you use a gradient conflict analysis to test whether this degradation is simply due to objective misalignment, and what qualitative gradient relationship (and resulting mechanistic hypothesis) would indicate a different cause?",
      "answer": "Compute and compare gradient-update similarity between (i) the substructure-classification loss and (ii) the losses for representative downstream evaluation tasks across minibatches. If the substructure objective were directly misaligned, you would expect strong negative cosine similarity (conflicting gradients) with the harmed tasks. Instead, the analysis shows evaluation-task gradients are highly consistent within-task, while substructure-classification gradients have lower (but still positive) within-task similarity and are close to orthogonal to the gradients for evaluation tasks. This suggests the mixed downstream effects are not explained by simple gradient misalignment; a plausible mechanistic explanation is that the substructure objective smooths residue representations by explicitly encouraging residues within the same substructure to share similar embeddings, which can hurt fine-grained residue-level distinctions.",
      "source_document": "papers/2512.18114v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a supervised substructure-classification fine-tuning setup where you must aggregate residue embeddings into a single embedding for each (possibly non-contiguous) protein substructure, what is a principled reason to prefer a fixed pooling operator (e.g., mean pooling) over a learnable pooling module, and what kind of ablation result would support that this design choice is not a major driver of downstream gains?",
      "answer": "A principled reason to use a fixed pooling operator (mean pooling) is to force the supervised signal to be absorbed by the underlying protein encoder rather than being \u201csolved\u201d by extra parameters in a learnable aggregation module; the intent is that substructure-tuning updates the base model\u2019s representations instead of shifting capacity into the pooling layer. An ablation supporting that pooling choice is not critical is that replacing mean pooling with alternatives such as max pooling or a learnable attention pooling yields similar substructure-tuning behavior/performance (i.e., the method is robust to the exact pooling used to map residue-level embeddings to substructure embeddings).",
      "source_document": "papers/2512.18114v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You fine-tune a pretrained protein encoder using supervised substructure-type classification, and you want to test whether the tuning improves the *intrinsic geometry* of substructure embeddings in a way that generalizes to substructure types that were completely held out from tuning. What evaluation protocol can you use based on silhouette score\u2014how do you define \u201cseen\u201d vs \u201cunseen\u201d substructure types, what embeddings are clustered, how is the score interpreted, and what qualitative result would support the claim that tuning learns general substructural features rather than memorizing specific label signatures?",
      "answer": "Use a clustering-based intrinsic evaluation of substructure embeddings measured by silhouette score. First, construct a single embedding per annotated substructure by pooling (mean pooling in the described setup) the encoder\u2019s residue-level embeddings over the residues that belong to that substructure. Next, group these substructure embeddings by substructure *type* and compute silhouette scores, where higher silhouette indicates tighter within-type clustering (and better separation from other types). Define \u201cseen\u201d vs \u201cunseen\u201d at the level of substructure types (not proteins): \u201cseen\u201d types are included in the substructure-type set used during tuning, whereas \u201cunseen\u201d types are excluded from the tuning training set entirely; scores can still be computed using substructures extracted from held-out test proteins. Evidence for generalization is that after substructure-tuning, silhouette scores increase not only for seen types but also for unseen types that had no training examples, supporting the interpretation that tuning encourages the encoder to learn general features of functional substructures rather than merely memorizing signatures of the tuned label set.",
      "source_document": "papers/2512.18114v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are designing a multi-site MRI lesion-segmentation benchmark and want a single leaderboard score that (i) reflects multiple aspects of performance (overlap, volume error, sensitivity, and precision) and (ii) discourages overfitting to the validation set by requiring consistency on an unseen test set. How can the final rank of each method be computed from per-method ranks in DSC, absolute volume difference (AVD), recall, and precision measured on both validation and test data?",
      "answer": "Compute ranks per method separately for each of the four metrics (DSC, AVD, recall, precision) in each phase (validation and test). For each phase, form a gross phase rank by averaging that method\u2019s four metric-specific ranks. Then compute the method\u2019s final rank by averaging its gross validation rank and gross test rank, giving equal weight to validation and test performance and to all four metrics within each phase.",
      "source_document": "papers/2512.18197v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You benchmark several EPVS (enlarged perivascular space) segmentation algorithms across the same set of MRI scans and find that per-scan metric values (DSC, AVD, recall, precision) are non-normally distributed. What nonparametric statistical testing setup can you use to compare methods pairwise within ROIs (e.g., basal ganglia and centrum semiovale), and how can you convert those pairwise test outcomes into a per-metric ranking of methods?",
      "answer": "Use paired (within-scan) Wilcoxon signed-rank tests for pairwise comparisons between methods, computed separately for each metric and ROI (e.g., CSO and BG), with significance defined at p \u2264 0.05. To turn the tests into a ranking for a given metric, count for each method how many pairwise comparisons it is significantly better than its competitors; methods with a larger number of significant \u201cwins\u201d receive a better rank for that metric.",
      "source_document": "papers/2512.18197v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In automatic segmentation of enlarged perivascular spaces (EPVS) from multi-modal brain MRI, algorithms often achieve higher overlap scores in the basal ganglia (BG) than in the centrum semiovale (CSO). What biological/imaging factors can explain this consistent BG>CSO performance gap?",
      "answer": "BG EPVS are typically easier to segment because they have (1) a more consistent morphology and orientation (often aligned along the lenticulostriate arteries) and generally better contrast, while CSO EPVS are (2) more numerous, smaller, and exhibit more variable orientations/appearances, making delineation harder; additionally (3) the CSO commonly contains other small white-matter hyperintensities/lesions that can be confused with EPVS, increasing false positives/ambiguity and lowering DSC.",
      "source_document": "papers/2512.18197v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training an automatic EPVS segmentation pipeline on multi-site MRI where slice thickness varies substantially across scans, what modeling strategy can be used to handle anisotropic volumes, and what slice-thickness rule can be used to decide when to run a 3D model versus a 2D model?",
      "answer": "Use a dual 2D/3D segmentation strategy to accommodate anisotropy: apply a 3D model to scans with relatively thin slices (slice thickness < 2.5 mm) and switch to a 2D model for scans with thicker slices.",
      "source_document": "papers/2512.18197v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are building a multi-site ground-truth dataset for segmenting very small MRI findings (e.g., enlarged perivascular spaces) and want to speed up labeling without sacrificing cross-site consistency. What annotation workflow can combine automated \u201cpriors\u201d with expert editing to produce reliable consensus masks, and what specific sources of annotation error/inconsistency should you plan for when defining QC procedures?",
      "answer": "A practical workflow is to (i) enforce a common labeling standard (here, STRIVE) and define the target ROIs; (ii) generate an initial segmentation prior (e.g., from an existing algorithm or a pretrained U-Net) to accelerate labeling where feasible; (iii) have two trained annotators independently and blindly manually edit the priors; (iv) produce the final mask by consensus, with a third experienced annotator adjudicating disagreements; and (v) run cross-center case-review discussions to align criteria when sites use different initial approaches (e.g., some sites may annotate from scratch without priors).\n\nKey sources of error/inconsistency to anticipate in QC include: EPVS being extremely small (on the order of 1\u20133 mm) and having appearance similar to surrounding tissue; confusion with other small hypointensities such as lacunes or imaging artifacts; bias introduced by different software used to generate priors (different sensitivity and boundary definitions); and limitations of manual annotation tools (brush-size precision, differences in EPVS visibility across viewing planes, single-plane vs multiplanar viewing constraints, and interface sensitivity), all of which can drive variability even with quality control.",
      "source_document": "papers/2512.18197v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In hybrid gene-regulation simulations where promoters switch stochastically but downstream mRNA/protein dynamics may be treated deterministically, under what conditions does \u201cpromoter-only stochasticity\u201d fail to reproduce protein noise, what additional noise sources must be represented to match SSA-level variability, and what copy-number-based guidelines can be used to choose between full SSA/CME, a full Markovian promoter model, promoter-only stochasticity, and pure ODEs?",
      "answer": "Promoter-only stochasticity fails when proteins are at low-to-moderate copy number (roughly <1,000 molecules, especially 100\u20131,000), because it produces an artificially narrow, near-Gaussian protein distribution and substantially underestimates the coefficient of variation (e.g., CV ratio 0.32 vs SSA, i.e., ~68% underestimation of total noise). Matching SSA-level variability requires accounting not only for promoter switching (bursting) noise but also intrinsic Poisson noise from transcription and\u2014critically\u2014translation, plus degradation noise; in one decomposition example, promoter switching contributes ~49% of total CV^2, translation ~31%, transcription ~12%, and degradation ~8%.\n\nCopy-number-based modeling guidance:\n\u2022 Low copy number proteins (<100 molecules): require full SSA or stochastic CME.\n\u2022 Moderate copy number proteins (100\u20131,000 molecules): best modeled with the full Markovian approach that includes stochastic promoters and stochasticity in synthesis/degradation to capture total noise accurately.\n\u2022 High copy number proteins (>1,000 molecules): promoter-only stochasticity becomes adequate (CV ratio >0.95) because synthesis/degradation noise is relatively small.\n\u2022 Very high copy number proteins (>10,000 molecules): pure deterministic ODEs may be sufficient.\nA practical multiscale whole-cell strategy is Markovian promoters for regulated genes, stochastic synthesis/degradation for low-copy proteins, and deterministic ODEs for high-copy proteins/metabolites.",
      "source_document": "papers/2512.18442v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When simulating gene regulation with a hybrid scheme in which promoter binding/unbinding is a continuous-time Markov chain but mRNA/protein dynamics are integrated deterministically, what constraints should the \u201ccommunication\u201d time step \u0394t satisfy to be both accurate and stable, and what practical convergence procedure/criteria can be used to pick the largest acceptable \u0394t for a new network?",
      "answer": "\u0394t should respect the assumed timescale separation and the numerical approximation used for promoter-state propagation: (i) it must lie between the fast promoter-switching and slow protein timescales, i.e., \u03c4_promoter \u223c 1/koff \u226a \u0394t \u226a \u03c4_protein \u223c 1/kdeg,p; (ii) the promoter rate-matrix step must be small so the matrix exponential/first-order update is valid, requiring \u2016Q(t)\u2016\u0394t \u226a 1 (with \u2016Q(t)\u2016 dominated by binding rates); and if using the first-order approximation specifically, \u0394t must be less than 1/(2\u2016Q(t)\u2016) to guarantee probability conservation.\n\nTo choose \u0394t in practice, run a convergence analysis over a range of candidate steps (e.g., 0.001\u20131.0 min) and compare each to a reference simulation at very small \u0394t using metrics such as trajectory correlation and mean absolute error (and for oscillators, period and amplitude errors). Convergence is defined by achieving trajectory correlation > 0.99 and MAE < 1% of the mean protein level; select the largest \u0394t meeting these criteria, then validate against SSA if available (also checking distribution overlap and promoter-state agreement metrics such as Cohen\u2019s \u03ba). A recommended workflow is to start with a conservative default \u0394t = 0.01 min, sweep \u0394t, pick the largest converged value, and document it.",
      "source_document": "papers/2512.18442v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When replacing a Hill-function transcription term with a mechanistic Markovian promoter (to keep reactions mass-action/CME-compatible), how can you represent *high cooperativity* in repression/activation, and what parameter relationships control the resulting ultrasensitive dose\u2013response? Describe (i) the multi-site promoter transition rates (including the cooperativity factor) and how dissociation constants are mapped, (ii) an alternative \u201ceffective Hill-like\u201d two-state representation for very large Hill coefficients, and (iii) how promoter architecture and cooperativity together determine the steepness of the dose\u2013response.",
      "answer": "(i) Multi-site Markovian promoter: model n binding sites with promoter state k\u2208{0,\u2026,n} bound TFs; binding transitions occur at rate kon\u00b7[TF]\u00b7(n\u2212k) and unbinding at koff\u00b7k, with Kd = koff/kon. Cooperativity is introduced by a factor qr>1 that increases binding once sites are occupied, i.e., binding rate kon\u00b7[TF]\u00b7(n\u2212k)\u00b7qr^{I(k>0)} (enhanced when k>0). Parameters are mapped via the Kd relationship (and in worked conversions, koff may be set to represent a fast equilibration timescale and kon computed from Kd); qr can be chosen to match the effective Hill-like behavior of the original model.\n(ii) For very high apparent Hill coefficients (e.g., n\u224810), an alternative is a two-state promoter with an effective Hill-like transition rate from active\u2192repressed of the form kon\u00b7[Z]^n/KM^n (reverse rate koff), instead of explicitly enumerating n sites.\n(iii) Ultrasensitivity/steepness emerges from the discrete state space: genes with more binding sites have sharper transitions, and the dose\u2013response slope is predicted to scale with the product of the number of binding sites and log(qr). Thus both binding-site count and the cooperativity factor jointly control how steep the response is, while preserving elementary (0th/1st/2nd order) reaction structure for CME/SSA compatibility.",
      "source_document": "papers/2512.18442v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a Markovian promoter framework for a gene regulated by two transcription factors\u2014an activator X and a repressor Y\u2014how can you encode an AND-NOT logic gate mechanistically using promoter states and a logic/activity function, and what qualitative promoter-state trajectory explains why an incoherent feed-forward loop (X activates Y; X activates Z; Y represses Z) generates a transient pulse in Z after a step increase in X?",
      "answer": "Encode the promoter as a multi-input occupancy state vector (kX,kY) giving how many X and Y molecules are bound. With one binding site each, the promoter has four discrete states: (0,0), (1,0), (0,1), (1,1), with transitions given by independent binding/unbinding reactions for X and Y. Implement the AND-NOT logic in the activity (logic) function L(k) by making the gene transcriptionally active if and only if X is bound and Y is not bound\u2014i.e., active when kX>0 and kY=0 (state (1,0)), and inactive in all other states.\n\nAfter a step increase in X, the promoter initially leaves the inactive (0,0) state and enters the active (1,0) state quickly because X binds, driving Z production. As time progresses, X also induces Y; as Y accumulates it binds the promoter, shifting occupancy to the repressed (1,1) state (and/or (0,1)), turning Z off. At long times the promoter is predominantly in the repressed (1,1) state with occasional flickers back to (1,0). This transient dominance of (1,0) followed by transition to (1,1) explains the pulse in Z despite sustained X.",
      "source_document": "papers/2512.18442v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You want to use a diffusion model as an unsupervised density estimator for protein\u2013ligand complexes represented as irregular 3D graphs with (i) continuous coordinates and (ii) discrete atom/residue identities, and you also want a single probability-flow ODE (PF-ODE) so you can compute per-complex log-likelihoods for OOD detection. How can you make the discrete identities compatible with a continuous variance-exploding diffusion, and how do you obtain a score for the categorical channel from class-posterior predictions? Also, explain why training the categorical channel via direct score regression on class prototypes is ill-posed and what training objective avoids this collapse.",
      "answer": "Make the categorical identities continuous by assigning each token i an L2-normalized prototype embedding ei\u2208R^d and diffusing the embedding with the same VE Gaussian corruption as coordinates. For a clean token y with prototype ey, sample t and \u03b5~N(0,I) and form the noisy embedding xt=ey+\u03c3(t)\u03b5. Let the denoiser output logits z(xt,t), giving class posteriors p(i|xt,t)=softmax(z)i. The posterior mean of the clean embedding is then the finite mixture \nbe0(xt,t)=\u2211_{i=1}^V p(i|xt,t) ei,\nwhich yields an analytic \u201cinterpolated\u201d score for the categorical block via the VE posterior-mean identity:\n\u015d(xt,t)=(be0(xt,t)\u2212xt)/\u03c3(t)^2.\nThis lets the categorical channel be trained with standard hard-label cross-entropy on logits, while still producing a continuous-time score field for the unified PF-ODE over concatenated coordinates\u2225embeddings.\nDirect score matching on the categorical prototypes is ill-posed because, under Gaussian corruption around each prototype, there is a degenerate joint optimum where all prototypes {ei} collapse to a single point\u2014making the noise easy to predict but destroying identity information. Cross-entropy training on logits (e.g., cosine logits with normalized ei and normalized predicted features) avoids this collapse and stabilizes the interpolation-derived score.",
      "source_document": "papers/2512.18454v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You have a diffusion model on protein\u2013ligand complexes and can compute a per-complex PF-ODE log-likelihood, but the likelihood distributions for ID and OOD overlap too much to threshold reliably. Describe a trajectory-aware, label-light OOD detector that instead uses multiscale PF-ODE trajectory summaries: (i) what feature vector is constructed and what kinds of dynamical behavior do these features capture, (ii) what preprocessing/embedding steps are applied before density modeling, (iii) how are the ID vs OOD densities fit and how is the final OOD score computed, and (iv) how are hyperparameters/thresholds and performance estimated under strong class imbalance?",
      "answer": "(i) For each complex, form a 19-dimensional trajectory feature vector consisting of the PF-ODE scalar log-likelihood plus 18 additional summaries extracted along the noising trajectory. These are designed to capture coordinated \u201cdifficulty\u201d signals when the model processes OOD inputs, including geometric inefficiency (path tortuosity / path efficiency), local instability and stiffness (a maximum Lipschitz estimate of the flow), vector-field activity and burstiness/smoothness (e.g., mean magnitude, spikiness ratio, mean acceleration), and energetic cost (total flow energy / directed work aggregated along the path). OOD samples tend to induce stiffer, burstier fields, higher acceleration, and longer, less efficient paths.\n\n(ii) Before fitting densities, apply a preprocessing map built from ID training features: a marginal quantile transform to normal scores (fit on ID), then standardization using ID mean/variance, then projection onto the top m principal components (PCA fit on ID) to obtain low-dimensional embeddings z in R^m.\n\n(iii) Fit separate class-conditional Gaussian kernel density estimators in the PCA space for ID and for OOD, using isotropic bandwidths h_ID and h_OOD. Select each bandwidth by K-fold cross-validation on the corresponding class, maximizing mean log-likelihood on held-out folds. For a test complex, compute z* and the log-density ratio L(x*) = log p\u0302_ID(z*) \u2212 log p\u0302_OOD(z*). Use the negated ratio S(x*) = \u2212L(x*) as a continuous OOD score (larger S = more OOD-like), and classify by thresholding S.\n\n(iv) Choose the decision threshold \u03c4 on a labeled calibration/validation set by sweeping \u03c4 to optimize a target trade-off such as F1 for the OOD class (the work fixes \u03c4 once by maximizing F1 on the validation split). Because ID examples can greatly outnumber OOD, estimate operating-point metrics with a balanced bootstrap protocol: keep the OOD test set fixed and repeatedly subsample an equal-sized ID test subset, then report mean \u00b1 standard deviation of AUROC (threshold-free) and thresholded metrics (accuracy, precision, recall, F1, specificity) over bootstrap replicates.",
      "source_document": "papers/2512.18454v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You want to bioinformatically validate whether a generative model\u2019s per-complex OOD scores for protein\u2013ligand structures really reflect \u201cnovelty\u201d relative to the training set. Propose a single structure-based similarity score between two protein\u2013ligand complexes that jointly accounts for (i) ligand chemistry, (ii) protein fold similarity, and (iii) binding-pose similarity. Specify how each component is computed (including any normalization choices), how you aggregate them into one score (including any clipping/inversion), what the maximum score represents, and one limitation of this aggregate when comparing complexes with very low similarity.",
      "answer": "A practical combined similarity score can be built from three complementary terms:\n\n1) **Ligand chemistry (Tanimoto):** compute Tanimoto similarity between ligand fingerprints (implemented with RDKit count-based fingerprints).\n\n2) **Protein fold similarity (TM-score):** align the two proteins with TM-align and use TM-scores **normalized by both chain lengths**, taking the higher normalized TM-score to reflect whether one protein is well represented within the other.\n\n3) **Binding-pose similarity (pocket-aligned ligand RMSD):** use the TM-align rotation/translation that aligns the protein pockets, apply it to the bound ligands, then compute ligand RMSD **between nearest points in the two ligand point clouds**; if atom counts differ, compute nearest-neighbor distances from each atom in the larger ligand to the smaller one, which increases RMSD when ligands have different sizes.\n\n**Aggregation:** define an interaction-level similarity\n\n\\[ S = \\max(\\text{Tanimoto} + \\text{TM-score} + (1 - \\text{RMSD}),\\ 0). \\]\n\nThe **maximum** is **3.0**, achieved when the complexes have an identical pocket (TM-score 1), identical ligands (Tanimoto 1), and the same binding pose (RMSD 0).\n\n**Limitation:** the score is designed to be robust for detecting/contrasting *highly similar* complexes, but its robustness degrades for ranking or comparing complexes in **low-similarity ranges** (the regime is less meaningful for their intended analysis), motivating the use of the outer \\(\\max(\\cdot,0)\\) clipping and caution when interpreting very dissimilar pairs.",
      "source_document": "papers/2512.18454v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You want to use a diffusion model\u2019s PF-ODE negative log-likelihood as a *label-light risk certificate* for a separate supervised predictor on protein\u2013ligand complexes. Let the diffusion model define a typicality score \\(L(x)=-\\log q(x)\\) and let \\(e_\\theta(x)\\) be the predictor\u2019s pointwise error. Under what assumptions can you turn concentration of \\(L(x)\\) on in-distribution (ID) data into a high-probability bound that controls the predictor error via a monotone calibration curve, and what is the resulting inequality? Also, how is the calibration curve obtained in practice from a held-out set so that \\(L(x)\\) becomes a quantitative risk stratifier?",
      "answer": "Assume (i) \\(x\\sim p_{ID}\\) and \\(L(x)=-\\log q(x)\\) has a finite ID variance \\(\\mathrm{Var}_{ID}(L(x))\\le \\sigma^2\\), where \\(L_{typ}=\\mathbb{E}_{ID}[L(x)]\\); and (ii) there exists a non-decreasing calibration curve \\(\\phi\\) such that \\(e_\\theta(x)\\le \\phi(L(x))\\) for (almost) all \\(x\\). Then for any margin \\(\\alpha>0\\),\n\\[\n\\mathbb{P}_{ID}\\big(e_\\theta(x) > \\phi(L_{typ}+\\alpha)\\big)\\;\\le\\;\\sigma^2/\\alpha^2,\n\\]\n(equivalently \\(\\mathbb{P}_{ID}(e_\\theta(x)\\le \\phi(L_{typ}+\\alpha))\\ge 1-\\sigma^2/\\alpha^2\\)).\nIn practice, \\(\\phi\\) is fit on a held-out calibration set by fitting a monotone upper envelope to the empirical scatter of \\((L(x), e_\\theta(x))\\), so that a complex\u2019s likelihood-derived typicality level maps to an empirically calibrated upper bound on expected error (risk strata).",
      "source_document": "papers/2512.18454v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a continuous diffusion / PF-ODE likelihood model for protein\u2013ligand complexes represented by 3D coordinates plus node features, how can you enforce *global translation invariance* in the coordinate channel while still computing a mathematically consistent per-complex log-likelihood? Describe (i) what coordinate transformation/projection is applied during training and PF-ODE integration, (ii) what other quantities must be projected for consistency (noise, drift, divergence probes), and (iii) how this constraint changes the terminal Gaussian term in the likelihood (i.e., the effective degrees of freedom).",
      "answer": "A consistent way to enforce translation invariance is to work in the center-of-mass (COM)-free subspace for the coordinate block.\n\n(i) Projection/constraint: For each complex, the coordinate block is COM-centered and projected to the COM-free subspace (removing global translation) at every step. This projection is applied throughout training and during PF-ODE integration (and also after numerical integration steps in reverse-time sampling / PF-ODE stepping).\n\n(ii) Consistency requirements: The same COM-free projection must be applied not only to the coordinates but also to all coordinate perturbations and dynamics that depend on them\u2014specifically the coordinate noise used to construct x_t, the coordinate components of the PF-ODE drift/updates, and the random probe vectors used for divergence estimation (Hutchinson trace estimator), so that the divergence matches the constrained dynamics.\n\n(iii) Effect on the terminal density term: Because the PF-ODE ends at a Gaussian terminal distribution, imposing the COM constraint reduces the coordinate degrees of freedom by 3 for a complex with n atoms in 3D. Accordingly, the coordinate terminal log-density uses (3n\u22123) degrees of freedom (rather than 3n), i.e. a DOF-corrected Gaussian normalization constant is used in log p1,coord in addition to the usual quadratic term \u2212||r1||^2/(2\u03c3(1)^2).",
      "source_document": "papers/2512.18454v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a chord-diagram\u2013based pipeline for quantifying RNA pseudoknotting, how does introducing a sequence-distance threshold \u03c4 change (i) which base-pair chords are grouped into the same \u201csegment\u201d and (ii) how the final pseudoknot count is computed from the resulting intersection graph? Give the formal definitions needed (distance, \u03c4-near, \u03c4-segment, and the vertex-cover objective), including how the \u03c4=0 case differs from \u03c4\u22651.",
      "answer": "Introduce a sequence-distance threshold by defining the chord distance between chords c1=(\u21131,r1) and c2=(\u21132,r2) as d(c1,c2)=max{|\u21131\u2212\u21132|, |r1\u2212r2|}. Two nested or crossed chords are called \u03c4-near if d(c1,c2)\u2264\u03c4 and the pair is not chord obstructed. A k-crossing or k-nesting {c1,\u2026,ck} is \u03c4-near if each consecutive pair ci,ci+1 is \u03c4-near. A \u03c4-segment is then a maximal nonempty set of chords forming a \u03c4-near k-nesting or \u03c4-near k-crossing; the \u03c4-segments partition the chord set. From these \u03c4-segments, form the \u03c4-segment intersection graph G\u03c4 whose vertices are \u03c4-segments and where two vertices are adjacent iff the corresponding \u03c4-segments cross.\n\nThe pseudoknot count is computed via vertex cover on G\u03c4: a secondary structure is \u03c4-pseudoknotted iff G\u03c4 has at least one edge, and for \u03c4\u22651 the number of pseudoknots is the minimum cardinality of a vertex cover of G\u03c4. The \u03c4=0 case is treated differently to match the segment-graph convention: the number of pseudoknots is the minimum cardinality over all vertex covers of minimum weight of the (\u03c4=0) segment graph (i.e., first minimize total vertex weight, then among those covers minimize cover size).",
      "source_document": "papers/2512.19939v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "RNA secondary structures can be encoded by a chord diagram D with intersection graph G (vertices are chords/base pairs; edges indicate crossings). If G is acyclic (a forest), how can you compute the genus \u03b3(D) from G, and what exact relationship holds between \u03b3(D) and the minimum vertex cover number \u03b2(G)? Briefly state the definitions needed (\u03b3(D) in terms of the adjacency matrix over \\(\\mathbb{Z}_2\\), and \u03b2(G)), and explain the methodological implication for using genus as a pseudoknot-complexity measure when intersection graphs are forests.",
      "answer": "Definitions: The vertex cover number \u03b2(G) is the size of a minimum vertex cover, i.e., a minimum-size subset A\u2286V(G) such that every edge of G has at least one endpoint in A. The genus of a chord diagram D is defined as \u03b3(D)=rank\u2082(A)/2, where A is the adjacency matrix of the intersection graph of D with coefficients in \\(\\mathbb{Z}_2\\) (equivalently, the topological genus of the band-surgery surface / can be computed by (P\u2212L)/2 using the associated double-line diagram). Relationship: If the intersection graph G of D is acyclic (a forest), then \u03b3(D)=\u03b2(G). Since both invariants are additive over disjoint unions, this holds componentwise (trees). Implication: for RNA structures whose intersection graphs are forests, genus and minimum vertex cover carry the same information\u2014so increases in genus correspond to increases in the minimum vertex cover, supporting genus as a robust quantifier/classifier of pseudoknot complexity in that regime.",
      "source_document": "papers/2512.19939v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a chord-diagram / segment-intersection-graph pipeline that identifies RNA pseudoknots by removing an \u201coptimal\u201d set of segments (a vertex cover) and then re-classifying the remaining crossingless structure into motifs (stems, bulges, interior loops, etc.), how should the algorithm handle the fact that there can be multiple optimal solutions? \n\nSpecifically: (i) describe the selection procedure that starts from independent sets and ends with choosing a single vertex cover, including how the objective ordering differs between the \u03c4=0 case and \u03c4>0, and (ii) explain why a deterministic tie-break rule (e.g., lexicographical order by segment left endpoints) can change the downstream pseudoknot *type* assignment even when the pseudoknot *count* is unchanged.",
      "answer": "(i) The procedure is:\n1) Enumerate all maximal independent sets I of the weighted segment (or \u03c4-segment) intersection graph.\n2) For \u03c4=0, rank independent sets by weight first: keep the maximum-weight independent sets, then among those keep the maximum-cardinality ones. For \u03c4>0, reverse the objective priority: keep the maximum-cardinality independent sets first, then among those keep the maximum-weight ones.\n3) Dualize the retained independent sets to vertex covers via P = V(G) \\ I.\n4) If multiple vertex covers remain optimal under the relevant objectives, choose a single cover deterministically by selecting the first cover under lexicographical order induced by indexing segments by their left endpoints.\n\n(ii) Different optimal vertex covers can remove different segments while achieving the same optimality criteria (same minimum size and/or weight), and removing different segments changes the crossingless chord diagram that remains. Since motif classification (bulge vs interior loop, etc.) is performed on this reduced structure, the same RNA can be assigned different pseudoknot interaction \u201ctypes\u201d depending on which optimal cover is removed (e.g., one optimal cover yields a pseudoknot connecting a bulge-to-bulge, while another yields a pseudoknot connecting an interior-loop-to-interior-loop). A lexicographical tie-break makes the typing reproducible but can select one of several biologically distinct motif interpretations when multiple optimal covers exist.",
      "source_document": "papers/2512.19939v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a \u03c4-segment\u2013based simplification of RNA chord diagrams, define the \u201cpersistence threshold\u201d \u03c4m (the smallest \u03c4 beyond which the \u03c4-segment partition stops changing). How would you compute \u03c4m for a given RNA structure in practice, and what structural feature(s) of the RNA does a large \u03c4m indicate? Explain the reason in terms of chord distances and why chords can remain nested yet not \u03c4-near.",
      "answer": "Define \u03c4m (for \u03c4\u22651) as the minimum value of \u03c4 such that for any \u03c4* \u2265 \u03c4m the \u03c4*-segment partition is identical to the \u03c4m-segment partition; equivalently, \u03c4m is the smallest \u03c4 for which the \u03c4-segment partition becomes equal to the augmented segment partition.\n\nTo compute \u03c4m for a structure: first compute the augmented segment partition; then compute \u03c4-segment partitions for increasing integers \u03c4>0 until the \u03c4-segment partition matches the augmented segment partition\u2014this first \u03c4 is \u03c4m.\n\nA large \u03c4m indicates the structure contains large bulges and/or large interior loops. The reason is that \u03c4-nearness is controlled by the chord distance d(c1,c2)=max(|\u21131\u2212\u21132|,|r1\u2212r2|): when there are large gaps between endpoints of chords that are nested, consecutive nested chords can fail the d(c_i,c_{i+1})\u2264\u03c4 condition for many \u03c4 values, so the segmentation does not merge those chords until \u03c4 becomes large. Thus persistent \u03c4-segment partitions (needing large \u03c4 to stabilize) are indicative of large bulges and internal loops.",
      "source_document": "papers/2512.19939v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a chord-diagram representation of an RNA secondary structure with no chord crossings, how can you algorithmically assign each unpaired nucleotide to a specific loop motif using only the segment (nested-chord) partition? Give precise criteria (in terms of chord endpoints and \u201cnested in a segment\u201d) for distinguishing: (i) stems, (ii) bulges vs (iii) interior loops between consecutive chords in a segment, (iv) multiloops created when multiple segments are nested inside another segment (including the possibility of length zero), and (v) the exterior loop and its dangling ends.",
      "answer": "For a crossingless chord diagram D, first take its segment partition S (each segment is a maximal nesting of chords).\n\n\u2022 \u201cNested in a segment\u201d: an unpaired base b is nested in a segment S if b lies strictly between the left and right endpoints of the innermost chord of S.\n\n\u2022 Stem: a stem is a k-nesting with no other bases between the endpoints of any two consecutive chords.\n\nNow fix a segment S and two consecutive chords c=(\u2113,r) and c\u2032=(\u2113\u2032,r\u2032) in S with \u2113<\u2113\u2032 (so r\u2032<r since the diagram is crossingless).\n\n\u2022 Interior loop vs bulge: look at the two intervals of bases between these consecutive chords, (\u2113,\u2113\u2032) and (r,r\u2032). If both intervals contain sequences of unpaired bases (equivalently \u2113\u2032\u2212\u2113\u22652 and r\u2212r\u2032\u22652), then the union of those two unpaired-base sequences is an interior loop. If exactly one of the two intervals contains unpaired bases, that unpaired-base sequence is a bulge.\n\n\u2022 Multiloop: if two or more segments T are nested in a segment S, then there is a multiloop consisting of all unpaired bases b that are nested in S but are not nested in any segment in T. Multiloops may have length zero (i.e., the multiloop can be the empty set).\n\n\u2022 Exterior loop and dangling ends: the exterior loop is the set of all unpaired bases that are not nested in any chord. If b0 is the first paired base and bf is the last paired base along the sequence, then unpaired bases before b0 and after bf are \u201cdangling ends,\u201d a subset of the exterior loop. The exterior loop can also be viewed as the multiloop obtained by adding an imaginary base pair between the 5\u2032 and 3\u2032 ends so that all chords become nested.",
      "source_document": "papers/2512.19939v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a zero-shot antibody design campaign, you want to triage binders not just by affinity but by \u201cdrug-like\u201d developability. What in vitro assay panel and explicit pass/fail criteria can you use to operationalize developability for aggregation, polyreactivity, hydrophobicity, and thermostability, and how are therapeutic control antibodies used to set these thresholds?",
      "answer": "A practical developability screen can combine four orthogonal biophysical assays with thresholds calibrated by therapeutic antibody controls:\n\n- Aggregation/monomericity: measure by SEC-HPLC; pass if the sample is at least 90% monomeric.\n- Polyreactivity: measure nonspecific binding by a baculovirus particle (BVP) ELISA; plate-normalize the signal so bococizumab = 1 (high polyreactivity control) and panitumumab = 0 (low polyreactivity control), and call designs non\u2011polyreactive if the normalized score is \u2264 1.\n- Hydrophobicity: measure by HIC-HPLC; normalize retention times so lirilumab = 1 (high-retention hydrophobicity reference) and require a normalized value \u2264 1 to pass.\n- Thermostability: measure by dye-free nanoDSF using the first melting transition (Tm1); classify designs as thermostable if Tm1 exceeds the bococizumab reference, measured at 61 \u00b0C.\n\nThese controls are used to calibrate each assay against empirically observed ranges for clinical-stage/approved antibodies and to define permissive but therapeutically grounded pass criteria.",
      "source_document": "papers/2512.20263v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You\u2019ve generated several de novo VHH binders and want to test, ex vivo, whether they elicit human immunogenic responses before any animal studies. Design an immunogenicity assay workflow using primary human PBMCs that captures donor-to-donor variability and includes appropriate positive/negative controls. Specify (i) how many donors, (ii) the concentration series format, (iii) the two main readouts and their timepoints, (iv) which cytokines are quantified, and (v) which controls are used to benchmark/validate the assay.",
      "answer": "A suitable ex vivo immunogenicity workflow is:\n\n(i) Donors: use primary PBMCs from 10 healthy human donors to capture donor-to-donor variability.\n\n(ii) Dose format: add each test antibody to PBMCs in a four-point, three-fold dilution series run in duplicate, with a top concentration of 30 \u00b5g/mL (delivered as 20 \u00b5L of 10\u00d7 antibody into 180 \u00b5L seeded cells).\n\n(iii) Readouts and timepoints: (1) cellular immune response/T-cell proliferation surrogate via a luminescence-based CellTiter-Glo assay measuring PBMC metabolic activity, assessed at both 48 h and 120 h; and (2) cytokine release quantified from culture supernatants using multiplex Luminex, with supernatants harvested at 48 h and 120 h (the cytokine panel is read out at the 120 h timepoint).\n\n(iv) Cytokines quantified by Luminex: IFN-\u03b3, IL-10, IL-16, IL-6, IL-8 (CXCL8), and TNF-\u03b1.\n\n(v) Controls: include (a) untreated PBMCs as a negative control baseline; (b) a clinically approved VHH therapeutic (caplacizumab) tested at equivalent concentrations as a reference negative/benchmark; and positive controls to validate immune activation consisting of ImmunoCult Human CD3/CD28 T-cell Activator plus IL-2, and an independent mitogenic stimulus with PHA-L (final treatment concentration 10 \u00b5g/mL).",
      "source_document": "papers/2512.20263v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are validating a small set of zero-shot\u2013designed antibody binders against multiple protein targets using bio-layer interferometry (BLI). Describe a tiered experimental decision rule for (i) calling a design a \u201chit\u201d in an initial single-point BLI screen, and (ii) deciding whether to accept and report kinetic parameters (kon, koff, KD) from follow-up multi-concentration measurements. In your answer, specify the screening threshold, how reference measurements are handled, what fitting approach/model is used, the quantitative fit-quality cutoffs (including any noted exception), and the key assay-level positive/negative controls needed to ensure the BLI data are interpretable.",
      "answer": "Tiered BLI validation can be run as:\n\n1) **Single-point screen (hit calling):** Run a single-point BLI binding assay with a **zero-analyte reference** (buffer alone) and use **reference-subtracted** binding responses. Classify designs as hits if the **reference-subtracted response is > 0.1 nm**, and advance only these to kinetic characterization.\n\n2) **Follow-up kinetics (accept/report KD):** Measure binding across multiple analyte concentrations and obtain kinetic parameters by **global fitting across all concentrations** (a **1:1 binding model** is used for the concentration series fitting). Assess fit quality with the **coefficient of determination (R\u00b2)** computed over association and dissociation phases; **report kinetic data only for fits with R\u00b2 > 0.95**. A noted exception is that if no interactions meet this criterion for a target (AHSP), fits with **R\u00b2 > 0.90** may be included **if they also pass visual quality assessment**. \n\n**Controls:** For each target, include a **target-level positive control** (either a commercially available IgG or the target\u2019s natural ligand) to verify assay performance; include the **zero-concentration analyte** as the reference control. Measurements are performed in **replicate** and include **positive controls with expected binding affinities** to check consistency.",
      "source_document": "papers/2512.20263v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are taking a set of de novo VHH/scFv candidates from a structure-conditioned generative model into wet-lab testing, and you want to (a) demonstrate they are not memorized from known antibodies, (b) avoid testing many near-duplicates, and (c) reduce obvious biophysical liabilities at the sequence level before expression. What concrete sequence-based novelty, diversity, and liability filters would you apply (including what databases/tools you would compare against, how novelty is quantified, how redundancy is removed, and what motifs/residues you would exclude in CDRs)?",
      "answer": "A practical pre-wet-lab filtering workflow is:\n\n1) Quantify novelty against known antibody sequences/structures\n- Compare generated antibody sequences against SAbDab by computing an edit distance over the concatenated CDR loops (for scFvs, compute separately for heavy and light chains).\n- Run sequence similarity searches with MMseqs2; retain all matches that pass MMseqs2 prefiltering/alignment by disabling E\u2011value filtering and setting the maximum number of reported matches larger than the database size.\n- Only advance designs whose closest SAbDab match is sufficiently far in CDR space (minimum CDR edit distance of 11; successful designs typically exceed 20).\n- Perform an additional novelty check with BLASTP against the non-redundant ClusteredNR database using full-length sequences; advanced designs are at least 16 residues different from the nearest neighbor (often >20).\n\n2) Remove redundancy / near-duplicates among the generated set\n- Cluster candidates by concatenated CDR sequences at 80% sequence identity (MMseqs2 clustering, default parameters) and advance only a single representative per cluster.\n\n3) Sequence-level liability filtering to increase odds of monomeric, well-behaved antibodies\n- Exclude designs containing cysteines within designed CDR loops.\n- Exclude designs containing N-linked glycosylation motifs within designed CDR loops.",
      "source_document": "papers/2512.20263v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are using a generative model to design macrocyclic peptide binders and want a fair, head-to-head comparison to RaPID-style trillion-member mRNA display libraries. What computational sampling and selection strategy can you use to (i) match the typical RaPID macrocycle search space, (ii) decide whether to include a synthesis-feasibility filter, and (iii) rank the surviving designs for synthesis\u2014specifying the ranking signal derived from structure-prediction uncertainty?",
      "answer": "A workable strategy is:\n(i) Sample macrocycles across a length range of 6\u201318 residues to mirror the typical RaPID mRNA display length distribution, enabling direct comparability.\n(ii) Do not apply an additional synthesis-feasibility filter if the goal is to maximize sequence diversity and minimize bias, especially given the lack of published evidence that vendor-recommended synthesis rules improve experimental success.\n(iii) After applying the remaining computational filters/novelty assessment, rank candidates using a structure-prediction uncertainty signal: compute the average across all inter-chain terms in the predicted alignment error matrix (ipae), and select the top designs (e.g., top 10 per target) for experimental characterization.",
      "source_document": "papers/2512.20263v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When optimizing an unsynthesizable, docked ligand by predicting atom-level edit operations, how can you incorporate protein\u2013ligand binding information so the model avoids deleting key pharmacophores\u2014specifically, what computational steps can convert 3D interaction data into promptable constraints, and how do you ensure the constraints refer to the correct atoms despite docking tools re-indexing atoms?",
      "answer": "Use an interaction-aware constraints pipeline: (1) generate a 3D conformer for the input SMILES and dock it into the target pocket with AutoDock Vina; (2) analyze the best-scoring pose with an interaction profiler (PLIP) to categorize noncovalent contacts (e.g., hydrogen bonds, \u03c0-stacking); (3) because docking can re-index atoms, perform graph alignment via substructure matching to map atom indices from the docked pose (PDB atom serials) back to the canonical atom-map numbers used in the 2D edit action space; (4) convert each critical interaction into a natural-language constraint of the form \u201cAtom [i] (Element E, connected to [N]): Dinteraction\u201d and inject these under a \u201cCRITICAL BIOLOGICAL CONSTRAINTS\u201d header, instructing the model to preserve those atoms or use bioisosteric replacements.",
      "source_document": "papers/2512.20333v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using a large language model to make minimal structural changes that improve a molecule\u2019s synthesizability (i.e., lead-optimization with a strong similarity constraint to the starting scaffold), what characteristic failure mode tends to appear if you ask the model to directly output a new SMILES string, and what two design choices can mitigate this failure\u2014one about the output representation and one about the inference strategy? Explain the mechanism and the expected impact on success rates at medium-to-high similarity thresholds.",
      "answer": "Direct SMILES generation tends to produce \u201challucinated\u201d but easy-to-synthesize molecules that drift away from the complex input scaffold: it can achieve a high rate of producing *some* valid molecule (\u201ctotal solved\u201d), but performance collapses once a similarity threshold is enforced because outputs bear little resemblance to the starting structure (structural drift/mode collapse toward generic, training-set-like chemistry).\n\nTwo mitigating design choices are:\n1) **Output representation: predict an executable edit sequence over the original molecular graph (atom-mapped edit operations such as DEL/ADD/MUTATE/CHANGE BOND, SET CHIRAL, STOP) rather than a full SMILES.** Mechanistically, editing constrains optimization to occur on the input graph, inherently preserving most structural features and reducing hallucination, which improves success rates under similarity constraints\u2014especially in the medium-to-high similarity regime (>0.6) that matters for lead optimization.\n2) **Inference strategy: use retrieval-augmented few-shot prompting (RAG) rather than zero-shot generation.** Mechanistically, retrieving multiple similar \u201cunsynthesizable\u2192synthesizable\u201d exemplars provides contextual grounding and diverse valid transformations, improving generalization; without retrieval, the model lacks the specific knowledge needed for surgical repairs and defaults to generic synthesizable outputs, worsening high-similarity success.\n\nOverall expectation: edit-based outputs with multi-example retrieval (e.g., 5-shot) outperform direct-SMILES and weaker/no retrieval (1-shot or zero-shot) once similarity thresholds are applied, even if direct-SMILES zero-shot may look better on unconstrained \u2018total solved\u2019.",
      "source_document": "papers/2512.20333v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an LLM-driven framework that optimizes a molecule\u2019s synthesizability by outputting atom-level edit commands (instead of regenerating SMILES), how can you *ground* the model\u2019s edits using a retrieval-augmented in-context learning setup? Describe (i) how to build the reference corpus of exemplars from paired \u201cunsynthesizable\u2192synthesizable\u201d molecules, including how the ground-truth edit sequences and rationales are obtained, and (ii) how exemplars are retrieved and used at inference time to reduce hallucinated/chemically unjustified edits.",
      "answer": "A retrieval-augmented in-context setup can be grounded by turning paired \u201cunsynthesizable\u2192synthesizable\u201d molecules into instructional exemplars and then retrieving the most relevant ones for each query.\n\n(i) Building the reference corpus:\n- Define a discrete editing action space where each operation targets unique atom-map numbers and is emitted as a JSON list of commands (e.g., DEL ATOM, MUTATE ATOM, ADD ATOM with new IDs \u2265500 to avoid conflicts, ADD/DEL BOND, CHANGE BOND, and stereochemistry-setting commands; terminated by STOP).\n- For each paired source molecule Msrc (unsynthesizable) and target molecule Mtgt (synthesizable), run a graph matching algorithm to extract the *minimal edit-distance path* between Msrc and Mtgt; this yields the ground-truth sequence of actions in the action space.\n- Because raw edit sequences are not self-explanatory, augment each (Msrc, Mtgt) pair by prompting an LLM post hoc to generate a coherent medicinal-chemistry rationale explaining the transformation (e.g., removing a chiral center to avoid stereoisomeric mixtures). Store each exemplar as (source/target, rationale, executable edit sequence).\n\n(ii) Retrieval and use at inference:\n- For a new query molecule, perform a similarity search against the exemplar database using Morgan fingerprints (radius 2, 2048-bit) and retrieve the top-k most similar examples (typically k=5).\n- Insert these retrieved \u201cReasoning \u2192 Edit\u201d exemplars as few-shot demonstrations in the prompt and enforce a Chain-of-Thought workflow: the model must first articulate the synthetic liabilities in natural language and only then output the JSON edit sequence.\n- This \u201creasoning then edits\u201d constraint, coupled with close structural retrieval, reduces hallucination and helps ensure the proposed modifications are chemically justified rather than arbitrary scaffold replacement.",
      "source_document": "papers/2512.20333v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You want to train/evaluate a \u201cmolecular editor\u201d that makes minimal changes to rescue synthetically infeasible, structure-based generated ligands. Describe a practical way to (i) operationally label a generated molecule as \u201cunsynthesizable\u201d using an automated retrosynthesis system, including what constraints to place on the retrosynthesis search, and (ii) construct supervised \u201csynthesis-cliff pairs\u201d by matching each unsynthesizable molecule to a purchasable analog so that the pair represents a small edit rather than a scaffold replacement. What similarity signals are combined for the matching, and what is the purpose of including a pharmacophore similarity criterion in addition to fingerprint similarity?",
      "answer": "(i) Labeling unsynthesizable: run a high-throughput retrosynthesis search (SimpRetro) using a broad reaction-template set from USPTO (filtered to templates seen at least twice) and restrict available starting materials to a commercial building-block catalog (Enamine). A generated molecule is treated as \u201cunsynthesizable\u201d if the retrosynthesis search cannot find any valid route within a fixed wall-clock budget (30 minutes CPU time), with no imposed maximum number of steps.\n\n(ii) Building synthesis-cliff pairs: for each unsynthesizable molecule, do a nearest-neighbor search in a large database of commercially available compounds (eMolecules) and accept a match only if it satisfies both (a) high 2D structural similarity by ECFP4 Tanimoto and (b) sufficient 2D pharmacophore similarity (Pharm2D). The added pharmacophore similarity constraint helps ensure that the \u201cclosest purchasable analog\u201d preserves key functional-feature patterns (interaction-relevant features) and thus corresponds to a minimal, meaningful edit across a synthesis cliff rather than a purely topological nearest neighbor that could lose the pharmacophoric arrangement (i.e., unwanted scaffold hopping/oversimplification).",
      "source_document": "papers/2512.20333v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a physics-based framework that models DMS mutational profiling across multiple RNA sequences, probe concentrations, and replicates, why might you fit parameters in two stages, and what specific parameter groups would belong to each stage? Also explain how the fitted model can be used to recover the RNA\u2019s native (zero-probe) secondary-structure ensemble.",
      "answer": "A two-stage fit separates (i) reagent/assay physics that should be shared across datasets from (ii) sequence-specific ensemble corrections needed to explain each RNA. First, optimize the global physical parameters shared across all sequences and concentrations: the reference chemical potential setting the concentration scale (\u00b5r), the binding penalty for structured/paired regions (\u2206\u00b5pairing), nucleotide-dependent adduct/binding probabilities (pbind(A, unpaired), pbind(C, unpaired), pbind(G), pbind(U)), and the false-negative parameter (m1); if no control is available, also fit a shared background factor (m0). Second, with those global parameters fixed, refine each RNA\u2019s ensemble by optimizing per-position soft-constraint parameters \u03bbi (one per nucleotide) that minimally perturb the baseline folding energies while matching the measured mutation rates. After fitting, the model can predict ensembles at any probe concentration and can be extrapolated to native conditions by setting the concentration-dependent perturbation term \u2206F[DMS](s) to zero, yielding the secondary-structure ensemble most consistent with the data at zero probe concentration.",
      "source_document": "papers/2512.20581v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You probe a designed bistable RNA where two flanking regions (A1 and A2) compete to base-pair with a shared central region (B), and you collect DMS-MaP mutation profiles over a range of probe concentrations with replicates. If the mutation rates within A1 and A2 show a concentration-dependent positional slope (a gradient along each competing strand), how can this be mechanistically interpreted in terms of strand displacement/strand invasion, and what specific qualitative signatures should appear in the inferred secondary-structure ensemble and pairing probabilities compared with a standard thermodynamic model that predicts only two fully formed helices?",
      "answer": "A concentration-dependent reactivity \u201cslope\u201d within each competing region is compatible with a strand displacement mechanism: nucleotides closest to the shared central domain (B) become protected (lower reactivity) earlier/stronger because pairing initiates near B and then progresses along the strand as displacement proceeds. An ensemble model that explains such data should therefore infer (i) a gradient in base-pairing probabilities within A1 and A2, with positions nearer B more likely to be paired than those farther away, and (ii) substantial populations of intermediate structures where B is partially paired with both competing strands (partial formation of each helix), rather than only the two end-state structures with one helix fully formed. In contrast, a baseline thermodynamic model tends to populate mainly the two fully formed alternative helices and largely misses these intermediate, co-occupancy strand-displacement states.",
      "source_document": "papers/2512.20581v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are modeling DMS-MaP mutation profiles for an RNA measured at multiple DMS concentrations with experimental replicates, and you also have a no-reagent (0 mM) control. In a physics-based likelihood model that links pairing probabilities to probe modification and then to observed mutations, how can you incorporate systematic, position-specific biases that are constant across concentrations so they do not get mistaken for structural signal? In that same framework, explain how the availability (or not) of the control affects whether you need to fit a global background mutation parameter, and what role the remaining fitted parameters play in capturing the concentration-dependent structural information.",
      "answer": "Introduce per-nucleotide correction factors (often denoted \u03b5_i) that act as site-specific offsets to the predicted mutation rates and are shared across all replicates and probe concentrations for that RNA. Calibrate \u03b5_i from the no-reagent control by choosing them so that the model\u2019s predicted mutation profile at zero probe concentration matches the control data; this makes \u03b5_i absorb systematic biases such as sequencing errors or reverse-transcription artifacts that are independent of probe concentration. With a control available, the constant background term associated with mutations on unmodified nucleotides (m0, i.e., false positives) can be omitted/absorbed because the baseline is already captured by \u03b5_i. Without a control, m0 must instead be fitted as a (sequence-shared) background parameter. After this separation, the fitted global \u201cphysical\u201d parameters (e.g., probe chemical potential scale, pairing penalty for probe interaction, nucleotide-specific modification probabilities, and the false-negative mutation parameter m1) together with sequence-specific soft constraints (\u03bb_i that adjust pairing energetics) explain the concentration-dependent increase in mutation rates via changes in modification probability driven by pairing probabilities, rather than by concentration-independent experimental noise.",
      "source_document": "papers/2512.20581v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In DMS-MaP (or related mutational profiling) experiments, you can infer alternative RNA conformations either by clustering individual sequencing reads that contain multiple co-occurring modifications (read-level ensemble deconvolution) or by fitting a physics-based model to the averaged per-nucleotide mutation fractions (ensemble-averaged inference). Compare these two strategies: what information does each exploit, what are their key failure modes/assumptions, and why does the ensemble-averaged approach remain informative in sparse-modification or rapidly interconverting regimes where read clustering can struggle?",
      "answer": "Read-level deconvolution methods cluster reads with multiple co-occurring modifications and use per-read correlation patterns to assign each cluster to a discrete structure, which can recover alternative conformations. However, they typically (i) discard the many reads containing zero or single modifications, (ii) require deep coverage to resolve low-probability states, (iii) can struggle to separate highly similar conformations and to represent continuous ensembles made of many closely related intermediates, and (iv) implicitly assume each read originates from a single structure\u2014an assumption that may break down for RNAs with rapid conformational dynamics and/or long probing times.\n\nAn ensemble-averaged physics-based approach instead calibrates a physical model against averaged mutational fractions, so every read\u2014including \u201cnegative\u201d reads with no modifications\u2014contributes signal, making it effective even in sparse-modification regimes. Because it directly models ensemble-averaged equilibrium behavior, it naturally accommodates fast interconversions between states, and it can quantify continuous ensembles (e.g., along strand-displacement pathways). The trade-off is that it does not explicitly use per-read co-variation, though read-level information could in principle be incorporated later to fine-tune the populations of specific structures.",
      "source_document": "papers/2512.20581v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You are fitting a physics-based model of DMS-MaP chemical probing data that predicts a per-nucleotide mutation probability for each RNA, nucleotide position, probe concentration, and experimental replicate, and you observe (Mi mutations out of ni reads) at each condition. How would you formulate an appropriate objective function for parameter estimation, explicitly accounting for the discrete count nature and variable coverage, and what optimization strategy can you use to keep fitted parameters physically meaningful (give examples of the kinds of bound constraints you would impose on probe-interaction, modification-probability, and reverse-transcription error parameters)?",
      "answer": "Use a binomial likelihood for the mutation counts because each read is a Bernoulli trial (mutation vs no mutation). For each data point i (indexing sequence, position, concentration, replicate), with coverage ni and model-predicted mutation probability Mi, the likelihood is Binomial(Mexp_i; ni, Mi). Fit parameters by minimizing the summed negative log-likelihood over all i.\n\nFor optimization, use a bound-constrained, quasi-Newton optimizer such as L-BFGS-B so parameters remain physically plausible. Impose bounds such as: probe chemical potential at a reference concentration \u00b5r in a finite energy range (e.g. \u22125 to 5 kcal/mol); pairing penalty \u2206\u00b5pairing constrained non-negative (e.g. 0 to 10 kcal/mol); chemical modification probabilities pbind constrained to [0,1]; false-positive mutation parameter m0 kept small (e.g. [0,0.1]); and false-negative parameter m1 bounded to a reasonable positive range (e.g. [0.1,10]). (Sequence-specific soft constraints \u03bbi can also be bounded, e.g. within [\u22121,1] kcal/mol, to regularize them.)",
      "source_document": "papers/2512.20581v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You suspect a ligand\u2013target activity benchmark built from the medicinal-chemistry literature contains provenance/\u201cchemist style\u201d shortcut signals that inflate model performance. Describe a concrete diagnostic experiment to test this hypothesis that (i) explicitly measures how predictable the data source is from structure, and (ii) tests whether activity can still be predicted when direct molecular descriptors are withheld. What inputs, model outputs, and split strategy would you use, and what qualitative outcome would constitute evidence of intent leakage rather than genuine structure\u2013activity learning?",
      "answer": "One diagnostic is a two-stage shortcut test:\n\n1) Source (author/lab) identification from structure: standardize molecules and featurize them with Morgan/ECFP fingerprints, then train a multiclass classifier to predict the publication author ID from the fingerprint. Use scaffold-grouped/scaffold-disjoint splitting (e.g., Bemis\u2013Murcko scaffolds as groups) so the task is not solved by near-duplicate analogues. The model outputs an author-probability vector a(x) (a softmax distribution over the author classes) for each molecule.\n\n2) Author-only activity prediction: build compound\u2013target activity examples but train an activity classifier that takes only (a) the author-probability vector a(x) from stage 1 and (b) a representation of the target/protein identity (e.g., a feature-hashed target-ID vector). Do not provide any direct molecular descriptors (no ECFP/graphs/SMILES embeddings) to the activity model.\n\nEvidence of intent leakage is when the author+protein model achieves activity-prediction performance close to a simple structure-based baseline (e.g., ECFP+protein under the same scaffold-disjoint split), and adding author probabilities on top of ECFPs yields only modest gains. This indicates that much of the benchmark\u2019s predictive signal can be recovered by learning which authors tend to work on which targets/chemotypes (dataset sociology) rather than learning causal structure\u2013activity relationships.",
      "source_document": "papers/2512.20924v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a hotspot-guided, fragment-assembly genetic algorithm for de novo ligand design, what are the components of the composite multi-objective fitness function used to evolve candidate molecules, and how is the \u201cpocket fit\u201d term computed to combine geometric containment within the pocket with alignment to predicted binding hotspots?",
      "answer": "The ligand generator scores each molecule m with a composite fitness that balances drug-like properties, target-pocket compatibility (including hotspot alignment), chemical novelty, and conformational realism while penalizing synthetic difficulty:\n\nF(m) = wp\u00b7Sproxy + wf\u00b7Sfit + wn\u00b7Snovelty + ws\u00b7Sstrain \u2212 \u03bb\u00b7PSA.\n\n\u2022 Proxy score Sproxy (drug-likeness): a weighted average of QED, LogP, normalized molecular weight, and rotatable bonds.\n\u2022 Pocket fit Sfit (pocket compatibility + hotspot alignment):\n  Sfit = 0.6\u00b7Finside + 0.4\u00b7(1 \u2212 dhot/(dhot + 4.0)),\n  where Finside is the fraction of heavy atoms lying within the pocket radius and dhot is the mean distance from ligand atoms to the nearest hotspot (smaller dhot increases the score).\n\u2022 Novelty Snovelty: 1 \u2212 max(Tc), where Tc is the Tanimoto similarity to reference/known ligands computed with 2048-bit Morgan fingerprints.\n\u2022 Strain energy term Sstrain: a penalty derived from per-atom UFF energy after constrained embedding (scaled 0\u20131) to discourage high-energy/unrealistic conformers.\n\u2022 PSA penalty (synthetic accessibility): a penalty derived from fragment complexity and ring topology; it is subtracted (scaled by \u03bb) to de-prioritize synthetically difficult molecules.\n\nThe weights are set empirically (wp = 0.45, wf = 0.35, wn = 0.15, ws = 0.2), and only top-scoring candidates are retained across generations to drive evolutionary refinement.",
      "source_document": "papers/2512.21301v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using an automated pocket-detection tool like DOGSiteScorer to choose a binding site for downstream docking/design, what geometric and physicochemical descriptors can be combined into a single \u201cdruggability\u201d score, and how is that composite score used to prioritize which pocket(s) to carry forward?",
      "answer": "Pocket druggability can be summarized by combining key geometric and physicochemical descriptors into a composite score. The descriptors include geometric cavity properties (Volume in \u00c5^3, Depth, and Enclosure/\u201cclosedness\u201d as a percent) plus physicochemical interaction potential (Hydrophobicity as a percent, Aromaticity, and counts of H-bond Donors+Acceptors). A composite scoring function is used:\n\nScore = 0.3\u00d7Volume + 0.2\u00d7Depth + 0.2\u00d7Enclosure\u00d7100 + 0.1\u00d7Hydrophobicity\u00d7100 + 0.1\u00d7Aromaticity + 0.1\u00d7(Donors+Acceptors).\n\nAfter computing this score for each detected pocket, scores are normalized to a 0\u20131 scale and pockets are sorted in descending order; the top-scoring pocket is selected as the most druggable region for downstream modeling (with the top 3 pockets carried forward).",
      "source_document": "papers/2512.21301v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When constructing a weighted gene co-expression network from bulk RNA-seq to prioritize AML biomarkers, how is the soft-thresholding power chosen to convert correlations into an adjacency matrix, and once modules are detected, how are modules summarized and \u201chub genes\u201d identified for target prioritization?",
      "answer": "Correlations are converted to a weighted adjacency using a power transform Aij = |rij|^\u03b2, where \u03b2 is selected to approximate scale-free topology at the inflection point that maximizes the scale-free fit (reported as R\u00b2 > 0.85) while keeping sufficient mean connectivity so the network is not overly sparse (here \u03b2 = 6). After module detection, each module is summarized by its module eigengene\u2014the first principal component (PC1) of the module\u2019s normalized expression matrix computed by PCA\u2014and hub genes are prioritized by intramodular connectivity, i.e., genes with the highest summed edge weights (kWithin) within their module.",
      "source_document": "papers/2512.21301v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using molecular docking to validate a de novo\u2013generated ligand against a putative AML protein target, what methodological choices can be used to (i) make the predicted binding pose robust and convergent, and (ii) interpret whether multiple top-ranked poses indicate a stable binding mode or an unstable prediction?",
      "answer": "Robustness/convergence can be increased by docking the same ligand\u2013protein pair with two independent docking platforms (SwissDock and CB.DOCK2), using a common docking engine/environment (AutoDock Vina), and including blind docking to help identify the most probable binding cavities. The binding site can be defined using conserved catalytic residues (from sequence homology/crystallographic knowledge) together with the top consensus predicted pocket. A rigorous search is enforced by selecting the lowest-energy binding mode for analysis and applying high sampling exhaustivity (exhaustivity = 35) to more thoroughly explore ligand conformational space and avoid local minima.\n\nTo interpret the set of top poses: binding free energies in roughly the \u22126 to \u22127 kcal/mol range are taken as evidence of spontaneous binding with moderate-to-high affinity (low micromolar Ki). If the energy gap between the best and other high-ranked poses is very small (e.g., ~0.4 kcal/mol across the top models), this indicates a \u201cflat\u201d energy landscape where the ligand can adopt several closely related, energetically favorable orientations\u2014supporting a stable binding mode rather than a single brittle pose prediction.",
      "source_document": "papers/2512.21301v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a fragment-assembly evolutionary de novo drug design pipeline, how can you quantify and enforce that generated molecules are genuinely novel relative to a reference ligand set, and what specific similarity metric and fingerprint representation can be used to define a novelty score suitable for inclusion in a multi-objective fitness function?",
      "answer": "Novelty can be enforced by computing, for each generated molecule, its maximum similarity to any molecule in a reference set and penalizing high similarity. A concrete definition is:\n\n- Compute Tanimoto similarity (Tc) between the generated molecule and each reference ligand using Morgan (circular) fingerprints of 2048 bits.\n- Define the novelty score as S_novelty = 1 \u2212 max(Tc), so a molecule is most \u201cnovel\u201d when even its closest reference neighbor has low similarity.\n\nThis S_novelty term can then be added to the composite fitness function (alongside drug-likeness, pocket-fit, and strain/feasibility penalties) to bias evolution toward structures that are distinct from known/training compounds.",
      "source_document": "papers/2512.21301v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an allele\u2013haplotype incidence representation for a cohort of H haplotypes, you can store each allele\u2019s carrier set either as (i) a dense bitmap of length H or (ii) a sparse list of the k carrier haplotype IDs. Using the simple storage-cost model for these two encodings, what break-even carrier count k* determines when you should switch between sparse and dense storage, and how does k relative to k* guide the encoding choice?",
      "answer": "Under the cost model, a dense bitmap costs Cdense(H)=H bits (ignoring constant overhead), while a sparse carrier list costs Csparse(k,H)=k\u2308log2 H\u2309 bits when haplotypes are stored as fixed-width integer identifiers. Setting Cdense=Csparse gives the break-even carrier count k*\u2248H/log2 H. Alleles with k\u226ak* are more efficiently stored as sparse lists, whereas alleles with k\u226bk* favor dense bitmaps; an adaptive scheme chooses the cheaper encoding per allele.",
      "source_document": "papers/2512.21320v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You have a population-scale allele\u2013haplotype incidence data structure where each allele\u2019s carrier set is stored either as a sparse list of carrier haplotype IDs or as a dense bitmap over H haplotypes (chosen adaptively per allele). How would you compute the set of haplotypes carrying *both* of two alleles, and what are the time complexities in (i) sparse\u2013sparse, (ii) dense\u2013dense, and (iii) mixed sparse\u2013dense cases? Explain briefly why this hybrid per-allele encoding tends to make such intersections efficient for rare variants.",
      "answer": "Compute the shared carriers by intersecting the two alleles\u2019 carrier representations:\n\n- Sparse\u2013sparse: merge the two sorted carrier lists, in O(k1 + k2) time.\n- Dense\u2013dense: perform a bitwise conjunction (AND) of the two bitmaps, in O(H / w) time, where w is the machine word size.\n- Mixed sparse\u2013dense: iterate over the smaller representation and test membership in the larger representation.\n\nThis tends to be efficient for rare variants because adaptive per-allele encoding stores rare alleles in the sparse regime, so intersections involving rare alleles usually operate on small carrier lists (and multi-allele filters can be implemented as successive intersections with early termination).",
      "source_document": "papers/2512.21320v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "You have an allele\u2013haplotype incidence representation (alleles as rows, haplotypes as columns) and want to produce a pangenome graph visualization without changing the underlying information about which haplotypes carry which variants. Describe two different backbone segmentation strategies that can be derived from the same incidence data\u2014one that yields an interpretable structural-variant\u2013focused graph and one that yields a denser graph that includes SNVs\u2014and explain (i) how SNVs and structural variants are represented in each strategy, (ii) what trade-off these strategies make, and (iii) what key property remains unchanged across the two graphs.",
      "answer": "Two graph constructions can be derived from the same allele\u2013haplotype incidence data by changing only how the reference backbone is segmented. (1) In an SV-backbone construction, the backbone is segmented only at structural-variant breakpoints; structural variants are represented explicitly as alternative paths (\u201cbubbles\u201d) between backbone segments, while SNVs/short indels do not split the backbone and are instead retained as annotations on the corresponding backbone segments. This yields a compact, largely linear, interpretable topology emphasizing large rearrangements. (2) In a coarse, SNV+SV construction, the backbone is additionally segmented at fixed genomic intervals (e.g., 1 kb tiling) as well as at SV boundaries; structural variants remain bubbles between segments, while SNVs are represented as short alternative nodes attached to the relevant backbone segments rather than splitting the path at single-base resolution, producing a much denser and visually complex graph.\nThe trade-off is positional resolution versus graph complexity/size: finer segmentation improves localization/detail but greatly increases nodes/edges and visual crowding, whereas SV-only segmentation simplifies interpretation. In both cases, the underlying allele\u2013haplotype incidence (which haplotypes realize each alternative) remains unchanged; only the abstraction level of the backbone segmentation differs.",
      "source_document": "papers/2512.21320v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an allele-centric pangenome representation where H1 stores, for each allele, the set of carrier haplotypes (as either a sparse list or dense bitmap), you also maintain a dual path-centric structure H2 to support questions that require *ordering* along haplotypes. Describe (i) how H2 represents each haplotype and what auxiliary index it maintains, and (ii) the precise information-equivalence relationship between an allele\u2019s carrier set in H1 and the corresponding representation in H2 that guarantees you can answer both carrier queries and local ordering/topology queries without materializing a full pangenome graph.",
      "answer": "(i) H2 represents each haplotype as an ordered sequence of abstract graph edges corresponding to reference segments and variant-induced alternative segments. It maintains an inverted index that maps each edge to the set of haplotypes that traverse it, enabling efficient adjacency/ordering/local-topology queries.\n\n(ii) H1 and H2 are information-equivalent with respect to allele\u2013haplotype incidence and ordered traversals: for any allele stored as a row in H1, the set of haplotypes carrying that allele is exactly the same as the set of haplotypes whose H2 paths traverse the corresponding edge(s). Conversely, the ordered haplotype paths in H2 can be reconstructed from the same underlying data used to build H1. This separation lets H1 answer carrier-set and population-incidence queries, while H2 supplies ordered genomic context (e.g., local neighborhoods around structural variants) without constructing a full pangenome graph.",
      "source_document": "papers/2512.21320v1.pdf",
      "mode": "textual",
      "content_refs": []
    }
  ],
  "timestamp": "2026-01-02T18:29:44.366967+00:00"
}