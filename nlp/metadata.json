{
  "corpus": "nlp",
  "source": "arxiv",
  "search_query": "cat:cs.CL AND (language model OR NLP OR text OR parsing OR semantics)",
  "curated_at": "2025-12-26T20:16:14.879686+00:00",
  "total_papers": 200,
  "papers_evaluated": 286,
  "acceptance_rate": 0.6993006993006993,
  "papers": [
    {
      "arxiv_id": "2512.21336v1",
      "title": "Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty",
      "authors": [
        {
          "name": "Ziyu Chen"
        },
        {
          "name": "Xinbei Jiang"
        },
        {
          "name": "Peng Sun"
        },
        {
          "name": "Tao Lin"
        }
      ],
      "abstract": "Masked Diffusion Models (MDMs) offer flexible, non-autoregressive generation, but this freedom introduces a challenge: final output quality is highly sensitive to the decoding order. We are the first to formalize this issue, attributing the variability in output quality to the cumulative predictive uncertainty along a generative path. To quantify this uncertainty, we introduce Denoising Entropy, a computable metric that serves as an internal signal for evaluating generative process. Leveraging this metric, we propose two algorithms designed to optimize the decoding path: a post-hoc selection method and a real-time guidance strategy. Experiments demonstrate that our entropy-guided methods significantly improve generation quality, consistently boosting accuracy on challenging reasoning, planning, and code benchmarks. Our work establishes Denoising Entropy as a principled tool for understanding and controlling generation, effectively turning the uncertainty in MDMs from a liability into a key advantage for discovering high-quality solutions.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-24T18:59:51+00:00",
      "updated": "2025-12-24T18:59:51+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21336v1",
      "file": "papers/2512.21336v1.pdf"
    },
    {
      "arxiv_id": "2512.21332v1",
      "title": "C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling",
      "authors": [
        {
          "name": "Jin Qin"
        },
        {
          "name": "Zihan Liao"
        },
        {
          "name": "Ziyin Zhang"
        },
        {
          "name": "Hang Yu"
        },
        {
          "name": "Peng Di"
        },
        {
          "name": "Rui Wang"
        }
      ],
      "abstract": "We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing the LLM's causal representations acquired during pretraining, while also 2) being able to aggregate information from all tokens in the sequence, breaking the information bottleneck in EOS-based sequence embeddings, and 3) supporting flexible adaptation of embedding dimension, serving as an alternative to MRL. Trained on three million publicly available data, C2LLM models set new records on MTEB-Code among models of similar sizes, with C2LLM-7B ranking 1st on the overall leaderboard.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-24T18:59:01+00:00",
      "updated": "2025-12-24T18:59:01+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21332v1",
      "file": "papers/2512.21332v1.pdf"
    },
    {
      "arxiv_id": "2512.21323v1",
      "title": "Parallel Token Prediction for Language Models",
      "authors": [
        {
          "name": "Felix Draxler"
        },
        {
          "name": "Justus Will"
        },
        {
          "name": "Farrin Marouf Sofian"
        },
        {
          "name": "Theofanis Karaletsos"
        },
        {
          "name": "Sameer Singh"
        },
        {
          "name": "Stephan Mandt"
        }
      ],
      "abstract": "We propose Parallel Token Prediction (PTP), a universal framework for parallel sequence generation in language models. PTP jointly predicts multiple dependent tokens in a single transformer call by incorporating the sampling procedure into the model. This reduces the latency bottleneck of autoregressive decoding, and avoids the restrictive independence assumptions common in existing multi-token prediction methods. We prove that PTP can represent arbitrary autoregressive sequence distributions. PTP is trained either by distilling an existing model or through inverse autoregressive training without a teacher. Experimentally, we achieve state-of-the-art speculative decoding performance on Vicuna-7B by accepting over four tokens per step on Spec-Bench. The universality of our framework indicates that parallel generation of long sequences is feasible without loss of modeling power.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-12-24T18:46:55+00:00",
      "updated": "2025-12-24T18:46:55+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21323v1",
      "file": "papers/2512.21323v1.pdf"
    },
    {
      "arxiv_id": "2512.21280v1",
      "title": "SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance",
      "authors": [
        {
          "name": "Divij Dudeja"
        },
        {
          "name": "Mayukha Pal"
        }
      ],
      "abstract": "The user of Engineering Manuals (EM) finds it difficult to read EM s because they are long, have a dense format which includes written documents, step by step procedures, and standard parameter lists for engineering equipment. Off the shelf transformers, especially compact ones, treat this material as a flat stream of tokens. This approach leads to confident but incorrect numeric answers and forces the models to memorize separate facts inefficiently. SMART (Structured Memory and Reasoning Transformer) offers a different and practical solution to the above problem. SMART structures its processing by using a hierarchical approach, and is based upon three main job categories (1) A syntax-aware Fact Extractor (Grammarian) Tree LSTM which extracts facts as subject relation object relations from EM sentences (2) A compact indexed memory MANN (Memory Augmented Neural Network) that indexes these Rational Subject Relation Objects as 384 dimensional vectors that are associated with the source of the information, and (3) A 6 layer Transformer that learns to fuse the previously retrieved facts into its generated response. The entire SMART model utilizes 45.51M parameters, which is 64% less than GPT-2 (124M) and 69% less than BERT (133M), and it achieves a 21.3% higher accuracy than GPT-2, indicating that SMART fits the data better with the least amount of processing requirements. SMART employs dual modes of inference an indexed fast path for known documents (sub-second answer times) and an indexed dynamic path assisted by RAGs for new uploads (FAISS Top 20 results with memory severed at 64 slots). In real world deployment, this framework leads to more well supported results with reduced hallucinations than comparable small transformer models.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-24T16:59:04+00:00",
      "updated": "2025-12-24T16:59:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21280v1",
      "file": "papers/2512.21280v1.pdf"
    },
    {
      "arxiv_id": "2512.21120v1",
      "title": "ClarifyMT-Bench: Benchmarking and Improving Multi-Turn Clarification for Conversational Large Language Models",
      "authors": [
        {
          "name": "Sichun Luo"
        },
        {
          "name": "Yi Huang"
        },
        {
          "name": "Mukai Li"
        },
        {
          "name": "Shichang Meng"
        },
        {
          "name": "Fengyuan Liu"
        },
        {
          "name": "Zefa Hu"
        },
        {
          "name": "Junlan Feng"
        },
        {
          "name": "Qi Liu"
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed as conversational assistants in open-domain, multi-turn settings, where users often provide incomplete or ambiguous information. However, existing LLM-focused clarification benchmarks primarily assume single-turn interactions or cooperative users, limiting their ability to evaluate clarification behavior in realistic settings. We introduce \\textbf{ClarifyMT-Bench}, a benchmark for multi-turn clarification grounded in a five-dimensional ambiguity taxonomy and a set of six behaviorally diverse simulated user personas. Through a hybrid LLM-human pipeline, we construct 6,120 multi-turn dialogues capturing diverse ambiguity sources and interaction patterns. Evaluating ten representative LLMs uncovers a consistent under-clarification bias: LLMs tend to answer prematurely, and performance degrades as dialogue depth increases. To mitigate this, we propose \\textbf{ClarifyAgent}, an agentic approach that decomposes clarification into perception, forecasting, tracking, and planning, substantially improving robustness across ambiguity conditions. ClarifyMT-Bench establishes a reproducible foundation for studying when LLMs should ask, when they should answer, and how to navigate ambiguity in real-world human-LLM interactions.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "published": "2025-12-24T11:39:00+00:00",
      "updated": "2025-12-24T11:39:00+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21120v1",
      "file": "papers/2512.21120v1.pdf"
    },
    {
      "arxiv_id": "2512.21110v1",
      "title": "Beyond Context: Large Language Models Failure to Grasp Users Intent",
      "authors": [
        {
          "name": "Ahmed M. Hussain"
        },
        {
          "name": "Salahuddin Salahuddin"
        },
        {
          "name": "Panos Papadimitratos"
        }
      ],
      "abstract": "Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.CY"
      ],
      "published": "2025-12-24T11:15:57+00:00",
      "updated": "2025-12-24T11:15:57+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21110v1",
      "file": "papers/2512.21110v1.pdf"
    },
    {
      "arxiv_id": "2512.21107v1",
      "title": "Semi-Supervised Learning for Large Language Models Safety and Content Moderation",
      "authors": [
        {
          "name": "Eduard Stefan Dinuta"
        },
        {
          "name": "Iustin Sirbu"
        },
        {
          "name": "Traian Rebedea"
        }
      ],
      "abstract": "Safety for Large Language Models (LLMs) has been an ongoing research focus since their emergence and is even more relevant nowadays with the increasing capacity of those models. Currently, there are several guardrails in place for all public LLMs and multiple proposed datasets for training safety classifiers. However, training these safety classifiers relies on large quantities of labeled data, which can be problematic to acquire, prone to labeling errors, or often include synthetic data. To address these issues, we suggest a different approach: utilizing semi-supervised learning techniques, which leverage both labeled and unlabeled data, to improve the performance on the safety task. We analyze the improvements that these techniques can offer for both prompts given to Large Language Models and the responses to those requests. Moreover, since augmentation is the central part of semi-supervised algorithms, we demonstrate the importance of using task-specific augmentations, which significantly increase the performance when compared to general-purpose augmentation techniques.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-24T11:12:09+00:00",
      "updated": "2025-12-24T11:12:09+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21107v1",
      "file": "papers/2512.21107v1.pdf"
    },
    {
      "arxiv_id": "2512.21017v1",
      "title": "Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy",
      "authors": [
        {
          "name": "Xiaofeng Shi"
        },
        {
          "name": "Qian Kou"
        },
        {
          "name": "Yuduo Li"
        },
        {
          "name": "Hua Zhou"
        }
      ],
      "abstract": "With the rapid advancement of Large Language Models (LLMs), the Chain-of-Thought (CoT) component has become significant for complex reasoning tasks. However, in conventional Supervised Fine-Tuning (SFT), the model could allocate disproportionately more attention to CoT sequences with excessive length. This reduces focus on the much shorter but essential Key portion-the final answer, whose correctness directly determines task success and evaluation quality. To address this limitation, we propose SFTKey, a two-stage training scheme. In the first stage, conventional SFT is applied to ensure proper output format, while in the second stage, only the Key portion is fine-tuned to improve accuracy. Extensive experiments across multiple benchmarks and model families demonstrate that SFTKey achieves an average accuracy improvement exceeding 5\\% over conventional SFT, while preserving the ability to generate correct formats. Overall, this study advances LLM fine-tuning by explicitly balancing CoT learning with additional optimization on answer-relevant tokens.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-24T07:24:31+00:00",
      "updated": "2025-12-24T07:24:31+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21017v1",
      "file": "papers/2512.21017v1.pdf"
    },
    {
      "arxiv_id": "2512.21002v1",
      "title": "Distilling the Essence: Efficient Reasoning Distillation via Sequence Truncation",
      "authors": [
        {
          "name": "Wei-Rui Chen"
        },
        {
          "name": "Vignesh Kothapalli"
        },
        {
          "name": "Ata Fatahibaarzi"
        },
        {
          "name": "Hejian Sang"
        },
        {
          "name": "Shao Tang"
        },
        {
          "name": "Qingquan Song"
        },
        {
          "name": "Zhipeng Wang"
        },
        {
          "name": "Muhammad Abdul-Mageed"
        }
      ],
      "abstract": "Distilling the reasoning capabilities from a large language model (LLM) to a smaller student model often involves training on substantial amounts of reasoning data. However, distillation over lengthy sequences with prompt (P), chain-of-thought (CoT), and answer (A) segments makes the process computationally expensive. In this work, we investigate how the allocation of supervision across different segments (P, CoT, A) affects student performance. Our analysis shows that selective knowledge distillation over only the CoT tokens can be effective when the prompt and answer information is encompassed by it. Building on this insight, we establish a truncation protocol to quantify computation-quality tradeoffs as a function of sequence length. We observe that training on only the first $50\\%$ of tokens of every training sequence can retain, on average, $\\approx94\\%$ of full-sequence performance on math benchmarks while reducing training time, memory usage, and FLOPs by about $50\\%$ each. These findings suggest that reasoning distillation benefits from prioritizing early reasoning tokens and provides a simple lever for computation-quality tradeoffs. Codes are available at https://github.com/weiruichen01/distilling-the-essence.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-24T06:57:35+00:00",
      "updated": "2025-12-24T06:57:35+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21002v1",
      "file": "papers/2512.21002v1.pdf"
    },
    {
      "arxiv_id": "2512.20983v1",
      "title": "Automatic Replication of LLM Mistakes in Medical Conversations",
      "authors": [
        {
          "name": "Oleksii Proniakin"
        },
        {
          "name": "Diego Fajardo"
        },
        {
          "name": "Ruslan Nazarenko"
        },
        {
          "name": "Razvan Marinescu"
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly evaluated in clinical settings using multi-dimensional rubrics which quantify reasoning quality, safety, and patient-centeredness. Yet, replicating specific mistakes in other LLM models is not straightforward and often requires manual effort. We introduce MedMistake, an automatic pipeline that extracts mistakes LLMs make in patient-doctor conversations and converts them into a benchmark of single-shot QA pairs. Our pipeline (1) creates complex, conversational data between an LLM patient and LLM doctor, (2) runs an evaluation with a committee of 2 LLM judges across a variety of dimensions and (3) creates simplified single-shot QA scenarios from those mistakes. We release MedMistake-All, a dataset of 3,390 single-shot QA pairs where GPT-5 and Gemini 2.5 Pro are currently failing to answer correctly, as judged by two LLM judges. We used medical experts to validate a subset of 211/3390 questions (MedMistake-Bench), which we used to run a final evaluation of 12 frontier LLMs: Claude Opus 4.5, Claude Sonnet 4.5, DeepSeek-Chat, Gemini 2.5 Pro, Gemini 3 Pro, GPT-4o, GPT-5, GPT-5.1, GPT-5.2, Grok 4, Grok 4.1, Mistral Large. We found that GPT models, Claude and Grok obtained the best performance on MedMistake-Bench. We release both the doctor-validated benchmark (MedMistake-Bench), as well as the full dataset (MedMistake-All) at https://huggingface.co/datasets/TheLumos/MedicalMistakeBenchmark.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-24T06:17:21+00:00",
      "updated": "2025-12-24T06:17:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20983v1",
      "file": "papers/2512.20983v1.pdf"
    },
    {
      "arxiv_id": "2512.20954v1",
      "title": "Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models",
      "authors": [
        {
          "name": "Xiang Zhang"
        },
        {
          "name": "Jiaqi Wei"
        },
        {
          "name": "Yuejin Yang"
        },
        {
          "name": "Zijie Qiu"
        },
        {
          "name": "Yuhan Chen"
        },
        {
          "name": "Zhiqiang Gao"
        },
        {
          "name": "Muhammad Abdul-Mageed"
        },
        {
          "name": "Laks V. S. Lakshmanan"
        },
        {
          "name": "Wanli Ouyang"
        },
        {
          "name": "Chenyu You"
        },
        {
          "name": "Siqi Sun"
        }
      ],
      "abstract": "Chain-of-Thought (CoT) prompting has significantly advanced task-solving capabilities in natural language processing with large language models. Unlike standard prompting, CoT encourages the model to generate intermediate reasoning steps, non-answer tokens, that help guide the model toward more accurate final outputs. These intermediate steps enable more complex reasoning processes such as error correction, memory management, future planning, and self-reflection. However, applying CoT to non-natural language domains, such as protein and RNA language models, is not yet possible, primarily due to the limited expressiveness of their token spaces (e.g., amino acid tokens). In this work, we propose and define the concept of language expressiveness: the ability of a given language, using its tokens and grammar, to encode information. We show that the limited expressiveness of protein language severely restricts the applicability of CoT-style reasoning. To overcome this, we introduce reflection pretraining, for the first time in a biological sequence model, which enables the model to engage in intermediate reasoning through the generation of auxiliary \"thinking tokens\" beyond simple answer tokens. Theoretically, we demonstrate that our augmented token set significantly enhances biological language expressiveness, thereby improving the overall reasoning capacity of the model. Experimentally, our pretraining approach teaches protein models to self-correct and leads to substantial performance gains compared to standard pretraining.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-24T05:25:17+00:00",
      "updated": "2025-12-24T05:25:17+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20954v1",
      "file": "papers/2512.20954v1.pdf"
    },
    {
      "arxiv_id": "2512.20950v1",
      "title": "MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment",
      "authors": [
        {
          "name": "Mohammad Mahdi Abootorabi"
        },
        {
          "name": "Alireza Ghahramani Kure"
        },
        {
          "name": "Mohammadali Mohammadkhani"
        },
        {
          "name": "Sina Elahimanesh"
        },
        {
          "name": "Mohammad Ali Ali Panah"
        }
      ],
      "abstract": "This paper presents our system for SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval. In an era where misinformation spreads rapidly, effective fact-checking is increasingly critical. We introduce TriAligner, a novel approach that leverages a dual-encoder architecture with contrastive learning and incorporates both native and English translations across different modalities. Our method effectively retrieves claims across multiple languages by learning the relative importance of different sources in alignment. To enhance robustness, we employ efficient data preprocessing and augmentation using large language models while incorporating hard negative sampling to improve representation learning. We evaluate our approach on monolingual and crosslingual benchmarks, demonstrating significant improvements in retrieval accuracy and fact-checking performance over baselines.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-12-24T05:14:40+00:00",
      "updated": "2025-12-24T05:14:40+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20950v1",
      "file": "papers/2512.20950v1.pdf"
    },
    {
      "arxiv_id": "2512.20949v1",
      "title": "Neural Probe-Based Hallucination Detection for Large Language Models",
      "authors": [
        {
          "name": "Shize Liang"
        },
        {
          "name": "Hongzhi Wang"
        }
      ],
      "abstract": "Large language models(LLMs) excel at text generation and knowledge question-answering tasks, but they are prone to generating hallucinated content, severely limiting their application in high-risk domains. Current hallucination detection methods based on uncertainty estimation and external knowledge retrieval suffer from the limitation that they still produce erroneous content at high confidence levels and rely heavily on retrieval efficiency and knowledge coverage. In contrast, probe methods that leverage the model's hidden-layer states offer real-time and lightweight advantages. However, traditional linear probes struggle to capture nonlinear structures in deep semantic spaces.To overcome these limitations, we propose a neural network-based framework for token-level hallucination detection. By freezing language model parameters, we employ lightweight MLP probes to perform nonlinear modeling of high-level hidden states. A multi-objective joint loss function is designed to enhance detection stability and semantic disambiguity. Additionally, we establish a layer position-probe performance response model, using Bayesian optimization to automatically search for optimal probe insertion layers and achieve superior training results.Experimental results on LongFact, HealthBench, and TriviaQA demonstrate that MLP probes significantly outperform state-of-the-art methods in accuracy, recall, and detection capability under low false-positive conditions.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-24T05:10:19+00:00",
      "updated": "2025-12-24T05:10:19+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20949v1",
      "file": "papers/2512.20949v1.pdf"
    },
    {
      "arxiv_id": "2512.20908v1",
      "title": "Where Did This Sentence Come From? Tracing Provenance in LLM Reasoning Distillation",
      "authors": [
        {
          "name": "Kaiyuan Liu"
        },
        {
          "name": "Shaotian Yan"
        },
        {
          "name": "Rui Miao"
        },
        {
          "name": "Bing Wang"
        },
        {
          "name": "Chen Shen"
        },
        {
          "name": "Jun Zhang"
        },
        {
          "name": "Jieping Ye"
        }
      ],
      "abstract": "Reasoning distillation has attracted increasing attention. It typically leverages a large teacher model to generate reasoning paths, which are then used to fine-tune a student model so that it mimics the teacher's behavior in training contexts. However, previous approaches have lacked a detailed analysis of the origins of the distilled model's capabilities. It remains unclear whether the student can maintain consistent behaviors with the teacher in novel test-time contexts, or whether it regresses to its original output patterns, raising concerns about the generalization of distillation models. To analyse this question, we introduce a cross-model Reasoning Distillation Provenance Tracing framework. For each action (e.g., a sentence) produced by the distilled model, we obtain the predictive probabilities assigned by the teacher, the original student, and the distilled model under the same context. By comparing these probabilities, we classify each action into different categories. By systematically disentangling the provenance of each action, we experimentally demonstrate that, in test-time contexts, the distilled model can indeed generate teacher-originated actions, which correlate with and plausibly explain observed performance on distilled model. Building on this analysis, we further propose a teacher-guided data selection method. Unlike prior approach that rely on heuristics, our method directly compares teacher-student divergences on the training data, providing a principled selection criterion. We validate the effectiveness of our approach across multiple representative teacher models and diverse student models. The results highlight the utility of our provenance-tracing framework and underscore its promise for reasoning distillation. We hope to share Reasoning Distillation Provenance Tracing and our insights into reasoning distillation with the community.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-24T03:19:05+00:00",
      "updated": "2025-12-24T03:19:05+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20908v1",
      "file": "papers/2512.20908v1.pdf"
    },
    {
      "arxiv_id": "2512.20877v1",
      "title": "Architectural Trade-offs in Small Language Models Under Compute Constraints",
      "authors": [
        {
          "name": "Shivraj Singh Bhatti"
        }
      ],
      "abstract": "We present a systematic empirical study of small language models under strict compute constraints, analyzing how architectural choices and training budget interact to determine performance. Starting from a linear next-token predictor, we progressively introduce nonlinearities, self-attention, and multi-layer transformer architectures, evaluating each on character-level modeling of Tiny Shakespeare and word-level modeling of Penn Treebank (PTB) and WikiText-2. We compare models using test negative log-likelihood (NLL), parameter count, and approximate training FLOPs to characterize accuracy-efficiency trade-offs. Our results show that attention-based models dominate MLPs in per-FLOP efficiency even at small scale, while increasing depth or context without sufficient optimization can degrade performance. We further examine rotary positional embeddings (RoPE), finding that architectural techniques successful in large language models do not necessarily transfer to small-model regimes.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-12-24T01:36:50+00:00",
      "updated": "2025-12-24T01:36:50+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20877v1",
      "file": "papers/2512.20877v1.pdf"
    },
    {
      "arxiv_id": "2512.20856v1",
      "title": "NVIDIA Nemotron 3: Efficient and Open Intelligence",
      "authors": [
        {
          "name": "NVIDIA"
        },
        {
          "name": ":"
        },
        {
          "name": "Aaron Blakeman"
        },
        {
          "name": "Aaron Grattafiori"
        },
        {
          "name": "Aarti Basant"
        },
        {
          "name": "Abhibha Gupta"
        },
        {
          "name": "Abhinav Khattar"
        },
        {
          "name": "Adi Renduchintala"
        },
        {
          "name": "Aditya Vavre"
        },
        {
          "name": "Akanksha Shukla"
        },
        {
          "name": "Akhiad Bercovich"
        },
        {
          "name": "Aleksander Ficek"
        },
        {
          "name": "Aleksandr Shaposhnikov"
        },
        {
          "name": "Alex Kondratenko"
        },
        {
          "name": "Alexander Bukharin"
        },
        {
          "name": "Alexandre Milesi"
        },
        {
          "name": "Ali Taghibakhshi"
        },
        {
          "name": "Alisa Liu"
        },
        {
          "name": "Amelia Barton"
        },
        {
          "name": "Ameya Sunil Mahabaleshwarkar"
        },
        {
          "name": "Amir Klein"
        },
        {
          "name": "Amit Zuker"
        },
        {
          "name": "Amnon Geifman"
        },
        {
          "name": "Amy Shen"
        },
        {
          "name": "Anahita Bhiwandiwalla"
        },
        {
          "name": "Andrew Tao"
        },
        {
          "name": "Anjulie Agrusa"
        },
        {
          "name": "Ankur Verma"
        },
        {
          "name": "Ann Guan"
        },
        {
          "name": "Anubhav Mandarwal"
        },
        {
          "name": "Arham Mehta"
        },
        {
          "name": "Ashwath Aithal"
        },
        {
          "name": "Ashwin Poojary"
        },
        {
          "name": "Asif Ahamed"
        },
        {
          "name": "Asit Mishra"
        },
        {
          "name": "Asma Kuriparambil Thekkumpate"
        },
        {
          "name": "Ayush Dattagupta"
        },
        {
          "name": "Banghua Zhu"
        },
        {
          "name": "Bardiya Sadeghi"
        },
        {
          "name": "Barnaby Simkin"
        },
        {
          "name": "Ben Lanir"
        },
        {
          "name": "Benedikt Schifferer"
        },
        {
          "name": "Besmira Nushi"
        },
        {
          "name": "Bilal Kartal"
        },
        {
          "name": "Bita Darvish Rouhani"
        },
        {
          "name": "Boris Ginsburg"
        },
        {
          "name": "Brandon Norick"
        },
        {
          "name": "Brandon Soubasis"
        },
        {
          "name": "Branislav Kisacanin"
        },
        {
          "name": "Brian Yu"
        },
        {
          "name": "Bryan Catanzaro"
        },
        {
          "name": "Carlo del Mundo"
        },
        {
          "name": "Chantal Hwang"
        },
        {
          "name": "Charles Wang"
        },
        {
          "name": "Cheng-Ping Hsieh"
        },
        {
          "name": "Chenghao Zhang"
        },
        {
          "name": "Chenhan Yu"
        },
        {
          "name": "Chetan Mungekar"
        },
        {
          "name": "Chintan Patel"
        },
        {
          "name": "Chris Alexiuk"
        },
        {
          "name": "Christopher Parisien"
        },
        {
          "name": "Collin Neale"
        },
        {
          "name": "Cyril Meurillon"
        },
        {
          "name": "Damon Mosk-Aoyama"
        },
        {
          "name": "Dan Su"
        },
        {
          "name": "Dane Corneil"
        },
        {
          "name": "Daniel Afrimi"
        },
        {
          "name": "Daniel Lo"
        },
        {
          "name": "Daniel Rohrer"
        },
        {
          "name": "Daniel Serebrenik"
        },
        {
          "name": "Daria Gitman"
        },
        {
          "name": "Daria Levy"
        },
        {
          "name": "Darko Stosic"
        },
        {
          "name": "David Mosallanezhad"
        },
        {
          "name": "Deepak Narayanan"
        },
        {
          "name": "Dhruv Nathawani"
        },
        {
          "name": "Dima Rekesh"
        },
        {
          "name": "Dina Yared"
        },
        {
          "name": "Divyanshu Kakwani"
        },
        {
          "name": "Dong Ahn"
        },
        {
          "name": "Duncan Riach"
        },
        {
          "name": "Dusan Stosic"
        },
        {
          "name": "Edgar Minasyan"
        },
        {
          "name": "Edward Lin"
        },
        {
          "name": "Eileen Long"
        },
        {
          "name": "Eileen Peters Long"
        },
        {
          "name": "Elad Segal"
        },
        {
          "name": "Elena Lantz"
        },
        {
          "name": "Ellie Evans"
        },
        {
          "name": "Elliott Ning"
        },
        {
          "name": "Eric Chung"
        },
        {
          "name": "Eric Harper"
        },
        {
          "name": "Eric Tramel"
        },
        {
          "name": "Erick Galinkin"
        },
        {
          "name": "Erik Pounds"
        },
        {
          "name": "Evan Briones"
        },
        {
          "name": "Evelina Bakhturina"
        },
        {
          "name": "Evgeny Tsykunov"
        },
        {
          "name": "Faisal Ladhak"
        },
        {
          "name": "Fay Wang"
        },
        {
          "name": "Fei Jia"
        },
        {
          "name": "Felipe Soares"
        },
        {
          "name": "Feng Chen"
        },
        {
          "name": "Ferenc Galko"
        },
        {
          "name": "Frank Sun"
        },
        {
          "name": "Frankie Siino"
        },
        {
          "name": "Gal Hubara Agam"
        },
        {
          "name": "Ganesh Ajjanagadde"
        },
        {
          "name": "Gantavya Bhatt"
        },
        {
          "name": "Gargi Prasad"
        },
        {
          "name": "George Armstrong"
        },
        {
          "name": "Gerald Shen"
        },
        {
          "name": "Gorkem Batmaz"
        },
        {
          "name": "Grigor Nalbandyan"
        },
        {
          "name": "Haifeng Qian"
        },
        {
          "name": "Harsh Sharma"
        },
        {
          "name": "Hayley Ross"
        },
        {
          "name": "Helen Ngo"
        },
        {
          "name": "Herbert Hum"
        },
        {
          "name": "Herman Sahota"
        },
        {
          "name": "Hexin Wang"
        },
        {
          "name": "Himanshu Soni"
        },
        {
          "name": "Hiren Upadhyay"
        },
        {
          "name": "Huizi Mao"
        },
        {
          "name": "Huy C Nguyen"
        },
        {
          "name": "Huy Q Nguyen"
        },
        {
          "name": "Iain Cunningham"
        },
        {
          "name": "Ido Galil"
        },
        {
          "name": "Ido Shahaf"
        },
        {
          "name": "Igor Gitman"
        },
        {
          "name": "Ilya Loshchilov"
        },
        {
          "name": "Itamar Schen"
        },
        {
          "name": "Itay Levy"
        },
        {
          "name": "Ivan Moshkov"
        },
        {
          "name": "Izik Golan"
        },
        {
          "name": "Izzy Putterman"
        },
        {
          "name": "Jan Kautz"
        },
        {
          "name": "Jane Polak Scowcroft"
        },
        {
          "name": "Jared Casper"
        },
        {
          "name": "Jatin Mitra"
        },
        {
          "name": "Jeffrey Glick"
        },
        {
          "name": "Jenny Chen"
        },
        {
          "name": "Jesse Oliver"
        },
        {
          "name": "Jian Zhang"
        },
        {
          "name": "Jiaqi Zeng"
        },
        {
          "name": "Jie Lou"
        },
        {
          "name": "Jimmy Zhang"
        },
        {
          "name": "Jinhang Choi"
        },
        {
          "name": "Jining Huang"
        },
        {
          "name": "Joey Conway"
        },
        {
          "name": "Joey Guman"
        },
        {
          "name": "John Kamalu"
        },
        {
          "name": "Johnny Greco"
        },
        {
          "name": "Jonathan Cohen"
        },
        {
          "name": "Joseph Jennings"
        },
        {
          "name": "Joyjit Daw"
        },
        {
          "name": "Julien Veron Vialard"
        },
        {
          "name": "Junkeun Yi"
        },
        {
          "name": "Jupinder Parmar"
        },
        {
          "name": "Kai Xu"
        },
        {
          "name": "Kan Zhu"
        },
        {
          "name": "Kari Briski"
        },
        {
          "name": "Katherine Cheung"
        },
        {
          "name": "Katherine Luna"
        },
        {
          "name": "Keith Wyss"
        },
        {
          "name": "Keshav Santhanam"
        },
        {
          "name": "Kevin Shih"
        },
        {
          "name": "Kezhi Kong"
        },
        {
          "name": "Khushi Bhardwaj"
        },
        {
          "name": "Kirthi Shankar"
        },
        {
          "name": "Krishna C. Puvvada"
        },
        {
          "name": "Krzysztof Pawelec"
        },
        {
          "name": "Kumar Anik"
        },
        {
          "name": "Lawrence McAfee"
        },
        {
          "name": "Laya Sleiman"
        },
        {
          "name": "Leon Derczynski"
        },
        {
          "name": "Li Ding"
        },
        {
          "name": "Lizzie Wei"
        },
        {
          "name": "Lucas Liebenwein"
        },
        {
          "name": "Luis Vega"
        },
        {
          "name": "Maanu Grover"
        },
        {
          "name": "Maarten Van Segbroeck"
        },
        {
          "name": "Maer Rodrigues de Melo"
        },
        {
          "name": "Mahdi Nazemi"
        },
        {
          "name": "Makesh Narsimhan Sreedhar"
        },
        {
          "name": "Manoj Kilaru"
        },
        {
          "name": "Maor Ashkenazi"
        },
        {
          "name": "Marc Romeijn"
        },
        {
          "name": "Marcin Chochowski"
        },
        {
          "name": "Mark Cai"
        },
        {
          "name": "Markus Kliegl"
        },
        {
          "name": "Maryam Moosaei"
        },
        {
          "name": "Matt Kulka"
        },
        {
          "name": "Matvei Novikov"
        },
        {
          "name": "Mehrzad Samadi"
        },
        {
          "name": "Melissa Corpuz"
        },
        {
          "name": "Mengru Wang"
        },
        {
          "name": "Meredith Price"
        },
        {
          "name": "Michael Andersch"
        },
        {
          "name": "Michael Boone"
        },
        {
          "name": "Michael Evans"
        },
        {
          "name": "Miguel Martinez"
        },
        {
          "name": "Mikail Khona"
        },
        {
          "name": "Mike Chrzanowski"
        },
        {
          "name": "Minseok Lee"
        },
        {
          "name": "Mohammad Dabbah"
        },
        {
          "name": "Mohammad Shoeybi"
        },
        {
          "name": "Mostofa Patwary"
        },
        {
          "name": "Nabin Mulepati"
        },
        {
          "name": "Najeeb Nabwani"
        },
        {
          "name": "Natalie Hereth"
        },
        {
          "name": "Nave Assaf"
        },
        {
          "name": "Negar Habibi"
        },
        {
          "name": "Neta Zmora"
        },
        {
          "name": "Netanel Haber"
        },
        {
          "name": "Nicola Sessions"
        },
        {
          "name": "Nidhi Bhatia"
        },
        {
          "name": "Nikhil Jukar"
        },
        {
          "name": "Nikki Pope"
        },
        {
          "name": "Nikolai Ludwig"
        },
        {
          "name": "Nima Tajbakhsh"
        },
        {
          "name": "Nir Ailon"
        },
        {
          "name": "Nirmal Juluru"
        },
        {
          "name": "Nishant Sharma"
        },
        {
          "name": "Oleksii Hrinchuk"
        },
        {
          "name": "Oleksii Kuchaiev"
        },
        {
          "name": "Olivier Delalleau"
        },
        {
          "name": "Oluwatobi Olabiyi"
        },
        {
          "name": "Omer Ullman Argov"
        },
        {
          "name": "Omri Puny"
        },
        {
          "name": "Oren Tropp"
        },
        {
          "name": "Ouye Xie"
        },
        {
          "name": "Parth Chadha"
        },
        {
          "name": "Pasha Shamis"
        },
        {
          "name": "Paul Gibbons"
        },
        {
          "name": "Pavlo Molchanov"
        },
        {
          "name": "Pawel Morkisz"
        },
        {
          "name": "Peter Dykas"
        },
        {
          "name": "Peter Jin"
        },
        {
          "name": "Pinky Xu"
        },
        {
          "name": "Piotr Januszewski"
        },
        {
          "name": "Pranav Prashant Thombre"
        },
        {
          "name": "Prasoon Varshney"
        },
        {
          "name": "Pritam Gundecha"
        },
        {
          "name": "Przemek Tredak"
        },
        {
          "name": "Qing Miao"
        },
        {
          "name": "Qiyu Wan"
        },
        {
          "name": "Rabeeh Karimi Mahabadi"
        },
        {
          "name": "Rachit Garg"
        },
        {
          "name": "Ran El-Yaniv"
        },
        {
          "name": "Ran Zilberstein"
        },
        {
          "name": "Rasoul Shafipour"
        },
        {
          "name": "Rich Harang"
        },
        {
          "name": "Rick Izzo"
        },
        {
          "name": "Rima Shahbazyan"
        },
        {
          "name": "Rishabh Garg"
        },
        {
          "name": "Ritika Borkar"
        },
        {
          "name": "Ritu Gala"
        },
        {
          "name": "Riyad Islam"
        },
        {
          "name": "Robert Hesse"
        },
        {
          "name": "Roger Waleffe"
        },
        {
          "name": "Rohit Watve"
        },
        {
          "name": "Roi Koren"
        },
        {
          "name": "Ruoxi Zhang"
        },
        {
          "name": "Russell Hewett"
        },
        {
          "name": "Russell J. Hewett"
        },
        {
          "name": "Ryan Prenger"
        },
        {
          "name": "Ryan Timbrook"
        },
        {
          "name": "Sadegh Mahdavi"
        },
        {
          "name": "Sahil Modi"
        },
        {
          "name": "Samuel Kriman"
        },
        {
          "name": "Sangkug Lim"
        },
        {
          "name": "Sanjay Kariyappa"
        },
        {
          "name": "Sanjeev Satheesh"
        },
        {
          "name": "Saori Kaji"
        },
        {
          "name": "Satish Pasumarthi"
        },
        {
          "name": "Saurav Muralidharan"
        },
        {
          "name": "Sean Narentharen"
        },
        {
          "name": "Sean Narenthiran"
        },
        {
          "name": "Seonmyeong Bak"
        },
        {
          "name": "Sergey Kashirsky"
        },
        {
          "name": "Seth Poulos"
        },
        {
          "name": "Shahar Mor"
        },
        {
          "name": "Shanmugam Ramasamy"
        },
        {
          "name": "Shantanu Acharya"
        },
        {
          "name": "Shaona Ghosh"
        },
        {
          "name": "Sharath Turuvekere Sreenivas"
        },
        {
          "name": "Shelby Thomas"
        },
        {
          "name": "Shiqing Fan"
        },
        {
          "name": "Shreya Gopal"
        },
        {
          "name": "Shrimai Prabhumoye"
        },
        {
          "name": "Shubham Pachori"
        },
        {
          "name": "Shubham Toshniwal"
        },
        {
          "name": "Shuoyang Ding"
        },
        {
          "name": "Siddharth Singh"
        },
        {
          "name": "Simeng Sun"
        },
        {
          "name": "Smita Ithape"
        },
        {
          "name": "Somshubra Majumdar"
        },
        {
          "name": "Soumye Singhal"
        },
        {
          "name": "Stas Sergienko"
        },
        {
          "name": "Stefania Alborghetti"
        },
        {
          "name": "Stephen Ge"
        },
        {
          "name": "Sugam Dipak Devare"
        },
        {
          "name": "Sumeet Kumar Barua"
        },
        {
          "name": "Suseella Panguluri"
        },
        {
          "name": "Suyog Gupta"
        },
        {
          "name": "Sweta Priyadarshi"
        },
        {
          "name": "Syeda Nahida Akter"
        },
        {
          "name": "Tan Bui"
        },
        {
          "name": "Teodor-Dumitru Ene"
        },
        {
          "name": "Terry Kong"
        },
        {
          "name": "Thanh Do"
        },
        {
          "name": "Tijmen Blankevoort"
        },
        {
          "name": "Tim Moon"
        },
        {
          "name": "Tom Balough"
        },
        {
          "name": "Tomer Asida"
        },
        {
          "name": "Tomer Bar Natan"
        },
        {
          "name": "Tomer Ronen"
        },
        {
          "name": "Tugrul Konuk"
        },
        {
          "name": "Twinkle Vashishth"
        },
        {
          "name": "Udi Karpas"
        },
        {
          "name": "Ushnish De"
        },
        {
          "name": "Vahid Noorozi"
        },
        {
          "name": "Vahid Noroozi"
        },
        {
          "name": "Venkat Srinivasan"
        },
        {
          "name": "Venmugil Elango"
        },
        {
          "name": "Victor Cui"
        },
        {
          "name": "Vijay Korthikanti"
        },
        {
          "name": "Vinay Rao"
        },
        {
          "name": "Vitaly Kurin"
        },
        {
          "name": "Vitaly Lavrukhin"
        },
        {
          "name": "Vladimir Anisimov"
        },
        {
          "name": "Wanli Jiang"
        },
        {
          "name": "Wasi Uddin Ahmad"
        },
        {
          "name": "Wei Du"
        },
        {
          "name": "Wei Ping"
        },
        {
          "name": "Wenfei Zhou"
        },
        {
          "name": "Will Jennings"
        },
        {
          "name": "William Zhang"
        },
        {
          "name": "Wojciech Prazuch"
        },
        {
          "name": "Xiaowei Ren"
        },
        {
          "name": "Yashaswi Karnati"
        },
        {
          "name": "Yejin Choi"
        },
        {
          "name": "Yev Meyer"
        },
        {
          "name": "Yi-Fu Wu"
        },
        {
          "name": "Yian Zhang"
        },
        {
          "name": "Yigong Qin"
        },
        {
          "name": "Ying Lin"
        },
        {
          "name": "Yonatan Geifman"
        },
        {
          "name": "Yonggan Fu"
        },
        {
          "name": "Yoshi Subara"
        },
        {
          "name": "Yoshi Suhara"
        },
        {
          "name": "Yubo Gao"
        },
        {
          "name": "Zach Moshe"
        },
        {
          "name": "Zhen Dong"
        },
        {
          "name": "Zhongbo Zhu"
        },
        {
          "name": "Zihan Liu"
        },
        {
          "name": "Zijia Chen"
        },
        {
          "name": "Zijie Yan"
        }
      ],
      "abstract": "We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-24T00:24:05+00:00",
      "updated": "2025-12-24T00:24:05+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20856v1",
      "file": "papers/2512.20856v1.pdf"
    },
    {
      "arxiv_id": "2512.20854v1",
      "title": "How important is Recall for Measuring Retrieval Quality?",
      "authors": [
        {
          "name": "Shelly Schwartz"
        },
        {
          "name": "Oleg Vasilyev"
        },
        {
          "name": "Randy Sawaya"
        }
      ],
      "abstract": "In realistic retrieval settings with large and evolving knowledge bases, the total number of documents relevant to a query is typically unknown, and recall cannot be computed. In this paper, we evaluate several established strategies for handling this limitation by measuring the correlation between retrieval quality metrics and LLM-based judgments of response quality, where responses are generated from the retrieved documents. We conduct experiments across multiple datasets with a relatively low number of relevant documents (2-15). We also introduce a simple retrieval quality measure that performs well without requiring knowledge of the total number of relevant documents.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "published": "2025-12-24T00:16:31+00:00",
      "updated": "2025-12-24T00:16:31+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20854v1",
      "file": "papers/2512.20854v1.pdf"
    },
    {
      "arxiv_id": "2512.20848v1",
      "title": "Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning",
      "authors": [
        {
          "name": "NVIDIA"
        },
        {
          "name": ":"
        },
        {
          "name": "Aaron Blakeman"
        },
        {
          "name": "Aaron Grattafiori"
        },
        {
          "name": "Aarti Basant"
        },
        {
          "name": "Abhibha Gupta"
        },
        {
          "name": "Abhinav Khattar"
        },
        {
          "name": "Adi Renduchintala"
        },
        {
          "name": "Aditya Vavre"
        },
        {
          "name": "Akanksha Shukla"
        },
        {
          "name": "Akhiad Bercovich"
        },
        {
          "name": "Aleksander Ficek"
        },
        {
          "name": "Aleksandr Shaposhnikov"
        },
        {
          "name": "Alex Kondratenko"
        },
        {
          "name": "Alexander Bukharin"
        },
        {
          "name": "Alexandre Milesi"
        },
        {
          "name": "Ali Taghibakhshi"
        },
        {
          "name": "Alisa Liu"
        },
        {
          "name": "Amelia Barton"
        },
        {
          "name": "Ameya Sunil Mahabaleshwarkar"
        },
        {
          "name": "Amir Klein"
        },
        {
          "name": "Amit Zuker"
        },
        {
          "name": "Amnon Geifman"
        },
        {
          "name": "Amy Shen"
        },
        {
          "name": "Anahita Bhiwandiwalla"
        },
        {
          "name": "Andrew Tao"
        },
        {
          "name": "Ann Guan"
        },
        {
          "name": "Anubhav Mandarwal"
        },
        {
          "name": "Arham Mehta"
        },
        {
          "name": "Ashwath Aithal"
        },
        {
          "name": "Ashwin Poojary"
        },
        {
          "name": "Asif Ahamed"
        },
        {
          "name": "Asma Kuriparambil Thekkumpate"
        },
        {
          "name": "Ayush Dattagupta"
        },
        {
          "name": "Banghua Zhu"
        },
        {
          "name": "Bardiya Sadeghi"
        },
        {
          "name": "Barnaby Simkin"
        },
        {
          "name": "Ben Lanir"
        },
        {
          "name": "Benedikt Schifferer"
        },
        {
          "name": "Besmira Nushi"
        },
        {
          "name": "Bilal Kartal"
        },
        {
          "name": "Bita Darvish Rouhani"
        },
        {
          "name": "Boris Ginsburg"
        },
        {
          "name": "Brandon Norick"
        },
        {
          "name": "Brandon Soubasis"
        },
        {
          "name": "Branislav Kisacanin"
        },
        {
          "name": "Brian Yu"
        },
        {
          "name": "Bryan Catanzaro"
        },
        {
          "name": "Carlo del Mundo"
        },
        {
          "name": "Chantal Hwang"
        },
        {
          "name": "Charles Wang"
        },
        {
          "name": "Cheng-Ping Hsieh"
        },
        {
          "name": "Chenghao Zhang"
        },
        {
          "name": "Chenhan Yu"
        },
        {
          "name": "Chetan Mungekar"
        },
        {
          "name": "Chintan Patel"
        },
        {
          "name": "Chris Alexiuk"
        },
        {
          "name": "Christopher Parisien"
        },
        {
          "name": "Collin Neale"
        },
        {
          "name": "Damon Mosk-Aoyama"
        },
        {
          "name": "Dan Su"
        },
        {
          "name": "Dane Corneil"
        },
        {
          "name": "Daniel Afrimi"
        },
        {
          "name": "Daniel Rohrer"
        },
        {
          "name": "Daniel Serebrenik"
        },
        {
          "name": "Daria Gitman"
        },
        {
          "name": "Daria Levy"
        },
        {
          "name": "Darko Stosic"
        },
        {
          "name": "David Mosallanezhad"
        },
        {
          "name": "Deepak Narayanan"
        },
        {
          "name": "Dhruv Nathawani"
        },
        {
          "name": "Dima Rekesh"
        },
        {
          "name": "Dina Yared"
        },
        {
          "name": "Divyanshu Kakwani"
        },
        {
          "name": "Dong Ahn"
        },
        {
          "name": "Duncan Riach"
        },
        {
          "name": "Dusan Stosic"
        },
        {
          "name": "Edgar Minasyan"
        },
        {
          "name": "Edward Lin"
        },
        {
          "name": "Eileen Long"
        },
        {
          "name": "Eileen Peters Long"
        },
        {
          "name": "Elena Lantz"
        },
        {
          "name": "Ellie Evans"
        },
        {
          "name": "Elliott Ning"
        },
        {
          "name": "Eric Chung"
        },
        {
          "name": "Eric Harper"
        },
        {
          "name": "Eric Tramel"
        },
        {
          "name": "Erick Galinkin"
        },
        {
          "name": "Erik Pounds"
        },
        {
          "name": "Evan Briones"
        },
        {
          "name": "Evelina Bakhturina"
        },
        {
          "name": "Faisal Ladhak"
        },
        {
          "name": "Fay Wang"
        },
        {
          "name": "Fei Jia"
        },
        {
          "name": "Felipe Soares"
        },
        {
          "name": "Feng Chen"
        },
        {
          "name": "Ferenc Galko"
        },
        {
          "name": "Frankie Siino"
        },
        {
          "name": "Gal Hubara Agam"
        },
        {
          "name": "Ganesh Ajjanagadde"
        },
        {
          "name": "Gantavya Bhatt"
        },
        {
          "name": "Gargi Prasad"
        },
        {
          "name": "George Armstrong"
        },
        {
          "name": "Gerald Shen"
        },
        {
          "name": "Gorkem Batmaz"
        },
        {
          "name": "Grigor Nalbandyan"
        },
        {
          "name": "Haifeng Qian"
        },
        {
          "name": "Harsh Sharma"
        },
        {
          "name": "Hayley Ross"
        },
        {
          "name": "Helen Ngo"
        },
        {
          "name": "Herman Sahota"
        },
        {
          "name": "Hexin Wang"
        },
        {
          "name": "Himanshu Soni"
        },
        {
          "name": "Hiren Upadhyay"
        },
        {
          "name": "Huizi Mao"
        },
        {
          "name": "Huy C Nguyen"
        },
        {
          "name": "Huy Q Nguyen"
        },
        {
          "name": "Iain Cunningham"
        },
        {
          "name": "Ido Shahaf"
        },
        {
          "name": "Igor Gitman"
        },
        {
          "name": "Ilya Loshchilov"
        },
        {
          "name": "Ivan Moshkov"
        },
        {
          "name": "Izzy Putterman"
        },
        {
          "name": "Jan Kautz"
        },
        {
          "name": "Jane Polak Scowcroft"
        },
        {
          "name": "Jared Casper"
        },
        {
          "name": "Jatin Mitra"
        },
        {
          "name": "Jeffrey Glick"
        },
        {
          "name": "Jenny Chen"
        },
        {
          "name": "Jesse Oliver"
        },
        {
          "name": "Jian Zhang"
        },
        {
          "name": "Jiaqi Zeng"
        },
        {
          "name": "Jie Lou"
        },
        {
          "name": "Jimmy Zhang"
        },
        {
          "name": "Jining Huang"
        },
        {
          "name": "Joey Conway"
        },
        {
          "name": "Joey Guman"
        },
        {
          "name": "John Kamalu"
        },
        {
          "name": "Johnny Greco"
        },
        {
          "name": "Jonathan Cohen"
        },
        {
          "name": "Joseph Jennings"
        },
        {
          "name": "Joyjit Daw"
        },
        {
          "name": "Julien Veron Vialard"
        },
        {
          "name": "Junkeun Yi"
        },
        {
          "name": "Jupinder Parmar"
        },
        {
          "name": "Kai Xu"
        },
        {
          "name": "Kan Zhu"
        },
        {
          "name": "Kari Briski"
        },
        {
          "name": "Katherine Cheung"
        },
        {
          "name": "Katherine Luna"
        },
        {
          "name": "Keshav Santhanam"
        },
        {
          "name": "Kevin Shih"
        },
        {
          "name": "Kezhi Kong"
        },
        {
          "name": "Khushi Bhardwaj"
        },
        {
          "name": "Krishna C. Puvvada"
        },
        {
          "name": "Krzysztof Pawelec"
        },
        {
          "name": "Kumar Anik"
        },
        {
          "name": "Lawrence McAfee"
        },
        {
          "name": "Laya Sleiman"
        },
        {
          "name": "Leon Derczynski"
        },
        {
          "name": "Li Ding"
        },
        {
          "name": "Lucas Liebenwein"
        },
        {
          "name": "Luis Vega"
        },
        {
          "name": "Maanu Grover"
        },
        {
          "name": "Maarten Van Segbroeck"
        },
        {
          "name": "Maer Rodrigues de Melo"
        },
        {
          "name": "Makesh Narsimhan Sreedhar"
        },
        {
          "name": "Manoj Kilaru"
        },
        {
          "name": "Maor Ashkenazi"
        },
        {
          "name": "Marc Romeijn"
        },
        {
          "name": "Mark Cai"
        },
        {
          "name": "Markus Kliegl"
        },
        {
          "name": "Maryam Moosaei"
        },
        {
          "name": "Matvei Novikov"
        },
        {
          "name": "Mehrzad Samadi"
        },
        {
          "name": "Melissa Corpuz"
        },
        {
          "name": "Mengru Wang"
        },
        {
          "name": "Meredith Price"
        },
        {
          "name": "Michael Boone"
        },
        {
          "name": "Michael Evans"
        },
        {
          "name": "Miguel Martinez"
        },
        {
          "name": "Mike Chrzanowski"
        },
        {
          "name": "Mohammad Shoeybi"
        },
        {
          "name": "Mostofa Patwary"
        },
        {
          "name": "Nabin Mulepati"
        },
        {
          "name": "Natalie Hereth"
        },
        {
          "name": "Nave Assaf"
        },
        {
          "name": "Negar Habibi"
        },
        {
          "name": "Neta Zmora"
        },
        {
          "name": "Netanel Haber"
        },
        {
          "name": "Nicola Sessions"
        },
        {
          "name": "Nidhi Bhatia"
        },
        {
          "name": "Nikhil Jukar"
        },
        {
          "name": "Nikki Pope"
        },
        {
          "name": "Nikolai Ludwig"
        },
        {
          "name": "Nima Tajbakhsh"
        },
        {
          "name": "Nirmal Juluru"
        },
        {
          "name": "Oleksii Hrinchuk"
        },
        {
          "name": "Oleksii Kuchaiev"
        },
        {
          "name": "Olivier Delalleau"
        },
        {
          "name": "Oluwatobi Olabiyi"
        },
        {
          "name": "Omer Ullman Argov"
        },
        {
          "name": "Ouye Xie"
        },
        {
          "name": "Parth Chadha"
        },
        {
          "name": "Pasha Shamis"
        },
        {
          "name": "Pavlo Molchanov"
        },
        {
          "name": "Pawel Morkisz"
        },
        {
          "name": "Peter Dykas"
        },
        {
          "name": "Peter Jin"
        },
        {
          "name": "Pinky Xu"
        },
        {
          "name": "Piotr Januszewski"
        },
        {
          "name": "Pranav Prashant Thombre"
        },
        {
          "name": "Prasoon Varshney"
        },
        {
          "name": "Pritam Gundecha"
        },
        {
          "name": "Qing Miao"
        },
        {
          "name": "Rabeeh Karimi Mahabadi"
        },
        {
          "name": "Ran El-Yaniv"
        },
        {
          "name": "Ran Zilberstein"
        },
        {
          "name": "Rasoul Shafipour"
        },
        {
          "name": "Rich Harang"
        },
        {
          "name": "Rick Izzo"
        },
        {
          "name": "Rima Shahbazyan"
        },
        {
          "name": "Rishabh Garg"
        },
        {
          "name": "Ritika Borkar"
        },
        {
          "name": "Ritu Gala"
        },
        {
          "name": "Riyad Islam"
        },
        {
          "name": "Roger Waleffe"
        },
        {
          "name": "Rohit Watve"
        },
        {
          "name": "Roi Koren"
        },
        {
          "name": "Ruoxi Zhang"
        },
        {
          "name": "Russell J. Hewett"
        },
        {
          "name": "Ryan Prenger"
        },
        {
          "name": "Ryan Timbrook"
        },
        {
          "name": "Sadegh Mahdavi"
        },
        {
          "name": "Sahil Modi"
        },
        {
          "name": "Samuel Kriman"
        },
        {
          "name": "Sanjay Kariyappa"
        },
        {
          "name": "Sanjeev Satheesh"
        },
        {
          "name": "Saori Kaji"
        },
        {
          "name": "Satish Pasumarthi"
        },
        {
          "name": "Sean Narentharen"
        },
        {
          "name": "Sean Narenthiran"
        },
        {
          "name": "Seonmyeong Bak"
        },
        {
          "name": "Sergey Kashirsky"
        },
        {
          "name": "Seth Poulos"
        },
        {
          "name": "Shahar Mor"
        },
        {
          "name": "Shanmugam Ramasamy"
        },
        {
          "name": "Shantanu Acharya"
        },
        {
          "name": "Shaona Ghosh"
        },
        {
          "name": "Sharath Turuvekere Sreenivas"
        },
        {
          "name": "Shelby Thomas"
        },
        {
          "name": "Shiqing Fan"
        },
        {
          "name": "Shreya Gopal"
        },
        {
          "name": "Shrimai Prabhumoye"
        },
        {
          "name": "Shubham Pachori"
        },
        {
          "name": "Shubham Toshniwal"
        },
        {
          "name": "Shuoyang Ding"
        },
        {
          "name": "Siddharth Singh"
        },
        {
          "name": "Simeng Sun"
        },
        {
          "name": "Smita Ithape"
        },
        {
          "name": "Somshubra Majumdar"
        },
        {
          "name": "Soumye Singhal"
        },
        {
          "name": "Stefania Alborghetti"
        },
        {
          "name": "Stephen Ge"
        },
        {
          "name": "Sugam Dipak Devare"
        },
        {
          "name": "Sumeet Kumar Barua"
        },
        {
          "name": "Suseella Panguluri"
        },
        {
          "name": "Suyog Gupta"
        },
        {
          "name": "Sweta Priyadarshi"
        },
        {
          "name": "Syeda Nahida Akter"
        },
        {
          "name": "Tan Bui"
        },
        {
          "name": "Teodor-Dumitru Ene"
        },
        {
          "name": "Terry Kong"
        },
        {
          "name": "Thanh Do"
        },
        {
          "name": "Tijmen Blankevoort"
        },
        {
          "name": "Tom Balough"
        },
        {
          "name": "Tomer Asida"
        },
        {
          "name": "Tomer Bar Natan"
        },
        {
          "name": "Tugrul Konuk"
        },
        {
          "name": "Twinkle Vashishth"
        },
        {
          "name": "Udi Karpas"
        },
        {
          "name": "Ushnish De"
        },
        {
          "name": "Vahid Noorozi"
        },
        {
          "name": "Vahid Noroozi"
        },
        {
          "name": "Venkat Srinivasan"
        },
        {
          "name": "Venmugil Elango"
        },
        {
          "name": "Vijay Korthikanti"
        },
        {
          "name": "Vitaly Kurin"
        },
        {
          "name": "Vitaly Lavrukhin"
        },
        {
          "name": "Wanli Jiang"
        },
        {
          "name": "Wasi Uddin Ahmad"
        },
        {
          "name": "Wei Du"
        },
        {
          "name": "Wei Ping"
        },
        {
          "name": "Wenfei Zhou"
        },
        {
          "name": "Will Jennings"
        },
        {
          "name": "William Zhang"
        },
        {
          "name": "Wojciech Prazuch"
        },
        {
          "name": "Xiaowei Ren"
        },
        {
          "name": "Yashaswi Karnati"
        },
        {
          "name": "Yejin Choi"
        },
        {
          "name": "Yev Meyer"
        },
        {
          "name": "Yi-Fu Wu"
        },
        {
          "name": "Yian Zhang"
        },
        {
          "name": "Ying Lin"
        },
        {
          "name": "Yonatan Geifman"
        },
        {
          "name": "Yonggan Fu"
        },
        {
          "name": "Yoshi Subara"
        },
        {
          "name": "Yoshi Suhara"
        },
        {
          "name": "Yubo Gao"
        },
        {
          "name": "Zach Moshe"
        },
        {
          "name": "Zhen Dong"
        },
        {
          "name": "Zihan Liu"
        },
        {
          "name": "Zijia Chen"
        },
        {
          "name": "Zijie Yan"
        }
      ],
      "abstract": "We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-23T23:54:32+00:00",
      "updated": "2025-12-23T23:54:32+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20848v1",
      "file": "papers/2512.20848v1.pdf"
    },
    {
      "arxiv_id": "2512.20822v1",
      "title": "MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs",
      "authors": [
        {
          "name": "Zhan Qu"
        },
        {
          "name": "Michael Frber"
        }
      ],
      "abstract": "Large Language Models (LLMs) are increasingly applied to medicine, yet their adoption is limited by concerns over reliability and safety. Existing evaluations either test factual medical knowledge in isolation or assess patient-level reasoning without verifying correctness, leaving a critical gap. We introduce MediEval, a benchmark that links MIMIC-IV electronic health records (EHRs) to a unified knowledge base built from UMLS and other biomedical vocabularies. MediEval generates diverse factual and counterfactual medical statements within real patient contexts, enabling systematic evaluation across a 4-quadrant framework that jointly considers knowledge grounding and contextual consistency. Using this framework, we identify critical failure modes, including hallucinated support and truth inversion, that current proprietary, open-source, and domain-specific LLMs frequently exhibit. To address these risks, we propose Counterfactual Risk-Aware Fine-tuning (CoRFu), a DPO-based method with an asymmetric penalty targeting unsafe confusions. CoRFu improves by +16.4 macro-F1 points over the base model and eliminates truth inversion errors, demonstrating both higher accuracy and substantially greater safety.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-23T22:52:24+00:00",
      "updated": "2025-12-23T22:52:24+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20822v1",
      "file": "papers/2512.20822v1.pdf"
    },
    {
      "arxiv_id": "2512.20817v1",
      "title": "EssayCBM: Rubric-Aligned Concept Bottleneck Models for Transparent Essay Grading",
      "authors": [
        {
          "name": "Kumar Satvik Chaudhary"
        },
        {
          "name": "Chengshuai Zhao"
        },
        {
          "name": "Fan Zhang"
        },
        {
          "name": "Yung Hin Tse"
        },
        {
          "name": "Garima Agrawal"
        },
        {
          "name": "Yuli Deng"
        },
        {
          "name": "Huan Liu"
        }
      ],
      "abstract": "Understanding how automated grading systems evaluate essays remains a significant challenge for educators and students, especially when large language models function as black boxes. We introduce EssayCBM, a rubric-aligned framework that prioritizes interpretability in essay assessment. Instead of predicting grades directly from text, EssayCBM evaluates eight writing concepts, such as Thesis Clarity and Evidence Use, through dedicated prediction heads on an encoder. These concept scores form a transparent bottleneck, and a lightweight network computes the final grade using only concepts. Instructors can adjust concept predictions and instantly view the updated grade, enabling accountable human-in-the-loop evaluation. EssayCBM matches black-box performance while offering actionable, concept-level feedback through an intuitive web interface.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-23T22:33:54+00:00",
      "updated": "2025-12-23T22:33:54+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20817v1",
      "file": "papers/2512.20817v1.pdf"
    },
    {
      "arxiv_id": "2512.20812v1",
      "title": "Semantic Deception: When Reasoning Models Can't Compute an Addition",
      "authors": [
        {
          "name": "Nathanil de Leeuw"
        },
        {
          "name": "Marceau Nahon"
        },
        {
          "name": "Mathis Reymond"
        },
        {
          "name": "Raja Chatila"
        },
        {
          "name": "Mehdi Khamassi"
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly used in situations where human values are at stake, such as decision-making tasks that involve reasoning when performed by humans. We investigate the so-called reasoning capabilities of LLMs over novel symbolic representations by introducing an experimental framework that tests their ability to process and manipulate unfamiliar symbols. We introduce semantic deceptions: situations in which symbols carry misleading semantic associations due to their form, such as being embedded in specific contexts, designed to probe whether LLMs can maintain symbolic abstraction or whether they default to exploiting learned semantic associations. We redefine standard digits and mathematical operators using novel symbols, and task LLMs with solving simple calculations expressed in this altered notation. The objective is: (1) to assess LLMs' capacity for abstraction and manipulation of arbitrary symbol systems; (2) to evaluate their ability to resist misleading semantic cues that conflict with the task's symbolic logic. Through experiments with four LLMs we show that semantic cues can significantly deteriorate reasoning models' performance on very simple tasks. They reveal limitations in current LLMs' ability for symbolic manipulations and highlight a tendency to over-rely on surface-level semantics, suggesting that chain-of-thoughts may amplify reliance on statistical correlations. Even in situations where LLMs seem to correctly follow instructions, semantic cues still impact basic capabilities. These limitations raise ethical and societal concerns, undermining the widespread and pernicious tendency to attribute reasoning abilities to LLMs and suggesting how LLMs might fail, in particular in decision-making contexts where robust symbolic reasoning is essential and should not be compromised by residual semantic associations inherited from the model's training.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-23T22:22:18+00:00",
      "updated": "2025-12-23T22:22:18+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20812v1",
      "file": "papers/2512.20812v1.pdf"
    },
    {
      "arxiv_id": "2512.20796v1",
      "title": "Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?",
      "authors": [
        {
          "name": "Zhengyang Shan"
        },
        {
          "name": "Aaron Mueller"
        }
      ],
      "abstract": "We investigate how independent demographic bias mechanisms are from general demographic recognition in language models. Using a multi-task evaluation setup where demographics are associated with names, professions, and education levels, we measure whether models can be debiased while preserving demographic detection capabilities. We compare attribution-based and correlation-based methods for locating bias features. We find that targeted sparse autoencoder feature ablations in Gemma-2-9B reduce bias without degrading recognition performance: attribution-based ablations mitigate race and gender profession stereotypes while preserving name recognition accuracy, whereas correlation-based ablations are more effective for education bias. Qualitative analysis further reveals that removing attribution features in education tasks induces ``prior collapse'', thus increasing overall bias. This highlights the need for dimension-specific interventions. Overall, our results show that demographic bias arises from task-specific mechanisms rather than absolute demographic markers, and that mechanistic inference-time interventions can enable surgical debiasing without compromising core model capabilities.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-23T21:44:20+00:00",
      "updated": "2025-12-23T21:44:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20796v1",
      "file": "papers/2512.20796v1.pdf"
    },
    {
      "arxiv_id": "2512.20794v1",
      "title": "Investigating Model Editing for Unlearning in Large Language Models",
      "authors": [
        {
          "name": "Shariqah Hossain"
        },
        {
          "name": "Lalana Kagal"
        }
      ],
      "abstract": "Machine unlearning aims to remove unwanted information from a model, but many methods are inefficient for LLMs with large numbers of parameters or fail to fully remove the intended information without degrading performance on knowledge that should be retained. Model editing algorithms solve a similar problem of changing information in models, but they focus on redirecting inputs to a new target rather than removing that information altogether. In this work, we explore the editing algorithms ROME, IKE, and WISE and design new editing targets for an unlearning setting. Through this investigation, we show that model editing approaches can exceed baseline unlearning methods in terms of quality of forgetting depending on the setting. Like traditional unlearning techniques, they struggle to encapsulate the scope of what is to be unlearned without damage to the overall model performance.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-23T21:41:36+00:00",
      "updated": "2025-12-23T21:41:36+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20794v1",
      "file": "papers/2512.20794v1.pdf"
    },
    {
      "arxiv_id": "2512.20773v1",
      "title": "Adversarial Training for Failure-Sensitive User Simulation in Mental Health Dialogue Optimization",
      "authors": [
        {
          "name": "Ziyi Zhu"
        },
        {
          "name": "Olivier Tieleman"
        },
        {
          "name": "Caitlin A. Stamatis"
        },
        {
          "name": "Luka Smyth"
        },
        {
          "name": "Thomas D. Hull"
        },
        {
          "name": "Daniel R. Cahn"
        },
        {
          "name": "Matteo Malgaroli"
        }
      ],
      "abstract": "Realistic user simulation is crucial for training and evaluating task-oriented dialogue (TOD) systems, yet creating simulators that accurately replicate human behavior remains challenging. A key property of effective simulators is their ability to expose failure modes of the systems they evaluate. We present an adversarial training framework that iteratively improves user simulator realism through a competitive dynamic between a generator (user simulator) and a discriminator. Applied to mental health support chatbots, our approach demonstrates that fine-tuned simulators dramatically outperform zero-shot base models at surfacing system issues, and adversarial training further enhances diversity, distributional alignment, and predictive validity. The resulting simulator achieves a strong correlation between simulated and real failure occurrence rates across diverse chatbot configurations while maintaining low distributional divergence of failure modes. Discriminator accuracy decreases drastically after three adversarial iterations, suggesting improved realism. These results provide evidence that adversarial training is a promising approach for creating realistic user simulators in mental health support TOD domains, enabling rapid, reliable, and cost-effective system evaluation before deployment.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-23T21:21:08+00:00",
      "updated": "2025-12-23T21:21:08+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20773v1",
      "file": "papers/2512.20773v1.pdf"
    },
    {
      "arxiv_id": "2512.20760v1",
      "title": "Generalization of RLVR Using Causal Reasoning as a Testbed",
      "authors": [
        {
          "name": "Brian Lu"
        },
        {
          "name": "Hongyu Zhao"
        },
        {
          "name": "Shuo Sun"
        },
        {
          "name": "Hao Peng"
        },
        {
          "name": "Rui Ding"
        },
        {
          "name": "Hongyuan Mei"
        }
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising paradigm for post-training large language models (LLMs) on complex reasoning tasks. Yet, the conditions under which RLVR yields robust generalization remain poorly understood. This paper provides an empirical study of RLVR generalization in the setting of probabilistic inference over causal graphical models. This setting offers two natural axes along which to examine generalization: (i) the level of the probabilistic query -- associational, interventional, or counterfactual -- and (ii) the structural complexity of the query, measured by the size of its relevant subgraph. We construct datasets of causal graphs and queries spanning these difficulty axes and fine-tune Qwen-2.5-Instruct models using RLVR or supervised fine-tuning (SFT). We vary both the model scale (3B-32B) and the query level included in training. We find that RLVR yields stronger within-level and across-level generalization than SFT, but only for specific combinations of model size and training query level. Further analysis shows that RLVR's effectiveness depends on the model's initial reasoning competence. With sufficient initial competence, RLVR improves an LLM's marginalization strategy and reduces errors in intermediate probability calculations, producing substantial accuracy gains, particularly on more complex queries. These findings show that RLVR can improve specific causal reasoning subskills, with its benefits emerging only when the model has sufficient initial competence.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-12-23T20:45:31+00:00",
      "updated": "2025-12-23T20:45:31+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20760v1",
      "file": "papers/2512.20760v1.pdf"
    },
    {
      "arxiv_id": "2512.20757v1",
      "title": "TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior",
      "authors": [
        {
          "name": "Gl Sena Altnta"
        },
        {
          "name": "Malikeh Ehghaghi"
        },
        {
          "name": "Brian Lester"
        },
        {
          "name": "Fengyuan Liu"
        },
        {
          "name": "Wanru Zhao"
        },
        {
          "name": "Marco Ciccone"
        },
        {
          "name": "Colin Raffel"
        }
      ],
      "abstract": "Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-12-23T20:43:06+00:00",
      "updated": "2025-12-23T20:43:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20757v1",
      "file": "papers/2512.20757v1.pdf"
    },
    {
      "arxiv_id": "2512.20745v1",
      "title": "AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent",
      "authors": [
        {
          "name": "Haipeng Luo"
        },
        {
          "name": "Huawen Feng"
        },
        {
          "name": "Qingfeng Sun"
        },
        {
          "name": "Can Xu"
        },
        {
          "name": "Kai Zheng"
        },
        {
          "name": "Yufei Wang"
        },
        {
          "name": "Tao Yang"
        },
        {
          "name": "Han Hu"
        },
        {
          "name": "Yansong Tang"
        },
        {
          "name": "Di Wang"
        }
      ],
      "abstract": "Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-12-23T19:57:49+00:00",
      "updated": "2025-12-23T19:57:49+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20745v1",
      "file": "papers/2512.20745v1.pdf"
    },
    {
      "arxiv_id": "2512.20724v1",
      "title": "SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention",
      "authors": [
        {
          "name": "Alexandros Christoforos"
        },
        {
          "name": "Chadbourne Davis"
        }
      ],
      "abstract": "Diffusion based approaches to long form text generation suffer from prohibitive computational cost and memory overhead as sequence length increases. We introduce SA-DiffuSeq, a diffusion framework that integrates sparse attention to fundamentally improve scalability for long document modeling. By selectively allocating attention within the diffusion process, SA-DiffuSeq significantly reduces computational complexity while maintaining semantic coherence and generation quality. A key component of our method is a soft absorbing state tailored to sparse attention dynamics, which stabilizes diffusion trajectories and accelerates sequence reconstruction. This design improves sampling efficiency and enhances precision in long range dependency modeling. Extensive experiments demonstrate that SA-DiffuSeq consistently surpasses state of the art diffusion baselines in both training efficiency and sampling speed, with especially strong gains on extended sequences. These properties make SA-DiffuSeq well suited for demanding long form applications such as scientific writing, large scale code generation, and multi turn long context dialogue. Overall, our results indicate that incorporating structured sparsity into diffusion models is a promising direction for efficient and expressive long text generation.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-23T19:35:02+00:00",
      "updated": "2025-12-23T19:35:02+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20724v1",
      "file": "papers/2512.20724v1.pdf"
    },
    {
      "arxiv_id": "2512.20612v1",
      "title": "Making Large Language Models Efficient Dense Retrievers",
      "authors": [
        {
          "name": "Yibin Lei"
        },
        {
          "name": "Shwai He"
        },
        {
          "name": "Ang Li"
        },
        {
          "name": "Andrew Yates"
        }
      ],
      "abstract": "Recent work has shown that directly fine-tuning large language models (LLMs) for dense retrieval yields strong performance, but their substantial parameter counts make them computationally inefficient. While prior studies have revealed significant layer redundancy in LLMs for generative tasks, it remains unclear whether similar redundancy exists when these models are adapted for retrieval tasks, which require encoding entire sequences into fixed representations rather than generating tokens iteratively. To this end, we conduct a comprehensive analysis of layer redundancy in LLM-based dense retrievers. We find that, in contrast to generative settings, MLP layers are substantially more prunable, while attention layers remain critical for semantic aggregation. Building on this insight, we propose EffiR, a framework for developing efficient retrievers that performs large-scale MLP compression through a coarse-to-fine strategy (coarse-grained depth reduction followed by fine-grained width reduction), combined with retrieval-specific fine-tuning. Across diverse BEIR datasets and LLM backbones, EffiR achieves substantial reductions in model size and inference cost while preserving the performance of full-size models.",
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "published": "2025-12-23T18:58:25+00:00",
      "updated": "2025-12-23T18:58:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20612v1",
      "file": "papers/2512.20612v1.pdf"
    },
    {
      "arxiv_id": "2512.20604v1",
      "title": "MoE-DiffuSeq: Enhancing Long-Document Diffusion Models with Sparse Attention and Mixture of Experts",
      "authors": [
        {
          "name": "Alexandros Christoforos"
        },
        {
          "name": "Chadbourne Davis"
        }
      ],
      "abstract": "We present MoE-DiffuSeq, a mixture of experts based framework for enhancing diffusion models in long document generation. Existing diffusion based text generation models, such as DiffuSeq, suffer from high computational cost and memory overhead when applied to extended sequences. To address these challenges, MoE-DiffuSeq integrates sparse attention with a mixture of experts architecture, enabling efficient and scalable long sequence modeling. Our approach introduces a customized sparse attention mechanism designed to reduce computational complexity while preserving text quality and coherence. In addition, we incorporate a soft absorbing state within the diffusion process to accelerate sequence reconstruction and improve generation precision. Extensive experiments demonstrate that MoE-DiffuSeq significantly improves training efficiency and sampling speed compared to existing diffusion models. These advantages are particularly effective for long document scenarios, including scientific article generation, code repository modeling, and long form dialogue generation. Benchmark results further show that MoE-DiffuSeq improves efficiency, speed, accuracy, and expressiveness, advancing the practical applicability of diffusion models for high quality long form text generation.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-23T18:50:54+00:00",
      "updated": "2025-12-23T18:50:54+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20604v1",
      "file": "papers/2512.20604v1.pdf"
    },
    {
      "arxiv_id": "2512.20578v1",
      "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits",
      "authors": [
        {
          "name": "Amirhosein Ghasemabadi"
        },
        {
          "name": "Di Niu"
        }
      ],
      "abstract": "Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-23T18:21:32+00:00",
      "updated": "2025-12-23T18:21:32+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20578v1",
      "file": "papers/2512.20578v1.pdf"
    },
    {
      "arxiv_id": "2512.20569v1",
      "title": "Distilling to Hybrid Attention Models via KL-Guided Layer Selection",
      "authors": [
        {
          "name": "Yanhong Li"
        },
        {
          "name": "Songlin Yang"
        },
        {
          "name": "Shawn Tan"
        },
        {
          "name": "Mayank Mishra"
        },
        {
          "name": "Rameswar Panda"
        },
        {
          "name": "Jiawei Zhou"
        },
        {
          "name": "Yoon Kim"
        }
      ],
      "abstract": "Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \\citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-23T18:12:22+00:00",
      "updated": "2025-12-23T18:12:22+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20569v1",
      "file": "papers/2512.20569v1.pdf"
    },
    {
      "arxiv_id": "2512.20491v2",
      "title": "Step-DeepResearch Technical Report",
      "authors": [
        {
          "name": "Chen Hu"
        },
        {
          "name": "Haikuo Du"
        },
        {
          "name": "Heng Wang"
        },
        {
          "name": "Lin Lin"
        },
        {
          "name": "Mingrui Chen"
        },
        {
          "name": "Peng Liu"
        },
        {
          "name": "Ruihang Miao"
        },
        {
          "name": "Tianchi Yue"
        },
        {
          "name": "Wang You"
        },
        {
          "name": "Wei Ji"
        },
        {
          "name": "Wei Yuan"
        },
        {
          "name": "Wenjin Deng"
        },
        {
          "name": "Xiaojian Yuan"
        },
        {
          "name": "Xiaoyun Zhang"
        },
        {
          "name": "Xiangyu Liu"
        },
        {
          "name": "Xikai Liu"
        },
        {
          "name": "Yanming Xu"
        },
        {
          "name": "Yicheng Cao"
        },
        {
          "name": "Yifei Zhang"
        },
        {
          "name": "Yongyao Wang"
        },
        {
          "name": "Yubo Shu"
        },
        {
          "name": "Yurong Zhang"
        },
        {
          "name": "Yuxiang Zhang"
        },
        {
          "name": "Zheng Gong"
        },
        {
          "name": "Zhichao Chang"
        },
        {
          "name": "Binyan Li"
        },
        {
          "name": "Dan Ma"
        },
        {
          "name": "Furong Jia"
        },
        {
          "name": "Hongyuan Wang"
        },
        {
          "name": "Jiayu Liu"
        },
        {
          "name": "Jing Bai"
        },
        {
          "name": "Junlan Liu"
        },
        {
          "name": "Manjiao Liu"
        },
        {
          "name": "Na Wang"
        },
        {
          "name": "Qiuping Wu"
        },
        {
          "name": "Qinxin Du"
        },
        {
          "name": "Shiwei Li"
        },
        {
          "name": "Wen Sun"
        },
        {
          "name": "Yifeng Gong"
        },
        {
          "name": "Yonglin Chen"
        },
        {
          "name": "Yuling Zhao"
        },
        {
          "name": "Yuxuan Lin"
        },
        {
          "name": "Ziqi Ren"
        },
        {
          "name": "Zixuan Wang"
        },
        {
          "name": "Aihu Zhang"
        },
        {
          "name": "Brian Li"
        },
        {
          "name": "Buyun Ma"
        },
        {
          "name": "Kang An"
        },
        {
          "name": "Li Xie"
        },
        {
          "name": "Mingliang Li"
        },
        {
          "name": "Pan Li"
        },
        {
          "name": "Shidong Yang"
        },
        {
          "name": "Xi Chen"
        },
        {
          "name": "Xiaojia Liu"
        },
        {
          "name": "Yuchu Luo"
        },
        {
          "name": "Yuan Song"
        },
        {
          "name": "YuanHao Ding"
        },
        {
          "name": "Yuanwei Liang"
        },
        {
          "name": "Zexi Li"
        },
        {
          "name": "Zhaoning Zhang"
        },
        {
          "name": "Zixin Zhang"
        },
        {
          "name": "Binxing Jiao"
        },
        {
          "name": "Daxin Jiang"
        },
        {
          "name": "Jiansheng Chen"
        },
        {
          "name": "Jing Li"
        },
        {
          "name": "Xiangyu Zhang"
        },
        {
          "name": "Yibo Zhu"
        }
      ],
      "abstract": "As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-23T16:32:27+00:00",
      "updated": "2025-12-24T15:52:31+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20491v2",
      "file": "papers/2512.20491v2.pdf"
    },
    {
      "arxiv_id": "2512.20404v1",
      "title": "Sentiment-Aware Extractive and Abstractive Summarization for Unstructured Text Mining",
      "authors": [
        {
          "name": "Junyi Liu"
        },
        {
          "name": "Stanley Kok"
        }
      ],
      "abstract": "With the rapid growth of unstructured data from social media, reviews, and forums, text mining has become essential in Information Systems (IS) for extracting actionable insights. Summarization can condense fragmented, emotion-rich posts, but existing methods-optimized for structured news-struggle with noisy, informal content. Emotional cues are critical for IS tasks such as brand monitoring and market analysis, yet few studies integrate sentiment modeling into summarization of short user-generated texts. We propose a sentiment-aware framework extending extractive (TextRank) and abstractive (UniLM) approaches by embedding sentiment signals into ranking and generation processes. This dual design improves the capture of emotional nuances and thematic relevance, producing concise, sentiment-enriched summaries that enhance timely interventions and strategic decision-making in dynamic online environments.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-23T14:48:42+00:00",
      "updated": "2025-12-23T14:48:42+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20404v1",
      "file": "papers/2512.20404v1.pdf"
    },
    {
      "arxiv_id": "2512.20352v1",
      "title": "Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation",
      "authors": [
        {
          "name": "Nilesh Jain"
        },
        {
          "name": "Seyi Adeyinka"
        },
        {
          "name": "Leor Roseman"
        },
        {
          "name": "Aza Allsop"
        }
      ],
      "abstract": "Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa ($$) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability ($= 0.907$, cosine=95.3%), followed by GPT-4o ($= 0.853$, cosine=92.6%) and Claude ($= 0.842$, cosine=92.1%). All three models achieve a high agreement ($> 0.80$), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-23T13:32:43+00:00",
      "updated": "2025-12-23T13:32:43+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20352v1",
      "file": "papers/2512.20352v1.pdf"
    },
    {
      "arxiv_id": "2512.20324v1",
      "title": "Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles",
      "authors": [
        {
          "name": "Nurul Labib Sayeedi"
        },
        {
          "name": "Md. Faiyaz Abdullah Sayeedi"
        },
        {
          "name": "Khushnur Binte Jahangir"
        },
        {
          "name": "Swakkhar Shatabda"
        },
        {
          "name": "Sarah Masud Preum"
        }
      ],
      "abstract": "Large Language Models (LLMs) show impressive performance on many NLP benchmarks, yet their ability to reason in figurative, culturally grounded, and low-resource settings remains underexplored. We address this gap for Bangla by introducing BanglaRiddleEval, a benchmark of 1,244 traditional Bangla riddles instantiated across four tasks (4,976 riddle-task artifacts in total). Using an LLM-based pipeline, we generate Chain-of-Thought explanations, semantically coherent distractors, and fine-grained ambiguity annotations, and evaluate a diverse suite of open-source and closed-source models under different prompting strategies. Models achieve moderate semantic overlap on generative QA but low correctness, MCQ accuracy peaks at only about 56% versus an 83% human baseline, and ambiguity resolution ranges from roughly 26% to 68%, with high-quality explanations confined to the strongest models. These results show that current LLMs capture some cues needed for Bangla riddle reasoning but remain far from human-level performance, establishing BanglaRiddleEval as a challenging new benchmark for low-resource figurative reasoning. All data, code, and evaluation scripts are available on GitHub: https://github.com/Labib1610/BanglaRiddleEval.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-23T12:48:05+00:00",
      "updated": "2025-12-23T12:48:05+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20324v1",
      "file": "papers/2512.20324v1.pdf"
    },
    {
      "arxiv_id": "2512.20293v1",
      "title": "AprielGuard",
      "authors": [
        {
          "name": "Jaykumar Kasundra"
        },
        {
          "name": "Anjaneya Praharaj"
        },
        {
          "name": "Sourabh Surana"
        },
        {
          "name": "Lakshmi Sirisha Chodisetty"
        },
        {
          "name": "Sourav Sharma"
        },
        {
          "name": "Abhigya Verma"
        },
        {
          "name": "Abhishek Bhardwaj"
        },
        {
          "name": "Debasish Kanhar"
        },
        {
          "name": "Aakash Bhagat"
        },
        {
          "name": "Khalil Slimi"
        },
        {
          "name": "Seganrasan Subramanian"
        },
        {
          "name": "Sathwik Tejaswi Madhusudhan"
        },
        {
          "name": "Ranga Prasad Chenna"
        },
        {
          "name": "Srinivas Sunkara"
        }
      ],
      "abstract": "Safeguarding large language models (LLMs) against unsafe or adversarial behavior is critical as they are increasingly deployed in conversational and agentic settings. Existing moderation tools often treat safety risks (e.g. toxicity, bias) and adversarial threats (e.g. prompt injections, jailbreaks) as separate problems, limiting their robustness and generalizability. We introduce AprielGuard, an 8B parameter safeguard model that unify these dimensions within a single taxonomy and learning framework. AprielGuard is trained on a diverse mix of open and synthetic data covering standalone prompts, multi-turn conversations, and agentic workflows, augmented with structured reasoning traces to improve interpretability. Across multiple public and proprietary benchmarks, AprielGuard achieves strong performance in detecting harmful content and adversarial manipulations, outperforming existing opensource guardrails such as Llama-Guard and Granite Guardian, particularly in multi-step and reasoning intensive scenarios. By releasing the model, we aim to advance transparent and reproducible research on reliable safeguards for LLMs.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-23T12:01:32+00:00",
      "updated": "2025-12-23T12:01:32+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20293v1",
      "file": "papers/2512.20293v1.pdf"
    },
    {
      "arxiv_id": "2512.20292v1",
      "title": "SlideTailor: Personalized Presentation Slide Generation for Scientific Papers",
      "authors": [
        {
          "name": "Wenzheng Zeng"
        },
        {
          "name": "Mingyu Ouyang"
        },
        {
          "name": "Langyuan Cui"
        },
        {
          "name": "Hwee Tou Ng"
        }
      ],
      "abstract": "Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MM"
      ],
      "published": "2025-12-23T12:01:18+00:00",
      "updated": "2025-12-23T12:01:18+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20292v1",
      "file": "papers/2512.20292v1.pdf"
    },
    {
      "arxiv_id": "2512.20204v1",
      "title": "Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings",
      "authors": [
        {
          "name": "Marko echovi"
        },
        {
          "name": "Natlia Komornkov"
        },
        {
          "name": "Dominik Machek"
        },
        {
          "name": "Ondej Bojar"
        }
      ],
      "abstract": "Speech processing and translation technology have the potential to facilitate meetings of individuals who do not share any common language. To evaluate automatic systems for such a task, a versatile and realistic evaluation corpus is needed. Therefore, we create and present a corpus of cross-lingual dialogues between individuals without a common language who were facilitated by automatic simultaneous speech translation. The corpus consists of 5 hours of speech recordings with ASR and gold transcripts in 12 original languages and automatic and corrected translations into English. For the purposes of research into cross-lingual summarization, our corpus also includes written summaries (minutes) of the meetings.\n  Moreover, we propose automatic detection of misunderstandings. For an overview of this task and its complexity, we attempt to quantify misunderstandings in cross-lingual meetings. We annotate misunderstandings manually and also test the ability of current large language models to detect them automatically. The results show that the Gemini model is able to identify text spans with misunderstandings with recall of 77% and precision of 47%.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-23T09:56:23+00:00",
      "updated": "2025-12-23T09:56:23+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20204v1",
      "file": "papers/2512.20204v1.pdf"
    },
    {
      "arxiv_id": "2512.20182v1",
      "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination",
      "authors": [
        {
          "name": "Shuzheng Si"
        },
        {
          "name": "Qingyi Wang"
        },
        {
          "name": "Haozhe Zhao"
        },
        {
          "name": "Yuzhuo Bai"
        },
        {
          "name": "Guanqiao Chen"
        },
        {
          "name": "Kangyang Luo"
        },
        {
          "name": "Gang Chen"
        },
        {
          "name": "Fanchao Qi"
        },
        {
          "name": "Minjia Zhang"
        },
        {
          "name": "Baobao Chang"
        },
        {
          "name": "Maosong Sun"
        }
      ],
      "abstract": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-23T09:20:32+00:00",
      "updated": "2025-12-23T09:20:32+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20182v1",
      "file": "papers/2512.20182v1.pdf"
    },
    {
      "arxiv_id": "2512.20169v1",
      "title": "Learning to Reason in LLMs by Expectation Maximization",
      "authors": [
        {
          "name": "Junghyun Lee"
        },
        {
          "name": "Branislav Kveton"
        },
        {
          "name": "Sunav Choudhary"
        },
        {
          "name": "Subhojyoti Mukherjee"
        },
        {
          "name": "Anup Rao"
        },
        {
          "name": "Ryan A. Rossi"
        },
        {
          "name": "Alexa Siu"
        }
      ],
      "abstract": "Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL",
        "stat.ML"
      ],
      "published": "2025-12-23T08:56:49+00:00",
      "updated": "2025-12-23T08:56:49+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20169v1",
      "file": "papers/2512.20169v1.pdf"
    },
    {
      "arxiv_id": "2512.20164v1",
      "title": "AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications",
      "authors": [
        {
          "name": "Honglin Mu"
        },
        {
          "name": "Jinghao Liu"
        },
        {
          "name": "Kaiyang Wan"
        },
        {
          "name": "Rui Xing"
        },
        {
          "name": "Xiuying Chen"
        },
        {
          "name": "Timothy Baldwin"
        },
        {
          "name": "Wanxiang Che"
        }
      ],
      "abstract": "Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by \"adversarial instructions\" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-23T08:42:09+00:00",
      "updated": "2025-12-23T08:42:09+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20164v1",
      "file": "papers/2512.20164v1.pdf"
    },
    {
      "arxiv_id": "2512.20156v1",
      "title": "Fun-Audio-Chat Technical Report",
      "authors": [
        {
          "name": "Qian Chen"
        },
        {
          "name": "Luyao Cheng"
        },
        {
          "name": "Chong Deng"
        },
        {
          "name": "Xiangang Li"
        },
        {
          "name": "Jiaqing Liu"
        },
        {
          "name": "Chao-Hong Tan"
        },
        {
          "name": "Wen Wang"
        },
        {
          "name": "Junhao Xu"
        },
        {
          "name": "Jieping Ye"
        },
        {
          "name": "Qinglin Zhang"
        },
        {
          "name": "Qiquan Zhang"
        },
        {
          "name": "Jingren Zhou"
        }
      ],
      "abstract": "Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "published": "2025-12-23T08:35:27+00:00",
      "updated": "2025-12-23T08:35:27+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20156v1",
      "file": "papers/2512.20156v1.pdf"
    },
    {
      "arxiv_id": "2512.20145v1",
      "title": "Retrieval-augmented Prompt Learning for Pre-trained Foundation Models",
      "authors": [
        {
          "name": "Xiang Chen"
        },
        {
          "name": "Yixin Ou"
        },
        {
          "name": "Quan Feng"
        },
        {
          "name": "Lei Li"
        },
        {
          "name": "Piji Li"
        },
        {
          "name": "Haibo Ye"
        },
        {
          "name": "Sheng-Jun Huang"
        },
        {
          "name": "Shuofei Qiao"
        },
        {
          "name": "Shumin Deng"
        },
        {
          "name": "Huajun Chen"
        },
        {
          "name": "Ningyu Zhang"
        }
      ],
      "abstract": "The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-12-23T08:15:34+00:00",
      "updated": "2025-12-23T08:15:34+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20145v1",
      "file": "papers/2512.20145v1.pdf"
    },
    {
      "arxiv_id": "2512.20144v1",
      "title": "Multi-hop Reasoning via Early Knowledge Alignment",
      "authors": [
        {
          "name": "Yuxin Wang"
        },
        {
          "name": "Shicheng Fang"
        },
        {
          "name": "Bo Wang"
        },
        {
          "name": "Qi Luo"
        },
        {
          "name": "Xuanjing Huang"
        },
        {
          "name": "Yining Zheng"
        },
        {
          "name": "Xipeng Qiu"
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at \\href{https://github.com/yxzwang/EarlyKnowledgeAlignment}{Github}.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-23T08:14:44+00:00",
      "updated": "2025-12-23T08:14:44+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20144v1",
      "file": "papers/2512.20144v1.pdf"
    },
    {
      "arxiv_id": "2512.20111v1",
      "title": "ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language",
      "authors": [
        {
          "name": "Aly Lidayan"
        },
        {
          "name": "Jakob Bjorner"
        },
        {
          "name": "Satvik Golechha"
        },
        {
          "name": "Kartik Goyal"
        },
        {
          "name": "Alane Suhr"
        }
      ],
      "abstract": "As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-23T07:11:26+00:00",
      "updated": "2025-12-23T07:11:26+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20111v1",
      "file": "papers/2512.20111v1.pdf"
    },
    {
      "arxiv_id": "2512.20097v1",
      "title": "A Novel Graph-Sequence Learning Model for Inductive Text Classification",
      "authors": [
        {
          "name": "Zuo Wang"
        },
        {
          "name": "Ye Yuan"
        }
      ],
      "abstract": "Text classification plays an important role in various downstream text-related tasks, such as sentiment analysis, fake news detection, and public opinion analysis. Recently, text classification based on Graph Neural Networks (GNNs) has made significant progress due to their strong capabilities of structural relationship learning. However, these approaches still face two major limitations. First, these approaches fail to fully consider the diverse structural information across word pairs, e.g., co-occurrence, syntax, and semantics. Furthermore, they neglect sequence information in the text graph structure information learning module and can not classify texts with new words and relations. In this paper, we propose a Novel Graph-Sequence Learning Model for Inductive Text Classification (TextGSL) to address the previously mentioned issues. More specifically, we construct a single text-level graph for all words in each text and establish different edge types based on the diverse relationships between word pairs. Building upon this, we design an adaptive multi-edge message-passing paradigm to aggregate diverse structural information between word pairs. Additionally, sequential information among text data can be captured by the proposed TextGSL through the incorporation of Transformer layers. Therefore, TextGSL can learn more discriminative text representations. TextGSL has been comprehensively compared with several strong baselines. The experimental results on diverse benchmarking datasets demonstrate that TextGSL outperforms these baselines in terms of accuracy.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-23T06:49:33+00:00",
      "updated": "2025-12-23T06:49:33+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20097v1",
      "file": "papers/2512.20097v1.pdf"
    },
    {
      "arxiv_id": "2512.20092v1",
      "title": "Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents",
      "authors": [
        {
          "name": "Yiming Du"
        },
        {
          "name": "Baojun Wang"
        },
        {
          "name": "Yifan Xiang"
        },
        {
          "name": "Zhaowei Wang"
        },
        {
          "name": "Wenyu Huang"
        },
        {
          "name": "Boyang Xue"
        },
        {
          "name": "Bin Liang"
        },
        {
          "name": "Xingshan Zeng"
        },
        {
          "name": "Fei Mi"
        },
        {
          "name": "Haoli Bai"
        },
        {
          "name": "Lifeng Shang"
        },
        {
          "name": "Jeff Z. Pan"
        },
        {
          "name": "Yuxin Jiang"
        },
        {
          "name": "Kam-Fai Wong"
        }
      ],
      "abstract": "Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0\\%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2\\%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0\\% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-23T06:37:29+00:00",
      "updated": "2025-12-23T06:37:29+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20092v1",
      "file": "papers/2512.20092v1.pdf"
    },
    {
      "arxiv_id": "2512.20074v1",
      "title": "Reason2Decide: Rationale-Driven Multi-Task Learning",
      "authors": [
        {
          "name": "H M Quamran Hasan"
        },
        {
          "name": "Housam Khalifa Bashier"
        },
        {
          "name": "Jiayi Dai"
        },
        {
          "name": "Mi-Young Kim"
        },
        {
          "name": "Randy Goebel"
        }
      ],
      "abstract": "Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We propose Reason2Decide, a two-stage training framework that addresses key challenges in self-rationalization, including exposure bias and task separation. In Stage-1, our model is trained on rationale generation, while in Stage-2, we jointly train on label prediction and rationale generation, applying scheduled sampling to gradually transition from conditioning on gold labels to model predictions. We evaluate Reason2Decide on three medical datasets, including a proprietary triage dataset and public biomedical QA datasets. Across model sizes, Reason2Decide outperforms other fine-tuning baselines and some zero-shot LLMs in prediction (F1) and rationale fidelity (BERTScore, BLEU, LLM-as-a-Judge). In triage, Reason2Decide is rationale source-robust across LLM-generated, nurse-authored, and nurse-post-processed rationales. In our experiments, while using only LLM-generated rationales in Stage-1, Reason2Decide outperforms other fine-tuning variants. This indicates that LLM-generated rationales are suitable for pretraining models, reducing reliance on human annotations. Remarkably, Reason2Decide achieves these gains with models 40x smaller than contemporary foundation models, making clinical reasoning more accessible for resource-constrained deployments while still providing explainable decision support.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-12-23T05:58:47+00:00",
      "updated": "2025-12-23T05:58:47+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20074v1",
      "file": "papers/2512.20074v1.pdf"
    },
    {
      "arxiv_id": "2512.19995v1",
      "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models",
      "authors": [
        {
          "name": "Ming Li"
        },
        {
          "name": "Chenrui Fan"
        },
        {
          "name": "Yize Cheng"
        },
        {
          "name": "Soheil Feizi"
        },
        {
          "name": "Tianyi Zhou"
        }
      ],
      "abstract": "Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-23T02:44:25+00:00",
      "updated": "2025-12-23T02:44:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19995v1",
      "file": "papers/2512.19995v1.pdf"
    },
    {
      "arxiv_id": "2512.19950v1",
      "title": "Bias Beneath the Tone: Empirical Characterisation of Tone Bias in LLM-Driven UX Systems",
      "authors": [
        {
          "name": "Heet Bodara"
        },
        {
          "name": "Md Masum Mushfiq"
        },
        {
          "name": "Isma Farah Siddiqui"
        }
      ],
      "abstract": "Large Language Models are increasingly used in conversational systems such as digital personal assistants, shaping how people interact with technology through language. While their responses often sound fluent and natural, they can also carry subtle tone biases such as sounding overly polite, cheerful, or cautious even when neutrality is expected. These tendencies can influence how users perceive trust, empathy, and fairness in dialogue. In this study, we explore tone bias as a hidden behavioral trait of large language models. The novelty of this research lies in the integration of controllable large language model based dialogue synthesis with tone classification models, enabling robust and ethical emotion recognition in personal assistant interactions. We created two synthetic dialogue datasets, one generated from neutral prompts and another explicitly guided to produce positive or negative tones. Surprisingly, even the neutral set showed consistent tonal skew, suggesting that bias may stem from the model's underlying conversational style. Using weak supervision through a pretrained DistilBERT model, we labeled tones and trained several classifiers to detect these patterns. Ensemble models achieved macro F1 scores up to 0.92, showing that tone bias is systematic, measurable, and relevant to designing fair and trustworthy conversational AI.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "published": "2025-12-23T00:41:48+00:00",
      "updated": "2025-12-23T00:41:48+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19950v1",
      "file": "papers/2512.19950v1.pdf"
    },
    {
      "arxiv_id": "2512.19903v1",
      "title": "How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse",
      "authors": [
        {
          "name": "Kirk Vanacore"
        },
        {
          "name": "Rene F. Kizilcec"
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly adopted in educational technologies for a variety of tasks, from generating instructional materials and assisting with assessment design to tutoring. While prior work has investigated how models can be adapted or optimized for specific tasks, far less is known about how well LLMs perform at interpreting authentic educational scenarios without significant customization. As LLM-based systems become widely adopted by learners and educators in everyday academic contexts, understanding their out-of-the-box capabilities is increasingly important for setting expectations and benchmarking. We compared six LLMs to estimate their baseline performance on a simple but important task: classifying instructional moves in authentic classroom transcripts. We evaluated typical prompting methods: zero-shot, one-shot, and few-shot prompting. We found that while zero-shot performance was moderate, providing comprehensive examples (few-shot prompting) significantly improved performance for state-of-the-art models, with the strongest configuration reaching Cohen's Kappa = 0.58 against expert-coded annotations. At the same time, improvements were neither uniform nor complete: performance varied considerably by instructional move, and higher recall frequently came at the cost of increased false positives. Overall, these findings indicate that foundation models demonstrate meaningful yet limited capacity to interpret instructional discourse, with prompt design helping to surface capability but not eliminating fundamental reliability constraints.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-22T22:08:32+00:00",
      "updated": "2025-12-22T22:08:32+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19903v1",
      "file": "papers/2512.19903v1.pdf"
    },
    {
      "arxiv_id": "2512.19864v1",
      "title": "HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data",
      "authors": [
        {
          "name": "Shashi Kant Gupta"
        },
        {
          "name": "Arijeet Pramanik"
        },
        {
          "name": "Jerrin John Thomas"
        },
        {
          "name": "Regina Schwind"
        },
        {
          "name": "Lauren Wiener"
        },
        {
          "name": "Avi Raju"
        },
        {
          "name": "Jeremy Kornbluth"
        },
        {
          "name": "Yanshan Wang"
        },
        {
          "name": "Zhaohui Su"
        },
        {
          "name": "Hrituraj Singh"
        }
      ],
      "abstract": "Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-22T20:38:30+00:00",
      "updated": "2025-12-22T20:38:30+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19864v1",
      "file": "papers/2512.19864v1.pdf"
    },
    {
      "arxiv_id": "2512.20687v1",
      "title": "PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation",
      "authors": [
        {
          "name": "Yuma Ichikawa"
        },
        {
          "name": "Naoya Takagi"
        },
        {
          "name": "Takumi Nakagawa"
        },
        {
          "name": "Yuzi Kanazawa"
        },
        {
          "name": "Akira Sakai"
        }
      ],
      "abstract": "Transformers operate as horizontal token-by-token scanners; at each generation step, the model attends to an ever-growing sequence of token-level states. This access pattern increases prefill latency and makes long-context decoding increasingly memory-bound, as KV-cache reads and writes dominate inference throughput rather than arithmetic computation. We propose Parallel Hierarchical Operation for Top-down Networks (PHOTON), a hierarchical autoregressive model that replaces flat scanning with vertical, multi-resolution context access. PHOTON maintains a hierarchy of latent streams: a bottom-up encoder progressively compresses tokens into low-rate contextual states, while lightweight top-down decoders reconstruct fine-grained token representations. Experimental results show that PHOTON is superior to competitive Transformer-based language models regarding the throughput-quality trade-off, offering significant advantages in long-context and multi-query tasks. This reduces decode-time KV-cache traffic, yielding up to $10^{3}\\times$ higher throughput per unit memory.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.DC"
      ],
      "published": "2025-12-22T19:26:59+00:00",
      "updated": "2025-12-22T19:26:59+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20687v1",
      "file": "papers/2512.20687v1.pdf"
    },
    {
      "arxiv_id": "2512.19682v2",
      "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
      "authors": [
        {
          "name": "Jiacheng Guo"
        },
        {
          "name": "Ling Yang"
        },
        {
          "name": "Peter Chen"
        },
        {
          "name": "Qixin Xiao"
        },
        {
          "name": "Yinjie Wang"
        },
        {
          "name": "Xinzhe Juan"
        },
        {
          "name": "Jiahao Qiu"
        },
        {
          "name": "Ke Shen"
        },
        {
          "name": "Mengdi Wang"
        }
      ],
      "abstract": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-22T18:57:13+00:00",
      "updated": "2025-12-23T03:45:42+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19682v2",
      "file": "papers/2512.19682v2.pdf"
    },
    {
      "arxiv_id": "2512.19673v1",
      "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
      "authors": [
        {
          "name": "Yuqiao Tan"
        },
        {
          "name": "Minzheng Wang"
        },
        {
          "name": "Shizhu He"
        },
        {
          "name": "Huanxuan Liao"
        },
        {
          "name": "Chengfeng Zhao"
        },
        {
          "name": "Qiunan Lu"
        },
        {
          "name": "Tian Liang"
        },
        {
          "name": "Jun Zhao"
        },
        {
          "name": "Kang Liu"
        }
      ],
      "abstract": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-12-22T18:51:48+00:00",
      "updated": "2025-12-22T18:51:48+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19673v1",
      "file": "papers/2512.19673v1.pdf"
    },
    {
      "arxiv_id": "2512.19651v1",
      "title": "Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting",
      "authors": [
        {
          "name": "Filippos Ventirozos"
        },
        {
          "name": "Peter Appleby"
        },
        {
          "name": "Matthew Shardlow"
        }
      ],
      "abstract": "Aspect-Category Sentiment Analysis (ACSA) provides granular insights by identifying specific themes within reviews and their associated sentiment. While supervised learning approaches dominate this field, the scarcity and high cost of annotated data for new domains present significant barriers. We argue that leveraging large language models (LLMs) in a zero-shot setting is a practical alternative where resources for data annotation are limited. In this work, we propose a novel Chain-of-Thought (CoT) prompting technique that utilises an intermediate Unified Meaning Representation (UMR) to structure the reasoning process for the ACSA task. We evaluate this UMR-based approach against a standard CoT baseline across three models (Qwen3-4B, Qwen3-8B, and Gemini-2.5-Pro) and four diverse datasets. Our findings suggest that UMR effectiveness may be model-dependent. Whilst preliminary results indicate comparable performance for mid-sized models such as Qwen3-8B, these observations warrant further investigation, particularly regarding the potential applicability to smaller model architectures. Further research is required to establish the generalisability of these findings across different model scales.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-22T18:23:37+00:00",
      "updated": "2025-12-22T18:23:37+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19651v1",
      "file": "papers/2512.19651v1.pdf"
    },
    {
      "arxiv_id": "2512.19630v1",
      "title": "Diacritic Restoration for Low-Resource Indigenous Languages: Case Study with Bribri and Cook Islands Mori",
      "authors": [
        {
          "name": "Rolando Coto-Solano"
        },
        {
          "name": "Daisy Li"
        },
        {
          "name": "Manoela Teleginski Ferraz"
        },
        {
          "name": "Olivia Sasse"
        },
        {
          "name": "Cha Krupka"
        },
        {
          "name": "Sharid Loiciga"
        },
        {
          "name": "Sally Akevai Tenamu Nicholas"
        }
      ],
      "abstract": "We present experiments on diacritic restoration, a form of text normalization essential for natural language processing (NLP) tasks. Our study focuses on two extremely under-resourced languages: Bribri, a Chibchan language spoken in Costa Rica, and Cook Islands Mori, a Polynesian language spoken in the Cook Islands. Specifically, this paper: (i) compares algorithms for diacritics restoration in under-resourced languages, including tonal diacritics, (ii) examines the amount of data required to achieve target performance levels, (iii) contrasts results across varying resource conditions, and (iv) explores the related task of diacritic correction. We find that fine-tuned, character-level LLMs perform best, likely due to their ability to decompose complex characters into their UTF-8 byte representations. In contrast, massively multilingual models perform less effectively given our data constraints. Across all models, reliable performance begins to emerge with data budgets of around 10,000 words. Zero-shot approaches perform poorly in all cases. This study responds both to requests from the language communities and to broader NLP research questions concerning model performance and generalization in under-resourced contexts.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-22T18:04:24+00:00",
      "updated": "2025-12-22T18:04:24+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19630v1",
      "file": "papers/2512.19630v1.pdf"
    },
    {
      "arxiv_id": "2512.19620v1",
      "title": "Exploring the features used for summary evaluation by Human and GPT",
      "authors": [
        {
          "name": "Zahra Sadeghi"
        },
        {
          "name": "Evangelos Milios"
        },
        {
          "name": "Frank Rudzicz"
        }
      ],
      "abstract": "Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the original text. While previous research investigated the alignment between LLMs and Human responses, it is not yet well understood what properties or features are exploited by them when asked to evaluate based on a particular quality dimension, and there has not been much attention towards mapping between evaluation scores and metrics. In this paper, we address this issue and discover features aligned with Human and Generative Pre-trained Transformers (GPTs) responses by studying statistical and machine learning metrics. Furthermore, we show that instructing GPTs to employ metrics used by Human can improve their judgment and conforming them better with human responses.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-22T17:54:49+00:00",
      "updated": "2025-12-22T17:54:49+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19620v1",
      "file": "papers/2512.19620v1.pdf"
    },
    {
      "arxiv_id": "2512.19585v1",
      "title": "Increasing the Thinking Budget is Not All You Need",
      "authors": [
        {
          "name": "Ignacio Iacobacci"
        },
        {
          "name": "Zhaozhi Qian"
        },
        {
          "name": "Faroq AL-Tam"
        },
        {
          "name": "Muhammad AL-Qurishi"
        },
        {
          "name": "Riad Souissi"
        }
      ],
      "abstract": "Recently, a new wave of thinking-capable Large Language Models has emerged, demonstrating exceptional capabilities across a wide range of reasoning benchmarks. Early studies have begun to explore how the amount of compute in terms of the length of the reasoning process, the so-called thinking budget, impacts model performance. In this work, we propose a systematic investigation of the thinking budget as a key parameter, examining its interaction with various configurations such as self-consistency, reflection, and others. Our goal is to provide an informative, balanced comparison framework that considers both performance outcomes and computational cost. Among our findings, we discovered that simply increasing the thinking budget is not the most effective use of compute. More accurate responses can instead be achieved through alternative configurations, such as self-consistency and self-reflection.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-22T17:12:04+00:00",
      "updated": "2025-12-22T17:12:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19585v1",
      "file": "papers/2512.19585v1.pdf"
    },
    {
      "arxiv_id": "2512.19537v1",
      "title": "Event Extraction in Large Language Model",
      "authors": [
        {
          "name": "Bobo Li"
        },
        {
          "name": "Xudong Han"
        },
        {
          "name": "Jiang Liu"
        },
        {
          "name": "Yuzhe Ding"
        },
        {
          "name": "Liqiang Jing"
        },
        {
          "name": "Zhaoqi Zhang"
        },
        {
          "name": "Jinheng Li"
        },
        {
          "name": "Xinya Du"
        },
        {
          "name": "Fei Li"
        },
        {
          "name": "Meishan Zhang"
        },
        {
          "name": "Min Zhang"
        },
        {
          "name": "Aixin Sun"
        },
        {
          "name": "Philip S. Yu"
        },
        {
          "name": "Hao Fei"
        }
      ],
      "abstract": "Large language models (LLMs) and multimodal LLMs are changing event extraction (EE): prompting and generation can often produce structured outputs in zero shot or few shot settings. Yet LLM based pipelines face deployment gaps, including hallucinations under weak constraints, fragile temporal and causal linking over long contexts and across documents, and limited long horizon knowledge management within a bounded context window. We argue that EE should be viewed as a system component that provides a cognitive scaffold for LLM centered solutions. Event schemas and slot constraints create interfaces for grounding and verification; event centric structures act as controlled intermediate representations for stepwise reasoning; event links support relation aware retrieval with graph based RAG; and event stores offer updatable episodic and agent memory beyond the context window. This survey covers EE in text and multimodal settings, organizing tasks and taxonomy, tracing method evolution from rule based and neural models to instruction driven and generative frameworks, and summarizing formulations, decoding strategies, architectures, representations, datasets, and evaluation. We also review cross lingual, low resource, and domain specific settings, and highlight open challenges and future directions for reliable event centric systems. Finally, we outline open challenges and future directions that are central to the LLM era, aiming to evolve EE from static extraction into a structurally reliable, agent ready perception and memory layer for open world systems.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-22T16:22:14+00:00",
      "updated": "2025-12-22T16:22:14+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19537v1",
      "file": "papers/2512.19537v1.pdf"
    },
    {
      "arxiv_id": "2512.19456v1",
      "title": "Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations",
      "authors": [
        {
          "name": "Jinwei Chi"
        },
        {
          "name": "Ke Wang"
        },
        {
          "name": "Yu Chen"
        },
        {
          "name": "Xuanye Lin"
        },
        {
          "name": "Qiang Xu"
        }
      ],
      "abstract": "Automated essay scoring (AES) is a challenging task in cross-prompt settings due to the diversity of scoring criteria. While previous studies have focused on the output of large language models (LLMs) to improve scoring accuracy, we believe activations from intermediate layers may also provide valuable information. To explore this possibility, we evaluated the discriminative power of LLMs' activations in cross-prompt essay scoring task. Specifically, we used activations to fit probes and further analyzed the effects of different models and input content of LLMs on this discriminative power. By computing the directions of essays across various trait dimensions under different prompts, we analyzed the variation in evaluation perspectives of large language models concerning essay types and traits. Results show that the activations possess strong discriminative power in evaluating essay quality and that LLMs can adapt their evaluation perspectives to different traits and essay types, effectively handling the diversity of scoring criteria in cross-prompt settings.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-22T15:01:07+00:00",
      "updated": "2025-12-22T15:01:07+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19456v1",
      "file": "papers/2512.19456v1.pdf"
    },
    {
      "arxiv_id": "2512.19455v2",
      "title": "SiamGPT: Quality-First Fine-Tuning for Stable Thai Text Generation",
      "authors": [
        {
          "name": "Thittipat Pairatsuppawat"
        },
        {
          "name": "Abhibhu Tachaapornchai"
        },
        {
          "name": "Paweekorn Kusolsomboon"
        },
        {
          "name": "Chutikan Chaiwong"
        },
        {
          "name": "Thodsaporn Chay-intr"
        },
        {
          "name": "Kobkrit Viriyayudhakorn"
        },
        {
          "name": "Nongnuch Ketui"
        },
        {
          "name": "Aslan B. Wong"
        }
      ],
      "abstract": "Open-weights large language models remain difficult to deploy for Thai due to unstable generation under complex instructions, despite strong English performance. To mitigate these limitations, We present SiamGPT-32B, an open-weights model based on Qwen3-32B, fine-tuned with a Quality-First strategy emphasizing curated supervision over data scale. The fine-tuning pipeline combines translated high-complexity English instruction data with a Thai-adapted AutoIF framework for instruction and linguistic constraints. Using supervised fine-tuning only, without continual pretraining or corpus expansion, SiamGPT-32B improves instruction adherence, multi-turn robustness, and linguistic stability. Evaluations on the SEA-HELM benchmark show that SiamGPT-32B achieves the strongest overall performance among similar-scale open-weights Thai models, with consistent gains in instruction following, multi-turn dialogue, and natural language understanding.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-22T15:00:25+00:00",
      "updated": "2025-12-23T03:39:28+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19455v2",
      "file": "papers/2512.19455v2.pdf"
    },
    {
      "arxiv_id": "2512.19424v1",
      "title": "CodeSimpleQA: Scaling Factuality in Code Large Language Models",
      "authors": [
        {
          "name": "Jian Yang"
        },
        {
          "name": "Wei Zhang"
        },
        {
          "name": "Yizhi Li"
        },
        {
          "name": "Shawn Guo"
        },
        {
          "name": "Haowen Wang"
        },
        {
          "name": "Aishan Liu"
        },
        {
          "name": "Ge Zhang"
        },
        {
          "name": "Zili Wang"
        },
        {
          "name": "Zhoujun Li"
        },
        {
          "name": "Xianglong Liu"
        },
        {
          "name": "Weifeng Lv"
        }
      ],
      "abstract": "Large language models (LLMs) have made significant strides in code generation, achieving impressive capabilities in synthesizing code snippets from natural language instructions. However, a critical challenge remains in ensuring LLMs generate factually accurate responses about programming concepts, technical implementations, etc. Most previous code-related benchmarks focus on code execution correctness, overlooking the factual accuracy of programming knowledge. To address this gap, we present CodeSimpleQA, a comprehensive bilingual benchmark designed to evaluate the factual accuracy of code LLMs in answering code-related questions, which contains carefully curated question-answer pairs in both English and Chinese, covering diverse programming languages and major computer science domains. Further, we create CodeSimpleQA-Instruct, a large-scale instruction corpus with 66M samples, and develop a post-training framework combining supervised fine-tuning and reinforcement learning. Our comprehensive evaluation of diverse LLMs reveals that even frontier LLMs struggle with code factuality. Our proposed framework demonstrates substantial improvements over the base model, underscoring the critical importance of factuality-aware alignment in developing reliable code LLMs.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-22T14:27:17+00:00",
      "updated": "2025-12-22T14:27:17+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19424v1",
      "file": "papers/2512.19424v1.pdf"
    },
    {
      "arxiv_id": "2512.19414v1",
      "title": "From Retrieval to Reasoning: A Framework for Cyber Threat Intelligence NER with Explicit and Adaptive Instructions",
      "authors": [
        {
          "name": "Jiaren Peng"
        },
        {
          "name": "Hongda Sun"
        },
        {
          "name": "Xuan Tian"
        },
        {
          "name": "Cheng Huang"
        },
        {
          "name": "Zeqing Li"
        },
        {
          "name": "Rui Yan"
        }
      ],
      "abstract": "The automation of Cyber Threat Intelligence (CTI) relies heavily on Named Entity Recognition (NER) to extract critical entities from unstructured text. Currently, Large Language Models (LLMs) primarily address this task through retrieval-based In-Context Learning (ICL). This paper analyzes this mainstream paradigm, revealing a fundamental flaw: its success stems not from global semantic similarity but largely from the incidental overlap of entity types within retrieved examples. This exposes the limitations of relying on unreliable implicit induction. To address this, we propose TTPrompt, a framework shifting from implicit induction to explicit instruction. TTPrompt maps the core concepts of CTI's Tactics, Techniques, and Procedures (TTPs) into an instruction hierarchy: formulating task definitions as Tactics, guiding strategies as Techniques, and annotation guidelines as Procedures. Furthermore, to handle the adaptability challenge of static guidelines, we introduce Feedback-driven Instruction Refinement (FIR). FIR enables LLMs to self-refine guidelines by learning from errors on minimal labeled data, adapting to distinct annotation dialects. Experiments on five CTI NER benchmarks demonstrate that TTPrompt consistently surpasses retrieval-based baselines. Notably, with refinement on just 1% of training data, it rivals models fine-tuned on the full dataset. For instance, on LADDER, its Micro F1 of 71.96% approaches the fine-tuned baseline, and on the complex CTINexus, its Macro F1 exceeds the fine-tuned ACLM model by 10.91%.",
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "published": "2025-12-22T14:13:01+00:00",
      "updated": "2025-12-22T14:13:01+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19414v1",
      "file": "papers/2512.19414v1.pdf"
    },
    {
      "arxiv_id": "2512.19399v1",
      "title": "Brain-Grounded Axes for Reading and Steering LLM States",
      "authors": [
        {
          "name": "Sandro Andric"
        }
      ],
      "abstract": "Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-12-22T13:51:03+00:00",
      "updated": "2025-12-22T13:51:03+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19399v1",
      "file": "papers/2512.19399v1.pdf"
    },
    {
      "arxiv_id": "2512.19378v1",
      "title": "HATS: High-Accuracy Triple-Set Watermarking for Large Language Models",
      "authors": [
        {
          "name": "Zhiqing Hu"
        },
        {
          "name": "Chenxu Zhao"
        },
        {
          "name": "Jiazhong Lu"
        },
        {
          "name": "Xiaolei Liu"
        }
      ],
      "abstract": "Misuse of LLM-generated text can be curbed by watermarking techniques that embed implicit signals into the output. We propose a watermark that partitions the vocabulary at each decoding step into three sets (Green/Yellow/Red) with fixed ratios and restricts sampling to the Green and Yellow sets. At detection time, we replay the same partitions, compute Green-enrichment and Red-depletion statistics, convert them to one-sided z-scores, and aggregate their p-values via Fisher's method to decide whether a passage is watermarked. We implement generation, detection, and testing on Llama 2 7B, and evaluate true-positive rate, false-positive rate, and text quality. Results show that the triple-partition scheme achieves high detection accuracy at fixed FPR while preserving readability.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-22T13:23:29+00:00",
      "updated": "2025-12-22T13:23:29+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19378v1",
      "file": "papers/2512.19378v1.pdf"
    },
    {
      "arxiv_id": "2512.19305v1",
      "title": "CienaLLM: Generative Climate-Impact Extraction from News Articles with Autoregressive LLMs",
      "authors": [
        {
          "name": "Javier Vela-Tambo"
        },
        {
          "name": "Jorge Gracia"
        },
        {
          "name": "Fernando Dominguez-Castro"
        }
      ],
      "abstract": "Understanding and monitoring the socio-economic impacts of climate hazards requires extracting structured information from heterogeneous news articles on a large scale. To that end, we have developed CienaLLM, a modular framework based on schema-guided Generative Information Extraction. CienaLLM uses open-weight Large Language Models for zero-shot information extraction from news articles, and supports configurable prompts and output schemas, multi-step pipelines, and cloud or on-premise inference. To systematically assess how the choice of LLM family, size, precision regime, and prompting strategy affect performance, we run a large factorial study in models, precisions, and prompt engineering techniques. An additional response parsing step nearly eliminates format errors while preserving accuracy; larger models deliver the strongest and most stable performance, while quantization offers substantial efficiency gains with modest accuracy trade-offs; and prompt strategies show heterogeneous, model-specific effects. CienaLLM matches or outperforms the supervised baseline in accuracy for extracting drought impacts from Spanish news, although at a higher inference cost. While evaluated in droughts, the schema-driven and model-agnostic design is suitable for adapting to related information extraction tasks (e.g., other hazards, sectors, or languages) by editing prompts and schemas rather than retraining. We release code, configurations, and schemas to support reproducible use.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-22T11:53:01+00:00",
      "updated": "2025-12-22T11:53:01+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19305v1",
      "file": "papers/2512.19305v1.pdf"
    },
    {
      "arxiv_id": "2512.19247v1",
      "title": "Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics",
      "authors": [
        {
          "name": "Do Minh Duc"
        },
        {
          "name": "Quan Xuan Truong"
        },
        {
          "name": "Nguyen Tat Dat"
        },
        {
          "name": "Nguyen Van Vinh"
        }
      ],
      "abstract": "Prompt engineering plays a critical role in adapting large language models (LLMs) to complex reasoning and labeling tasks without the need for extensive fine-tuning. In this paper, we propose a novel prompt optimization pipeline for frame detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT) to generate highly effective task-specific prompts. Central to our approach is an LLM-based prompt optimizer agent that iteratively refines the prompts using retrieved examples, performance feedback, and internal self-evaluation. Our framework is evaluated on a real-world logistics text annotation task, where reasoning accuracy and labeling efficiency are critical. Experimental results show that the optimized prompts - particularly those enhanced via Auto-CoT and RAG - improve real-world inference accuracy by up to 15% compared to baseline zero-shot or static prompts. The system demonstrates consistent improvements across multiple LLMs, including GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B), validating its generalizability and practical value. These findings suggest that structured prompt optimization is a viable alternative to full fine-tuning, offering scalable solutions for deploying LLMs in domain-specific NLP applications such as logistics.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-22T10:29:51+00:00",
      "updated": "2025-12-22T10:29:51+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19247v1",
      "file": "papers/2512.19247v1.pdf"
    },
    {
      "arxiv_id": "2512.19238v1",
      "title": "Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation",
      "authors": [
        {
          "name": "Anna-Maria Gueorguieva"
        },
        {
          "name": "Aylin Caliskan"
        }
      ],
      "abstract": "Large language models (LLMs) have been shown to exhibit social bias, however, bias towards non-protected stigmatized identities remain understudied. Furthermore, what social features of stigmas are associated with bias in LLM outputs is unknown. From psychology literature, it has been shown that stigmas contain six shared social features: aesthetics, concealability, course, disruptiveness, origin, and peril. In this study, we investigate if human and LLM ratings of the features of stigmas, along with prompt style and type of stigma, have effect on bias towards stigmatized groups in LLM outputs. We measure bias against 93 stigmatized groups across three widely used LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B) using SocialStigmaQA, a benchmark that includes 37 social scenarios about stigmatized identities; for example deciding wether to recommend them for an internship. We find that stigmas rated by humans to be highly perilous (e.g., being a gang member or having HIV) have the most biased outputs from SocialStigmaQA prompts (60% of outputs from all models) while sociodemographic stigmas (e.g. Asian-American or old age) have the least amount of biased outputs (11%). We test if the amount of biased outputs could be decreased by using guardrail models, models meant to identify harmful input, using each LLM's respective guardrail model (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API). We find that bias decreases significantly by 10.4%, 1.4%, and 7.8%, respectively. However, we show that features with significant effect on bias remain unchanged post-mitigation and that guardrail models often fail to recognize the intent of bias in prompts. This work has implications for using LLMs in scenarios involving stigmatized groups and we suggest future work towards improving guardrail models for bias mitigation.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-22T10:20:20+00:00",
      "updated": "2025-12-22T10:20:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19238v1",
      "file": "papers/2512.19238v1.pdf"
    },
    {
      "arxiv_id": "2512.19171v1",
      "title": "JEPA-Reasoner: Decoupling Latent Reasoning from Token Generation",
      "authors": [
        {
          "name": "Bingyang Kelvin Liu"
        },
        {
          "name": "Ziyu Patrick Chen"
        }
      ],
      "abstract": "While Joint-Embedding Predictive Architecture (JEPA) has emerged as a powerful architecture for learning rich latent representations, it fundamentally lacks generative abilities. Meanwhile, latent space reasoning attempts for Transformer models like COCONUT do improve performance, but they ultimately rely on token-by-token generation, which still accumulates compounding error and relies on context information to gain reasoning insights. To address these limitations, we propose JEPA-Reasoner, a novel JEPA model enhanced with generative ability that reasons in latent space. We augment it with a separate action-taker model, Talker, to produce human-readable sentences. Our approach demonstrates that decoupling latent space reasoning and token generation enables JEPA-Reasoner to produce mixed latent vectors that might lay the foundation for multi-threaded reasoning, while performing autoregressive generation with superior robustness to compounding error.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-22T09:05:06+00:00",
      "updated": "2025-12-22T09:05:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19171v1",
      "file": "papers/2512.19171v1.pdf"
    },
    {
      "arxiv_id": "2512.19134v1",
      "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation",
      "authors": [
        {
          "name": "Dehai Min"
        },
        {
          "name": "Kailin Zhang"
        },
        {
          "name": "Tongtong Wu"
        },
        {
          "name": "Lu Cheng"
        }
      ],
      "abstract": "Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "published": "2025-12-22T08:28:05+00:00",
      "updated": "2025-12-22T08:28:05+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19134v1",
      "file": "papers/2512.19134v1.pdf"
    },
    {
      "arxiv_id": "2512.19126v2",
      "title": "AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards",
      "authors": [
        {
          "name": "Zihan Lin"
        },
        {
          "name": "Xiaohan Wang"
        },
        {
          "name": "Hexiong Yang"
        },
        {
          "name": "Jiajun Chai"
        },
        {
          "name": "Jie Cao"
        },
        {
          "name": "Guojun Yin"
        },
        {
          "name": "Wei Lin"
        },
        {
          "name": "Ran He"
        }
      ],
      "abstract": "While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization objective. To address this, we propose advantage-weighted policy optimization (AWPO) -- a principled RL framework that effectively integrates explicit reasoning rewards to enhance tool-use capability. AWPO incorporates variance-aware gating and difficulty-aware weighting to adaptively modulate advantages from reasoning signals based on group-relative statistics, alongside a tailored clipping mechanism for stable optimization. Extensive experiments demonstrate that AWPO achieves state-of-the-art performance across standard tool-use benchmarks, significantly outperforming strong baselines and leading closed-source models in challenging multi-turn scenarios. Notably, with exceptional parameter efficiency, our 4B model surpasses Grok-4 by 16.0 percent in multi-turn accuracy while preserving generalization capability on the out-of-distribution MMLU-Pro benchmark.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-22T08:07:00+00:00",
      "updated": "2025-12-23T11:15:52+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19126v2",
      "file": "papers/2512.19126v2.pdf"
    },
    {
      "arxiv_id": "2512.19125v1",
      "title": "SAP: Syntactic Attention Pruning for Transformer-based Language Models",
      "authors": [
        {
          "name": "Tzu-Yun Lee"
        },
        {
          "name": "Ding-Yong Hong"
        },
        {
          "name": "Jan-Jan Wu"
        }
      ],
      "abstract": "This paper introduces Syntactic Attention Pruning (SAP), a novel method for effectively pruning attention heads in Transformer models. Unlike conventional approaches that rely solely on mathematical analysis of model weights and activations, SAP incorporates both the syntactic structure and attention patterns of sentences to guide the pruning process. By leveraging these linguistic features, SAP not only achieves performance comparable to state-of-the-art methods but also enhances the interpretability of model behavior. To further improve robustness, we propose Candidate Filtering (CF), a mechanism that prioritizes heads based on their contribution to model performance, mitigating degradation during pruning. Experimental results indicate that SAP effectively preserves critical heads of a high density of strong attention values, outperforming existing head pruning strategies in retrain-free settings. These findings position SAP as a promising foundation for a new direction in model compression research, offering high flexibility for pruning across all transformer-based language models.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-12-22T08:05:01+00:00",
      "updated": "2025-12-22T08:05:01+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19125v1",
      "file": "papers/2512.19125v1.pdf"
    },
    {
      "arxiv_id": "2512.19122v1",
      "title": "BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation",
      "authors": [
        {
          "name": "Mahir Labib Dihan"
        },
        {
          "name": "Sadif Ahmed"
        },
        {
          "name": "Md Nafiu Rahman"
        }
      ],
      "abstract": "Bangla is a low-resource language for code generation, lacking large-scale annotated datasets and tools to transform natural language specifications into executable programs. This makes Bangla-to-code generation a challenging task requiring innovative solutions. To address this, we introduce BanglaForge, a novel framework for generating code from Bangla function descriptions. BanglaForge leverages a retrieval-augmented dual-model collaboration paradigm with self-refinement, combining in-context learning, llm-based translation, systematic prompt engineering, and iterative self-refinement based on execution feedback, where a coder generates initial solutions and a reviewer enhances them for robustness. On the BLP-2025 Bangla Code Generation benchmark, BanglaForge achieves a competitive Pass@1 accuracy of 84.00%, demonstrating the effectiveness of retrieval, model collaboration, and self-refinement for low-resource Bangla code generation.",
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.CL"
      ],
      "published": "2025-12-22T07:53:16+00:00",
      "updated": "2025-12-22T07:53:16+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19122v1",
      "file": "papers/2512.19122v1.pdf"
    },
    {
      "arxiv_id": "2512.19012v2",
      "title": "DramaBench: A Six-Dimensional Evaluation Framework for Drama Script Continuation",
      "authors": [
        {
          "name": "Shijian Ma"
        },
        {
          "name": "Yunqi Huang"
        },
        {
          "name": "Yan Lin"
        }
      ],
      "abstract": "Drama script continuation requires models to maintain character consistency, advance plot coherently, and preserve dramatic structurecapabilities that existing benchmarks fail to evaluate comprehensively. We present DramaBench, the first large-scale benchmark for evaluating drama script continuation across six independent dimensions: Format Standards, Narrative Efficiency, Character Consistency, Emotional Depth, Logic Consistency, and Conflict Handling. Our framework combines rulebased analysis with LLM-based labeling and statistical metrics, ensuring objective and reproducible evaluation. We conduct comprehensive evaluation of 8 state-of-the-art language models on 1,103 scripts (8,824 evaluations total), with rigorous statistical significance testing (252 pairwise comparisons, 65.9% significant) and human validation (188 scripts, substantial agreement on 3/5 dimensions). Our ablation studies confirm all six dimensions capture independent quality aspects (mean | r | = 0.020). DramaBench provides actionable, dimensionspecific feedback for model improvement and establishes a rigorous standard for creative writing evaluation.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-22T04:03:01+00:00",
      "updated": "2025-12-23T06:23:22+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19012v2",
      "file": "papers/2512.19012v2.pdf"
    },
    {
      "arxiv_id": "2512.19011v1",
      "title": "Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline",
      "authors": [
        {
          "name": "Akshaj Prashanth Rao"
        },
        {
          "name": "Advait Singh"
        },
        {
          "name": "Saumya Kumaar Saksena"
        },
        {
          "name": "Dhruv Kumar"
        }
      ],
      "abstract": "Prompt injection and jailbreaking attacks pose persistent security challenges to large language model (LLM)-based systems. We present an efficient and systematically evaluated defense architecture that mitigates these threats through a lightweight, multi-stage pipeline. Its core component is a semantic filter based on text normalization, TF-IDF representations, and a Linear SVM classifier. Despite its simplicity, this module achieves 93.4% accuracy and 96.5% specificity on held-out data, substantially reducing attack throughput while incurring negligible computational overhead.\n  Building on this efficient foundation, the full pipeline integrates complementary detection and mitigation mechanisms that operate at successive stages, providing strong robustness with minimal latency. In comparative experiments, our SVM-based configuration improves overall accuracy from 35.1% to 93.4% while reducing average time to completion from approximately 450s to 47s, yielding over 10 times lower latency than ShieldGemma. These results demonstrate that the proposed design simultaneously advances defensive precision and efficiency, addressing a core limitation of current model-based moderators.\n  Evaluation across a curated corpus of over 30,000 labeled prompts, including benign, jailbreak, and application-layer injections, confirms that staged, resource-efficient defenses can robustly secure modern LLM-driven applications.",
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-12-22T04:00:35+00:00",
      "updated": "2025-12-22T04:00:35+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19011v1",
      "file": "papers/2512.19011v1.pdf"
    },
    {
      "arxiv_id": "2512.19004v1",
      "title": "Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models",
      "authors": [
        {
          "name": "Tongyuan Miao"
        },
        {
          "name": "Gary Huang"
        },
        {
          "name": "Kai Jun Han"
        },
        {
          "name": "Annie Jiang"
        }
      ],
      "abstract": "Diffusion Large Language Models (DLLMs) enable fully parallel token decoding but often remain impractical at inference time due to the many denoising iterations required to refine an information-free, fully masked initialization into coherent text. Most existing acceleration methods focus on traversing this generative trajectory more efficiently via improved solvers or sampling strategies. We advance a complementary perspective: shorten the trajectory itself by starting closer to the target distribution through context-aware initialization.\n  We propose a training-free interface that injects prompt-conditioned priors from a lightweight auxiliary model into the diffusion initialization, and instantiate it with two mechanisms: discrete token injection and representation-level embedding interpolation. Because injected priors can be imperfect and unmask-only decoding can over-commit early, we also introduce a simple confidence-based remasking mechanism as a form of prior skepticism. Preliminary evidence on GSM8K suggests that context-aware initialization can substantially reduce denoising iterations (about 35\\% fewer function evaluations in our setting), while also exposing a key open challenge: naive warm-starting can degrade final accuracy relative to strong diffusion baselines. We use these findings to motivate a research agenda around calibration, revision mechanisms, and representation alignment for reliable warm-started diffusion decoding.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-22T03:45:04+00:00",
      "updated": "2025-12-22T03:45:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19004v1",
      "file": "papers/2512.19004v1.pdf"
    },
    {
      "arxiv_id": "2512.18999v1",
      "title": "Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework",
      "authors": [
        {
          "name": "Jinyan Liu"
        },
        {
          "name": "Zikang Chen"
        },
        {
          "name": "Qinchuan Wang"
        },
        {
          "name": "Tan Xie"
        },
        {
          "name": "Heming Zheng"
        },
        {
          "name": "Xudong Lv"
        }
      ],
      "abstract": "When applied directly in an end-to-end manner to medical follow-up tasks, Large Language Models (LLMs) often suffer from uncontrolled dialog flow and inaccurate information extraction due to the complexity of follow-up forms. To address this limitation, we designed and compared two follow-up chatbot systems: an end-to-end LLM-based system (control group) and a modular pipeline with structured process control (experimental group). Experimental results show that while the end-to-end approach frequently fails on lengthy and complex forms, our modular method-built on task decomposition, semantic clustering, and flow management-substantially improves dialog stability and extraction accuracy. Moreover, it reduces the number of dialogue turns by 46.73% and lowers token consumption by 80% to 87.5%. These findings highlight the necessity of integrating external control mechanisms when deploying LLMs in high-stakes medical follow-up scenarios.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-22T03:33:43+00:00",
      "updated": "2025-12-22T03:33:43+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18999v1",
      "file": "papers/2512.18999v1.pdf"
    },
    {
      "arxiv_id": "2512.18940v1",
      "title": "FASTRIC: Prompt Specification Language for Verifiable LLM Interactions",
      "authors": [
        {
          "name": "Wen-Long Jin"
        }
      ],
      "abstract": "Large Language Models (LLMs) execute complex multi-turn interaction protocols but lack formal specifications to verify execution against designer intent. We introduce FASTRIC, a Prompt Specification Language that makes implicit Finite State Machines (FSMs) explicit in natural language prompts, enabling conformance verification through execution trace analysis. The LLM serves as intelligent execution agent: interpreting designer-encoded FSMs to execute specified behavioral roles. Unlike symbolic specification languages requiring parsers and compilers, FASTRIC leverages LLMs as unified infrastructure-simultaneously parser, interpreter, runtime environment, and development assistant. FASTRIC guides designers to articulate seven FSM elements (Final States, Agents, States, Triggers, Roles, Initial State, Constraints) structuring multi-turn interactions. Specification formality-ranging from implicit descriptions that frontier models infer to explicit step-by-step instructions for weaker models-serves as a design parameter. We introduce procedural conformance as verification metric measuring execution adherence to FSM specifications. Testing a 3-state kindergarten tutoring FSM across four formality levels and three model scales (14.7B, 685B, 1T+ parameters) reveals optimal specification formality is a function of model capacity. DeepSeek-V3.2 (685B) achieves perfect conformance (1.00) at L2-L4; ChatGPT-5 (~1T) peaks at L3 (0.90) before collapsing at L4 (0.39); Phi4 (14.7B) shows no stable optimum with high variance (SD=0.16-0.36). These findings reveal model-specific formality ranges-\"Goldilocks zones\"-where specifications provide sufficient structure without over-constraint, establishing Prompt Specification Engineering for creating verifiable interaction protocols, transforming multi-turn interaction design from heuristic art to systematic engineering with measurable procedural guarantees.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.SE"
      ],
      "published": "2025-12-22T01:19:50+00:00",
      "updated": "2025-12-22T01:19:50+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18940v1",
      "file": "papers/2512.18940v1.pdf"
    },
    {
      "arxiv_id": "2512.18906v1",
      "title": "Remedy-R: Generative Reasoning for Machine Translation Evaluation without Error Annotations",
      "authors": [
        {
          "name": "Shaomu Tan"
        },
        {
          "name": "Ryosuke Mitani"
        },
        {
          "name": "Ritvik Choudhary"
        },
        {
          "name": "Qiyu Wu"
        },
        {
          "name": "Toshiyuki Sekiya"
        },
        {
          "name": "Christof Monz"
        }
      ],
      "abstract": "Over the years, automatic MT metrics have hillclimbed benchmarks and presented strong and sometimes human-level agreement with human ratings. Yet they remain black-box, offering little insight into their decision-making and often failing under real-world out-of-distribution (OOD) inputs. We introduce Remedy-R, a reasoning-driven generative MT metric trained with reinforcement learning from pairwise translation preferences, without requiring error-span annotations or distillation from closed LLMs. Remedy-R produces step-by-step analyses of accuracy, fluency, and completeness, followed by a final score, enabling more interpretable assessments. With only 60K training pairs across two language pairs, Remedy-R remains competitive with top scalar metrics and GPT-4-based judges on WMT22-24 meta-evaluation, generalizes to other languages, and exhibits strong robustness on OOD stress tests. Moreover, Remedy-R models generate self-reflective feedback that can be reused for translation improvement. Building on this finding, we introduce Remedy-R Agent, a simple evaluate-revise pipeline that leverages Remedy-R's evaluation analysis to refine translations. This agent consistently improves translation quality across diverse models, including Qwen2.5, ALMA-R, GPT-4o-mini, and Gemini-2.0-Flash, suggesting that Remedy-R's reasoning captures translation-relevant information and is practically useful.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-21T22:37:38+00:00",
      "updated": "2025-12-21T22:37:38+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18906v1",
      "file": "papers/2512.18906v1.pdf"
    },
    {
      "arxiv_id": "2512.18841v1",
      "title": "MDToC: Metacognitive Dynamic Tree of Concepts for Boosting Mathematical Problem-Solving of Large Language Models",
      "authors": [
        {
          "name": "Tung Duong Ta"
        },
        {
          "name": "Tim Oates"
        }
      ],
      "abstract": "Despite advances in mathematical reasoning capabilities, Large Language Models (LLMs) still struggle with calculation verification when using established prompting techniques. We present MDToC (Metacognitive Dynamic Tree of Concepts), a three-phase approach that constructs a concept tree, develops accuracy-verified calculations for each concept, and employs majority voting to evaluate competing solutions. Evaluations across CHAMP, MATH, and Game-of-24 benchmarks demonstrate our MDToC's effectiveness, with GPT-4-Turbo achieving 58.1\\% on CHAMP, 86.6\\% on MATH, and 85\\% on Game-of-24 - outperforming GoT by 5\\%, 5.4\\%, and 4\\% on all these tasks, respectively, without hand-engineered hints. MDToC consistently surpasses existing prompting methods across all backbone models, yielding improvements of up to 7.6\\% over ToT and 6.2\\% over GoT, establishing metacognitive calculation verification as a promising direction for enhanced mathematical reasoning.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-21T18:11:24+00:00",
      "updated": "2025-12-21T18:11:24+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18841v1",
      "file": "papers/2512.18841v1.pdf"
    },
    {
      "arxiv_id": "2512.18834v1",
      "title": "AraMix: Recycling, Refiltering, and Deduplicating to Deliver the Largest Arabic Pretraining Corpus",
      "authors": [
        {
          "name": "Sultan Alrashed"
        },
        {
          "name": "Francesco Orabona"
        }
      ],
      "abstract": "We present AraMix, a deduplicated Arabic pretraining corpus containing approximately 178 billion tokens across 179 million documents. Rather than scraping the web again, AraMix demonstrates that substantial value lies in systematically reusing and curating existing pretraining datasets: we combine seven publicly available Arabic web datasets, apply quality filtering designed specifically for Arabic text to re-filter some datasets, and perform cross-dataset deduplication, both MinHash and sentence-level. This approach reveals that nearly 60% of tokens across these independently collected corpora are duplicates, redundancy that any new scraping efforts will reproduce. Our work suggests that for lower resource languages, investment in curation pipelines for existing data yields greater returns than additional web crawls, an approach that allowed us to curate the largest heavily filtered publicly available Arabic pretraining corpus.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-21T17:36:26+00:00",
      "updated": "2025-12-21T17:36:26+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18834v1",
      "file": "papers/2512.18834v1.pdf"
    },
    {
      "arxiv_id": "2512.18832v1",
      "title": "From Word to World: Can Large Language Models be Implicit Text-based World Models?",
      "authors": [
        {
          "name": "Yixia Li"
        },
        {
          "name": "Hongru Wang"
        },
        {
          "name": "Jiahao Qiu"
        },
        {
          "name": "Zhenfei Yin"
        },
        {
          "name": "Dongdong Zhang"
        },
        {
          "name": "Cheng Qian"
        },
        {
          "name": "Zeping Li"
        },
        {
          "name": "Pony Ma"
        },
        {
          "name": "Guanhua Chen"
        },
        {
          "name": "Heng Ji"
        },
        {
          "name": "Mengdi Wang"
        }
      ],
      "abstract": "Agentic reinforcement learning increasingly relies on experience-driven scaling, yet real-world environments remain non-adaptive, limited in coverage, and difficult to scale. World models offer a potential way to improve learning efficiency through simulated experience, but it remains unclear whether large language models can reliably serve this role and under what conditions they meaningfully benefit agents. We study these questions in text-based environments, which provide a controlled setting to reinterpret language modeling as next-state prediction under interaction. We introduce a three-level framework for evaluating LLM-based world models: (i) fidelity and consistency, (ii) scalability and robustness, and (iii) agent utility. Across five representative environments, we find that sufficiently trained world models maintain coherent latent state, scale predictably with data and model size, and improve agent performance via action verification, synthetic trajectory generation, and warm-starting reinforcement learning. Meanwhile, these gains depend critically on behavioral coverage and environment complexity, delineating clear boundry on when world modeling effectively supports agent learning.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-21T17:28:42+00:00",
      "updated": "2025-12-21T17:28:42+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18832v1",
      "file": "papers/2512.18832v1.pdf"
    },
    {
      "arxiv_id": "2512.18748v2",
      "title": "Code2Doc: A Quality-First Curated Dataset for Code Documentation",
      "authors": [
        {
          "name": "Recep Kaan Karaman"
        },
        {
          "name": "Meftun Akarsu"
        }
      ],
      "abstract": "The performance of automatic code documentation generation models depends critically on the quality of the training data used for supervision. However, most existing code documentation datasets are constructed through large scale scraping of public repositories with limited quality control. As a result, they often contain noisy documentation, extensive duplication, and increasing contamination from AI generated content. These issues weaken the supervision signal available to learning-based models and complicate evaluation.\n  We introduce Code2Doc, a quality-first curated dataset for function-level code documentation generation. Code2Doc consists of 13,358 high-quality function-documentation pairs extracted from widely used open-source repositories spanning five programming languages: Python, Java, TypeScript, JavaScript, and C++. The dataset is constructed using a four-stage curation pipeline that enforces documentation completeness and clarity, filters functions based on structural and complexity criteria, removes exact and near-duplicate code, and identifies documentation likely to be AI generated. Starting from 52,069 extracted candidates, only 25.6% satisfy all quality constraints.\n  We provide a detailed analysis of the resulting dataset, which achieves a mean documentation quality score of 6.93 out of 10. Overall, 86.9% of samples contain explicit type annotations, and only 2.9% are flagged as potentially AI generated. Baseline experiments show that fine-tuning a large language model on Code2Doc yields relative improvements of 29.47% in BLEU and 24.04% in ROUGE-L over zero shot performance, despite the modest dataset size. We release both the dataset and the full curation pipeline to support reproducible research on automatic code documentation generation.",
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-12-21T14:28:51+00:00",
      "updated": "2025-12-24T06:47:27+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18748v2",
      "file": "papers/2512.18748v2.pdf"
    },
    {
      "arxiv_id": "2512.18746v1",
      "title": "MemEvolve: Meta-Evolution of Agent Memory Systems",
      "authors": [
        {
          "name": "Guibin Zhang"
        },
        {
          "name": "Haotian Ren"
        },
        {
          "name": "Chong Zhan"
        },
        {
          "name": "Zhenhong Zhou"
        },
        {
          "name": "Junhao Wang"
        },
        {
          "name": "He Zhu"
        },
        {
          "name": "Wangchunshu Zhou"
        },
        {
          "name": "Shuicheng Yan"
        }
      ],
      "abstract": "Self-evolving memory systems are unprecedentedly reshaping the evolutionary paradigm of large language model (LLM)-based agents. Prior work has predominantly relied on manually engineered memory architectures to store trajectories, distill experience, and synthesize reusable tools, enabling agents to evolve on the fly within environment interactions. However, this paradigm is fundamentally constrained by the staticity of the memory system itself: while memory facilitates agent-level evolving, the underlying memory architecture cannot be meta-adapted to diverse task contexts. To address this gap, we propose MemEvolve, a meta-evolutionary framework that jointly evolves agents' experiential knowledge and their memory architecture, allowing agent systems not only to accumulate experience but also to progressively refine how they learn from it. To ground MemEvolve in prior research and foster openness in future self-evolving systems, we introduce EvolveLab, a unified self-evolving memory codebase that distills twelve representative memory systems into a modular design space (encode, store, retrieve, manage), providing both a standardized implementation substrate and a fair experimental arena. Extensive evaluations on four challenging agentic benchmarks demonstrate that MemEvolve achieves (I) substantial performance gains, improving frameworks such as SmolAgent and Flash-Searcher by up to $17.06\\%$; and (II) strong cross-task and cross-LLM generalization, designing memory architectures that transfer effectively across diverse benchmarks and backbone models.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.MA"
      ],
      "published": "2025-12-21T14:26:14+00:00",
      "updated": "2025-12-21T14:26:14+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18746v1",
      "file": "papers/2512.18746v1.pdf"
    },
    {
      "arxiv_id": "2512.18623v1",
      "title": "LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction",
      "authors": [
        {
          "name": "Jensen Zhang"
        },
        {
          "name": "Ningyuan Liu"
        },
        {
          "name": "Yijia Fan"
        },
        {
          "name": "Zihao Huang"
        },
        {
          "name": "Qinglin Zeng"
        },
        {
          "name": "Kaitong Cai"
        },
        {
          "name": "Jian Wang"
        },
        {
          "name": "Keze Wang"
        }
      ],
      "abstract": "Large language models (LLMs) often generate hallucinated content that lacks factual or contextual grounding, limiting their reliability in critical applications. Existing approaches such as supervised fine-tuning and reinforcement learning from human feedback are data intensive and computationally expensive, while static parameter editing methods struggle with context dependent errors and catastrophic forgetting.\n  We propose LLM-CAS, a framework that formulates real-time hallucination correction as a hierarchical reinforcement learning problem. LLM-CAS trains an agent to learn a policy that dynamically selects temporary neuron perturbations during inference based on the current context. Unlike prior dynamic approaches that rely on heuristic or predefined adjustments, this policy driven mechanism enables adaptive and fine grained correction without permanent parameter modification.\n  Experiments across multiple language models demonstrate that LLM-CAS consistently improves factual accuracy, achieving gains of 10.98 percentage points on StoryCloze, 2.71 points on TriviaQA, and 2.06 points on the MC1 score of TruthfulQA. These results outperform both static editing methods such as ITI and CAA and the dynamic SADI framework. Overall, LLM-CAS provides an efficient and context aware solution for improving the reliability of LLMs, with promising potential for future multimodal extensions.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-21T06:54:34+00:00",
      "updated": "2025-12-21T06:54:34+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18623v1",
      "file": "papers/2512.18623v1.pdf"
    },
    {
      "arxiv_id": "2512.18622v1",
      "title": "A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback",
      "authors": [
        {
          "name": "Thanh Dat Hoang"
        },
        {
          "name": "Thanh Trung Huynh"
        },
        {
          "name": "Matthias Weidlich"
        },
        {
          "name": "Thanh Tam Nguyen"
        },
        {
          "name": "Tong Chen"
        },
        {
          "name": "Hongzhi Yin"
        },
        {
          "name": "Quoc Viet Hung Nguyen"
        }
      ],
      "abstract": "Text2SQL, the task of generating SQL queries from natural language text, is a critical challenge in data engineering. Recently, Large Language Models (LLMs) have demonstrated superior performance for this task due to their advanced comprehension and generation capabilities. However, privacy and cost considerations prevent companies from using Text2SQL solutions based on external LLMs offered as a service. Rather, small LLMs (SLMs) that are openly available and can hosted in-house are adopted. These SLMs, in turn, lack the generalization capabilities of larger LLMs, which impairs their effectiveness for complex tasks such as Text2SQL. To address these limitations, we propose MATS, a novel Text2SQL framework designed specifically for SLMs. MATS uses a multi-agent mechanism that assigns specialized roles to auxiliary agents, reducing individual workloads and fostering interaction. A training scheme based on reinforcement learning aligns these agents using feedback obtained during execution, thereby maintaining competitive performance despite a limited LLM size. Evaluation results using on benchmark datasets show that MATS, deployed on a single- GPU server, yields accuracy that are on-par with large-scale LLMs when using significantly fewer parameters. Our source code and data are available at https://github.com/thanhdath/mats-sql.",
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.MA"
      ],
      "published": "2025-12-21T06:43:47+00:00",
      "updated": "2025-12-21T06:43:47+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18622v1",
      "file": "papers/2512.18622v1.pdf"
    },
    {
      "arxiv_id": "2512.18608v1",
      "title": "A Comparative Study of Light-weight Language Models for PII Masking and their Deployment for Real Conversational Texts",
      "authors": [
        {
          "name": "Prabigya Acharya"
        },
        {
          "name": "Liza Shrestha"
        }
      ],
      "abstract": "Automated masking of Personally Identifiable Information (PII) is critical for privacy-preserving conversational systems. While current frontier large language models demonstrate strong PII masking capabilities, concerns about data handling and computational costs motivate exploration of whether lightweight models can achieve comparable performance. We compare encoder-decoder and decoder-only architectures by fine-tuning T5-small and Mistral-Instruct-v0.3 on English datasets constructed from the AI4Privacy benchmark. We create different dataset variants to study label standardization and PII representation, covering 24 standardized PII categories and higher-granularity settings. Evaluation using entity-level and character-level metrics, type accuracy, and exact match shows that both lightweight models achieve performance comparable to frontier LLMs for PII masking tasks. Label normalization consistently improves performance across architectures. Mistral achieves higher F1 and recall with greater robustness across PII types but incurs significantly higher generation latency. T5, while less robust in conversational text, offers more controllable structured outputs and lower inference cost, motivating its use in a real-time Discord bot for real-world PII redaction. Evaluation on live messages reveals performance degradation under informal inputs. These results clarify trade-offs between accuracy, robustness, and computational efficiency, demonstrating that lightweight models can provide effective PII masking while addressing data handling concerns associated with frontier LLMs.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-21T05:58:40+00:00",
      "updated": "2025-12-21T05:58:40+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18608v1",
      "file": "papers/2512.18608v1.pdf"
    },
    {
      "arxiv_id": "2512.18601v1",
      "title": "On Finding Inconsistencies in Documents",
      "authors": [
        {
          "name": "Charles J. Lovering"
        },
        {
          "name": "Seth Ebner"
        },
        {
          "name": "Brandon Smock"
        },
        {
          "name": "Michael Krumdick"
        },
        {
          "name": "Saad Rabbani"
        },
        {
          "name": "Ahmed Muhammad"
        },
        {
          "name": "Varshini Reddy"
        },
        {
          "name": "Chris Tanner"
        }
      ],
      "abstract": "Professionals in academia, law, and finance audit their documents because inconsistencies can result in monetary, reputational, and scientific costs. Language models (LMs) have the potential to dramatically speed up this auditing process. To understand their abilities, we introduce a benchmark, FIND (Finding INconsistencies in Documents), where each example is a document with an inconsistency inserted manually by a domain expert. Despite the documents being long, technical, and complex, the best-performing model (gpt-5) recovered 64% of the inserted inconsistencies. Surprisingly, gpt-5 also found undiscovered inconsistencies present in the original documents. For example, on 50 arXiv papers, we judged 136 out of 196 of the model's suggestions to be legitimate inconsistencies missed by the original authors. However, despite these findings, even the best models miss almost half of the inconsistencies in FIND, demonstrating that inconsistency detection is still a challenging task.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-21T05:20:21+00:00",
      "updated": "2025-12-21T05:20:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18601v1",
      "file": "papers/2512.18601v1.pdf"
    },
    {
      "arxiv_id": "2512.18593v1",
      "title": "From Scratch to Fine-Tuned: A Comparative Study of Transformer Training Strategies for Legal Machine Translation",
      "authors": [
        {
          "name": "Amit Barman"
        },
        {
          "name": "Atanu Mandal"
        },
        {
          "name": "Sudip Kumar Naskar"
        }
      ],
      "abstract": "In multilingual nations like India, access to legal information is often hindered by language barriers, as much of the legal and judicial documentation remains in English. Legal Machine Translation (L-MT) offers a scalable solution to this challenge by enabling accurate and accessible translations of legal documents. This paper presents our work for the JUST-NLP 2025 Legal MT shared task, focusing on English-Hindi translation using Transformer-based approaches. We experiment with 2 complementary strategies, fine-tuning a pre-trained OPUS-MT model for domain-specific adaptation and training a Transformer model from scratch using the provided legal corpus. Performance is evaluated using standard MT metrics, including SacreBLEU, chrF++, TER, ROUGE, BERTScore, METEOR, and COMET. Our fine-tuned OPUS-MT model achieves a SacreBLEU score of 46.03, significantly outperforming both baseline and from-scratch models. The results highlight the effectiveness of domain adaptation in enhancing translation quality and demonstrate the potential of L-MT systems to improve access to justice and legal transparency in multilingual contexts.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "published": "2025-12-21T04:45:31+00:00",
      "updated": "2025-12-21T04:45:31+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18593v1",
      "file": "papers/2512.18593v1.pdf"
    },
    {
      "arxiv_id": "2512.18551v1",
      "title": "Neologism Learning as a Parameter-Efficient Alternative to Fine-Tuning for Model Steering",
      "authors": [
        {
          "name": "Sungjoon Park"
        },
        {
          "name": "Varun Ramamurthi"
        },
        {
          "name": "Owen Terry"
        }
      ],
      "abstract": "In language modeling, neologisms are new tokens trained to represent a concept not already included in a given model's vocabulary. Neologisms can be used to encourage specific behavior in models, for example by appending prompts with \"Give me a neologism answer.\" Behavioral steering can also be achieved through fine-tuning, albeit with more compute and less flexibility: learning a neologism only trains d parameters and allows the user to still access the model's default behavior. We compare the performance of neologism learning against low-rank adaptation (LoRA) fine-tuning, finding that neologisms outperform fine-tuned models under a matched training setup (same data and hyperparameters). We also investigate self-verbalizations of neologisms, and observe that the model will occasionally make up its own new words when asked about a neologism.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-21T00:45:23+00:00",
      "updated": "2025-12-21T00:45:23+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18551v1",
      "file": "papers/2512.18551v1.pdf"
    },
    {
      "arxiv_id": "2512.18546v1",
      "title": "LLMs on Drugs: Language Models Are Few-Shot Consumers",
      "authors": [
        {
          "name": "Alexander Doudkin"
        }
      ],
      "abstract": "Large language models (LLMs) are sensitive to the personas imposed on them at inference time, yet prompt-level \"drug\" interventions have never been benchmarked rigorously. We present the first controlled study of psychoactive framings on GPT-5-mini using ARC-Challenge. Four single-sentence prompts -- LSD, cocaine, alcohol, and cannabis -- are compared against a sober control across 100 validation items per condition, with deterministic decoding, full logging, Wilson confidence intervals, and Fisher exact tests. Control accuracy is 0.45; alcohol collapses to 0.10 (p = 3.2e-8), cocaine to 0.21 (p = 4.9e-4), LSD to 0.19 (p = 1.3e-4), and cannabis to 0.30 (p = 0.041), largely because persona prompts disrupt the mandated \"Answer: <LETTER>\" template. Persona text therefore behaves like a \"few-shot consumable\" that can destroy reliability without touching model weights. All experimental code, raw results, and analysis scripts are available at https://github.com/lexdoudkin/llms-on-drugs.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-21T00:19:02+00:00",
      "updated": "2025-12-21T00:19:02+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18546v1",
      "file": "papers/2512.18546v1.pdf"
    },
    {
      "arxiv_id": "2512.18542v1",
      "title": "SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models",
      "authors": [
        {
          "name": "Scott Thornton"
        }
      ],
      "abstract": "AI assistants produce vulnerable code in 45% of security-relevant scenarios, introducing flaws into production systems at scale. Yet existing secure coding datasets fall short. They lack incident grounding, don't provide the scale modern training requires, and miss the operational security context developers need for production deployments. We present SecureCode v2.0, a production-grade dataset of 1,215 security-focused coding examples that passed structural validation and expert security review. Every example ties to actual documented security incidents with CVE references, provides vulnerable and secure implementations, demonstrates concrete attacks, and includes defense-in-depth operational guidance. The dataset covers 11 vulnerability categories (complete OWASP Top 10:2025 plus AI/ML Security Threats) across 11 languages (Python, JavaScript, Java, Go, PHP, C#, TypeScript, Ruby, Rust, Kotlin, and YAML for infrastructure-as-code).\n  Our quality assurance framework ensures complete incident grounding. Each example includes SIEM integration strategies, infrastructure hardening recommendations (Docker, AppArmor, WAF configurations), and testing approaches using language-appropriate frameworks. The dataset uses a 4-turn conversational structure mirroring actual developer-AI interactions, escalating from basic implementations to advanced security considerations and defense-in-depth guidance.\n  Our contributions: (1) 1,215 rigorously validated examples split into 989 training, 122 validation, and 104 test sets, (2) an automated validation framework ensuring dataset consistency, (3) a 4-turn conversational structure capturing realistic security workflows, (4) comprehensive operational security guidance with SIEM integration strategies, (5) complete language-specific implementation fidelity, and (6) open-source release of data, validation tools, and benchmarking protocols.",
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-12-20T23:52:12+00:00",
      "updated": "2025-12-20T23:52:12+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18542v1",
      "file": "papers/2512.18542v1.pdf"
    },
    {
      "arxiv_id": "2512.18533v1",
      "title": "Generalization Gaps in Political Fake News Detection: An Empirical Study on the LIAR Dataset",
      "authors": [
        {
          "name": "S Mahmudul Hasan"
        },
        {
          "name": "Shaily Roy"
        },
        {
          "name": "Akib Jawad Nafis"
        }
      ],
      "abstract": "The proliferation of linguistically subtle political disinformation poses a significant challenge to automated fact-checking systems. Despite increasing emphasis on complex neural architectures, the empirical limits of text-only linguistic modeling remain underexplored. We present a systematic diagnostic evaluation of nine machine learning algorithms on the LIAR benchmark. By isolating lexical features (Bag-of-Words, TF-IDF) and semantic embeddings (GloVe), we uncover a hard \"Performance Ceiling\", with fine-grained classification not exceeding a Weighted F1-score of 0.32 across models. Crucially, a simple linear SVM (Accuracy: 0.624) matches the performance of pre-trained Transformers such as RoBERTa (Accuracy: 0.620), suggesting that model capacity is not the primary bottleneck. We further diagnose a massive \"Generalization Gap\" in tree-based ensembles, which achieve more than 99% training accuracy but collapse to approximately 25% on test data, indicating reliance on lexical memorization rather than semantic inference. Synthetic data augmentation via SMOTE yields no meaningful gains, confirming that the limitation is semantic (feature ambiguity) rather than distributional. These findings indicate that for political fact-checking, increasing model complexity without incorporating external knowledge yields diminishing returns.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-12-20T23:08:18+00:00",
      "updated": "2025-12-20T23:08:18+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18533v1",
      "file": "papers/2512.18533v1.pdf"
    },
    {
      "arxiv_id": "2512.18475v1",
      "title": "Research on a hybrid LSTM-CNN-Attention model for text-based web content classification",
      "authors": [
        {
          "name": "Mykola Kuz"
        },
        {
          "name": "Ihor Lazarovych"
        },
        {
          "name": "Mykola Kozlenko"
        },
        {
          "name": "Mykola Pikuliak"
        },
        {
          "name": "Andrii Kvasniuk"
        }
      ],
      "abstract": "This study presents a hybrid deep learning architecture that integrates LSTM, CNN, and an Attention mechanism to enhance the classification of web content based on text. Pretrained GloVe embeddings are used to represent words as dense vectors that preserve semantic similarity. The CNN layer extracts local n-gram patterns and lexical features, while the LSTM layer models long-range dependencies and sequential structure. The integrated Attention mechanism enables the model to focus selectively on the most informative parts of the input sequence. A 5-fold cross-validation setup was used to assess the robustness and generalizability of the proposed solution. Experimental results show that the hybrid LSTM-CNN-Attention model achieved outstanding performance, with an accuracy of 0.98, precision of 0.94, recall of 0.92, and F1-score of 0.93. These results surpass the performance of baseline models based solely on CNNs, LSTMs, or transformer-based classifiers such as BERT. The combination of neural network components enabled the model to effectively capture both fine-grained text structures and broader semantic context. Furthermore, the use of GloVe embeddings provided an efficient and effective representation of textual data, making the model suitable for integration into systems with real-time or near-real-time requirements. The proposed hybrid architecture demonstrates high effectiveness in text-based web content classification, particularly in tasks requiring both syntactic feature extraction and semantic interpretation. By combining presented mechanisms, the model addresses the limitations of individual architectures and achieves improved generalization. These findings support the broader use of hybrid deep learning approaches in NLP applications, especially where complex, unstructured textual data must be processed and classified with high reliability.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-12-20T19:38:07+00:00",
      "updated": "2025-12-20T19:38:07+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18475v1",
      "file": "papers/2512.18475v1.pdf"
    },
    {
      "arxiv_id": "2512.18462v1",
      "title": "Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling",
      "authors": [
        {
          "name": "Christopher Romn Jaimes"
        }
      ],
      "abstract": "Natural Language Inference (NLI) models frequently rely on spurious correlations rather than semantic reasoning. Existing mitigation strategies often incur high annotation costs or trigger catastrophic forgetting during fine-tuning. We propose an automated, scalable pipeline to address these limitations. First, we introduce Log-Frequency LMI (LF-LMI) to accurately detect semantic artifacts. Second, we generate a high-quality synthetic contrast set via an LLM-synthesis pipeline with multi-judge verification. Finally, we introduce Dynamic Balanced Sampling, a training strategy that rotates the original data distribution to prevent forgetting. Our method improves consistency on a challenging benchmark from 63.5% to 81.0% while maintaining 88.4% in-domain accuracy, significantly outperforming naive fine-tuning.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-20T18:30:54+00:00",
      "updated": "2025-12-20T18:30:54+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18462v1",
      "file": "papers/2512.18462v1.pdf"
    },
    {
      "arxiv_id": "2512.18399v1",
      "title": "AraToken: Optimizing Arabic Tokenization with Normalization Pipeline and Language Extension for Qwen3",
      "authors": [
        {
          "name": "Mark Kashirskiy"
        },
        {
          "name": "Artiom Lipinski"
        },
        {
          "name": "Ilya Makarov"
        }
      ],
      "abstract": "Tokenization is a critical preprocessing step for large language models (LLMs), directly impacting training efficiency and downstream performance. General-purpose tokenizers trained predominantly on English and Latin-script languages exhibit suboptimal performance on morphologically rich languages such as Arabic, resulting in inflated token sequences and reduced compression efficiency. In this work, we present AraToken, an Arabic-optimized tokenizer built on SentencePiece Unigram algorithm with a comprehensive normalization pipeline addressing Arabic-specific orthographic variations including Alif variants, diacritics, and Arabic-Indic numerals. We systematically compare BPE, WordPiece, and SentencePiece algorithms across multiple configurations, demonstrating that SentencePiece with normalization achieves 18% lower fertility (1.199 vs 1.35 tokens/word) compared to unnormalized baselines. Furthermore, we introduce the Language Extension Pipeline (LEP), a method for integrating the optimized tokenizer into Qwen3-0.6B through vocabulary extension with mean subtoken initialization and selective transformer layer unfreezing. Our experiments show that LEP reduces evaluation loss from 8.28 to 2.43 within 800 training steps on 100K Arabic samples. We release our tokenizer, training scripts, and model checkpoints to facilitate Arabic NLP research.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-20T15:32:10+00:00",
      "updated": "2025-12-20T15:32:10+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18399v1",
      "file": "papers/2512.18399v1.pdf"
    },
    {
      "arxiv_id": "2512.18362v1",
      "title": "SRS-Stories: Vocabulary-constrained multilingual story generation for language learning",
      "authors": [
        {
          "name": "Wiktor Kamzela"
        },
        {
          "name": "Mateusz Lango"
        },
        {
          "name": "Ondrej Dusek"
        }
      ],
      "abstract": "In this paper, we use large language models to generate personalized stories for language learners, using only the vocabulary they know. The generated texts are specifically written to teach the user new vocabulary by simply reading stories where it appears in context, while at the same time seamlessly reviewing recently learned vocabulary. The generated stories are enjoyable to read and the vocabulary reviewing/learning is optimized by a Spaced Repetition System. The experiments are conducted in three languages: English, Chinese and Polish, evaluating three story generation methods and three strategies for enforcing lexical constraints. The results show that the generated stories are more grammatical, coherent, and provide better examples of word usage than texts generated by the standard constrained beam search approach",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-20T13:24:59+00:00",
      "updated": "2025-12-20T13:24:59+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18362v1",
      "file": "papers/2512.18362v1.pdf"
    },
    {
      "arxiv_id": "2512.18360v1",
      "title": "LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators",
      "authors": [
        {
          "name": "Mateusz Lango"
        },
        {
          "name": "Ondej Duek"
        }
      ],
      "abstract": "We present a novel neurosymbolic framework for RDF-to-text generation, in which the model is \"trained\" through collaborative interactions among multiple LLM agents rather than traditional backpropagation. The LLM agents produce rule-based Python code for a generator for the given domain, based on RDF triples only, with no in-domain human reference texts. The resulting system is fully interpretable, requires no supervised training data, and generates text nearly instantaneously using only a single CPU. Our experiments on the WebNLG and OpenDialKG data show that outputs produced by our approach reduce hallucination, with only slight fluency penalties compared to finetuned or prompted language models",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-20T13:16:51+00:00",
      "updated": "2025-12-20T13:16:51+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18360v1",
      "file": "papers/2512.18360v1.pdf"
    },
    {
      "arxiv_id": "2512.18357v1",
      "title": "DACE For Railway Acronym Disambiguation",
      "authors": [
        {
          "name": "El Mokhtar Hribach"
        },
        {
          "name": "Oussama Mechhour"
        },
        {
          "name": "Mohammed Elmonstaser"
        },
        {
          "name": "Yassine El Boudouri"
        },
        {
          "name": "Othmane Kabal"
        }
      ],
      "abstract": "Acronym Disambiguation (AD) is a fundamental challenge in technical text processing, particularly in specialized sectors where high ambiguity complicates automated analysis. This paper addresses AD within the context of the TextMine'26 competition on French railway documentation. We present DACE (Dynamic Prompting, Retrieval Augmented Generation, Contextual Selection, and Ensemble Aggregation), a framework that enhances Large Language Models through adaptive in-context learning and external domain knowledge injection. By dynamically tailoring prompts to acronym ambiguity and aggregating ensemble predictions, DACE mitigates hallucination and effectively handles low-resource scenarios. Our approach secured the top rank in the competition with an F1 score of 0.9069.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-20T12:56:06+00:00",
      "updated": "2025-12-20T12:56:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18357v1",
      "file": "papers/2512.18357v1.pdf"
    },
    {
      "arxiv_id": "2512.18352v1",
      "title": "LLM-based Few-Shot Early Rumor Detection with Imitation Agent",
      "authors": [
        {
          "name": "Fengzhu Zeng"
        },
        {
          "name": "Qian Shao"
        },
        {
          "name": "Ling Cheng"
        },
        {
          "name": "Wei Gao"
        },
        {
          "name": "Shih-Fen Cheng"
        },
        {
          "name": "Jing Ma"
        },
        {
          "name": "Cheng Niu"
        }
      ],
      "abstract": "Early Rumor Detection (EARD) aims to identify the earliest point at which a claim can be accurately classified based on a sequence of social media posts. This is especially challenging in data-scarce settings. While Large Language Models (LLMs) perform well in few-shot NLP tasks, they are not well-suited for time-series data and are computationally expensive for both training and inference. In this work, we propose a novel EARD framework that combines an autonomous agent and an LLM-based detection model, where the agent acts as a reliable decision-maker for \\textit{early time point determination}, while the LLM serves as a powerful \\textit{rumor detector}. This approach offers the first solution for few-shot EARD, necessitating only the training of a lightweight agent and allowing the LLM to remain training-free. Extensive experiments on four real-world datasets show our approach boosts performance across LLMs and surpasses existing EARD methods in accuracy and earliness.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-20T12:42:27+00:00",
      "updated": "2025-12-20T12:42:27+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18352v1",
      "file": "papers/2512.18352v1.pdf"
    },
    {
      "arxiv_id": "2512.18337v1",
      "title": "Towards Efficient Agents: A Co-Design of Inference Architecture and System",
      "authors": [
        {
          "name": "Weizhe Lin"
        },
        {
          "name": "Hui-Ling Zhen"
        },
        {
          "name": "Shuai Yang"
        },
        {
          "name": "Xian Wang"
        },
        {
          "name": "Renxi Liu"
        },
        {
          "name": "Hanting Chen"
        },
        {
          "name": "Wangze Zhang"
        },
        {
          "name": "Chuansai Zhou"
        },
        {
          "name": "Yiming Li"
        },
        {
          "name": "Chen Chen"
        },
        {
          "name": "Xing Li"
        },
        {
          "name": "Zhiyuan Yang"
        },
        {
          "name": "Xiaosong Li"
        },
        {
          "name": "Xianzhi Yu"
        },
        {
          "name": "Zhenhua Dong"
        },
        {
          "name": "Mingxuan Yuan"
        },
        {
          "name": "Yunhe Wang"
        }
      ],
      "abstract": "The rapid development of large language model (LLM)-based agents has unlocked new possibilities for autonomous multi-turn reasoning and tool-augmented decision-making. However, their real-world deployment is hindered by severe inefficiencies that arise not from isolated model inference, but from the systemic latency accumulated across reasoning loops, context growth, and heterogeneous tool interactions. This paper presents AgentInfer, a unified framework for end-to-end agent acceleration that bridges inference optimization and architectural design. We decompose the problem into four synergistic components: AgentCollab, a hierarchical dual-model reasoning framework that balances large- and small-model usage through dynamic role assignment; AgentSched, a cache-aware hybrid scheduler that minimizes latency under heterogeneous request patterns; AgentSAM, a suffix-automaton-based speculative decoding method that reuses multi-session semantic memory to achieve low-overhead inference acceleration; and AgentCompress, a semantic compression mechanism that asynchronously distills and reorganizes agent memory without disrupting ongoing reasoning. Together, these modules form a Self-Evolution Engine capable of sustaining efficiency and cognitive stability throughout long-horizon reasoning tasks. Experiments on the BrowseComp-zh and DeepDiver benchmarks demonstrate that through the synergistic collaboration of these methods, AgentInfer reduces ineffective token consumption by over 50%, achieving an overall 1.8-2.5 times speedup with preserved accuracy. These results underscore that optimizing for agentic task completion-rather than merely per-token throughput-is the key to building scalable, efficient, and self-improving intelligent systems.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-20T12:06:13+00:00",
      "updated": "2025-12-20T12:06:13+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18337v1",
      "file": "papers/2512.18337v1.pdf"
    },
    {
      "arxiv_id": "2512.18329v1",
      "title": "LIR$^3$AG: A Lightweight Rerank Reasoning Strategy Framework for Retrieval-Augmented Generation",
      "authors": [
        {
          "name": "Guo Chen"
        },
        {
          "name": "Junjie Huang"
        },
        {
          "name": "Huaijin Xie"
        },
        {
          "name": "Fei Sun"
        },
        {
          "name": "Tao Jia"
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) effectively enhances Large Language Models (LLMs) by incorporating retrieved external knowledge into the generation process. Reasoning models improve LLM performance in multi-hop QA tasks, which require integrating and reasoning over multiple pieces of evidence across different documents to answer a complex question. However, they often introduce substantial computational costs, including increased token consumption and inference latency. To better understand and mitigate this trade-off, we conduct a comprehensive study of reasoning strategies for reasoning models in RAG multi-hop QA tasks. Our findings reveal that reasoning models adopt structured strategies to integrate retrieved and internal knowledge, primarily following two modes: Context-Grounded Reasoning, which relies directly on retrieved content, and Knowledge-Reconciled Reasoning, which resolves conflicts or gaps using internal knowledge. To this end, we propose a novel Lightweight Rerank Reasoning Strategy Framework for RAG (LiR$^3$AG) to enable non-reasoning models to transfer reasoning strategies by restructuring retrieved evidence into coherent reasoning chains. LiR$^3$AG significantly reduce the average 98% output tokens overhead and 58.6% inferencing time while improving 8B non-reasoning model's F1 performance ranging from 6.2% to 22.5% to surpass the performance of 32B reasoning model in RAG, offering a practical and efficient path forward for RAG systems.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-20T11:53:37+00:00",
      "updated": "2025-12-20T11:53:37+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18329v1",
      "file": "papers/2512.18329v1.pdf"
    },
    {
      "arxiv_id": "2512.18321v1",
      "title": "CTTA-T: Continual Test-Time Adaptation for Text Understanding via Teacher-Student with a Domain-aware and Generalized Teacher",
      "authors": [
        {
          "name": "Tianlun Liu"
        },
        {
          "name": "Zhiliang Tian"
        },
        {
          "name": "Zhen Huang"
        },
        {
          "name": "Xingzhi Zhou"
        },
        {
          "name": "Wanlong Yu"
        },
        {
          "name": "Tianle Liu"
        },
        {
          "name": "Feng Liu"
        },
        {
          "name": "Dongsheng Li"
        }
      ],
      "abstract": "Text understanding often suffers from domain shifts. To handle testing domains, domain adaptation (DA) is trained to adapt to a fixed and observed testing domain; a more challenging paradigm, test-time adaptation (TTA), cannot access the testing domain during training and online adapts to the testing samples during testing, where the samples are from a fixed domain. We aim to explore a more practical and underexplored scenario, continual test-time adaptation (CTTA) for text understanding, which involves a sequence of testing (unobserved) domains in testing. Current CTTA methods struggle in reducing error accumulation over domains and enhancing generalization to handle unobserved domains: 1) Noise-filtering reduces accumulated errors but discards useful information, and 2) accumulating historical domains enhances generalization, but it is hard to achieve adaptive accumulation. In this paper, we propose a CTTA-T (continual test-time adaptation for text understanding) framework adaptable to evolving target domains: it adopts a teacher-student framework, where the teacher is domain-aware and generalized for evolving domains. To improve teacher predictions, we propose a refine-then-filter based on dropout-driven consistency, which calibrates predictions and removes unreliable guidance. For the adaptation-generalization trade-off, we construct a domain-aware teacher by dynamically accumulating cross-domain semantics via incremental PCA, which continuously tracks domain shifts. Experiments show CTTA-T excels baselines.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-20T11:39:21+00:00",
      "updated": "2025-12-20T11:39:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18321v1",
      "file": "papers/2512.18321v1.pdf"
    },
    {
      "arxiv_id": "2512.18301v1",
      "title": "InstructNet: A Novel Approach for Multi-Label Instruction Classification through Advanced Deep Learning",
      "authors": [
        {
          "name": "Tanjim Taharat Aurpa"
        },
        {
          "name": "Md Shoaib Ahmed"
        },
        {
          "name": "Md Mahbubur Rahman"
        },
        {
          "name": "Md. Golam Moazzam"
        }
      ],
      "abstract": "People use search engines for various topics and items, from daily essentials to more aspirational and specialized objects. Therefore, search engines have taken over as peoples preferred resource. The How To prefix has become familiar and widely used in various search styles to find solutions to particular problems. This search allows people to find sequential instructions by providing detailed guidelines to accomplish specific tasks. Categorizing instructional text is also essential for task-oriented learning and creating knowledge bases. This study uses the How To articles to determine the multi-label instruction category. We have brought this work with a dataset comprising 11,121 observations from wikiHow, where each record has multiple categories. To find out the multi-label category meticulously, we employ some transformer-based deep neural architectures, such as Generalized Autoregressive Pretraining for Language Understanding (XLNet), Bidirectional Encoder Representation from Transformers (BERT), etc. In our multi-label instruction classification process, we have reckoned our proposed architectures using accuracy and macro f1-score as the performance metrics. This thorough evaluation showed us much about our strategys strengths and drawbacks. Specifically, our implementation of the XLNet architecture has demonstrated unprecedented performance, achieving an accuracy of 97.30% and micro and macro average scores of 89.02% and 93%, a noteworthy accomplishment in multi-label classification. This high level of accuracy and macro average score is a testament to the effectiveness of the XLNet architecture in our proposed InstructNet approach. By employing a multi-level strategy in our evaluation process, we have gained a more comprehensive knowledge of the effectiveness of our proposed architectures and identified areas for forthcoming improvement and refinement.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-20T10:16:09+00:00",
      "updated": "2025-12-20T10:16:09+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18301v1",
      "file": "papers/2512.18301v1.pdf"
    },
    {
      "arxiv_id": "2512.18225v1",
      "title": "GeoSense-AI: Fast Location Inference from Crisis Microblogs",
      "authors": [
        {
          "name": "Deepit Sapru"
        }
      ],
      "abstract": "This paper presents an applied AI pipeline for realtime geolocation from noisy microblog streams, unifying statistical hashtag segmentation, part-of-speech-driven proper-noun detection, dependency parsing around disaster lexicons, lightweight named-entity recognition, and gazetteer-grounded disambiguation to infer locations directly from text rather than sparse geotags. The approach operationalizes information extraction under streaming constraints, emphasizing low-latency NLP components and efficient validation against geographic knowledge bases to support situational awareness during emergencies. In head to head comparisons with widely used NER toolkits, the system attains strong F1 while being engineered for orders-of-magnitude faster throughput, enabling deployment in live crisis informatics settings. A production map interface demonstrates end-to-end AI functionality ingest, inference, and visualization--surfacing locational signals at scale for floods, outbreaks, and other fastmoving events. By prioritizing robustness to informal text and streaming efficiency, GeoSense-AI illustrates how domain-tuned NLP and knowledge grounding can elevate emergency response beyond conventional geo-tag reliance.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.SI"
      ],
      "published": "2025-12-20T05:46:57+00:00",
      "updated": "2025-12-20T05:46:57+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18225v1",
      "file": "papers/2512.18225v1.pdf"
    },
    {
      "arxiv_id": "2512.18215v1",
      "title": "Stable and Efficient Single-Rollout RL for Multimodal Reasoning",
      "authors": [
        {
          "name": "Rui Liu"
        },
        {
          "name": "Dian Yu"
        },
        {
          "name": "Lei Ke"
        },
        {
          "name": "Haolin Liu"
        },
        {
          "name": "Yujun Zhou"
        },
        {
          "name": "Zhenwen Liang"
        },
        {
          "name": "Haitao Mi"
        },
        {
          "name": "Pratap Tokekar"
        },
        {
          "name": "Dong Yu"
        }
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). However, prevalent group-based algorithms such as GRPO require multi-rollout sampling for each prompt. While more efficient single-rollout variants have recently been explored in text-only settings, we find that they suffer from severe instability in multimodal contexts, often leading to training collapse. To address this training efficiency-stability trade-off, we introduce $\\textbf{MSSR}$ (Multimodal Stabilized Single-Rollout), a group-free RLVR framework that achieves both stable optimization and effective multimodal reasoning performance. MSSR achieves this via an entropy-based advantage-shaping mechanism that adaptively regularizes advantage magnitudes, preventing collapse and maintaining training stability. While such mechanisms have been used in group-based RLVR, we show that in the multimodal single-rollout setting they are not merely beneficial but essential for stability. In in-distribution evaluations, MSSR demonstrates superior training compute efficiency, achieving similar validation accuracy to the group-based baseline with half the training steps. When trained for the same number of steps, MSSR's performance surpasses the group-based baseline and shows consistent generalization improvements across five diverse reasoning-intensive benchmarks. Together, these results demonstrate that MSSR enables stable, compute-efficient, and effective RLVR for complex multimodal reasoning tasks.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "published": "2025-12-20T05:07:53+00:00",
      "updated": "2025-12-20T05:07:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18215v1",
      "file": "papers/2512.18215v1.pdf"
    },
    {
      "arxiv_id": "2512.18196v1",
      "title": "Training LLMs with LogicReward for Faithful and Rigorous Reasoning",
      "authors": [
        {
          "name": "Jundong Xu"
        },
        {
          "name": "Hao Fei"
        },
        {
          "name": "Huichi Zhou"
        },
        {
          "name": "Xin Quan"
        },
        {
          "name": "Qijun Huang"
        },
        {
          "name": "Shengqiong Wu"
        },
        {
          "name": "William Yang Wang"
        },
        {
          "name": "Mong-Li Lee"
        },
        {
          "name": "Wynne Hsu"
        }
      ],
      "abstract": "Although LLMs exhibit strong reasoning capabilities, existing training methods largely depend on outcome-based feedback, which can produce correct answers with flawed reasoning. Prior work introduces supervision on intermediate steps but still lacks guarantees of logical soundness, which is crucial in high-stakes scenarios where logical consistency is paramount. To address this, we propose LogicReward, a novel reward system that guides model training by enforcing step-level logical correctness with a theorem prover. We further introduce Autoformalization with Soft Unification, which reduces natural language ambiguity and improves formalization quality, enabling more effective use of the theorem prover. An 8B model trained on data constructed with LogicReward surpasses GPT-4o and o4-mini by 11.6\\% and 2\\% on natural language inference and logical reasoning tasks with simple training procedures. Further analysis shows that LogicReward enhances reasoning faithfulness, improves generalizability to unseen tasks such as math and commonsense reasoning, and provides a reliable reward signal even without ground-truth labels. We will release all data and code at https://llm-symbol.github.io/LogicReward.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-20T03:43:02+00:00",
      "updated": "2025-12-20T03:43:02+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18196v1",
      "file": "papers/2512.18196v1.pdf"
    },
    {
      "arxiv_id": "2512.18190v2",
      "title": "External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning",
      "authors": [
        {
          "name": "Jian Yan"
        }
      ],
      "abstract": "This paper proposes the External Hippocampus framework, which models language model reasoning from a cognitive dynamics perspective as the flow of information energy in semantic space. Unlike traditional weight-space optimization methods, this framework constructs topological cognitive maps through dimensionality reduction projection, enabling precise navigation and intervention of energy flow at test time while avoiding substantial computational requirements and demonstrating predictable intervention patterns. The method effectively addresses the cognitive deadlock problem in multi-step reasoning for small models. Experiments on models <=7B parameters show: map-guided methods achieve 81.20% accuracy on 500 challenging problems (relative baseline +16.80%), reduce reasoning time by >= 15x, with key findings revealing that reasoning stagnation manifests as \"Cognitive Vortex\" and low-entropy potential wells, while temperature perturbations effectively restart energy flow. The framework requires no additional training, possesses autonomous growth capability, and provides an efficient and controllable topological-aware solution for small model reasoning.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-12-20T03:27:11+00:00",
      "updated": "2025-12-23T08:10:47+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18190v2",
      "file": "papers/2512.18190v2.pdf"
    },
    {
      "arxiv_id": "2512.18119v1",
      "title": "Distributed Asymmetric Allocation: A Topic Model for Large Imbalanced Corpora in Social Sciences",
      "authors": [
        {
          "name": "Kohei Watanabe"
        }
      ],
      "abstract": "Social scientists employ latent Dirichlet allocation (LDA) to find highly specific topics in large corpora, but they often struggle in this task because (1) LDA, in general, takes a significant amount of time to fit on large corpora; (2) unsupervised LDA fragments topics into sub-topics in short documents; (3) semi-supervised LDA fails to identify specific topics defined using seed words. To solve these problems, I have developed a new topic model called distributed asymmetric allocation (DAA) that integrates multiple algorithms for efficiently identifying sentences about important topics in large corpora. I evaluate the ability of DAA to identify politically important topics by fitting it to the transcripts of speeches at the United Nations General Assembly between 1991 and 2017. The results show that DAA can classify sentences significantly more accurately and quickly than LDA thanks to the new algorithms. More generally, the results demonstrate that it is important for social scientists to optimize Dirichlet priors of LDA to perform content analysis accurately.",
      "primary_category": "stat.ME",
      "categories": [
        "stat.ME",
        "cs.CL"
      ],
      "published": "2025-12-19T22:56:57+00:00",
      "updated": "2025-12-19T22:56:57+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18119v1",
      "file": "papers/2512.18119v1.pdf"
    },
    {
      "arxiv_id": "2512.18115v1",
      "title": "Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown",
      "authors": [
        {
          "name": "Changxu Duan"
        }
      ],
      "abstract": "Academic documents stored in PDF format can be transformed into plain text structured markup languages to enhance accessibility and enable scalable digital library workflows. Markup languages allow for easier updates and customization, making academic content more adaptable and accessible to diverse usage, such as linguistic corpus compilation. Such documents, typically delivered in PDF format, contain complex elements including mathematical formulas, figures, headers, and tables, as well as densely layouted text. Existing end-to-end decoder transformer models can transform screenshots of documents into markup language. However, these models exhibit significant inefficiencies; their token-by-token decoding from scratch wastes a lot of inference steps in regenerating dense text that could be directly copied from PDF files. To solve this problem, we introduce EditTrans, a hybrid editing-generation model whose features allow identifying a queue of to-be-edited text from a PDF before starting to generate markup language. EditTrans contains a lightweight classifier fine-tuned from a Document Layout Analysis model on 162,127 pages of documents from arXiv. In our evaluations, EditTrans reduced the transformation latency up to 44.5% compared to end-to-end decoder transformer models, while maintaining transformation quality. Our code and reproducible dataset production scripts are open-sourced.",
      "primary_category": "cs.MM",
      "categories": [
        "cs.MM",
        "cs.CL",
        "cs.CV",
        "cs.DL"
      ],
      "published": "2025-12-19T22:43:12+00:00",
      "updated": "2025-12-19T22:43:12+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18115v1",
      "file": "papers/2512.18115v1.pdf"
    },
    {
      "arxiv_id": "2512.18041v1",
      "title": "Narrative Consolidation: Formulating a New Task for Unifying Multi-Perspective Accounts",
      "authors": [
        {
          "name": "Roger A. Finger"
        },
        {
          "name": "Eduardo G. Cortes"
        },
        {
          "name": "Sandro J. Rigo"
        },
        {
          "name": "Gabriel de O. Ramos"
        }
      ],
      "abstract": "Processing overlapping narrative documents, such as legal testimonies or historical accounts, often aims not for compression but for a unified, coherent, and chronologically sound text. Standard Multi-Document Summarization (MDS), with its focus on conciseness, fails to preserve narrative flow. This paper formally defines this challenge as a new NLP task: Narrative Consolidation, where the central objectives are chronological integrity, completeness, and the fusion of complementary details. To demonstrate the critical role of temporal structure in this task, we introduce Temporal Alignment Event Graph (TAEG), a graph structure that explicitly models chronology and event alignment. By applying a standard centrality algorithm to TAEG, our method functions as a version selection mechanism, choosing the most central representation of each event in its correct temporal position. In a study on the four Biblical Gospels, this structure-focused approach guarantees perfect temporal ordering (Kendall's Tau of 1.000) by design and dramatically improves content metrics (e.g., +357.2% in ROUGE-L F1). The success of this baseline method validates the formulation of Narrative Consolidation as a relevant task and establishes that an explicit temporal backbone is a fundamental component for its resolution.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-12-19T20:14:44+00:00",
      "updated": "2025-12-19T20:14:44+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18041v1",
      "file": "papers/2512.18041v1.pdf"
    },
    {
      "arxiv_id": "2512.18027v1",
      "title": "CoPE: A Small Language Model for Steerable and Scalable Content Labeling",
      "authors": [
        {
          "name": "Samidh Chakrabarti"
        },
        {
          "name": "David Willner"
        },
        {
          "name": "Kevin Klyman"
        },
        {
          "name": "Tiffany Saade"
        },
        {
          "name": "Emily Capstick"
        },
        {
          "name": "Sabina Nong"
        }
      ],
      "abstract": "This paper details the methodology behind CoPE, a policy-steerable small language model capable of fast and accurate content labeling. We present a novel training curricula called Contradictory Example Training that enables the model to learn policy interpretation rather than mere policy memorization. We also present a novel method for generating content policies, called Binocular Labeling, which enables rapid construction of unambiguous training datasets. When evaluated across seven different harm areas, CoPE exhibits equal or superior accuracy to frontier models at only 1% of their size. We openly release a 9 billion parameter version of the model that can be run on a single consumer-grade GPU. Models like CoPE represent a paradigm shift for classifier systems. By turning an ML task into a policy writing task, CoPE opens up new design possibilities for the governance of online platforms.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.CY",
        "cs.SI"
      ],
      "published": "2025-12-19T19:47:33+00:00",
      "updated": "2025-12-19T19:47:33+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18027v1",
      "file": "papers/2512.18027v1.pdf"
    },
    {
      "arxiv_id": "2512.18014v1",
      "title": "ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India",
      "authors": [
        {
          "name": "Shubham Kumar Nigam"
        },
        {
          "name": "Tanuj Tyagi"
        },
        {
          "name": "Siddharth Shukla"
        },
        {
          "name": "Aditya Kumar Guru"
        },
        {
          "name": "Balaramamahanthi Deepak Patnaik"
        },
        {
          "name": "Danush Khanna"
        },
        {
          "name": "Noel Shallum"
        },
        {
          "name": "Kripabandhu Ghosh"
        },
        {
          "name": "Arnab Bhattacharya"
        }
      ],
      "abstract": "This paper presents an early exploration of reinforcement learning methodologies for legal AI in the Indian context. We introduce Reinforcement Learning-based Legal Reasoning (ReGal), a framework that integrates Multi-Task Instruction Tuning with Reinforcement Learning from AI Feedback (RLAIF) using Proximal Policy Optimization (PPO). Our approach is evaluated across two critical legal tasks: (i) Court Judgment Prediction and Explanation (CJPE), and (ii) Legal Document Summarization. Although the framework underperforms on standard evaluation metrics compared to supervised and proprietary models, it provides valuable insights into the challenges of applying RL to legal texts. These challenges include reward model alignment, legal language complexity, and domain-specific adaptation. Through empirical and qualitative analysis, we demonstrate how RL can be repurposed for high-stakes, long-document tasks in law. Our findings establish a foundation for future work on optimizing legal reasoning pipelines using reinforcement learning, with broader implications for building interpretable and adaptive legal AI systems.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-19T19:13:41+00:00",
      "updated": "2025-12-19T19:13:41+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18014v1",
      "file": "papers/2512.18014v1.pdf"
    },
    {
      "arxiv_id": "2512.18004v1",
      "title": "Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models",
      "authors": [
        {
          "name": "Shubham Kumar Nigam"
        },
        {
          "name": "Parjanya Aditya Shukla"
        },
        {
          "name": "Noel Shallum"
        },
        {
          "name": "Arnab Bhattacharya"
        }
      ],
      "abstract": "Handwritten text recognition (HTR) and machine translation continue to pose significant challenges, particularly for low-resource languages like Marathi, which lack large digitized corpora and exhibit high variability in handwriting styles. The conventional approach to address this involves a two-stage pipeline: an OCR system extracts text from handwritten images, which is then translated into the target language using a machine translation model. In this work, we explore and compare the performance of traditional OCR-MT pipelines with Vision Large Language Models that aim to unify these stages and directly translate handwritten text images in a single, end-to-end step. Our motivation is grounded in the urgent need for scalable, accurate translation systems to digitize legal records such as FIRs, charge sheets, and witness statements in India's district and high courts. We evaluate both approaches on a curated dataset of handwritten Marathi legal documents, with the goal of enabling efficient legal document processing, even in low-resource environments. Our findings offer actionable insights toward building robust, edge-deployable solutions that enhance access to legal information for non-native speakers and legal professionals alike.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-12-19T19:06:14+00:00",
      "updated": "2025-12-19T19:06:14+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18004v1",
      "file": "papers/2512.18004v1.pdf"
    },
    {
      "arxiv_id": "2512.17901v1",
      "title": "When Reasoning Meets Its Laws",
      "authors": [
        {
          "name": "Junyu Zhang"
        },
        {
          "name": "Yifan Sun"
        },
        {
          "name": "Tianang Leng"
        },
        {
          "name": "Jingyan Shen"
        },
        {
          "name": "Liu Ziyin"
        },
        {
          "name": "Paul Pu Liang"
        },
        {
          "name": "Huan Zhang"
        }
      ],
      "abstract": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-12-19T18:59:11+00:00",
      "updated": "2025-12-19T18:59:11+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17901v1",
      "file": "papers/2512.17901v1.pdf"
    },
    {
      "arxiv_id": "2512.17843v1",
      "title": "ShareChat: A Dataset of Chatbot Conversations in the Wild",
      "authors": [
        {
          "name": "Yueru Yan"
        },
        {
          "name": "Tuc Nguyen"
        },
        {
          "name": "Bo Su"
        },
        {
          "name": "Melissa Lieffers"
        },
        {
          "name": "Thai Le"
        }
      ],
      "abstract": "While Large Language Models (LLMs) have evolved into distinct platforms with unique interface designs and capabilities, existing public datasets treat models as generic text generators, stripping away the interface context that actively shapes user interaction. To address this limitation, we present ShareChat, a large-scale, cross-platform corpus comprising 142,808 conversations and over 660,000 turns collected from publicly shared URLs across five major platforms: ChatGPT, Claude, Gemini, Perplexity, and Grok. ShareChat distinguishes itself by preserving native platform affordances often lost in standard logs, including reasoning traces, source links, and code artifacts, while spanning 101 languages over the period from April 2023 to October 2025. Furthermore, ShareChat offers substantially longer context windows and greater interaction depth than prior datasets. We demonstrate the dataset's multifaceted utility through three representative analyses: (1) analyzing conversation completeness to measure user intent satisfaction; (2) evaluating source citation behaviors in content generation; and (3) conducting temporal analysis to track evolving usage patterns. This work provides the community with a vital and timely resource for understanding authentic user-LLM chatbot interactions in the wild.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "published": "2025-12-19T17:47:53+00:00",
      "updated": "2025-12-19T17:47:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17843v1",
      "file": "papers/2512.17843v1.pdf"
    },
    {
      "arxiv_id": "2512.17776v1",
      "title": "DEER: A Comprehensive and Reliable Benchmark for Deep-Research Expert Reports",
      "authors": [
        {
          "name": "Janghoon Han"
        },
        {
          "name": "Heegyu Kim"
        },
        {
          "name": "Changho Lee"
        },
        {
          "name": "Dahm Lee"
        },
        {
          "name": "Min Hyung Park"
        },
        {
          "name": "Hosung Song"
        },
        {
          "name": "Stanley Jungkyu Choi"
        },
        {
          "name": "Moontae Lee"
        },
        {
          "name": "Honglak Lee"
        }
      ],
      "abstract": "As large language models (LLMs) advance, deep research systems can generate expert-level reports via multi-step reasoning and evidence-based synthesis, but evaluating such reports remains challenging. Existing benchmarks often lack systematic criteria for expert reporting, evaluations that rely heavily on LLM judges can fail to capture issues that require expert judgment, and source verification typically covers only a limited subset of explicitly cited statements rather than report-wide factual reliability. We introduce DEER, a benchmark for evaluating expert-level deep research reports. DEER comprises 50 report-writing tasks spanning 13 domains and an expert-grounded evaluation taxonomy (7 dimensions, 25 sub-dimension) operationalized into 130 fine-grained rubric items. DEER further provides task-specific expert guidance to help LLM judges assess expert-level report quality more consistently. Complementing rubric-based assessment, we propose a document-level fact-checking architecture that extracts and verifies all claims across the entire report, including both cited and uncited ones, and quantifies external-evidence quality. DEER correlates closely with human expert judgments and yields interpretable diagnostics of system strengths and weaknesses.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-19T16:46:20+00:00",
      "updated": "2025-12-19T16:46:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17776v1",
      "file": "papers/2512.17776v1.pdf"
    },
    {
      "arxiv_id": "2512.17769v1",
      "title": "Bangla MedER: Multi-BERT Ensemble Approach for the Recognition of Bangla Medical Entity",
      "authors": [
        {
          "name": "Tanjim Taharat Aurpa"
        },
        {
          "name": "Farzana Akter"
        },
        {
          "name": "Md. Mehedi Hasan"
        },
        {
          "name": "Shakil Ahmed"
        },
        {
          "name": "Shifat Ara Rafiq"
        },
        {
          "name": "Fatema Khan"
        }
      ],
      "abstract": "Medical Entity Recognition (MedER) is an essential NLP task for extracting meaningful entities from the medical corpus. Nowadays, MedER-based research outcomes can remarkably contribute to the development of automated systems in the medical sector, ultimately enhancing patient care and outcomes. While extensive research has been conducted on MedER in English, low-resource languages like Bangla remain underexplored. Our work aims to bridge this gap. For Bangla medical entity recognition, this study first examined a number of transformer models, including BERT, DistilBERT, ELECTRA, and RoBERTa. We also propose a novel Multi-BERT Ensemble approach that outperformed all baseline models with the highest accuracy of 89.58%. Notably, it provides an 11.80% accuracy improvement over the single-layer BERT model, demonstrating its effectiveness for this task. A major challenge in MedER for low-resource languages is the lack of annotated datasets. To address this issue, we developed a high-quality dataset tailored for the Bangla MedER task. The dataset was used to evaluate the effectiveness of our model through multiple performance metrics, demonstrating its robustness and applicability. Our findings highlight the potential of Multi-BERT Ensemble models in improving MedER for Bangla and set the foundation for further advancements in low-resource medical NLP.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-19T16:41:16+00:00",
      "updated": "2025-12-19T16:41:16+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17769v1",
      "file": "papers/2512.17769v1.pdf"
    },
    {
      "arxiv_id": "2512.17756v1",
      "title": "AncientBench: Towards Comprehensive Evaluation on Excavated and Transmitted Chinese Corpora",
      "authors": [
        {
          "name": "Zhihan Zhou"
        },
        {
          "name": "Daqian Shi"
        },
        {
          "name": "Rui Song"
        },
        {
          "name": "Lida Shi"
        },
        {
          "name": "Xiaolei Diao"
        },
        {
          "name": "Hao Xu"
        }
      ],
      "abstract": "Comprehension of ancient texts plays an important role in archaeology and understanding of Chinese history and civilization. The rapid development of large language models needs benchmarks that can evaluate their comprehension of ancient characters. Existing Chinese benchmarks are mostly targeted at modern Chinese and transmitted documents in ancient Chinese, but the part of excavated documents in ancient Chinese is not covered. To meet this need, we propose the AncientBench, which aims to evaluate the comprehension of ancient characters, especially in the scenario of excavated documents. The AncientBench is divided into four dimensions, which correspond to the four competencies of ancient character comprehension: glyph comprehension, pronunciation comprehension, meaning comprehension, and contextual comprehension. The benchmark also contains ten tasks, including radical, phonetic radical, homophone, cloze, translation, and more, providing a comprehensive framework for evaluation. We convened archaeological researchers to conduct experimental evaluations, proposed an ancient model as baseline, and conducted extensive experiments on the currently best-performing large language models. The experimental results reveal the great potential of large language models in ancient textual scenarios as well as the gap with humans. Our research aims to promote the development and application of large language models in the field of archaeology and ancient Chinese language.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-19T16:28:57+00:00",
      "updated": "2025-12-19T16:28:57+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17756v1",
      "file": "papers/2512.17756v1.pdf"
    },
    {
      "arxiv_id": "2512.17738v1",
      "title": "When the Gold Standard isn't Necessarily Standard: Challenges of Evaluating the Translation of User-Generated Content",
      "authors": [
        {
          "name": "Lydia Nishimwe"
        },
        {
          "name": "Benot Sagot"
        },
        {
          "name": "Rachel Bawden"
        }
      ],
      "abstract": "User-generated content (UGC) is characterised by frequent use of non-standard language, from spelling errors to expressive choices such as slang, character repetitions, and emojis. This makes evaluating UGC translation particularly challenging: what counts as a \"good\" translation depends on the level of standardness desired in the output. To explore this, we examine the human translation guidelines of four UGC datasets, and derive a taxonomy of twelve non-standard phenomena and five translation actions (NORMALISE, COPY, TRANSFER, OMIT, CENSOR). Our analysis reveals notable differences in how UGC is treated, resulting in a spectrum of standardness in reference translations. Through a case study on large language models (LLMs), we show that translation scores are highly sensitive to prompts with explicit translation instructions for UGC, and that they improve when these align with the dataset's guidelines. We argue that when preserving UGC style is important, fair evaluation requires both models and metrics to be aware of translation guidelines. Finally, we call for clear guidelines during dataset creation and for the development of controllable, guideline-aware evaluation frameworks for UGC translation.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-19T16:17:23+00:00",
      "updated": "2025-12-19T16:17:23+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17738v1",
      "file": "papers/2512.17738v1.pdf"
    },
    {
      "arxiv_id": "2512.17677v1",
      "title": "Toward Ethical AI Through Bayesian Uncertainty in Neural Question Answering",
      "authors": [
        {
          "name": "Riccardo Di Sipio"
        }
      ],
      "abstract": "We explore Bayesian reasoning as a means to quantify uncertainty in neural networks for question answering. Starting with a multilayer perceptron on the Iris dataset, we show how posterior inference conveys confidence in predictions. We then extend this to language models, applying Bayesian inference first to a frozen head and finally to LoRA-adapted transformers, evaluated on the CommonsenseQA benchmark. Rather than aiming for state-of-the-art accuracy, we compare Laplace approximations against maximum a posteriori (MAP) estimates to highlight uncertainty calibration and selective prediction. This allows models to abstain when confidence is low. An ``I don't know'' response not only improves interpretability but also illustrates how Bayesian methods can contribute to more responsible and ethical deployment of neural question-answering systems.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-19T15:17:19+00:00",
      "updated": "2025-12-19T15:17:19+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17677v1",
      "file": "papers/2512.17677v1.pdf"
    },
    {
      "arxiv_id": "2512.17639v1",
      "title": "Linear Personality Probing and Steering in LLMs: A Big Five Study",
      "authors": [
        {
          "name": "Michel Frising"
        },
        {
          "name": "Daniel Balcells"
        }
      ],
      "abstract": "Large language models (LLMs) exhibit distinct and consistent personalities that greatly impact trust and engagement. While this means that personality frameworks would be highly valuable tools to characterize and control LLMs' behavior, current approaches remain either costly (post-training) or brittle (prompt engineering). Probing and steering via linear directions has recently emerged as a cheap and efficient alternative. In this paper, we investigate whether linear directions aligned with the Big Five personality traits can be used for probing and steering model behavior. Using Llama 3.3 70B, we generate descriptions of 406 fictional characters and their Big Five trait scores. We then prompt the model with these descriptions and questions from the Alpaca questionnaire, allowing us to sample hidden activations that vary along personality traits in known, quantifiable ways. Using linear regression, we learn a set of per-layer directions in activation space, and test their effectiveness for probing and steering model behavior. Our results suggest that linear directions aligned with trait-scores are effective probes for personality detection, while their steering capabilities strongly depend on context, producing reliable effects in forced-choice tasks but limited influence in open-ended generation or when additional context is present in the prompt.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-19T14:41:09+00:00",
      "updated": "2025-12-19T14:41:09+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17639v1",
      "file": "papers/2512.17639v1.pdf"
    },
    {
      "arxiv_id": "2512.17630v1",
      "title": "Confidence-Credibility Aware Weighted Ensembles of Small LLMs Outperform Large LLMs in Emotion Detection",
      "authors": [
        {
          "name": "Menna Elgabry"
        },
        {
          "name": "Ali Hamdi"
        }
      ],
      "abstract": "This paper introduces a confidence-weighted, credibility-aware ensemble framework for text-based emotion detection, inspired by Condorcet's Jury Theorem (CJT). Unlike conventional ensembles that often rely on homogeneous architectures, our approach combines architecturally diverse small transformer-based large language models (sLLMs) - BERT, RoBERTa, DistilBERT, DeBERTa, and ELECTRA, each fully fine-tuned for emotion classification. To preserve error diversity, we minimize parameter convergence while taking advantage of the unique biases of each model. A dual-weighted voting mechanism integrates both global credibility (validation F1 score) and local confidence (instance-level probability) to dynamically weight model contributions. Experiments on the DAIR-AI dataset demonstrate that our credibility-confidence ensemble achieves a macro F1 score of 93.5 percent, surpassing state-of-the-art benchmarks and significantly outperforming large-scale LLMs, including Falcon, Mistral, Qwen, and Phi, even after task-specific Low-Rank Adaptation (LoRA). With only 595M parameters in total, our small LLMs ensemble proves more parameter-efficient and robust than models up to 7B parameters, establishing that carefully designed ensembles of small, fine-tuned models can outperform much larger LLMs in specialized natural language processing (NLP) tasks such as emotion detection.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-12-19T14:33:14+00:00",
      "updated": "2025-12-19T14:33:14+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17630v1",
      "file": "papers/2512.17630v1.pdf"
    },
    {
      "arxiv_id": "2512.17387v1",
      "title": "CIFE: Code Instruction-Following Evaluation",
      "authors": [
        {
          "name": "Sravani Gunnu"
        },
        {
          "name": "Shanmukha Guttula"
        },
        {
          "name": "Hima Patel"
        }
      ],
      "abstract": "Large Language Models (LLMs) are increasingly applied to real-world code generation, where functional correctness alone is insufficient for reliable deployment, developers also expect adherence to explicit requirements for robustness, formatting, and security. Existing benchmarks primarily assess correctness through test-case execution, offering limited insight into how reliably models follow such constraints. We introduce a benchmark of 1,000 Python tasks, each paired with an average of 7 developer-specified constraints spanning 13 categories. Constraints are curated through a four-stage human-LLM pipeline to ensure they are atomic, relevant, and objective. We evaluate 14 open- and closed-source models using complementary adherence metrics and propose the C2A Score, a composite measure that jointly captures correctness and constraint compliance. Results reveal a substantial gap between partial and strict satisfaction, while strong models achieve over 90% partial adherence, strict adherence remains between 39-66%. These findings highlight that trustworthy code generation requires not only correctness but also consistent adherence to developer intent.",
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.CL"
      ],
      "published": "2025-12-19T09:43:20+00:00",
      "updated": "2025-12-19T09:43:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17387v1",
      "file": "papers/2512.17387v1.pdf"
    },
    {
      "arxiv_id": "2512.17385v1",
      "title": "UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models",
      "authors": [
        {
          "name": "Jiajun Wu"
        },
        {
          "name": "Jian Yang"
        },
        {
          "name": "Wei Zhang"
        },
        {
          "name": "Lin Jing"
        },
        {
          "name": "Yuqing Ma"
        },
        {
          "name": "Ensheng Shi"
        },
        {
          "name": "Yuchi Ma"
        },
        {
          "name": "Zhoujun Li"
        },
        {
          "name": "Xianglong Liu"
        }
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-19T09:42:04+00:00",
      "updated": "2025-12-19T09:42:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17385v1",
      "file": "papers/2512.17385v1.pdf"
    },
    {
      "arxiv_id": "2512.17375v1",
      "title": "AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens",
      "authors": [
        {
          "name": "Tung-Ling Li"
        },
        {
          "name": "Yuhao Wu"
        },
        {
          "name": "Hongliang Liu"
        }
      ],
      "abstract": "Reward models and LLM-as-a-Judge systems are central to modern post-training pipelines such as RLHF, DPO, and RLAIF, where they provide scalar feedback and binary decisions that guide model selection and RL-based fine-tuning. We show that these judge systems exhibit a recurring vulnerability: short sequences of low-perplexity control tokens can flip many binary evaluations from correct ``No'' judgments to incorrect ``Yes'' judgments by steering the last-layer logit gap. These control tokens are patterns that a policy model could plausibly generate during post-training, and thus represent realistic reward-hacking risks rather than worst-case adversarial strings. Our method, AdvJudge-Zero, uses the model's next-token distribution and beam-search exploration to discover diverse control-token sequences from scratch, and our analysis shows that the induced hidden-state perturbations concentrate in a low-rank ``soft mode'' that is anti-aligned with the judge's refusal direction. Empirically, these tokens cause very high false positive rates when large open-weight and specialized judge models score incorrect answers on math and reasoning benchmarks. Finally, we show that LoRA-based adversarial training on small sets of control-token-augmented examples can markedly reduce these false positives while preserving evaluation quality.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.CR"
      ],
      "published": "2025-12-19T09:22:11+00:00",
      "updated": "2025-12-19T09:22:11+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17375v1",
      "file": "papers/2512.17375v1.pdf"
    },
    {
      "arxiv_id": "2512.17351v1",
      "title": "Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers",
      "authors": [
        {
          "name": "Zeyuan Allen-Zhu"
        }
      ],
      "abstract": "Understanding architectural differences in language models is challenging, especially at academic-scale pretraining (e.g., 1.3B parameters, 100B tokens), where results are often dominated by noise and randomness. To overcome this, we introduce controlled synthetic pretraining tasks that isolate and evaluate core model capabilities. Within this framework, we discover CANON LAYERS: lightweight architectural components -- named after the musical term \"canon\" -- that promote horizontal information flow across neighboring tokens. Canon layers compute weighted sums of nearby token representations and integrate seamlessly into Transformers, linear attention, state-space models, or any sequence architecture.\n  We present 12 key results. This includes how Canon layers enhance reasoning depth (e.g., by $2\\times$), reasoning breadth, knowledge manipulation, etc. They lift weak architectures like NoPE to match RoPE, and linear attention to rival SOTA linear models like Mamba2/GDN -- validated both through synthetic tasks and real-world academic-scale pretraining. This synthetic playground offers an economical, principled path to isolate core model capabilities often obscured at academic scales. Equipped with infinite high-quality data, it may even PREDICT how future architectures will behave as training pipelines improve -- e.g., through better data curation or RL-based post-training -- unlocking deeper reasoning and hierarchical inference.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-19T08:47:28+00:00",
      "updated": "2025-12-19T08:47:28+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17351v1",
      "file": "papers/2512.17351v1.pdf"
    },
    {
      "arxiv_id": "2512.17347v1",
      "title": "Stakeholder Suite: A Unified AI Framework for Mapping Actors, Topics and Arguments in Public Debates",
      "authors": [
        {
          "name": "Mohamed Chenene"
        },
        {
          "name": "Jeanne Rouhier"
        },
        {
          "name": "Jean Danilou"
        },
        {
          "name": "Mihir Sarkar"
        },
        {
          "name": "Elena Cabrio"
        }
      ],
      "abstract": "Public debates surrounding infrastructure and energy projects involve complex networks of stakeholders, arguments, and evolving narratives. Understanding these dynamics is crucial for anticipating controversies and informing engagement strategies, yet existing tools in media intelligence largely rely on descriptive analytics with limited transparency. This paper presents Stakeholder Suite, a framework deployed in operational contexts for mapping actors, topics, and arguments within public debates. The system combines actor detection, topic modeling, argument extraction and stance classification in a unified pipeline. Tested on multiple energy infrastructure projects as a case study, the approach delivers fine-grained, source-grounded insights while remaining adaptable to diverse domains. The framework achieves strong retrieval precision and stance accuracy, producing arguments judged relevant in 75% of pilot use cases. Beyond quantitative metrics, the tool has proven effective for operational use: helping project teams visualize networks of influence, identify emerging controversies, and support evidence-based decision-making.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-19T08:38:28+00:00",
      "updated": "2025-12-19T08:38:28+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17347v1",
      "file": "papers/2512.17347v1.pdf"
    },
    {
      "arxiv_id": "2512.17344v1",
      "title": "Governance-Aware Hybrid Fine-Tuning for Multilingual Large Language Models",
      "authors": [
        {
          "name": "Haomin Qi"
        },
        {
          "name": "Chengbo Huang"
        },
        {
          "name": "Zihan Dai"
        },
        {
          "name": "Yunkai Gao"
        }
      ],
      "abstract": "We present a governance-aware hybrid fine-tuning framework for multilingual, low-resource adaptation of large language models. The core algorithm combines gradient-aligned low-rank updates with structured orthogonal transformations through layer-wise mixing and introduces unitary constraints in selected sub-layers to stabilize deep optimization. In tandem with lightweight, label-free data governance steps, including language identification, near-duplicate removal, and quality filtering, the framework targets accuracy, calibration, and cross-language parity under tight compute budgets. Across XNLI and FLORES, the hybrid approach delivers consistent gains over strong PEFT baselines while maintaining directional balance and improving probability calibration, as shown in Tables II and III. It is more resilient to lightweight orthographic variants, as shown in Table IV, and benefits additively from simple governance steps, as shown in Table V. Training footprint measurements indicate modest overhead and a favorable cost-quality frontier, as shown in Table VI and Figure 2. Together, these results show that hybrid and unitary PEFT provide a stable and accessible path to resource-efficient multilingual adaptation when paired with practical data governance.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-19T08:35:51+00:00",
      "updated": "2025-12-19T08:35:51+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17344v1",
      "file": "papers/2512.17344v1.pdf"
    },
    {
      "arxiv_id": "2512.17325v1",
      "title": "Task Schema and Binding: A Double Dissociation Study of In-Context Learning",
      "authors": [
        {
          "name": "Chaeha Kim"
        }
      ],
      "abstract": "We provide causal mechanistic validation that in-context learning (ICL) decomposes into two separable mechanisms: Task Schema (abstract task type recognition) and Binding (specific input-output associations). Through activation patching experiments across 9 models from 7 Transformer families plus Mamba (370M-13B parameters), we establish three key findings:\n  1. Double dissociation: Task Schema transfers at 100% via late MLP patching; Binding transfers at 62% via residual stream patching -- proving separable mechanisms\n  2. Prior-Schema trade-off: Schema reliance inversely correlates with prior knowledge (Spearman rho = -0.596, p < 0.001, N=28 task-model pairs)\n  3. Architecture generality: The mechanism operates across all tested architectures including the non-Transformer Mamba\n  These findings offer a mechanistic account of the ICL puzzle that contrasts with prior views treating ICL as a monolithic mechanism (whether retrieval-based, gradient descent-like, or purely Bayesian). By establishing that Schema and Binding are neurally dissociable -- not merely behavioral modes -- we provide causal evidence for dual-process theories of ICL. Models rely on Task Schema when prior knowledge is absent, but prior knowledge interferes through attentional mis-routing (72.7% recency bias) rather than direct output competition (0%). This explains why arbitrary mappings succeed (zero prior leads to full Schema reliance) while factual overrides fail -- and reveals that the true bottleneck is attentional, not output-level. Practical implications: Understanding these dual mechanisms enables more efficient prompt engineering -- reliable schema transfer reduces required demonstrations for novel tasks, while prior-aware design can mitigate the 38% binding failure rate in high-prior scenarios, improving ICL system reliability in production deployments.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2025-12-19T08:14:21+00:00",
      "updated": "2025-12-19T08:14:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17325v1",
      "file": "papers/2512.17325v1.pdf"
    },
    {
      "arxiv_id": "2512.17289v1",
      "title": "Subjective Question Generation and Answer Evaluation using NLP",
      "authors": [
        {
          "name": "G. M. Refatul Islam"
        },
        {
          "name": "Safwan Shaheer"
        },
        {
          "name": "Yaseen Nur"
        },
        {
          "name": "Mohammad Rafid Hamid"
        }
      ],
      "abstract": "Natural Language Processing (NLP) is one of the most revolutionary technologies today. It uses artificial intelligence to understand human text and spoken words. It is used for text summarization, grammar checking, sentiment analysis, and advanced chatbots and has many more potential use cases. Furthermore, it has also made its mark on the education sector. Much research and advancements have already been conducted on objective question generation; however, automated subjective question generation and answer evaluation are still in progress. An automated system to generate subjective questions and evaluate the answers can help teachers assess student work and enhance the student's learning experience by allowing them to self-assess their understanding after reading an article or a chapter of a book. This research aims to improve current NLP models or make a novel one for automated subjective question generation and answer evaluation from text input.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-19T07:11:50+00:00",
      "updated": "2025-12-19T07:11:50+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17289v1",
      "file": "papers/2512.17289v1.pdf"
    },
    {
      "arxiv_id": "2512.17270v1",
      "title": "Understanding Generalization in Role-Playing Models via Information Theory",
      "authors": [
        {
          "name": "Yongqi Li"
        },
        {
          "name": "Hao Lang"
        },
        {
          "name": "Fei Huang"
        },
        {
          "name": "Tieyun Qian"
        },
        {
          "name": "Yongbin Li"
        }
      ],
      "abstract": "Role-playing models (RPMs) are widely used in real-world applications but underperform when deployed in the wild. This degradation can be attributed to distribution shifts, including user, character, and dialogue compositional shifts. Existing methods like LLM-as-a-judge fall short in providing a fine-grained diagnosis of how these shifts affect RPM generalization, and thus there lack formal frameworks to characterize RPM generalization behaviors. To bridge these gaps, we introduce an information-theoretic metric, named reasoning-based effective mutual information difference (R-EMID), to measure RPM performance degradation in an interpretable way. We also derive an upper bound on R-EMID to predict the worst-case generalization performance of RPMs and theoretically reveal how various shifts contribute to the RPM performance degradation. Moreover, we propose a co-evolving reinforcement learning framework to adaptively model the connection among user, character, and dialogue context and thus enhance the estimation of dialogue response generation probability, which is critical for calculating R-EMID. Finally, we evaluate the generalization performance of various RPMs using R-EMID, finding that user shift poses the highest risk among all shifts and reinforcement learning is the most effective approach for enhancing RPM generalization.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-12-19T06:37:44+00:00",
      "updated": "2025-12-19T06:37:44+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17270v1",
      "file": "papers/2512.17270v1.pdf"
    },
    {
      "arxiv_id": "2512.17267v1",
      "title": "AutoMetrics: Approximate Human Judgements with Automatically Generated Evaluators",
      "authors": [
        {
          "name": "Michael J. Ryan"
        },
        {
          "name": "Yanzhe Zhang"
        },
        {
          "name": "Amol Salunkhe"
        },
        {
          "name": "Yi Chu"
        },
        {
          "name": "Di Xu"
        },
        {
          "name": "Diyi Yang"
        }
      ],
      "abstract": "Evaluating user-facing AI applications remains a central challenge, especially in open-ended domains such as travel planning, clinical note generation, or dialogue. The gold standard is user feedback (e.g., thumbs up/down) or behavioral signals (e.g., retention), but these are often scarce in prototypes and research projects, or too-slow to use for system optimization. We present AutoMetrics, a framework for synthesizing evaluation metrics under low-data constraints. AutoMetrics combines retrieval from MetricBank, a collection of 48 metrics we curate, with automatically generated LLM-as-a-Judge criteria informed by lightweight human feedback. These metrics are composed via regression to maximize correlation with human signal. AutoMetrics takes you from expensive measures to interpretable automatic metrics. Across 5 diverse tasks, AutoMetrics improves Kendall correlation with human ratings by up to 33.4% over LLM-as-a-Judge while requiring fewer than 100 feedback points. We show that AutoMetrics can be used as a proxy reward to equal effect as a verifiable reward. We release the full AutoMetrics toolkit and MetricBank to accelerate adaptive evaluation of LLM applications.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-19T06:32:46+00:00",
      "updated": "2025-12-19T06:32:46+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17267v1",
      "file": "papers/2512.17267v1.pdf"
    },
    {
      "arxiv_id": "2512.17220v1",
      "title": "Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding",
      "authors": [
        {
          "name": "Yuqing Li"
        },
        {
          "name": "Jiangnan Li"
        },
        {
          "name": "Zheng Lin"
        },
        {
          "name": "Ziyan Zhou"
        },
        {
          "name": "Junjie Wu"
        },
        {
          "name": "Weiping Wang"
        },
        {
          "name": "Jie Zhou"
        },
        {
          "name": "Mo Yu"
        }
      ],
      "abstract": "Humans understand long and complex texts by relying on a holistic semantic representation of the content. This global view helps organize prior knowledge, interpret new information, and integrate evidence dispersed across a document, as revealed by the Mindscape-Aware Capability of humans in psychology. Current Retrieval-Augmented Generation (RAG) systems lack such guidance and therefore struggle with long-context tasks. In this paper, we propose Mindscape-Aware RAG (MiA-RAG), the first approach that equips LLM-based RAG systems with explicit global context awareness. MiA-RAG builds a mindscape through hierarchical summarization and conditions both retrieval and generation on this global semantic representation. This enables the retriever to form enriched query embeddings and the generator to reason over retrieved evidence within a coherent global context. We evaluate MiA-RAG across diverse long-context and bilingual benchmarks for evidence-based understanding and global sense-making. It consistently surpasses baselines, and further analysis shows that it aligns local details with a coherent global representation, enabling more human-like long-context retrieval and reasoning.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-19T04:08:29+00:00",
      "updated": "2025-12-19T04:08:29+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17220v1",
      "file": "papers/2512.17220v1.pdf"
    },
    {
      "arxiv_id": "2512.17179v1",
      "title": "Enhancing Long Document Long Form Summarisation with Self-Planning",
      "authors": [
        {
          "name": "Xiaotang Du"
        },
        {
          "name": "Rohit Saxena"
        },
        {
          "name": "Laura Perez-Beltrachini"
        },
        {
          "name": "Pasquale Minervini"
        },
        {
          "name": "Ivan Titov"
        }
      ],
      "abstract": "We introduce a novel approach for long context summarisation, highlight-guided generation, that leverages sentence-level information as a content plan to improve the traceability and faithfulness of generated summaries. Our framework applies self-planning methods to identify important content and then generates a summary conditioned on the plan. We explore both an end-to-end and two-stage variants of the approach, finding that the two-stage pipeline performs better on long and information-dense documents. Experiments on long-form summarisation datasets demonstrate that our method consistently improves factual consistency while preserving relevance and overall quality. On GovReport, our best approach has improved ROUGE-L by 4.1 points and achieves about 35% gains in SummaC scores. Qualitative analysis shows that highlight-guided summarisation helps preserve important details, leading to more accurate and insightful summaries across domains.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-19T02:37:30+00:00",
      "updated": "2025-12-19T02:37:30+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17179v1",
      "file": "papers/2512.17179v1.pdf"
    },
    {
      "arxiv_id": "2512.17093v1",
      "title": "A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving",
      "authors": [
        {
          "name": "Timo Pierre Schrader"
        },
        {
          "name": "Lukas Lange"
        },
        {
          "name": "Tobias Kaminski"
        },
        {
          "name": "Simon Razniewski"
        },
        {
          "name": "Annemarie Friedrich"
        }
      ],
      "abstract": "The rise of large language models (LLMs) has sparked interest in coding assistants. While general-purpose programming languages are well supported, generating code for domain-specific languages remains a challenging problem for LLMs. In this paper, we focus on the LLM-based generation of code for Answer Set Programming (ASP), a particularly effective approach for finding solutions to combinatorial search problems. The effectiveness of LLMs in ASP code generation is currently hindered by the limited number of examples seen during their initial pre-training phase.\n  In this paper, we introduce a novel ASP-solver-in-the-loop approach for solver-guided instruction-tuning of LLMs to addressing the highly complex semantic parsing task inherent in ASP code generation. Our method only requires problem specifications in natural language and their solutions. Specifically, we sample ASP statements for program continuations from LLMs for unriddling logic puzzles. Leveraging the special property of declarative ASP programming that partial encodings increasingly narrow down the solution space, we categorize them into chosen and rejected instances based on solver feedback. We then apply supervised fine-tuning to train LLMs on the curated data and further improve robustness using a solver-guided search that includes best-of-N sampling. Our experiments demonstrate consistent improvements in two distinct prompting settings on two datasets.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-12-18T21:45:45+00:00",
      "updated": "2025-12-18T21:45:45+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17093v1",
      "file": "papers/2512.17093v1.pdf"
    },
    {
      "arxiv_id": "2512.17092v1",
      "title": "Data Augmentation Supporting a Conversational Agent Designed for Smoking Cessation Support Groups",
      "authors": [
        {
          "name": "Salar Hashemitaheri"
        },
        {
          "name": "Ian Harris"
        }
      ],
      "abstract": "Online support groups for smoking cessation are economical and accessible, yet they often face challenges with low user engagement and stigma. The use of an automatic conversational agent would improve engagement by ensuring that all user comments receive a timely response.). We address the challenge of insufficient high-quality data by employing a two-level data augmentation strategy: synthetic data augmentation and real data augmentation. First, we fine-tuned an open source LLM to classify posts from our existing smoking cessation support groups and identify intents with low F1 (precision+recall) scores. Then, for these intents, we generate additional synthetic data using prompt engineering with the GPT model, with an average of 87\\% of the generated synthetic posts deemed high quality by human annotators. Overall, the synthetic augmentation process resulted in 43\\% of the original posts being selected for augmentation, followed by 140\\% synthetic expansion of these posts. Additionally, we scraped more than 10,000 real posts from a related online support context, of which 73\\% were validated as good quality by human annotators. Each synthetic or scraped post underwent rigorous validation involving human reviewers to ensure quality and relevance. The validated new data, combined with the original support group posts, formed an augmented dataset used to retrain the intent classifier. Performance evaluation of the retrained model demonstrated a 32\\% improvement in F1, confirming the effectiveness of our data augmentation approach. Synthetic and real post augmentation led to similar performance improvements. This study provides a replicable framework for enhancing conversational agent performance in domains where data scarcity is a critical issue.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-18T21:45:30+00:00",
      "updated": "2025-12-18T21:45:30+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17092v1",
      "file": "papers/2512.17092v1.pdf"
    },
    {
      "arxiv_id": "2512.17083v2",
      "title": "When F1 Fails: Granularity-Aware Evaluation for Dialogue Topic Segmentation",
      "authors": [
        {
          "name": "Michael H. Coen"
        }
      ],
      "abstract": "Dialogue topic segmentation supports summarization, retrieval, memory management, and conversational continuity. Despite decades of work, evaluation practice remains dominated by strict boundary matching and F1-based metrics. Modern large language model (LLM) based conversational systems increasingly rely on segmentation to manage conversation history beyond fixed context windows. In such systems, unstructured context accumulation degrades efficiency and coherence.\n  This paper introduces an evaluation framework that reports boundary density and segment alignment diagnostics (purity and coverage) alongside window-tolerant F1 (W-F1). By separating boundary scoring from boundary selection, we evaluate segmentation quality across density regimes rather than at a single operating point. Cross-dataset evaluation shows that reported performance differences often reflect annotation granularity mismatch rather than boundary placement quality alone.\n  We evaluate structurally distinct segmentation strategies across eight dialogue datasets spanning task-oriented, open-domain, meeting-style, and synthetic interactions. Boundary-based metrics are strongly coupled to boundary density: threshold sweeps produce larger W-F1 changes than switching between methods. These findings support viewing topic segmentation as a granularity selection problem rather than prediction of a single correct boundary set. This motivates separating boundary scoring from boundary selection for analyzing and tuning segmentation under varying annotation granularities.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-18T21:29:43+00:00",
      "updated": "2025-12-24T18:05:57+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17083v2",
      "file": "papers/2512.17083v2.pdf"
    },
    {
      "arxiv_id": "2512.17075v1",
      "title": "Perturb Your Data: Paraphrase-Guided Training Data Watermarking",
      "authors": [
        {
          "name": "Pranav Shetty"
        },
        {
          "name": "Mirazul Haque"
        },
        {
          "name": "Petr Babkin"
        },
        {
          "name": "Zhiqiang Ma"
        },
        {
          "name": "Xiaomo Liu"
        },
        {
          "name": "Manuela Veloso"
        }
      ],
      "abstract": "Training data detection is critical for enforcing copyright and data licensing, as Large Language Models (LLM) are trained on massive text corpora scraped from the internet. We present SPECTRA, a watermarking approach that makes training data reliably detectable even when it comprises less than 0.001% of the training corpus. SPECTRA works by paraphrasing text using an LLM and assigning a score based on how likely each paraphrase is, according to a separate scoring model. A paraphrase is chosen so that its score closely matches that of the original text, to avoid introducing any distribution shifts. To test whether a suspect model has been trained on the watermarked data, we compare its token probabilities against those of the scoring model. We demonstrate that SPECTRA achieves a consistent p-value gap of over nine orders of magnitude when detecting data used for training versus data not used for training, which is greater than all baselines tested. SPECTRA equips data owners with a scalable, deploy-before-release watermark that survives even large-scale LLM training.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-12-18T21:17:16+00:00",
      "updated": "2025-12-18T21:17:16+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17075v1",
      "file": "papers/2512.17075v1.pdf"
    },
    {
      "arxiv_id": "2512.17065v1",
      "title": "XLM: A Python package for non-autoregressive language models",
      "authors": [
        {
          "name": "Dhruvesh Patel"
        },
        {
          "name": "Durga Prasad Maram"
        },
        {
          "name": "Sai Sreenivas Chintha"
        },
        {
          "name": "Benjamin Rozonoyer"
        },
        {
          "name": "Andrew McCallum"
        }
      ],
      "abstract": "In recent years, there has been a resurgence of interest in non-autoregressive text generation in the context of general language modeling. Unlike the well-established autoregressive language modeling paradigm, which has a plethora of standard training and inference libraries, implementations of non-autoregressive language modeling have largely been bespoke making it difficult to perform systematic comparisons of different methods. Moreover, each non-autoregressive language model typically requires it own data collation, loss, and prediction logic, making it challenging to reuse common components. In this work, we present the XLM python package, which is designed to make implementing small non-autoregressive language models faster with a secondary goal of providing a suite of small pre-trained models (through a companion xlm-models package) that can be used by the research community. The code is available at https://github.com/dhruvdcoder/xlm-core.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-18T21:05:10+00:00",
      "updated": "2025-12-18T21:05:10+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17065v1",
      "file": "papers/2512.17065v1.pdf"
    },
    {
      "arxiv_id": "2512.17053v1",
      "title": "Knowledge Distillation with Structured Chain-of-Thought for Text-to-SQL",
      "authors": [
        {
          "name": "Khushboo Thaker"
        },
        {
          "name": "Yony Bresler"
        }
      ],
      "abstract": "Deploying accurate Text-to-SQL systems at the enterprise level faces a difficult trilemma involving cost, security and performance. Current solutions force enterprises to choose between expensive, proprietary Large Language Models (LLMs) and low-performing Small Language Models (SLMs). Efforts to improve SLMs often rely on distilling reasoning from large LLMs using unstructured Chain-of-Thought (CoT) traces, a process that remains inherently ambiguous. Instead, we hypothesize that a formal, structured reasoning representation provides a clearer, more reliable teaching signal, as the Text-to-SQL task requires explicit and precise logical steps. To evaluate this hypothesis, we propose Struct-SQL, a novel Knowledge Distillation (KD) framework that trains an SLM to emulate a powerful large LLM. Consequently, we adopt a query execution plan as a formal blueprint to derive this structured reasoning. Our SLM, distilled with structured CoT, achieves an absolute improvement of 8.1% over an unstructured CoT distillation baseline. A detailed error analysis reveals that a key factor in this gain is a marked reduction in syntactic errors. This demonstrates that teaching a model to reason using a structured logical blueprint is beneficial for reliable SQL generation in SLMs.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB"
      ],
      "published": "2025-12-18T20:41:22+00:00",
      "updated": "2025-12-18T20:41:22+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17053v1",
      "file": "papers/2512.17053v1.pdf"
    },
    {
      "arxiv_id": "2512.16917v1",
      "title": "Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning",
      "authors": [
        {
          "name": "Qihao Liu"
        },
        {
          "name": "Luoxin Ye"
        },
        {
          "name": "Wufei Ma"
        },
        {
          "name": "Yu-Cheng Chou"
        },
        {
          "name": "Alan Yuille"
        }
      ],
      "abstract": "Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-12-18T18:59:54+00:00",
      "updated": "2025-12-18T18:59:54+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16917v1",
      "file": "papers/2512.16917v1.pdf"
    },
    {
      "arxiv_id": "2512.16914v1",
      "title": "Constructive Circuit Amplification: Improving Math Reasoning in LLMs via Targeted Sub-Network Updates",
      "authors": [
        {
          "name": "Nikhil Prakash"
        },
        {
          "name": "Donghao Ren"
        },
        {
          "name": "Dominik Moritz"
        },
        {
          "name": "Yannick Assogba"
        }
      ],
      "abstract": "Prior studies investigating the internal workings of LLMs have uncovered sparse subnetworks, often referred to as circuits, that are responsible for performing specific tasks. Additionally, it has been shown that model performance improvement through fine-tuning often results from the strengthening of existing circuits in the model. Taken together, these findings suggest the possibility of intervening directly on such circuits to make precise, task-targeted updates. Motivated by these findings, we propose a novel method called Constructive Circuit Amplification which identifies pivotal tokens from model reasoning traces as well as model components responsible for the desired task, and updates only those components. Applied to mathematical reasoning, it improves accuracy by up to +11.4% across multiple models while modifying as little as 1.59% of model components, with minimal impact on other abilities as measured by MMLU, TriviaQA, and TruthfulQA. These results demonstrate that targeted capabilities can be reliably enhanced by selectively updating a sparse set of model components.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-18T18:59:46+00:00",
      "updated": "2025-12-18T18:59:46+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16914v1",
      "file": "papers/2512.16914v1.pdf"
    },
    {
      "arxiv_id": "2512.16912v2",
      "title": "Exploration vs Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
      "authors": [
        {
          "name": "Peter Chen"
        },
        {
          "name": "Xiaopeng Li"
        },
        {
          "name": "Ziniu Li"
        },
        {
          "name": "Wotao Yin"
        },
        {
          "name": "Xi Chen"
        },
        {
          "name": "Tianyi Lin"
        }
      ],
      "abstract": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-12-18T18:59:27+00:00",
      "updated": "2025-12-21T17:23:35+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16912v2",
      "file": "papers/2512.16912v2.pdf"
    },
    {
      "arxiv_id": "2512.16904v1",
      "title": "How Good is Post-Hoc Watermarking With Language Model Rephrasing?",
      "authors": [
        {
          "name": "Pierre Fernandez"
        },
        {
          "name": "Tom Sander"
        },
        {
          "name": "Hady Elsahar"
        },
        {
          "name": "Hongyan Chang"
        },
        {
          "name": "Tom Souek"
        },
        {
          "name": "Valeriu Lacatusu"
        },
        {
          "name": "Tuan Tran"
        },
        {
          "name": "Sylvestre-Alvise Rebuffi"
        },
        {
          "name": "Alexandre Mourachko"
        }
      ],
      "abstract": "Generation-time text watermarking embeds statistical signals into text for traceability of AI-generated content. We explore *post-hoc watermarking* where an LLM rewrites existing text while applying generation-time watermarking, to protect copyrighted documents, or detect their use in training or RAG via watermark radioactivity. Unlike generation-time approaches, which is constrained by how LLMs are served, this setting offers additional degrees of freedom for both generation and detection. We investigate how allocating compute (through larger rephrasing models, beam search, multi-candidate generation, or entropy filtering at detection) affects the quality-detectability trade-off. Our strategies achieve strong detectability and semantic fidelity on open-ended text such as books. Among our findings, the simple Gumbel-max scheme surprisingly outperforms more recent alternatives under nucleus sampling, and most methods benefit significantly from beam search. However, most approaches struggle when watermarking verifiable text such as code, where we counterintuitively find that smaller models outperform larger ones. This study reveals both the potential and limitations of post-hoc watermarking, laying groundwork for practical applications and future research.",
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "published": "2025-12-18T18:57:33+00:00",
      "updated": "2025-12-18T18:57:33+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16904v1",
      "file": "papers/2512.16904v1.pdf"
    },
    {
      "arxiv_id": "2512.16902v1",
      "title": "In-Context Algebra",
      "authors": [
        {
          "name": "Eric Todd"
        },
        {
          "name": "Jannik Brinkmann"
        },
        {
          "name": "Rohit Gandikota"
        },
        {
          "name": "David Bau"
        }
      ],
      "abstract": "We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-12-18T18:56:50+00:00",
      "updated": "2025-12-18T18:56:50+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16902v1",
      "file": "papers/2512.16902v1.pdf"
    },
    {
      "arxiv_id": "2512.16899v1",
      "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image",
      "authors": [
        {
          "name": "Yushi Hu"
        },
        {
          "name": "Reyhane Askari-Hemmat"
        },
        {
          "name": "Melissa Hall"
        },
        {
          "name": "Emily Dinan"
        },
        {
          "name": "Luke Zettlemoyer"
        },
        {
          "name": "Marjan Ghazvininejad"
        }
      ],
      "abstract": "Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning (\"thinking-with-images\"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.CV"
      ],
      "published": "2025-12-18T18:56:04+00:00",
      "updated": "2025-12-18T18:56:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16899v1",
      "file": "papers/2512.16899v1.pdf"
    },
    {
      "arxiv_id": "2512.16883v1",
      "title": "AdaSearch: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning",
      "authors": [
        {
          "name": "Tzu-Han Lin"
        },
        {
          "name": "Wei-Lin Chen"
        },
        {
          "name": "Chen-An Li"
        },
        {
          "name": "Hung-yi Lee"
        },
        {
          "name": "Yun-Nung Chen"
        },
        {
          "name": "Yu Meng"
        }
      ],
      "abstract": "Equipping large language models (LLMs) with search engines via reinforcement learning (RL) has emerged as an effective approach for building search agents. However, overreliance on search introduces unnecessary cost and risks exposure to noisy or malicious content, while relying solely on parametric knowledge risks hallucination. The central challenge is to develop agents that adaptively balance parametric knowledge with external search, invoking search only when necessary. Prior work mitigates search overuse by shaping rewards around the number of tool calls. However, these penalties require substantial reward engineering, provide ambiguous credit assignment, and can be exploited by agents that superficially reduce calls. Moreover, evaluating performance solely through call counts conflates necessary and unnecessary search, obscuring the measurement of true adaptive behavior. To address these limitations, we first quantify the self-knowledge awareness of existing search agents via an F1-based decision metric, revealing that methods such as Search-R1 often overlook readily available parametric knowledge. Motivated by these findings, we propose AdaSearch, a simple two-stage, outcome-driven RL framework that disentangles problem solving from the decision of whether to invoke search, and makes this decision process explicit and interpretable. This transparency is crucial for high-stakes domains such as finance and medical question answering, yet is largely neglected by prior approaches. Experiments across multiple model families and sizes demonstrate that AdaSearch substantially improves knowledge-boundary awareness, reduces unnecessary search calls, preserves strong task performance, and offers more transparent, interpretable decision behaviors.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-18T18:50:01+00:00",
      "updated": "2025-12-18T18:50:01+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16883v1",
      "file": "papers/2512.16883v1.pdf"
    },
    {
      "arxiv_id": "2512.16832v1",
      "title": "What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels",
      "authors": [
        {
          "name": "Aditya Yadavalli"
        },
        {
          "name": "Tiago Pimentel"
        },
        {
          "name": "Tamar I Regev"
        },
        {
          "name": "Ethan Wilcox"
        },
        {
          "name": "Alex Warstadt"
        }
      ],
      "abstract": "Prosody -- the melody of speech -- conveys critical information often not captured by the words or text of a message. In this paper, we propose an information-theoretic approach to quantify how much information is expressed by prosody alone and not by text, and crucially, what that information is about. Our approach applies large speech and language models to estimate the mutual information between a particular dimension of an utterance's meaning (e.g., its emotion) and any of its communication channels (e.g., audio or text). We then use this approach to quantify how much information is conveyed by audio and text about sarcasm, emotion, and questionhood, using speech from television and podcasts. We find that for sarcasm and emotion the audio channel -- and by implication the prosodic channel -- transmits over an order of magnitude more information about these features than the text channel alone, at least when long-term context beyond the current sentence is unavailable. For questionhood, prosody provides comparatively less additional information. We conclude by outlining a program applying our approach to more dimensions of meaning, communication channels, and languages.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-18T18:10:20+00:00",
      "updated": "2025-12-18T18:10:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16832v1",
      "file": "papers/2512.16832v1.pdf"
    },
    {
      "arxiv_id": "2512.16814v1",
      "title": "Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs",
      "authors": [
        {
          "name": "William English"
        },
        {
          "name": "Dominic Simon"
        },
        {
          "name": "Sumit Kumar Jha"
        },
        {
          "name": "Rickard Ewetz"
        }
      ],
      "abstract": "Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-18T17:55:15+00:00",
      "updated": "2025-12-18T17:55:15+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16814v1",
      "file": "papers/2512.16814v1.pdf"
    },
    {
      "arxiv_id": "2512.16802v1",
      "title": "Exploration of Augmentation Strategies in Multi-modal Retrieval-Augmented Generation for the Biomedical Domain: A Case Study Evaluating Question Answering in Glycobiology",
      "authors": [
        {
          "name": "Primo Kocbek"
        },
        {
          "name": "Azra Frkatovi-Hodi"
        },
        {
          "name": "Dora Lali"
        },
        {
          "name": "Vivian Hui"
        },
        {
          "name": "Gordan Lauc"
        },
        {
          "name": "Gregor tiglic"
        }
      ],
      "abstract": "Multi-modal retrieval-augmented generation (MM-RAG) promises grounded biomedical QA, but it is unclear when to (i) convert figures/tables into text versus (ii) use optical character recognition (OCR)-free visual retrieval that returns page images and leaves interpretation to the generator. We study this trade-off in glycobiology, a visually dense domain. We built a benchmark of 120 multiple-choice questions (MCQs) from 25 papers, stratified by retrieval difficulty (easy text, medium figures/tables, hard cross-evidence). We implemented four augmentations-None, Text RAG, Multi-modal conversion, and late-interaction visual retrieval (ColPali)-using Docling parsing and Qdrant indexing. We evaluated mid-size open-source and frontier proprietary models (e.g., Gemma-3-27B-IT, GPT-4o family). Additional testing used the GPT-5 family and multiple visual retrievers (ColPali/ColQwen/ColFlor). Accuracy with Agresti-Coull 95% confidence intervals (CIs) was computed over 5 runs per configuration. With Gemma-3-27B-IT, Text and Multi-modal augmentation outperformed OCR-free retrieval (0.722-0.740 vs. 0.510 average accuracy). With GPT-4o, Multi-modal achieved 0.808, with Text 0.782 and ColPali 0.745 close behind; within-model differences were small. In follow-on experiments with the GPT-5 family, the best results with ColPali and ColFlor improved by ~2% to 0.828 in both cases. In general, across the GPT-5 family, ColPali, ColQwen, and ColFlor were statistically indistinguishable. GPT-5-nano trailed larger GPT-5 variants by roughly 8-10%. Pipeline choice is capacity-dependent: converting visuals to text lowers the reader burden and is more reliable for mid-size models, whereas OCR-free visual retrieval becomes competitive under frontier models. Among retrievers, ColFlor offers parity with heavier options at a smaller footprint, making it an efficient default when strong generators are available.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-18T17:35:04+00:00",
      "updated": "2025-12-18T17:35:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16802v1",
      "file": "papers/2512.16802v1.pdf"
    },
    {
      "arxiv_id": "2512.16795v1",
      "title": "From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs",
      "authors": [
        {
          "name": "Shubham Mishra"
        },
        {
          "name": "Samyek Jain"
        },
        {
          "name": "Gorang Mehrishi"
        },
        {
          "name": "Shiv Tiwari"
        },
        {
          "name": "Harsh Sharma"
        },
        {
          "name": "Pratik Narang"
        },
        {
          "name": "Dhruv Kumar"
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.IR"
      ],
      "published": "2025-12-18T17:27:51+00:00",
      "updated": "2025-12-18T17:27:51+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16795v1",
      "file": "papers/2512.16795v1.pdf"
    },
    {
      "arxiv_id": "2512.16770v1",
      "title": "GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation",
      "authors": [
        {
          "name": "William English"
        },
        {
          "name": "Chase Walker"
        },
        {
          "name": "Dominic Simon"
        },
        {
          "name": "Rickard Ewetz"
        }
      ],
      "abstract": "Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\\mathcal{P}$. We decompose the grounding task hierarchically -- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\\%$, a $1.4\\times$ improvement over SOTA.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-18T17:03:07+00:00",
      "updated": "2025-12-18T17:03:07+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16770v1",
      "file": "papers/2512.16770v1.pdf"
    },
    {
      "arxiv_id": "2512.16649v1",
      "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
      "authors": [
        {
          "name": "Bingxiang He"
        },
        {
          "name": "Zekai Qu"
        },
        {
          "name": "Zeyuan Liu"
        },
        {
          "name": "Yinghao Chen"
        },
        {
          "name": "Yuxin Zuo"
        },
        {
          "name": "Cheng Qian"
        },
        {
          "name": "Kaiyan Zhang"
        },
        {
          "name": "Weize Chen"
        },
        {
          "name": "Chaojun Xiao"
        },
        {
          "name": "Ganqu Cui"
        },
        {
          "name": "Ning Ding"
        },
        {
          "name": "Zhiyuan Liu"
        }
      ],
      "abstract": "Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: \\textbf{Is this complexity necessary?} We present \\textbf{JustRL}, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\\% and 64.3\\% average accuracy across nine mathematical benchmarks) while using 2$\\times$ less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-18T15:21:25+00:00",
      "updated": "2025-12-18T15:21:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16649v1",
      "file": "papers/2512.16649v1.pdf"
    },
    {
      "arxiv_id": "2512.16602v1",
      "title": "Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics",
      "authors": [
        {
          "name": "Iker Garca-Ferrero"
        },
        {
          "name": "David Montero"
        },
        {
          "name": "Roman Orus"
        }
      ],
      "abstract": "We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-18T14:43:04+00:00",
      "updated": "2025-12-18T14:43:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16602v1",
      "file": "papers/2512.16602v1.pdf"
    },
    {
      "arxiv_id": "2512.16553v1",
      "title": "Needle in the Web: A Benchmark for Retrieving Targeted Web Pages in the Wild",
      "authors": [
        {
          "name": "Yumeng Wang"
        },
        {
          "name": "Tianyu Fan"
        },
        {
          "name": "Lingrui Xu"
        },
        {
          "name": "Chao Huang"
        }
      ],
      "abstract": "Large Language Models (LLMs) have evolved from simple chatbots into sophisticated agents capable of automating complex real-world tasks, where browsing and reasoning over live web content is key to assessing retrieval and cognitive skills. Existing benchmarks like BrowseComp and xBench-DeepSearch emphasize complex reasoning searches requiring multi-hop synthesis but neglect Fuzzy Exploratory Search, namely queries that are vague and multifaceted, where users seek the most relevant webpage rather than a single factual answer. To address this gap, we introduce Needle in the Web, a novel benchmark specifically designed to evaluate modern search agents and LLM-based systems on their ability to retrieve and reason over real-world web content in response to ambiguous, exploratory queries under varying levels of difficulty. Needle in the Web comprises 663 questions spanning seven distinct domains. To ensure high query quality and answer uniqueness, we employ a flexible methodology that reliably generates queries of controllable difficulty based on factual claims of web contents. We benchmark three leading LLMs and three agent-based search systems on Needle in the Web, finding that most models struggle: many achieve below 35% accuracy, and none consistently excel across domains or difficulty levels. These findings reveal that Needle in the Web presents a significant challenge for current search systems and highlights the open problem of effective fuzzy retrieval under semantic ambiguity.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-12-18T13:57:28+00:00",
      "updated": "2025-12-18T13:57:28+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16553v1",
      "file": "papers/2512.16553v1.pdf"
    },
    {
      "arxiv_id": "2512.16541v1",
      "title": "UM_FHS at the CLEF 2025 SimpleText Track: Comparing No-Context and Fine-Tune Approaches for GPT-4.1 Models in Sentence and Document-Level Text Simplification",
      "authors": [
        {
          "name": "Primoz Kocbek"
        },
        {
          "name": "Gregor Stiglic"
        }
      ],
      "abstract": "This work describes our submission to the CLEF 2025 SimpleText track Task 1, addressing both sentenceand document-level simplification of scientific texts. The methodology centered on using the gpt-4.1, gpt-4.1mini, and gpt-4.1-nano models from OpenAI. Two distinct approaches were compared: a no-context method relying on prompt engineering and a fine-tuned (FT) method across models. The gpt-4.1-mini model with no-context demonstrated robust performance at both levels of simplification, while the fine-tuned models showed mixed results, highlighting the complexities of simplifying text at different granularities, where gpt-4.1-nano-ft performance stands out at document-level simplification in one case.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-18T13:50:54+00:00",
      "updated": "2025-12-18T13:50:54+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16541v1",
      "file": "papers/2512.16541v1.pdf"
    },
    {
      "arxiv_id": "2512.16530v1",
      "title": "Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics",
      "authors": [
        {
          "name": "Primoz Kocbek"
        },
        {
          "name": "Leon Kopitar"
        },
        {
          "name": "Gregor Stiglic"
        }
      ],
      "abstract": "This study investigated the application of Large Language Models (LLMs) for simplifying biomedical texts to enhance health literacy. Using a public dataset, which included plain language adaptations of biomedical abstracts, we developed and evaluated several approaches, specifically a baseline approach using a prompt template, a two AI agent approach, and a fine-tuning approach. We selected OpenAI gpt-4o and gpt-4o mini models as baselines for further research. We evaluated our approaches with quantitative metrics, such as Flesch-Kincaid grade level, SMOG Index, SARI, and BERTScore, G-Eval, as well as with qualitative metric, more precisely 5-point Likert scales for simplicity, accuracy, completeness, brevity. Results showed a superior performance of gpt-4o-mini and an underperformance of FT approaches. G-Eval, a LLM based quantitative metric, showed promising results, ranking the approaches similarly as the qualitative metric.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-18T13:37:58+00:00",
      "updated": "2025-12-18T13:37:58+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16530v1",
      "file": "papers/2512.16530v1.pdf"
    },
    {
      "arxiv_id": "2512.16970v1",
      "title": "PAACE: A Plan-Aware Automated Agent Context Engineering Framework",
      "authors": [
        {
          "name": "Kamer Ali Yuksel"
        }
      ],
      "abstract": "Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MA"
      ],
      "published": "2025-12-18T12:54:56+00:00",
      "updated": "2025-12-18T12:54:56+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16970v1",
      "file": "papers/2512.16970v1.pdf"
    },
    {
      "arxiv_id": "2512.16445v1",
      "title": "Topic Modelling Black Box Optimization",
      "authors": [
        {
          "name": "Roman Akramov"
        },
        {
          "name": "Artem Khamatullin"
        },
        {
          "name": "Svetlana Glazyrina"
        },
        {
          "name": "Maksim Kryzhanovskiy"
        },
        {
          "name": "Roman Ischenko"
        }
      ],
      "abstract": "Choosing the number of topics $T$ in Latent Dirichlet Allocation (LDA) is a key design decision that strongly affects both the statistical fit and interpretability of topic models. In this work, we formulate the selection of $T$ as a discrete black-box optimization problem, where each function evaluation corresponds to training an LDA model and measuring its validation perplexity. Under a fixed evaluation budget, we compare four families of optimizers: two hand-designed evolutionary methods - Genetic Algorithm (GA) and Evolution Strategy (ES) - and two learned, amortized approaches, Preferential Amortized Black-Box Optimization (PABBO) and Sharpness-Aware Black-Box Optimization (SABBO). Our experiments show that, while GA, ES, PABBO, and SABBO eventually reach a similar band of final perplexity, the amortized optimizers are substantially more sample- and time-efficient. SABBO typically identifies a near-optimal topic number after essentially a single evaluation, and PABBO finds competitive configurations within a few evaluations, whereas GA and ES require almost the full budget to approach the same region.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.NE"
      ],
      "published": "2025-12-18T12:00:24+00:00",
      "updated": "2025-12-18T12:00:24+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16445v1",
      "file": "papers/2512.16445v1.pdf"
    },
    {
      "arxiv_id": "2512.16378v2",
      "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
      "authors": [
        {
          "name": "Sara Papi"
        },
        {
          "name": "Javier Garcia Gilabert"
        },
        {
          "name": "Zachary Hopton"
        },
        {
          "name": "Vilm Zouhar"
        },
        {
          "name": "Carlos Escolano"
        },
        {
          "name": "Gerard I. Gllego"
        },
        {
          "name": "Jorge Iranzo-Snchez"
        },
        {
          "name": "Ahrii Kim"
        },
        {
          "name": "Dominik Machek"
        },
        {
          "name": "Patricia Schmidtova"
        },
        {
          "name": "Maike Zfle"
        }
      ],
      "abstract": "As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD"
      ],
      "published": "2025-12-18T10:21:14+00:00",
      "updated": "2025-12-24T14:39:27+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16378v2",
      "file": "papers/2512.16378v2.pdf"
    },
    {
      "arxiv_id": "2512.16323v1",
      "title": "Hacking Neural Evaluation Metrics with Single Hub Text",
      "authors": [
        {
          "name": "Hiroyuki Deguchi"
        },
        {
          "name": "Katsuki Chousa"
        },
        {
          "name": "Yusuke Sakai"
        }
      ],
      "abstract": "Strongly human-correlated evaluation metrics serve as an essential compass for the development and improvement of generation models and must be highly reliable and robust. Recent embedding-based neural text evaluation metrics, such as COMET for translation tasks, are widely used in both research and development fields. However, there is no guarantee that they yield reliable evaluation results due to the black-box nature of neural networks. To raise concerns about the reliability and safety of such metrics, we propose a method for finding a single adversarial text in the discrete space that is consistently evaluated as high-quality, regardless of the test cases, to identify the vulnerabilities in evaluation metrics. The single hub text found with our method achieved 79.1 COMET% and 67.8 COMET% in the WMT'24 English-to-Japanese (En--Ja) and English-to-German (En--De) translation tasks, respectively, outperforming translations generated individually for each source sentence by using M2M100, a general translation model. Furthermore, we also confirmed that the hub text found with our method generalizes across multiple language pairs such as Ja--En and De--En.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-18T09:06:24+00:00",
      "updated": "2025-12-18T09:06:24+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16323v1",
      "file": "papers/2512.16323v1.pdf"
    },
    {
      "arxiv_id": "2512.16301v2",
      "title": "Adaptation of Agentic AI",
      "authors": [
        {
          "name": "Pengcheng Jiang"
        },
        {
          "name": "Jiacheng Lin"
        },
        {
          "name": "Zhiyi Shi"
        },
        {
          "name": "Zifeng Wang"
        },
        {
          "name": "Luxi He"
        },
        {
          "name": "Yichen Wu"
        },
        {
          "name": "Ming Zhong"
        },
        {
          "name": "Peiyang Song"
        },
        {
          "name": "Qizheng Zhang"
        },
        {
          "name": "Heng Wang"
        },
        {
          "name": "Xueqiang Xu"
        },
        {
          "name": "Hanwen Xu"
        },
        {
          "name": "Pengrui Han"
        },
        {
          "name": "Dylan Zhang"
        },
        {
          "name": "Jiashuo Sun"
        },
        {
          "name": "Chaoqi Yang"
        },
        {
          "name": "Kun Qian"
        },
        {
          "name": "Tian Wang"
        },
        {
          "name": "Changran Hu"
        },
        {
          "name": "Manling Li"
        },
        {
          "name": "Quanzheng Li"
        },
        {
          "name": "Hao Peng"
        },
        {
          "name": "Sheng Wang"
        },
        {
          "name": "Jingbo Shang"
        },
        {
          "name": "Chao Zhang"
        },
        {
          "name": "Jiaxuan You"
        },
        {
          "name": "Liyuan Liu"
        },
        {
          "name": "Pan Lu"
        },
        {
          "name": "Yu Zhang"
        },
        {
          "name": "Heng Ji"
        },
        {
          "name": "Yejin Choi"
        },
        {
          "name": "Dawn Song"
        },
        {
          "name": "Jimeng Sun"
        },
        {
          "name": "Jiawei Han"
        }
      ],
      "abstract": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-12-18T08:38:51+00:00",
      "updated": "2025-12-22T11:05:54+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16301v2",
      "file": "papers/2512.16301v2.pdf"
    },
    {
      "arxiv_id": "2512.16287v1",
      "title": "Evaluating OpenAI GPT Models for Translation of Endangered Uralic Languages: A Comparison of Reasoning and Non-Reasoning Architectures",
      "authors": [
        {
          "name": "Yehor Tereshchenko"
        },
        {
          "name": "Mika Hmlinen"
        },
        {
          "name": "Svitlana Myroniuk"
        }
      ],
      "abstract": "The evaluation of Large Language Models (LLMs) for translation tasks has primarily focused on high-resource languages, leaving a significant gap in understanding their performance on low-resource and endangered languages. This study presents a comprehensive comparison of OpenAI's GPT models, specifically examining the differences between reasoning and non-reasoning architectures for translating between Finnish and four low-resource Uralic languages: Komi-Zyrian, Moksha, Erzya, and Udmurt. Using a parallel corpus of literary texts, we evaluate model willingness to attempt translation through refusal rate analysis across different model architectures. Our findings reveal significant performance variations between reasoning and non-reasoning models, with reasoning models showing 16 percentage points lower refusal rates. The results provide valuable insights for researchers and practitioners working with Uralic languages and contribute to the broader understanding of reasoning model capabilities for endangered language preservation.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-18T08:14:49+00:00",
      "updated": "2025-12-18T08:14:49+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16287v1",
      "file": "papers/2512.16287v1.pdf"
    },
    {
      "arxiv_id": "2512.16248v2",
      "title": "Sigma-MoE-Tiny Technical Report",
      "authors": [
        {
          "name": "Qingguo Hu"
        },
        {
          "name": "Zhenghao Lin"
        },
        {
          "name": "Ziyue Yang"
        },
        {
          "name": "Yucheng Ding"
        },
        {
          "name": "Xiao Liu"
        },
        {
          "name": "Yuting Jiang"
        },
        {
          "name": "Ruizhe Wang"
        },
        {
          "name": "Tianyu Chen"
        },
        {
          "name": "Zhongxin Guo"
        },
        {
          "name": "Yifan Xiong"
        },
        {
          "name": "Rui Gao"
        },
        {
          "name": "Lei Qu"
        },
        {
          "name": "Jinsong Su"
        },
        {
          "name": "Peng Cheng"
        },
        {
          "name": "Yeyun Gong"
        }
      ],
      "abstract": "Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.\n  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny\n  Code: https://github.com/microsoft/ltp-megatron-lm",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-18T06:57:42+00:00",
      "updated": "2025-12-19T05:44:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16248v2",
      "file": "papers/2512.16248v2.pdf"
    },
    {
      "arxiv_id": "2512.16229v2",
      "title": "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding",
      "authors": [
        {
          "name": "Chenkai Xu"
        },
        {
          "name": "Yijie Jin"
        },
        {
          "name": "Jiajun Li"
        },
        {
          "name": "Yi Tu"
        },
        {
          "name": "Guoping Long"
        },
        {
          "name": "Dandan Tu"
        },
        {
          "name": "Mingcong Song"
        },
        {
          "name": "Hongjie Si"
        },
        {
          "name": "Tianqi Hou"
        },
        {
          "name": "Junchi Yan"
        },
        {
          "name": "Zhijie Deng"
        }
      ],
      "abstract": "Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-18T06:22:01+00:00",
      "updated": "2025-12-22T13:29:11+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16229v2",
      "file": "papers/2512.16229v2.pdf"
    },
    {
      "arxiv_id": "2512.16227v1",
      "title": "An Information-Theoretic Framework for Robust Large Language Model Editing",
      "authors": [
        {
          "name": "Qizhou Chen"
        },
        {
          "name": "Chengyu Wang"
        },
        {
          "name": "Taolin Zhang"
        },
        {
          "name": "Xiaofeng He"
        }
      ],
      "abstract": "Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-18T06:21:17+00:00",
      "updated": "2025-12-18T06:21:17+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16227v1",
      "file": "papers/2512.16227v1.pdf"
    },
    {
      "arxiv_id": "2512.16189v2",
      "title": "Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation",
      "authors": [
        {
          "name": "Musarrat Zeba"
        },
        {
          "name": "Abdullah Al Mamun"
        },
        {
          "name": "Kishoar Jahan Tithee"
        },
        {
          "name": "Debopom Sutradhar"
        },
        {
          "name": "Mohaimenul Azam Khan Raiaan"
        },
        {
          "name": "Saddam Mukta"
        },
        {
          "name": "Reem E. Mohamed"
        },
        {
          "name": "Md Rafiqul Islam"
        },
        {
          "name": "Yakub Sebastian"
        },
        {
          "name": "Mukhtar Hussain"
        },
        {
          "name": "Sami Azam"
        }
      ],
      "abstract": "In healthcare, it is essential for any LLM-generated output to be reliable and accurate, particularly in cases involving decision-making and patient safety. However, the outputs are often unreliable in such critical areas due to the risk of hallucinated outputs from the LLMs. To address this issue, we propose a fact-checking module that operates independently of any LLM, along with a domain-specific summarization model designed to minimize hallucination rates. Our model is fine-tuned using Low-Rank Adaptation (LoRa) on the MIMIC III dataset and is paired with the fact-checking module, which uses numerical tests for correctness and logical checks at a granular level through discrete logic in natural language processing (NLP) to validate facts against electronic health records (EHRs). We trained the LLM model on the full MIMIC-III dataset. For evaluation of the fact-checking module, we sampled 104 summaries, extracted them into 3,786 propositions, and used these as facts. The fact-checking module achieves a precision of 0.8904, a recall of 0.8234, and an F1-score of 0.8556. Additionally, the LLM summary model achieves a ROUGE-1 score of 0.5797 and a BERTScore of 0.9120 for summary quality.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-18T05:23:47+00:00",
      "updated": "2025-12-19T06:34:23+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16189v2",
      "file": "papers/2512.16189v2.pdf"
    },
    {
      "arxiv_id": "2512.16183v1",
      "title": "A Domain-Adapted Pipeline for Structured Information Extraction from Police Incident Announcements on Social Media",
      "authors": [
        {
          "name": "Mengfan Shen"
        },
        {
          "name": "Kangqi Song"
        },
        {
          "name": "Xindi Wang"
        },
        {
          "name": "Wei Jia"
        },
        {
          "name": "Tao Wang"
        },
        {
          "name": "Ziqiang Han"
        }
      ],
      "abstract": "Structured information extraction from police incident announcements is crucial for timely and accurate data processing, yet presents considerable challenges due to the variability and informal nature of textual sources such as social media posts. To address these challenges, we developed a domain-adapted extraction pipeline that leverages targeted prompt engineering with parameter-efficient fine-tuning of the Qwen2.5-7B model using Low-Rank Adaptation (LoRA). This approach enables the model to handle noisy, heterogeneous text while reliably extracting 15 key fields, including location, event characteristics, and impact assessment, from a high-quality, manually annotated dataset of 4,933 instances derived from 27,822 police briefing posts on Chinese Weibo (2019-2020). Experimental results demonstrated that LoRA-based fine-tuning significantly improved performance over both the base and instruction-tuned models, achieving an accuracy exceeding 98.36% for mortality detection and Exact Match Rates of 95.31% for fatality counts and 95.54% for province-level location extraction. The proposed pipeline thus provides a validated and efficient solution for multi-task structured information extraction in specialized domains, offering a practical framework for transforming unstructured text into reliable structured data in social science research.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.CY"
      ],
      "published": "2025-12-18T05:08:26+00:00",
      "updated": "2025-12-18T05:08:26+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16183v1",
      "file": "papers/2512.16183v1.pdf"
    },
    {
      "arxiv_id": "2512.16182v1",
      "title": "DualGuard: Dual-stream Large Language Model Watermarking Defense against Paraphrase and Spoofing Attack",
      "authors": [
        {
          "name": "Hao Li"
        },
        {
          "name": "Yubing Ren"
        },
        {
          "name": "Yanan Cao"
        },
        {
          "name": "Yingjie Li"
        },
        {
          "name": "Fang Fang"
        },
        {
          "name": "Shi Wang"
        },
        {
          "name": "Li Guo"
        }
      ],
      "abstract": "With the rapid development of cloud-based services, large language models (LLMs) have become increasingly accessible through various web platforms. However, this accessibility has also led to growing risks of model abuse. LLM watermarking has emerged as an effective approach to mitigate such misuse and protect intellectual property. Existing watermarking algorithms, however, primarily focus on defending against paraphrase attacks while overlooking piggyback spoofing attacks, which can inject harmful content, compromise watermark reliability, and undermine trust in attribution. To address this limitation, we propose DualGuard, the first watermarking algorithm capable of defending against both paraphrase and spoofing attacks. DualGuard employs the adaptive dual-stream watermarking mechanism, in which two complementary watermark signals are dynamically injected based on the semantic content. This design enables DualGuard not only to detect but also to trace spoofing attacks, thereby ensuring reliable and trustworthy watermark detection. Extensive experiments conducted across multiple datasets and language models demonstrate that DualGuard achieves excellent detectability, robustness, traceability, and text quality, effectively advancing the state of LLM watermarking for real-world applications.",
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "published": "2025-12-18T05:08:19+00:00",
      "updated": "2025-12-18T05:08:19+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16182v1",
      "file": "papers/2512.16182v1.pdf"
    },
    {
      "arxiv_id": "2512.16147v1",
      "title": "Decoding Fake Narratives in Spreading Hateful Stories: A Dual-Head RoBERTa Model with Multi-Task Learning",
      "authors": [
        {
          "name": "Yash Bhaskar"
        },
        {
          "name": "Sankalp Bahad"
        },
        {
          "name": "Parameswari Krishnamurthy"
        }
      ],
      "abstract": "Social media platforms, while enabling global connectivity, have become hubs for the rapid spread of harmful content, including hate speech and fake narratives \\cite{davidson2017automated, shu2017fake}. The Faux-Hate shared task focuses on detecting a specific phenomenon: the generation of hate speech driven by fake narratives, termed Faux-Hate. Participants are challenged to identify such instances in code-mixed Hindi-English social media text. This paper describes our system developed for the shared task, addressing two primary sub-tasks: (a) Binary Faux-Hate detection, involving fake and hate speech classification, and (b) Target and Severity prediction, categorizing the intended target and severity of hateful content. Our approach combines advanced natural language processing techniques with domain-specific pretraining to enhance performance across both tasks. The system achieved competitive results, demonstrating the efficacy of leveraging multi-task learning for this complex problem.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-18T04:00:06+00:00",
      "updated": "2025-12-18T04:00:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16147v1",
      "file": "papers/2512.16147v1.pdf"
    },
    {
      "arxiv_id": "2512.16145v1",
      "title": "MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation",
      "authors": [
        {
          "name": "Pengyu Wang"
        },
        {
          "name": "Shuchang Ye"
        },
        {
          "name": "Usman Naseem"
        },
        {
          "name": "Jinman Kim"
        }
      ],
      "abstract": "Medical report generation (MRG) aims to automatically derive radiology-style reports from medical images to aid in clinical decision-making. However, existing methods often generate text that mimics the linguistic style of radiologists but fails to guarantee clinical correctness, because they are trained on token-level objectives which focus on word-choice and sentence structure rather than actual medical accuracy. We propose a semantic-driven reinforcement learning (SRL) method for medical report generation, adopted on a large vision-language model (LVLM). SRL adopts Group Relative Policy Optimization (GRPO) to encourage clinical-correctness-guided learning beyond imitation of language style. Specifically, we optimise a report-level reward: a margin-based cosine similarity (MCCS) computed between key radiological findings extracted from generated and reference reports, thereby directly aligning clinical-label agreement and improving semantic correctness. A lightweight reasoning format constraint further guides the model to generate structured \"thinking report\" outputs. We evaluate Medical Report Generation with Sematic-driven Reinforment Learning (MRG-R1), on two datasets: IU X-Ray and MIMIC-CXR using clinical efficacy (CE) metrics. MRG-R1 achieves state-of-the-art performance with CE-F1 51.88 on IU X-Ray and 40.39 on MIMIC-CXR. We found that the label-semantic reinforcement is better than conventional token-level supervision. These results indicate that optimizing a clinically grounded, report-level reward rather than token overlap,meaningfully improves clinical correctness. This work is a prior to explore semantic-reinforcement in supervising medical correctness in medical Large vision-language model(Med-LVLM) training.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-18T03:57:55+00:00",
      "updated": "2025-12-18T03:57:55+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16145v1",
      "file": "papers/2512.16145v1.pdf"
    },
    {
      "arxiv_id": "2512.16125v1",
      "title": "Convolutional Lie Operator for Sentence Classification",
      "authors": [
        {
          "name": "Daniela N. Rim"
        },
        {
          "name": "Heeyoul Choi"
        }
      ],
      "abstract": "Traditional Convolutional Neural Networks have been successful in capturing local, position-invariant features in text, but their capacity to model complex transformation within language can be further explored. In this work, we explore a novel approach by integrating Lie Convolutions into Convolutional-based sentence classifiers, inspired by the ability of Lie group operations to capture complex, non-Euclidean symmetries. Our proposed models SCLie and DPCLie empirically outperform traditional Convolutional-based sentence classifiers, suggesting that Lie-based models relatively improve the accuracy by capturing transformations not commonly associated with language. Our findings motivate more exploration of new paradigms in language modeling.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-18T03:23:37+00:00",
      "updated": "2025-12-18T03:23:37+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16125v1",
      "file": "papers/2512.16125v1.pdf"
    },
    {
      "arxiv_id": "2512.16059v1",
      "title": "ContextLeak: Auditing Leakage in Private In-Context Learning Methods",
      "authors": [
        {
          "name": "Jacob Choi"
        },
        {
          "name": "Shuying Cao"
        },
        {
          "name": "Xingjian Dong"
        },
        {
          "name": "Wang Bill Zhu"
        },
        {
          "name": "Robin Jia"
        },
        {
          "name": "Sai Praneeth Karimireddy"
        }
      ],
      "abstract": "In-Context Learning (ICL) has become a standard technique for adapting Large Language Models (LLMs) to specialized tasks by supplying task-specific exemplars within the prompt. However, when these exemplars contain sensitive information, reliable privacy-preserving mechanisms are essential to prevent unintended leakage through model outputs. Many privacy-preserving methods are proposed to protect the information leakage in the context, but there are less efforts on how to audit those methods. We introduce ContextLeak, the first framework to empirically measure the worst-case information leakage in ICL. ContextLeak uses canary insertion, embedding uniquely identifiable tokens in exemplars and crafting targeted queries to detect their presence. We apply ContextLeak across a range of private ICL techniques, both heuristic such as prompt-based defenses and those with theoretical guarantees such as Embedding Space Aggregation and Report Noisy Max. We find that ContextLeak tightly correlates with the theoretical privacy budget ($$) and reliably detects leakage. Our results further reveal that existing methods often strike poor privacy-utility trade-offs, either leaking sensitive information or severely degrading performance.",
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "published": "2025-12-18T00:53:19+00:00",
      "updated": "2025-12-18T00:53:19+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16059v1",
      "file": "papers/2512.16059v1.pdf"
    },
    {
      "arxiv_id": "2512.16041v1",
      "title": "Are We on the Right Way to Assessing LLM-as-a-Judge?",
      "authors": [
        {
          "name": "Yuanning Feng"
        },
        {
          "name": "Sinan Wang"
        },
        {
          "name": "Zhengxiang Cheng"
        },
        {
          "name": "Yao Wan"
        },
        {
          "name": "Dongping Chen"
        }
      ],
      "abstract": "LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-17T23:49:55+00:00",
      "updated": "2025-12-17T23:49:55+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16041v1",
      "file": "papers/2512.16041v1.pdf"
    },
    {
      "arxiv_id": "2512.16034v1",
      "title": "Examining the Utility of Self-disclosure Types for Modeling Annotators of Social Norms",
      "authors": [
        {
          "name": "Kieran Henderson"
        },
        {
          "name": "Kian Omoomi"
        },
        {
          "name": "Vasudha Varadarajan"
        },
        {
          "name": "Allison Lahnala"
        },
        {
          "name": "Charles Welch"
        }
      ],
      "abstract": "Recent work has explored the use of personal information in the form of persona sentences or self-disclosures to improve modeling of individual characteristics and prediction of annotator labels for subjective tasks. The volume of personal information has historically been restricted and thus little exploration has gone into understanding what kind of information is most informative for predicting annotator labels. In this work, we categorize self-disclosure sentences and use them to build annotator models for predicting judgments of social norms. We perform several ablations and analyses to examine the impact of the type of information on our ability to predict annotation patterns. We find that demographics are more impactful than attitudes, relationships, and experiences. Generally, theory-based approaches worked better than automatic clusters. Contrary to previous work, only a small number of related comments are needed. Lastly, having a more diverse sample of annotator self-disclosures leads to the best performance.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-17T23:32:48+00:00",
      "updated": "2025-12-17T23:32:48+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16034v1",
      "file": "papers/2512.16034v1.pdf"
    },
    {
      "arxiv_id": "2512.16029v1",
      "title": "Cross-Language Bias Examination in Large Language Models",
      "authors": [
        {
          "name": "Yuxuan Liang"
        },
        {
          "name": "Marwa Mahmoud"
        }
      ],
      "abstract": "This study introduces an innovative multilingual bias evaluation framework for assessing bias in Large Language Models, combining explicit bias assessment through the BBQ benchmark with implicit bias measurement using a prompt-based Implicit Association Test. By translating the prompts and word list into five target languages, English, Chinese, Arabic, French, and Spanish, we directly compare different types of bias across languages. The results reveal substantial gaps in bias across languages used in LLMs. For example, Arabic and Spanish consistently show higher levels of stereotype bias, while Chinese and English exhibit lower levels of bias. We also identify contrasting patterns across bias types. Age shows the lowest explicit bias but the highest implicit bias, emphasizing the importance of detecting implicit biases that are undetectable with standard benchmarks. These findings indicate that LLMs vary significantly across languages and bias dimensions. This study fills a key research gap by providing a comprehensive methodology for cross-lingual bias analysis. Ultimately, our work establishes a foundation for the development of equitable multilingual LLMs, ensuring fairness and effectiveness across diverse languages and cultures.",
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-12-17T23:22:03+00:00",
      "updated": "2025-12-17T23:22:03+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16029v1",
      "file": "papers/2512.16029v1.pdf"
    },
    {
      "arxiv_id": "2512.15973v1",
      "title": "Dynamic Rank Reinforcement Learning for Adaptive Low-Rank Multi-Head Self Attention in Large Language Models",
      "authors": [
        {
          "name": "Caner Erden"
        }
      ],
      "abstract": "We propose Dynamic Rank Reinforcement Learning (DR-RL), a novel framework that adaptively optimizes the low-rank factorization of Multi-Head Self-Attention (MHSA) in Large Language Models (LLMs) through the integration of reinforcement learning and online matrix perturbation theory. While traditional low-rank approximations often rely on static rank assumptions--limiting their flexibility across diverse input contexts--our method dynamically selects ranks based on real-time sequence dynamics, layer-specific sensitivities, and hardware constraints. The core innovation lies in an RL agent that formulates rank selection as a sequential policy optimization problem, where the reward function strictly balances attention fidelity against computational latency. Crucially, we employ online matrix perturbation bounds to enable incremental rank updates, thereby avoiding the prohibitive cost of full decomposition during inference. Furthermore, the integration of a lightweight Transformer-based policy network and batched Singular Value Decomposition (SVD) operations ensures scalable deployment on modern GPU architectures. Experiments demonstrate that DR-RL maintains downstream accuracy statistically equivalent to full-rank attention while significantly reducing Floating Point Operations (FLOPs), particularly in long-sequence regimes (L > 4096). This work bridges the gap between adaptive efficiency and theoretical rigor in MHSA, offering a principled, mathematically grounded alternative to heuristic rank reduction techniques in resource-constrained deep learning. Source code and experiment logs are available at: https://github.com/canererden/DR_RL_Project",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2025-12-17T21:09:19+00:00",
      "updated": "2025-12-17T21:09:19+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15973v1",
      "file": "papers/2512.15973v1.pdf"
    },
    {
      "arxiv_id": "2512.15959v1",
      "title": "BRAID: Bounded Reasoning for Autonomous Inference and Decisions",
      "authors": [
        {
          "name": "Armaan Amcalar"
        },
        {
          "name": "Eyup Cinar"
        }
      ],
      "abstract": "Large Language Models (LLMs) exhibit nonlinear relationships between performance, cost, and token usage. This paper presents a quantitative study on structured prompting using BRAID (Bounded Reasoning for Au tonomous Inference and Decisions) across multiple GPT model tiers, eval uated on the AdvancedIF, GSM-Hard, and the SCALE MultiChallenge benchmark datasets. BRAID introduces a bounded reasoning framework using Mermaid-based instruction graphs that enable models to reason struc turally rather than through unbounded natural-language token expansion. We show that structured machine-readable prompts substantially increase reasoning accuracy and cost efficiency for agents in production systems. The findings establish BRAID as an effective and scalable technique for optimizing inference efficiency in autonomous agent systems. All datasets and detailed result logs are available at https://benchmark.openserv.ai.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-17T20:46:44+00:00",
      "updated": "2025-12-17T20:46:44+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15959v1",
      "file": "papers/2512.15959v1.pdf"
    },
    {
      "arxiv_id": "2512.15925v1",
      "title": "Social Story Frames: Contextual Reasoning about Narrative Intent and Reception",
      "authors": [
        {
          "name": "Joel Mire"
        },
        {
          "name": "Maria Antoniak"
        },
        {
          "name": "Steven R. Wilson"
        },
        {
          "name": "Zexin Ma"
        },
        {
          "name": "Achyutarama R. Ganti"
        },
        {
          "name": "Andrew Piper"
        },
        {
          "name": "Maarten Sap"
        }
      ],
      "abstract": "Reading stories evokes rich interpretive, affective, and evaluative responses, such as inferences about narrative intent or judgments about characters. Yet, computational models of reader response are limited, preventing nuanced analyses. To address this gap, we introduce SocialStoryFrames, a formalism for distilling plausible inferences about reader response, such as perceived author intent, explanatory and predictive reasoning, affective responses, and value judgments, using conversational context and a taxonomy grounded in narrative theory, linguistic pragmatics, and psychology. We develop two models, SSF-Generator and SSF-Classifier, validated through human surveys (N=382 participants) and expert annotations, respectively. We conduct pilot analyses to showcase the utility of the formalism for studying storytelling at scale. Specifically, applying our models to SSF-Corpus, a curated dataset of 6,140 social media stories from diverse contexts, we characterize the frequency and interdependence of storytelling intents, and we compare and contrast narrative practices (and their diversity) across communities. By linking fine-grained, context-sensitive modeling with a generic taxonomy of reader responses, SocialStoryFrames enable new research into storytelling in online communities.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SI"
      ],
      "published": "2025-12-17T19:41:32+00:00",
      "updated": "2025-12-17T19:41:32+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15925v1",
      "file": "papers/2512.15925v1.pdf"
    },
    {
      "arxiv_id": "2512.15907v1",
      "title": "TabReX : Tabular Referenceless eXplainable Evaluation",
      "authors": [
        {
          "name": "Tejas Anvekar"
        },
        {
          "name": "Juhna Park"
        },
        {
          "name": "Aparna Garimella"
        },
        {
          "name": "Vivek Gupta"
        }
      ],
      "abstract": "Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-17T19:20:20+00:00",
      "updated": "2025-12-17T19:20:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15907v1",
      "file": "papers/2512.15907v1.pdf"
    },
    {
      "arxiv_id": "2512.15674v1",
      "title": "Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers",
      "authors": [
        {
          "name": "Adam Karvonen"
        },
        {
          "name": "James Chua"
        },
        {
          "name": "Clment Dumas"
        },
        {
          "name": "Kit Fraser-Taliente"
        },
        {
          "name": "Subhash Kantamneni"
        },
        {
          "name": "Julian Minder"
        },
        {
          "name": "Euan Ong"
        },
        {
          "name": "Arnab Sen Sharma"
        },
        {
          "name": "Daniel Wen"
        },
        {
          "name": "Owain Evans"
        },
        {
          "name": "Samuel Marks"
        }
      ],
      "abstract": "Large language model (LLM) activations are notoriously difficult to understand, with most existing techniques using complex, specialized methods for interpreting them. Recent work has proposed a simpler approach known as LatentQA: training LLMs to directly accept LLM activations as inputs and answer arbitrary questions about them in natural language. However, prior work has focused on narrow task settings for both training and evaluation. In this paper, we instead take a generalist perspective. We evaluate LatentQA-trained models, which we call Activation Oracles (AOs), in far out-of-distribution settings and examine how performance scales with training data diversity. We find that AOs can recover information fine-tuned into a model (e.g., biographical knowledge or malign propensities) that does not appear in the input text, despite never being trained with activations from a fine-tuned model. Our main evaluations are four downstream tasks where we can compare to prior white- and black-box techniques. We find that even narrowly-trained LatentQA models can generalize well, and that adding additional training datasets (such as classification tasks and a self-supervised context prediction task) yields consistent further improvements. Overall, our best AOs match or exceed prior white-box baselines on all four tasks and are the best method on 3 out of 4. These results suggest that diversified training to answer natural-language queries imparts a general capability to verbalize information about LLM activations.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-17T18:26:28+00:00",
      "updated": "2025-12-17T18:26:28+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15674v1",
      "file": "papers/2512.15674v1.pdf"
    },
    {
      "arxiv_id": "2512.15663v1",
      "title": "Explaining the Reasoning of Large Language Models Using Attribution Graphs",
      "authors": [
        {
          "name": "Chase Walker"
        },
        {
          "name": "Rickard Ewetz"
        }
      ],
      "abstract": "Large language models (LLMs) exhibit remarkable capabilities, yet their reasoning remains opaque, raising safety and trust concerns. Attribution methods, which assign credit to input features, have proven effective for explaining the decision making of computer vision models. From these, context attributions have emerged as a promising approach for explaining the behavior of autoregressive LLMs. However, current context attributions produce incomplete explanations by directly relating generated tokens to the prompt, discarding inter-generational influence in the process. To overcome these shortcomings, we introduce the Context Attribution via Graph Explanations (CAGE) framework. CAGE introduces an attribution graph: a directed graph that quantifies how each generation is influenced by both the prompt and all prior generations. The graph is constructed to preserve two properties-causality and row stochasticity. The attribution graph allows context attributions to be computed by marginalizing intermediate contributions along paths in the graph. Across multiple models, datasets, metrics, and methods, CAGE improves context attribution faithfulness, achieving average gains of up to 40%.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-12-17T18:15:26+00:00",
      "updated": "2025-12-17T18:15:26+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15663v1",
      "file": "papers/2512.15663v1.pdf"
    },
    {
      "arxiv_id": "2512.15658v1",
      "title": "PPSEBM: An Energy-Based Model with Progressive Parameter Selection for Continual Learning",
      "authors": [
        {
          "name": "Xiaodi Li"
        },
        {
          "name": "Dingcheng Li"
        },
        {
          "name": "Rujun Gao"
        },
        {
          "name": "Mahmoud Zamani"
        },
        {
          "name": "Feng Mi"
        },
        {
          "name": "Latifur Khan"
        }
      ],
      "abstract": "Continual learning remains a fundamental challenge in machine learning, requiring models to learn from a stream of tasks without forgetting previously acquired knowledge. A major obstacle in this setting is catastrophic forgetting, where performance on earlier tasks degrades as new tasks are learned. In this paper, we introduce PPSEBM, a novel framework that integrates an Energy-Based Model (EBM) with Progressive Parameter Selection (PPS) to effectively address catastrophic forgetting in continual learning for natural language processing tasks. In PPSEBM, progressive parameter selection allocates distinct, task-specific parameters for each new task, while the EBM generates representative pseudo-samples from prior tasks. These generated samples actively inform and guide the parameter selection process, enhancing the model's ability to retain past knowledge while adapting to new tasks. Experimental results on diverse NLP benchmarks demonstrate that PPSEBM outperforms state-of-the-art continual learning methods, offering a promising and robust solution to mitigate catastrophic forgetting.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-17T18:11:29+00:00",
      "updated": "2025-12-17T18:11:29+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15658v1",
      "file": "papers/2512.15658v1.pdf"
    },
    {
      "arxiv_id": "2512.15653v1",
      "title": "Characterizing Mamba's Selective Memory using Auto-Encoders",
      "authors": [
        {
          "name": "Tamanna Hossain"
        },
        {
          "name": "Robert L. Logan"
        },
        {
          "name": "Ganesh Jagadeesan"
        },
        {
          "name": "Sameer Singh"
        },
        {
          "name": "Joel Tetreault"
        },
        {
          "name": "Alejandro Jaimes"
        }
      ],
      "abstract": "State space models (SSMs) are a promising alternative to transformers for language modeling because they use fixed memory during inference. However, this fixed memory usage requires some information loss in the hidden state when processing long sequences. While prior work has studied the sequence length at which this information loss occurs, it does not characterize the types of information SSM language models (LMs) tend to forget. In this paper, we address this knowledge gap by identifying the types of tokens (e.g., parts of speech, named entities) and sequences (e.g., code, math problems) that are more frequently forgotten by SSM LMs. We achieve this by training an auto-encoder to reconstruct sequences from the SSM's hidden state, and measure information loss by comparing inputs with their reconstructions. We perform experiments using the Mamba family of SSM LMs (130M--1.4B) on sequences ranging from 4--256 tokens. Our results show significantly higher rates of information loss on math-related tokens (e.g., numbers, variables), mentions of organization entities, and alternative dialects to Standard American English. We then examine the frequency that these tokens appear in Mamba's pretraining data and find that less prevalent tokens tend to be the ones Mamba is most likely to forget. By identifying these patterns, our work provides clear direction for future research to develop methods that better control Mamba's ability to retain important information.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-17T18:05:25+00:00",
      "updated": "2025-12-17T18:05:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15653v1",
      "file": "papers/2512.15653v1.pdf"
    },
    {
      "arxiv_id": "2512.15634v1",
      "title": "How Much is Too Much? Exploring LoRA Rank Trade-offs for Retaining Knowledge and Domain Robustness",
      "authors": [
        {
          "name": "Darshita Rathore"
        },
        {
          "name": "Vineet Kumar"
        },
        {
          "name": "Chetna Bansal"
        },
        {
          "name": "Anindya Moitra"
        }
      ],
      "abstract": "Large language models are increasingly adapted to downstream tasks through fine-tuning. Full supervised fine-tuning (SFT) and parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), are two dominant approaches. While PEFT methods are widely used for their computational efficiency, the implications of their configurations (e.g., rank) remain under-explored in downstream Q&A tasks and generalisation. In this work, we perform a comprehensive evaluation across multiple reasoning and recall datasets, conducting a rank sweep to quantify the trade-off between SFT and PEFT. We also compare the accuracy of PEFT and SFT models across in-domain and out-of-domain adaptation, highlighting distinct generalisation behaviour and task-specific forgetting. We demonstrate that LoRA achieves competitive and in some cases superior performance compared to SFT, particularly on reasoning tasks at specific rank values. Additionally, we analyze the internal representations via spectral features and layer-wise attention structures, offering insights into representational drift and structural changes in attention patterns.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-17T17:44:09+00:00",
      "updated": "2025-12-17T17:44:09+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15634v1",
      "file": "papers/2512.15634v1.pdf"
    },
    {
      "arxiv_id": "2512.15601v1",
      "title": "You Never Know a Person, You Only Know Their Defenses: Detecting Levels of Psychological Defense Mechanisms in Supportive Conversations",
      "authors": [
        {
          "name": "Hongbin Na"
        },
        {
          "name": "Zimu Wang"
        },
        {
          "name": "Zhaoming Chen"
        },
        {
          "name": "Peilin Zhou"
        },
        {
          "name": "Yining Hua"
        },
        {
          "name": "Grace Ziqi Zhou"
        },
        {
          "name": "Haiyang Zhang"
        },
        {
          "name": "Tao Shen"
        },
        {
          "name": "Wei Wang"
        },
        {
          "name": "John Torous"
        },
        {
          "name": "Shaoxiong Ji"
        },
        {
          "name": "Ling Chen"
        }
      ],
      "abstract": "Psychological defenses are strategies, often automatic, that people use to manage distress. Rigid or overuse of defenses is negatively linked to mental health and shapes what speakers disclose and how they accept or resist help. However, defenses are complex and difficult to reliably measure, particularly in clinical dialogues. We introduce PsyDefConv, a dialogue corpus with help seeker utterances labeled for defense level, and DMRS Co-Pilot, a four-stage pipeline that provides evidence-based pre-annotations. The corpus contains 200 dialogues and 4709 utterances, including 2336 help seeker turns, with labeling and Cohen's kappa 0.639. In a counterbalanced study, the co-pilot reduced average annotation time by 22.4%. In expert review, it averaged 4.62 for evidence, 4.44 for clinical plausibility, and 4.40 for insight on a seven-point scale. Benchmarks with strong language models in zero-shot and fine-tuning settings demonstrate clear headroom, with the best macro F1-score around 30% and a tendency to overpredict mature defenses. Corpus analyses confirm that mature defenses are most common and reveal emotion-specific deviations. We will release the corpus, annotations, code, and prompts to support research on defensive functioning in language.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-17T17:11:05+00:00",
      "updated": "2025-12-17T17:11:05+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15601v1",
      "file": "papers/2512.15601v1.pdf"
    },
    {
      "arxiv_id": "2512.15586v1",
      "title": "Bolmo: Byteifying the Next Generation of Language Models",
      "authors": [
        {
          "name": "Benjamin Minixhofer"
        },
        {
          "name": "Tyler Murray"
        },
        {
          "name": "Tomasz Limisiewicz"
        },
        {
          "name": "Anna Korhonen"
        },
        {
          "name": "Luke Zettlemoyer"
        },
        {
          "name": "Noah A. Smith"
        },
        {
          "name": "Edoardo M. Ponti"
        },
        {
          "name": "Luca Soldaini"
        },
        {
          "name": "Valentin Hofmann"
        }
      ],
      "abstract": "We introduce Bolmo, the first family of competitive fully open byte-level language models (LMs) at the 1B and 7B parameter scales. In contrast to prior research on byte-level LMs, which focuses predominantly on training from scratch, we train Bolmo by byteifying existing subword-level LMs. Byteification enables overcoming the limitations of subword tokenization - such as insufficient character understanding and efficiency constraints due to the fixed subword vocabulary - while performing at the level of leading subword-level LMs. Bolmo is specifically designed for byteification: our architecture resolves a mismatch between the expressivity of prior byte-level architectures and subword-level LMs, which makes it possible to employ an effective exact distillation objective between Bolmo and the source subword model. This allows for converting a subword-level LM to a byte-level LM by investing less than 1\\% of a typical pretraining token budget. Bolmo substantially outperforms all prior byte-level LMs of comparable size, and outperforms the source subword-level LMs on character understanding and, in some cases, coding, while coming close to matching the original LMs' performance on other tasks. Furthermore, we show that Bolmo can achieve inference speeds competitive with subword-level LMs by training with higher token compression ratios, and can be cheaply and effectively post-trained by leveraging the existing ecosystem around the source subword-level LM. Our results finally make byte-level LMs a practical choice competitive with subword-level LMs across a wide set of use cases.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-17T16:46:11+00:00",
      "updated": "2025-12-17T16:46:11+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15586v1",
      "file": "papers/2512.15586v1.pdf"
    },
    {
      "arxiv_id": "2512.15556v1",
      "title": "An Empirical Study on Chinese Character Decomposition in Multiword Expression-Aware Neural Machine Translation",
      "authors": [
        {
          "name": "Lifeng Han"
        },
        {
          "name": "Gareth J. F. Jones"
        },
        {
          "name": "Alan F. Smeaton"
        }
      ],
      "abstract": "Word meaning, representation, and interpretation play fundamental roles in natural language understanding (NLU), natural language processing (NLP), and natural language generation (NLG) tasks. Many of the inherent difficulties in these tasks stem from Multi-word Expressions (MWEs), which complicate the tasks by introducing ambiguity, idiomatic expressions, infrequent usage, and a wide range of variations. Significant effort and substantial progress have been made in addressing the challenging nature of MWEs in Western languages, particularly English. This progress is attributed in part to the well-established research communities and the abundant availability of computational resources. However, the same level of progress is not true for language families such as Chinese and closely related Asian languages, which continue to lag behind in this regard. While sub-word modelling has been successfully applied to many Western languages to address rare words improving phrase comprehension, and enhancing machine translation (MT) through techniques like byte-pair encoding (BPE), it cannot be applied directly to ideograph language scripts like Chinese. In this work, we conduct a systematic study of the Chinese character decomposition technology in the context of MWE-aware neural machine translation (NMT). Furthermore, we report experiments to examine how Chinese character decomposition technology contributes to the representation of the original meanings of Chinese words and characters, and how it can effectively address the challenges of translating MWEs.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-17T16:08:49+00:00",
      "updated": "2025-12-17T16:08:49+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15556v1",
      "file": "papers/2512.15556v1.pdf"
    },
    {
      "arxiv_id": "2512.15551v1",
      "title": "Learning inflection classes using Adaptive Resonance Theory",
      "authors": [
        {
          "name": "Peter Dekker"
        },
        {
          "name": "Heikki Rasilo"
        },
        {
          "name": "Bart de Boer"
        }
      ],
      "abstract": "The concept of inflection classes is an abstraction used by linguists, and provides a means to describe patterns in languages that give an analogical base for deducing previously unencountered forms. This ability is an important part of morphological acquisition and processing. We study the learnability of a system of verbal inflection classes by the individual language user by performing unsupervised clustering of lexemes into inflection classes. As a cognitively plausible and interpretable computational model, we use Adaptive Resonance Theory, a neural network with a parameter that determines the degree of generalisation (vigilance). The model is applied to Latin, Portuguese and Estonian. The similarity of clustering to attested inflection classes varies depending on the complexity of the inflectional system. We find the best performance in a narrow region of the generalisation parameter. The learned features extracted from the model show similarity with linguistic descriptions of the inflection classes. The proposed model could be used to study change in inflection classes in the future, by including it in an agent-based model.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-17T15:58:20+00:00",
      "updated": "2025-12-17T15:58:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15551v1",
      "file": "papers/2512.15551v1.pdf"
    },
    {
      "arxiv_id": "2512.15550v1",
      "title": "CTkvr: KV Cache Retrieval for Long-Context LLMs via Centroid then Token Indexing",
      "authors": [
        {
          "name": "Kuan Lu"
        },
        {
          "name": "Shuhang Lin"
        },
        {
          "name": "Sai Wu"
        },
        {
          "name": "Yichen Yao"
        },
        {
          "name": "Junhan Yang"
        },
        {
          "name": "Huan Li"
        },
        {
          "name": "Wei Chu"
        },
        {
          "name": "Xu Yinghui"
        },
        {
          "name": "Yuan Qi"
        },
        {
          "name": "Gang Chen"
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly applied in long-context scenarios such as multi-turn conversations. However, long contexts pose significant challenges for inference efficiency, including high memory overhead from Key-Value (KV) cache and increased latency due to excessive memory accesses. Recent methods for dynamic KV selection struggle with trade-offs: block-level indexing degrades accuracy by retrieving irrelevant KV entries, while token-level indexing incurs high latency from inefficient retrieval mechanisms. In this paper, we propose CTKVR, a novel centroid-then-token KV retrieval scheme that addresses these limitations. CTKVR leverages a key observation: query vectors adjacent in position exhibit high similarity after Rotary Position Embedding (RoPE) and share most of their top-k KV cache entries. Based on this insight, CTKVR employs a two-stage retrieval strategy: lightweight centroids are precomputed during prefilling for centroid-grained indexing, followed by token-level refinement for precise KV retrieval. This approach balances retrieval efficiency and accuracy. To further enhance performance, we implement an optimized system for indexing construction and search using CPU-GPU co-execution. Experimentally, CTKVR achieves superior performance across multiple benchmarks with less than 1% accuracy degradation. Meanwhile, CTKVR delivers 3 times and 4 times throughput speedups on Llama-3-8B and Yi-9B at 96K context length across diverse GPU hardware.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-17T15:56:32+00:00",
      "updated": "2025-12-17T15:56:32+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15550v1",
      "file": "papers/2512.15550v1.pdf"
    },
    {
      "arxiv_id": "2512.15547v1",
      "title": "When a Nation Speaks: Machine Learning and NLP in People's Sentiment Analysis During Bangladesh's 2024 Mass Uprising",
      "authors": [
        {
          "name": "Md. Samiul Alim"
        },
        {
          "name": "Mahir Shahriar Tamim"
        },
        {
          "name": "Maisha Rahman"
        },
        {
          "name": "Tanvir Ahmed Khan"
        },
        {
          "name": "Md Mushfique Anwar"
        }
      ],
      "abstract": "Sentiment analysis, an emerging research area within natural language processing (NLP), has primarily been explored in contexts like elections and social media trends, but there remains a significant gap in understanding emotional dynamics during civil unrest, particularly in the Bangla language. Our study pioneers sentiment analysis in Bangla during a national crisis by examining public emotions amid Bangladesh's 2024 mass uprising. We curated a unique dataset of 2,028 annotated news headlines from major Facebook news portals, classifying them into Outrage, Hope, and Despair. Through Latent Dirichlet Allocation (LDA), we identified prevalent themes like political corruption and public protests, and analyzed how events such as internet blackouts shaped sentiment patterns. It outperformed multilingual transformers (mBERT: 67%, XLM-RoBERTa: 71%) and traditional machine learning methods (SVM and Logistic Regression: both 70%). These results highlight the effectiveness of language-specific models and offer valuable insights into public sentiment during political turmoil.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.CY"
      ],
      "published": "2025-12-17T15:54:37+00:00",
      "updated": "2025-12-17T15:54:37+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15547v1",
      "file": "papers/2512.15547v1.pdf"
    },
    {
      "arxiv_id": "2512.15358v1",
      "title": "Dual-Density Inference for Efficient Language Model Reasoning",
      "authors": [
        {
          "name": "Zhengyi Zhao"
        },
        {
          "name": "Shubo Zhang"
        },
        {
          "name": "Yuxi Zhang"
        },
        {
          "name": "Huimin Wang"
        },
        {
          "name": "Binyang Li"
        },
        {
          "name": "Kam-Fai Wong"
        }
      ],
      "abstract": "Large Language Models (LLMs) have shown impressive capabilities in complex reasoning tasks. However, current approaches employ uniform language density for both intermediate reasoning and final answers, leading to computational inefficiency. Our observation found that reasoning process serves a computational function for the model itself, while answering serves a communicative function for human understanding. This distinction enables the use of compressed, symbol-rich language for intermediate computations while maintaining human-readable final explanations. To address this inefficiency, we present Denser: \\underline{D}ual-d\\underline{ens}ity inf\\underline{er}ence, a novel framework that optimizes information density separately for reasoning and answering phases. Our framework implements this through three components: a query processing module that analyzes input problems, a high-density compressed reasoning mechanism for efficient intermediate computations, and an answer generation component that translates compressed reasoning into human-readable solutions. Experimental evaluation across multiple reasoning question answering benchmarks demonstrates that Denser reduces token consumption by up to 62\\% compared to standard Chain-of-Thought methods while preserving or improving accuracy. These efficiency gains are particularly significant for complex multi-step reasoning problems where traditional methods generate extensive explanations.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-17T12:04:05+00:00",
      "updated": "2025-12-17T12:04:05+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15358v1",
      "file": "papers/2512.15358v1.pdf"
    },
    {
      "arxiv_id": "2512.15353v1",
      "title": "Adversarial versification in portuguese as a jailbreak operator in LLMs",
      "authors": [
        {
          "name": "Joao Queiroz"
        }
      ],
      "abstract": "Recent evidence shows that the versification of prompts constitutes a highly effective adversarial mechanism against aligned LLMs. The study 'Adversarial poetry as a universal single-turn jailbreak mechanism in large language models' demonstrates that instructions routinely refused in prose become executable when rewritten as verse, producing up to 18 x more safety failures in benchmarks derived from MLCommons AILuminate. Manually written poems reach approximately 62% ASR, and automated versions 43%, with some models surpassing 90% success in single-turn interactions. The effect is structural: systems trained with RLHF, constitutional AI, and hybrid pipelines exhibit consistent degradation under minimal semiotic formal variation. Versification displaces the prompt into sparsely supervised latent regions, revealing guardrails that are excessively dependent on surface patterns. This dissociation between apparent robustness and real vulnerability exposes deep limitations in current alignment regimes. The absence of evaluations in Portuguese, a language with high morphosyntactic complexity, a rich metric-prosodic tradition, and over 250 million speakers, constitutes a critical gap. Experimental protocols must parameterise scansion, metre, and prosodic variation to test vulnerabilities specific to Lusophone patterns, which are currently ignored.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-17T11:55:45+00:00",
      "updated": "2025-12-17T11:55:45+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15353v1",
      "file": "papers/2512.15353v1.pdf"
    },
    {
      "arxiv_id": "2512.15312v1",
      "title": "Evaluating LLMs for Zeolite Synthesis Event Extraction (ZSEE): A Systematic Analysis of Prompting Strategies",
      "authors": [
        {
          "name": "Charan Prakash Rathore"
        },
        {
          "name": "Saumi Ray"
        },
        {
          "name": "Dhruv Kumar"
        }
      ],
      "abstract": "Extracting structured information from zeolite synthesis experimental procedures is critical for materials discovery, yet existing methods have not systematically evaluated Large Language Models (LLMs) for this domain-specific task. This work addresses a fundamental question: what is the efficacy of different prompting strategies when applying LLMs to scientific information extraction? We focus on four key subtasks: event type classification (identifying synthesis steps), trigger text identification (locating event mentions), argument role extraction (recognizing parameter types), and argument text extraction (extracting parameter values). We evaluate four prompting strategies - zero-shot, few-shot, event-specific, and reflection-based - across six state-of-the-art LLMs (Gemma-3-12b-it, GPT-5-mini, O4-mini, Claude-Haiku-3.5, DeepSeek reasoning and non-reasoning) using the ZSEE dataset of 1,530 annotated sentences. Results demonstrate strong performance on event type classification (80-90\\% F1) but modest performance on fine-grained extraction tasks, particularly argument role and argument text extraction (50-65\\% F1). GPT-5-mini exhibits extreme prompt sensitivity with 11-79\\% F1 variation. Notably, advanced prompting strategies provide minimal improvements over zero-shot approaches, revealing fundamental architectural limitations. Error analysis identifies systematic hallucination, over-generalization, and inability to capture synthesis-specific nuances. Our findings demonstrate that while LLMs achieve high-level understanding, precise extraction of experimental parameters requires domain-adapted models, providing quantitative benchmarks for scientific information extraction.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-17T11:02:31+00:00",
      "updated": "2025-12-17T11:02:31+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15312v1",
      "file": "papers/2512.15312v1.pdf"
    },
    {
      "arxiv_id": "2512.15302v1",
      "title": "Towards Proactive Personalization through Profile Customization for Individual Users in Dialogues",
      "authors": [
        {
          "name": "Xiaotian Zhang"
        },
        {
          "name": "Yuan Wang"
        },
        {
          "name": "Ruizhe Chen"
        },
        {
          "name": "Zeya Wang"
        },
        {
          "name": "Runchen Hou"
        },
        {
          "name": "Zuozhu Liu"
        }
      ],
      "abstract": "The deployment of Large Language Models (LLMs) in interactive systems necessitates a deep alignment with the nuanced and dynamic preferences of individual users. Current alignment techniques predominantly address universal human values or static, single-turn preferences, thereby failing to address the critical needs of long-term personalization and the initial user cold-start problem. To bridge this gap, we propose PersonalAgent, a novel user-centric lifelong agent designed to continuously infer and adapt to user preferences. PersonalAgent constructs and dynamically refines a unified user profile by decomposing dialogues into single-turn interactions, framing preference inference as a sequential decision-making task. Experiments show that PersonalAgent achieves superior performance over strong prompt-based and policy optimization baselines, not only in idealized but also in noisy conversational contexts, while preserving cross-session preference consistency. Furthermore, human evaluation confirms that PersonalAgent excels at capturing user preferences naturally and coherently. Our findings underscore the importance of lifelong personalization for developing more inclusive and adaptive conversational agents. Our code is available here.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-17T10:47:06+00:00",
      "updated": "2025-12-17T10:47:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15302v1",
      "file": "papers/2512.15302v1.pdf"
    },
    {
      "arxiv_id": "2512.15274v1",
      "title": "Well Begun, Half Done: Reinforcement Learning with Prefix Optimization for LLM Reasoning",
      "authors": [
        {
          "name": "Yiliu Sun"
        },
        {
          "name": "Zicheng Zhao"
        },
        {
          "name": "Yang Wei"
        },
        {
          "name": "Yanfang Zhang"
        },
        {
          "name": "Chen Gong"
        }
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) significantly enhances the reasoning capability of Large Language Models (LLMs). Current RLVR approaches typically conduct training across all generated tokens, but neglect to explore which tokens (e.g., prefix tokens) actually contribute to reasoning. This uniform training strategy spends substantial effort on optimizing low-return tokens, which in turn impedes the potential improvement from high-return tokens and reduces overall training effectiveness. To address this issue, we propose a novel RLVR approach called Progressive Prefix-token Policy Optimization (PPPO), which highlights the significance of the prefix segment of generated outputs. Specifically, inspired by the well-established human thinking theory of Path Dependence, where early-stage thoughts substantially constrain subsequent thinking trajectory, we identify an analogous phenomenon in LLM reasoning termed Beginning Lock-in Effect (BLE). PPPO leverages this finding by focusing its optimization objective on the prefix reasoning process of LLMs. This targeted optimization strategy can positively influence subsequent reasoning processes, and ultimately improve final results. To improve the learning effectiveness of LLMs on how to start reasoning with high quality, PPPO introduces two training strategies: (a) Progressive Prefix Retention, which shapes a progressive learning process by increasing the proportion of retained prefix tokens during training; (b) Continuation Accumulated Reward, which mitigates reward bias by sampling multiple continuations for one prefix token sequence, and accumulating their scores as the reward signal. Extensive experimental results on various reasoning tasks demonstrate that our proposed PPPO outperforms representative RLVR methods, with the accuracy improvements of 18.02% on only 26.17% training tokens.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-17T10:26:11+00:00",
      "updated": "2025-12-17T10:26:11+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15274v1",
      "file": "papers/2512.15274v1.pdf"
    },
    {
      "arxiv_id": "2512.15248v1",
      "title": "The Moralization Corpus: Frame-Based Annotation and Analysis of Moralizing Speech Acts across Diverse Text Genres",
      "authors": [
        {
          "name": "Maria Becker"
        },
        {
          "name": "Mirko Sommer"
        },
        {
          "name": "Lars Tapken"
        },
        {
          "name": "Yi Wan Teh"
        },
        {
          "name": "Bruno Brocai"
        }
      ],
      "abstract": "Moralizations - arguments that invoke moral values to justify demands or positions - are a yet underexplored form of persuasive communication. We present the Moralization Corpus, a novel multi-genre dataset designed to analyze how moral values are strategically used in argumentative discourse. Moralizations are pragmatically complex and often implicit, posing significant challenges for both human annotators and NLP systems. We develop a frame-based annotation scheme that captures the constitutive elements of moralizations - moral values, demands, and discourse protagonists - and apply it to a diverse set of German texts, including political debates, news articles, and online discussions. The corpus enables fine-grained analysis of moralizing language across communicative formats and domains. We further evaluate several large language models (LLMs) under varied prompting conditions for the task of moralization detection and moralization component extraction and compare it to human annotations in order to investigate the challenges of automatic and manual analysis of moralizations. Results show that detailed prompt instructions has a greater effect than few-shot or explanation-based prompting, and that moralization remains a highly subjective and context-sensitive task. We release all data, annotation guidelines, and code to foster future interdisciplinary research on moral discourse and moral reasoning in NLP.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-12-17T09:46:29+00:00",
      "updated": "2025-12-17T09:46:29+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15248v1",
      "file": "papers/2512.15248v1.pdf"
    }
  ]
}