{
  "corpus": "ml_interpretability",
  "source": "arxiv",
  "search_query": "(cat:cs.LG OR cat:cs.AI) AND (interpretability OR explainability OR XAI OR SHAP OR LIME)",
  "curated_at": "2025-12-26T18:06:56.638082+00:00",
  "total_papers": 200,
  "papers_evaluated": 706,
  "acceptance_rate": 0.28328611898017,
  "papers": [
    {
      "arxiv_id": "2512.21113v1",
      "title": "A Mechanistic Analysis of Transformers for Dynamical Systems",
      "authors": [
        {
          "name": "Gregory Duthé"
        },
        {
          "name": "Nikolaos Evangelou"
        },
        {
          "name": "Wei Liu"
        },
        {
          "name": "Ioannis G. Kevrekidis"
        },
        {
          "name": "Eleni Chatzi"
        }
      ],
      "abstract": "Transformers are increasingly adopted for modeling and forecasting time-series, yet their internal mechanisms remain poorly understood from a dynamical systems perspective. In contrast to classical autoregressive and state-space models, which benefit from well-established theoretical foundations, Transformer architectures are typically treated as black boxes. This gap becomes particularly relevant as attention-based models are considered for general-purpose or zero-shot forecasting across diverse dynamical regimes. In this work, we do not propose a new forecasting model, but instead investigate the representational capabilities and limitations of single-layer Transformers when applied to dynamical data. Building on a dynamical systems perspective we interpret causal self-attention as a linear, history-dependent recurrence and analyze how it processes temporal information. Through a series of linear and nonlinear case studies, we identify distinct operational regimes. For linear systems, we show that the convexity constraint imposed by softmax attention fundamentally restricts the class of dynamics that can be represented, leading to oversmoothing in oscillatory settings. For nonlinear systems under partial observability, attention instead acts as an adaptive delay-embedding mechanism, enabling effective state reconstruction when sufficient temporal context and latent dimensionality are available. These results help bridge empirical observations with classical dynamical systems theory, providing insight into when and why Transformers succeed or fail as models of dynamical systems.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CE"
      ],
      "published": "2025-12-24T11:21:07+00:00",
      "updated": "2025-12-24T11:21:07+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21113v1",
      "file": "papers/2512.21113v1.pdf"
    },
    {
      "arxiv_id": "2512.21066v1",
      "title": "Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation",
      "authors": [
        {
          "name": "Tomoaki Yamaguchi"
        },
        {
          "name": "Yutong Zhou"
        },
        {
          "name": "Masahiro Ryo"
        },
        {
          "name": "Keisuke Katsura"
        }
      ],
      "abstract": "Explainable artificial intelligence (XAI) enables data-driven understanding of factor associations with response variables, yet communicating XAI outputs to laypersons remains challenging, hindering trust in AI-based predictions. Large language models (LLMs) have emerged as promising tools for translating technical explanations into accessible narratives, yet the integration of agentic AI, where LLMs operate as autonomous agents through iterative refinement, with XAI remains unexplored. This study proposes an agentic XAI framework combining SHAP-based explainability with multimodal LLM-driven iterative refinement to generate progressively enhanced explanations. As a use case, we tested this framework as an agricultural recommendation system using rice yield data from 26 fields in Japan. The Agentic XAI initially provided a SHAP result and explored how to improve the explanation through additional analysis iteratively across 11 refinement rounds (Rounds 0-10). Explanations were evaluated by human experts (crop scientists) (n=12) and LLMs (n=14) against seven metrics: Specificity, Clarity, Conciseness, Practicality, Contextual Relevance, Cost Consideration, and Crop Science Credibility. Both evaluator groups confirmed that the framework successfully enhanced recommendation quality with an average score increase of 30-33% from Round 0, peaking at Rounds 3-4. However, excessive refinement showed a substantial drop in recommendation quality, indicating a bias-variance trade-off where early rounds lacked explanation depth (bias) while excessive iteration introduced verbosity and ungrounded abstraction (variance), as revealed by metric-specific analysis. These findings suggest that strategic early stopping (regularization) is needed for optimizing practical utility, challenging assumptions about monotonic improvement and providing evidence-based design principles for agentic XAI systems.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "published": "2025-12-24T09:19:15+00:00",
      "updated": "2025-12-24T09:19:15+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21066v1",
      "file": "papers/2512.21066v1.pdf"
    },
    {
      "arxiv_id": "2512.20762v1",
      "title": "Subgroup Discovery with the Cox Model",
      "authors": [
        {
          "name": "Zachary Izzo"
        },
        {
          "name": "Iain Melvin"
        }
      ],
      "abstract": "We study the problem of subgroup discovery for survival analysis, where the goal is to find an interpretable subset of the data on which a Cox model is highly accurate. Our work is the first to study this particular subgroup problem, for which we make several contributions.\n  Subgroup discovery methods generally require a \"quality function\" in order to sift through and select the most advantageous subgroups. We first examine why existing natural choices for quality functions are insufficient to solve the subgroup discovery problem for the Cox model. To address the shortcomings of existing metrics, we introduce two technical innovations: the *expected prediction entropy (EPE)*, a novel metric for evaluating survival models which predict a hazard function; and the *conditional rank statistics (CRS)*, a statistical object which quantifies the deviation of an individual point to the distribution of survival times in an existing subgroup. We study the EPE and CRS theoretically and show that they can solve many of the problems with existing metrics.\n  We introduce a total of eight algorithms for the Cox subgroup discovery problem. The main algorithm is able to take advantage of both the EPE and the CRS, allowing us to give theoretical correctness results for this algorithm in a well-specified setting. We evaluate all of the proposed methods empirically on both synthetic and real data. The experiments confirm our theory, showing that our contributions allow for the recovery of a ground-truth subgroup in well-specified cases, as well as leading to better model fit compared to naively fitting the Cox model to the whole dataset in practical settings. Lastly, we conduct a case study on jet engine simulation data from NASA. The discovered subgroups uncover known nonlinearities/homogeneity in the data, and which suggest design choices which have been mirrored in practice.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.ST",
        "stat.ML"
      ],
      "published": "2025-12-23T20:49:05+00:00",
      "updated": "2025-12-23T20:49:05+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20762v1",
      "file": "papers/2512.20762v1.pdf"
    },
    {
      "arxiv_id": "2512.20514v1",
      "title": "Explainable time-series forecasting with sampling-free SHAP for Transformers",
      "authors": [
        {
          "name": "Matthias Hertel"
        },
        {
          "name": "Sebastian Pütz"
        },
        {
          "name": "Ralf Mikut"
        },
        {
          "name": "Veit Hagenmeyer"
        },
        {
          "name": "Benjamin Schäfer"
        }
      ],
      "abstract": "Time-series forecasts are essential for planning and decision-making in many domains. Explainability is key to building user trust and meeting transparency requirements. Shapley Additive Explanations (SHAP) is a popular explainable AI framework, but it lacks efficient implementations for time series and often assumes feature independence when sampling counterfactuals. We introduce SHAPformer, an accurate, fast and sampling-free explainable time-series forecasting model based on the Transformer architecture. It leverages attention manipulation to make predictions based on feature subsets. SHAPformer generates explanations in under one second, several orders of magnitude faster than the SHAP Permutation Explainer. On synthetic data with ground truth explanations, SHAPformer provides explanations that are true to the data. Applied to real-world electrical load data, it achieves competitive predictive performance and delivers meaningful local and global insights, such as identifying the past load as the key predictor and revealing a distinct model behavior during the Christmas period.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-23T17:02:35+00:00",
      "updated": "2025-12-23T17:02:35+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20514v1",
      "file": "papers/2512.20514v1.pdf"
    },
    {
      "arxiv_id": "2512.20328v1",
      "title": "Toward Explaining Large Language Models in Software Engineering Tasks",
      "authors": [
        {
          "name": "Antonio Vitale"
        },
        {
          "name": "Khai-Nguyen Nguyen"
        },
        {
          "name": "Denys Poshyvanyk"
        },
        {
          "name": "Rocco Oliveto"
        },
        {
          "name": "Simone Scalabrino"
        },
        {
          "name": "Antonio Mastropaolo"
        }
      ],
      "abstract": "Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software engineering, existing methods lack domain-specific explanations aligned with how practitioners reason about SE artifacts. To address this gap, we introduce FeatureSHAP, the first fully automated, model-agnostic explainability framework tailored to software engineering tasks. Based on Shapley values, FeatureSHAP attributes model outputs to high-level input features through systematic input perturbation and task-specific similarity comparisons, while remaining compatible with both open-source and proprietary LLMs. We evaluate FeatureSHAP on two bi-modal SE tasks: code generation and code summarization. The results show that FeatureSHAP assigns less importance to irrelevant input features and produces explanations with higher fidelity than baseline methods. A practitioner survey involving 37 participants shows that FeatureSHAP helps practitioners better interpret model outputs and make more informed decisions. Collectively, FeatureSHAP represents a meaningful step toward practical explainable AI in software engineering. FeatureSHAP is available at https://github.com/deviserlab/FeatureSHAP.",
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-23T12:56:18+00:00",
      "updated": "2025-12-23T12:56:18+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20328v1",
      "file": "papers/2512.20328v1.pdf"
    },
    {
      "arxiv_id": "2512.20305v1",
      "title": "KAN-AFT: An Interpretable Nonlinear Survival Model Integrating Kolmogorov-Arnold Networks with Accelerated Failure Time Analysis",
      "authors": [
        {
          "name": "Mebin Jose"
        },
        {
          "name": "Jisha Francis"
        },
        {
          "name": "Sudheesh Kumar Kattumannil"
        }
      ],
      "abstract": "Survival analysis relies fundamentally on the semi-parametric Cox Proportional Hazards (CoxPH) model and the parametric Accelerated Failure Time (AFT) model. CoxPH assumes constant hazard ratios, often failing to capture real-world dynamics, while traditional AFT models are limited by rigid distributional assumptions. Although deep learning models like DeepAFT address these constraints by improving predictive accuracy and handling censoring, they inherit the significant challenge of black-box interpretability. The recent introduction of CoxKAN demonstrated the successful integration of Kolmogorov-Arnold Networks (KANs), a novel architecture that yields highly accurate and interpretable symbolic representations, within the CoxPH framework. Motivated by the interpretability gains of CoxKAN, we introduce KAN-AFT (Kolmogorov Arnold Network-based AFT), the first framework to apply KANs to the AFT model. KAN-AFT effectively models complex nonlinear relationships within the AFT framework. Our primary contributions include: (i) a principled AFT-KAN formulation, (ii) robust optimization strategies for right-censored observations (e.g., Buckley-James and IPCW), and (iii) an interpretability pipeline that converts the learned spline functions into closed-form symbolic equations for survival time. Empirical results on multiple datasets confirm that KAN-AFT achieves performance comparable to or better than DeepAFT, while uniquely providing transparent, symbolic models of the survival process.",
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-12-23T12:16:06+00:00",
      "updated": "2025-12-23T12:16:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20305v1",
      "file": "papers/2512.20305v1.pdf"
    },
    {
      "arxiv_id": "2512.20288v1",
      "title": "UbiQVision: Quantifying Uncertainty in XAI for Image Recognition",
      "authors": [
        {
          "name": "Akshat Dubey"
        },
        {
          "name": "Aleksandar Anžel"
        },
        {
          "name": "Bahar İlgen"
        },
        {
          "name": "Georges Hattab"
        }
      ],
      "abstract": "Recent advances in deep learning have led to its widespread adoption across diverse domains, including medical imaging. This progress is driven by increasingly sophisticated model architectures, such as ResNets, Vision Transformers, and Hybrid Convolutional Neural Networks, that offer enhanced performance at the cost of greater complexity. This complexity often compromises model explainability and interpretability. SHAP has emerged as a prominent method for providing interpretable visualizations that aid domain experts in understanding model predictions. However, SHAP explanations can be unstable and unreliable in the presence of epistemic and aleatoric uncertainty. In this study, we address this challenge by using Dirichlet posterior sampling and Dempster-Shafer theory to quantify the uncertainty that arises from these unstable explanations in medical imaging applications. The framework uses a belief, plausible, and fusion map approach alongside statistical quantitative analysis to produce quantification of uncertainty in SHAP. Furthermore, we evaluated our framework on three medical imaging datasets with varying class distributions, image qualities, and modality types which introduces noise due to varying image resolutions and modality-specific aspect covering the examples from pathology, ophthalmology, and radiology, introducing significant epistemic uncertainty.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-23T11:57:34+00:00",
      "updated": "2025-12-23T11:57:34+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20288v1",
      "file": "papers/2512.20288v1.pdf"
    },
    {
      "arxiv_id": "2512.20182v1",
      "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination",
      "authors": [
        {
          "name": "Shuzheng Si"
        },
        {
          "name": "Qingyi Wang"
        },
        {
          "name": "Haozhe Zhao"
        },
        {
          "name": "Yuzhuo Bai"
        },
        {
          "name": "Guanqiao Chen"
        },
        {
          "name": "Kangyang Luo"
        },
        {
          "name": "Gang Chen"
        },
        {
          "name": "Fanchao Qi"
        },
        {
          "name": "Minjia Zhang"
        },
        {
          "name": "Baobao Chang"
        },
        {
          "name": "Maosong Sun"
        }
      ],
      "abstract": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-23T09:20:32+00:00",
      "updated": "2025-12-23T09:20:32+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20182v1",
      "file": "papers/2512.20182v1.pdf"
    },
    {
      "arxiv_id": "2512.20074v1",
      "title": "Reason2Decide: Rationale-Driven Multi-Task Learning",
      "authors": [
        {
          "name": "H M Quamran Hasan"
        },
        {
          "name": "Housam Khalifa Bashier"
        },
        {
          "name": "Jiayi Dai"
        },
        {
          "name": "Mi-Young Kim"
        },
        {
          "name": "Randy Goebel"
        }
      ],
      "abstract": "Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We propose Reason2Decide, a two-stage training framework that addresses key challenges in self-rationalization, including exposure bias and task separation. In Stage-1, our model is trained on rationale generation, while in Stage-2, we jointly train on label prediction and rationale generation, applying scheduled sampling to gradually transition from conditioning on gold labels to model predictions. We evaluate Reason2Decide on three medical datasets, including a proprietary triage dataset and public biomedical QA datasets. Across model sizes, Reason2Decide outperforms other fine-tuning baselines and some zero-shot LLMs in prediction (F1) and rationale fidelity (BERTScore, BLEU, LLM-as-a-Judge). In triage, Reason2Decide is rationale source-robust across LLM-generated, nurse-authored, and nurse-post-processed rationales. In our experiments, while using only LLM-generated rationales in Stage-1, Reason2Decide outperforms other fine-tuning variants. This indicates that LLM-generated rationales are suitable for pretraining models, reducing reliance on human annotations. Remarkably, Reason2Decide achieves these gains with models 40x smaller than contemporary foundation models, making clinical reasoning more accessible for resource-constrained deployments while still providing explainable decision support.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-12-23T05:58:47+00:00",
      "updated": "2025-12-23T05:58:47+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20074v1",
      "file": "papers/2512.20074v1.pdf"
    },
    {
      "arxiv_id": "2512.20028v1",
      "title": "DecoKAN: Interpretable Decomposition for Forecasting Cryptocurrency Market Dynamics",
      "authors": [
        {
          "name": "Yuan Gao"
        },
        {
          "name": "Zhenguo Dong"
        },
        {
          "name": "Xuelong Wang"
        },
        {
          "name": "Zhiqiang Wang"
        },
        {
          "name": "Yong Zhang"
        },
        {
          "name": "Shaofan Wang"
        }
      ],
      "abstract": "Accurate and interpretable forecasting of multivariate time series is crucial for understanding the complex dynamics of cryptocurrency markets in digital asset systems. Advanced deep learning methodologies, particularly Transformer-based and MLP-based architectures, have achieved competitive predictive performance in cryptocurrency forecasting tasks. However, cryptocurrency data is inherently composed of long-term socio-economic trends and local high-frequency speculative oscillations. Existing deep learning-based 'black-box' models fail to effectively decouple these composite dynamics or provide the interpretability needed for trustworthy financial decision-making. To overcome these limitations, we propose DecoKAN, an interpretable forecasting framework that integrates multi-level Discrete Wavelet Transform (DWT) for decoupling and hierarchical signal decomposition with Kolmogorov-Arnold Network (KAN) mixers for transparent and interpretable nonlinear modeling. The DWT component decomposes complex cryptocurrency time series into distinct frequency components, enabling frequency-specific analysis, while KAN mixers provide intrinsically interpretable spline-based mappings within each decomposed subseries. Furthermore, interpretability is enhanced through a symbolic analysis pipeline involving sparsification, pruning, and symbolization, which produces concise analytical expressions offering symbolic representations of the learned patterns. Extensive experiments demonstrate that DecoKAN achieves the lowest average Mean Squared Error on all tested real-world cryptocurrency datasets (BTC, ETH, XMR), consistently outperforming a comprehensive suite of competitive state-of-the-art baselines. These results validate DecoKAN's potential to bridge the gap between predictive accuracy and model transparency, advancing trustworthy decision support within complex cryptocurrency markets.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-12-23T03:44:49+00:00",
      "updated": "2025-12-23T03:44:49+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20028v1",
      "file": "papers/2512.20028v1.pdf"
    },
    {
      "arxiv_id": "2512.19980v1",
      "title": "Neuron-Guided Interpretation of Code LLMs: Where, Why, and How?",
      "authors": [
        {
          "name": "Zhe Yin"
        },
        {
          "name": "Xiaodong Gu"
        },
        {
          "name": "Beijun Shen"
        }
      ],
      "abstract": "Code language models excel on code intelligence tasks, yet their internal interpretability is underexplored. Existing neuron interpretability techniques from NLP are suboptimal for source code due to programming languages formal, hierarchical, and executable nature. We empirically investigate code LLMs at the neuron level, localizing language-specific neurons (selectively responsive to one language) and concept layers (feed-forward layers encoding language-agnostic code representations). We analyze Llama-3.1-8B and Qwen2.5-Coder-32B on multilingual inputs in C++, Java, Python, Go, and JavaScript, measuring neuron selectivity and layerwise contributions during generation. We find (1) neurons specialized for individual languages alongside a universal subset supporting general-purpose generation; and (2) lower layers mainly encode language-specific syntax, while middle layers capture semantic abstractions shared across languages, emerging as concept layers. We demonstrate utility on three tasks: neuron-guided fine-tuning for code generation, clone detection via concept-layer embeddings, and concept-layer-guided transfer for code summarization, each yielding consistent gains in multilingual settings.",
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "published": "2025-12-23T02:04:13+00:00",
      "updated": "2025-12-23T02:04:13+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19980v1",
      "file": "papers/2512.19980v1.pdf"
    },
    {
      "arxiv_id": "2512.19941v1",
      "title": "Block-Recurrent Dynamics in Vision Transformers",
      "authors": [
        {
          "name": "Mozes Jacobs"
        },
        {
          "name": "Thomas Fel"
        },
        {
          "name": "Richard Hakim"
        },
        {
          "name": "Alessandra Brondetta"
        },
        {
          "name": "Demba Ba"
        },
        {
          "name": "T. Andy Keller"
        }
      ],
      "abstract": "As Vision Transformers (ViTs) become standard vision backbones, a mechanistic account of their computational phenomenology is essential. Despite architectural cues that hint at dynamical structure, there is no settled framework that interprets Transformer depth as a well-characterized flow. In this work, we introduce the Block-Recurrent Hypothesis (BRH), arguing that trained ViTs admit a block-recurrent depth structure such that the computation of the original $L$ blocks can be accurately rewritten using only $k \\ll L$ distinct blocks applied recurrently. Across diverse ViTs, between-layer representational similarity matrices suggest few contiguous phases. To determine whether these phases reflect genuinely reusable computation, we train block-recurrent surrogates of pretrained ViTs: Recurrent Approximations to Phase-structured TransfORmers (Raptor). In small-scale, we demonstrate that stochastic depth and training promote recurrent structure and subsequently correlate with our ability to accurately fit Raptor. We then provide an empirical existence proof for BRH by training a Raptor model to recover $96\\%$ of DINOv2 ImageNet-1k linear probe accuracy in only 2 blocks at equivalent computational cost. Finally, we leverage our hypothesis to develop a program of Dynamical Interpretability. We find i) directional convergence into class-dependent angular basins with self-correcting trajectories under small perturbations, ii) token-specific dynamics, where cls executes sharp late reorientations while patch tokens exhibit strong late-stage coherence toward their mean direction, and iii) a collapse to low rank updates in late depth, consistent with convergence to low-dimensional attractors. Altogether, we find a compact recurrent program emerges along ViT depth, pointing to a low-complexity normative solution that enables these models to be studied through principled dynamical systems analysis.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-23T00:18:23+00:00",
      "updated": "2025-12-23T00:18:23+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19941v1",
      "file": "papers/2512.19941v1.pdf"
    },
    {
      "arxiv_id": "2512.19557v1",
      "title": "Augmenting Intelligence: A Hybrid Framework for Scalable and Stable Explanations",
      "authors": [
        {
          "name": "Lawrence Krukrubo"
        },
        {
          "name": "Julius Odede"
        },
        {
          "name": "Olawande Olusegun"
        }
      ],
      "abstract": "Current approaches to Explainable AI (XAI) face a \"Scalability-Stability Dilemma.\" Post-hoc methods (e.g., LIME, SHAP) may scale easily but suffer from instability, while supervised explanation frameworks (e.g., TED) offer stability but require prohibitive human effort to label every training instance. This paper proposes a Hybrid LRR-TED framework that addresses this dilemma through a novel \"Asymmetry of Discovery.\" When applied to customer churn prediction, we demonstrate that automated rule learners (GLRM) excel at identifying broad \"Safety Nets\" (retention patterns) but struggle to capture specific \"Risk Traps\" (churn triggers)-a phenomenon we term the Anna Karenina Principle of Churn. By initialising the explanation matrix with automated safety rules and augmenting it with a Pareto-optimal set of just four human-defined risk rules, our approach achieves 94.00% predictive accuracy. This configuration outperforms the full 8-rule manual expert baseline while reducing human annotation effort by 50%, proposing a shift in the paradigm for Human-in-the-Loop AI: moving experts from the role of \"Rule Writers\" to \"Exception Handlers.\"",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-12-22T16:40:14+00:00",
      "updated": "2025-12-22T16:40:14+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19557v1",
      "file": "papers/2512.19557v1.pdf"
    },
    {
      "arxiv_id": "2512.19399v1",
      "title": "Brain-Grounded Axes for Reading and Steering LLM States",
      "authors": [
        {
          "name": "Sandro Andric"
        }
      ],
      "abstract": "Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-12-22T13:51:03+00:00",
      "updated": "2025-12-22T13:51:03+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19399v1",
      "file": "papers/2512.19399v1.pdf"
    },
    {
      "arxiv_id": "2512.19373v1",
      "title": "Cluster-Based Generalized Additive Models Informed by Random Fourier Features",
      "authors": [
        {
          "name": "Xin Huang"
        },
        {
          "name": "Jia Li"
        },
        {
          "name": "Jun Yu"
        }
      ],
      "abstract": "Explainable machine learning aims to strike a balance between prediction accuracy and model transparency, particularly in settings where black-box predictive models, such as deep neural networks or kernel-based methods, achieve strong empirical performance but remain difficult to interpret. This work introduces a mixture of generalized additive models (GAMs) in which random Fourier feature (RFF) representations are leveraged to uncover locally adaptive structure in the data. In the proposed method, an RFF-based embedding is first learned and then compressed via principal component analysis. The resulting low-dimensional representations are used to perform soft clustering of the data through a Gaussian mixture model. These cluster assignments are then applied to construct a mixture-of-GAMs framework, where each local GAM captures nonlinear effects through interpretable univariate smooth functions. Numerical experiments on real-world regression benchmarks, including the California Housing, NASA Airfoil Self-Noise, and Bike Sharing datasets, demonstrate improved predictive performance relative to classical interpretable models. Overall, this construction provides a principled approach for integrating representation learning with transparent statistical modeling.",
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-12-22T13:15:52+00:00",
      "updated": "2025-12-22T13:15:52+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19373v1",
      "file": "papers/2512.19373v1.pdf"
    },
    {
      "arxiv_id": "2512.19135v1",
      "title": "Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis",
      "authors": [
        {
          "name": "Chenghao Li"
        },
        {
          "name": "Chaoning Zhang"
        },
        {
          "name": "Yi Lu"
        },
        {
          "name": "Shuxu Chen"
        },
        {
          "name": "Xudong Wang"
        },
        {
          "name": "Jiaquan Zhang"
        },
        {
          "name": "Zhicheng Wang"
        },
        {
          "name": "Zhengxun Jin"
        },
        {
          "name": "Kuien Liu"
        },
        {
          "name": "Sung-Ho Bae"
        },
        {
          "name": "Guoqing Wang"
        },
        {
          "name": "Yang Yang"
        },
        {
          "name": "Hen Tao Shen"
        }
      ],
      "abstract": "With the development of large language models (LLMs), particularly with the introduction of the long reasoning chain technique, the reasoning ability of LLMs in complex problem-solving has been significantly enhanced. While acknowledging the power of long reasoning chains, we cannot help but wonder: Why do different reasoning chains perform differently in reasoning? What components of the reasoning chains play a key role? Existing studies mainly focus on evaluating reasoning chains from a functional perspective, with little attention paid to their structural mechanisms. To address this gap, this work is the first to analyze and evaluate the quality of the reasoning chain from a structural perspective. We apply persistent homology from Topological Data Analysis (TDA) to map reasoning steps into semantic space, extract topological features, and analyze structural changes. These changes reveal semantic coherence, logical redundancy, and identify logical breaks and gaps. By calculating homology groups, we assess connectivity and redundancy at various scales, using barcode and persistence diagrams to quantify stability and consistency. Our results show that the topological structural complexity of reasoning chains correlates positively with accuracy. More complex chains identify correct answers sooner, while successful reasoning exhibits simpler topologies, reducing redundancy and cycles, enhancing efficiency and interpretability. This work provides a new perspective on reasoning chain quality assessment and offers guidance for future optimization.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-12-22T08:28:08+00:00",
      "updated": "2025-12-22T08:28:08+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19135v1",
      "file": "papers/2512.19135v1.pdf"
    },
    {
      "arxiv_id": "2512.18986v1",
      "title": "R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression",
      "authors": [
        {
          "name": "Kun Zhao"
        },
        {
          "name": "Siyuan Dai"
        },
        {
          "name": "Yingying Zhang"
        },
        {
          "name": "Guodong Liu"
        },
        {
          "name": "Pengfei Gu"
        },
        {
          "name": "Chenghua Lin"
        },
        {
          "name": "Paul M. Thompson"
        },
        {
          "name": "Alex Leow"
        },
        {
          "name": "Heng Huang"
        },
        {
          "name": "Lifang He"
        },
        {
          "name": "Liang Zhan"
        },
        {
          "name": "Haoteng Tang"
        }
      ],
      "abstract": "Early detection of Alzheimer's disease (AD) requires models capable of integrating macro-scale neuroanatomical alterations with micro-scale genetic susceptibility, yet existing multimodal approaches struggle to align these heterogeneous signals. We introduce R-GenIMA, an interpretable multimodal large language model that couples a novel ROI-wise vision transformer with genetic prompting to jointly model structural MRI and single nucleotide polymorphisms (SNPs) variations. By representing each anatomically parcellated brain region as a visual token and encoding SNP profiles as structured text, the framework enables cross-modal attention that links regional atrophy patterns to underlying genetic factors. Applied to the ADNI cohort, R-GenIMA achieves state-of-the-art performance in four-way classification across normal cognition (NC), subjective memory concerns (SMC), mild cognitive impairment (MCI), and AD. Beyond predictive accuracy, the model yields biologically meaningful explanations by identifying stage-specific brain regions and gene signatures, as well as coherent ROI-Gene association patterns across the disease continuum. Attention-based attribution revealed genes consistently enriched for established GWAS-supported AD risk loci, including APOE, BIN1, CLU, and RBFOX1. Stage-resolved neuroanatomical signatures identified shared vulnerability hubs across disease stages alongside stage-specific patterns: striatal involvement in subjective decline, frontotemporal engagement during prodromal impairment, and consolidated multimodal network disruption in AD. These results demonstrate that interpretable multimodal AI can synthesize imaging and genetics to reveal mechanistic insights, providing a foundation for clinically deployable tools that enable earlier risk stratification and inform precision therapeutic strategies in Alzheimer's disease.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-12-22T02:54:10+00:00",
      "updated": "2025-12-22T02:54:10+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18986v1",
      "file": "papers/2512.18986v1.pdf"
    },
    {
      "arxiv_id": "2512.18930v1",
      "title": "LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer",
      "authors": [
        {
          "name": "Raina Panda"
        },
        {
          "name": "Daniel Fein"
        },
        {
          "name": "Arpita Singhal"
        },
        {
          "name": "Mark Fiore"
        },
        {
          "name": "Maneesh Agrawala"
        },
        {
          "name": "Matyas Bohacek"
        }
      ],
      "abstract": "Artistic style transfer in generative models remains a significant challenge, as existing methods often introduce style only via model fine-tuning, additional adapters, or prompt engineering, all of which can be computationally expensive and may still entangle style with subject matter. In this paper, we introduce a training- and inference-light, interpretable method for representing and transferring artistic style. Our approach leverages an art-specific Sparse Autoencoder (SAE) on top of latent embeddings of generative image models. Trained on artistic data, our SAE learns an emergent, largely disentangled set of stylistic and compositional concepts, corresponding to style-related elements pertaining brushwork, texture, and color palette, as well as semantic and structural concepts. We call it LouvreSAE and use it to construct style profiles: compact, decomposable steering vectors that enable style transfer without any model updates or optimization. Unlike prior concept-based style transfer methods, our method requires no fine-tuning, no LoRA training, and no additional inference passes, enabling direct steering of artistic styles from only a few reference images. We validate our method on ArtBench10, achieving or surpassing existing methods on style evaluations (VGG Style Loss and CLIP Score Style) while being 1.7-20x faster and, critically, interpretable.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "published": "2025-12-22T00:36:22+00:00",
      "updated": "2025-12-22T00:36:22+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18930v1",
      "file": "papers/2512.18930v1.pdf"
    },
    {
      "arxiv_id": "2512.18792v1",
      "title": "The Dead Salmons of AI Interpretability",
      "authors": [
        {
          "name": "Maxime Méloux"
        },
        {
          "name": "Giada Dirupo"
        },
        {
          "name": "François Portet"
        },
        {
          "name": "Maxime Peyrard"
        }
      ],
      "abstract": "In a striking neuroscience study, the authors placed a dead salmon in an MRI scanner and showed it images of humans in social situations. Astonishingly, standard analyses of the time reported brain regions predictive of social emotions. The explanation, of course, was not supernatural cognition but a cautionary tale about misapplied statistical inference. In AI interpretability, reports of similar ''dead salmon'' artifacts abound: feature attribution, probing, sparse auto-encoding, and even causal analyses can produce plausible-looking explanations for randomly initialized neural networks. In this work, we examine this phenomenon and argue for a pragmatic statistical-causal reframing: explanations of computational systems should be treated as parameters of a (statistical) model, inferred from computational traces. This perspective goes beyond simply measuring statistical variability of explanations due to finite sampling of input data; interpretability methods become statistical estimators, and findings should be tested against explicit and meaningful alternative computational hypotheses, with uncertainty quantified with respect to the postulated statistical model. It also highlights important theoretical issues, such as the identifiability of common interpretability queries, which we argue is critical to understand the field's susceptibility to false discoveries, poor generalizability, and high variance. More broadly, situating interpretability within the standard toolkit of statistical inference opens promising avenues for future work aimed at turning AI interpretability into a pragmatic and rigorous science.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-12-21T16:07:44+00:00",
      "updated": "2025-12-21T16:07:44+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18792v1",
      "file": "papers/2512.18792v1.pdf"
    },
    {
      "arxiv_id": "2512.18733v1",
      "title": "Explainable and Fine-Grained Safeguarding of LLM Multi-Agent Systems via Bi-Level Graph Anomaly Detection",
      "authors": [
        {
          "name": "Junjun Pan"
        },
        {
          "name": "Yixin Liu"
        },
        {
          "name": "Rui Miao"
        },
        {
          "name": "Kaize Ding"
        },
        {
          "name": "Yu Zheng"
        },
        {
          "name": "Quoc Viet Hung Nguyen"
        },
        {
          "name": "Alan Wee-Chung Liew"
        },
        {
          "name": "Shirui Pan"
        }
      ],
      "abstract": "Large language model (LLM)-based multi-agent systems (MAS) have shown strong capabilities in solving complex tasks. As MAS become increasingly autonomous in various safety-critical tasks, detecting malicious agents has become a critical security concern. Although existing graph anomaly detection (GAD)-based defenses can identify anomalous agents, they mainly rely on coarse sentence-level information and overlook fine-grained lexical cues, leading to suboptimal performance. Moreover, the lack of interpretability in these methods limits their reliability and real-world applicability. To address these limitations, we propose XG-Guard, an explainable and fine-grained safeguarding framework for detecting malicious agents in MAS. To incorporate both coarse and fine-grained textual information for anomalous agent identification, we utilize a bi-level agent encoder to jointly model the sentence- and token-level representations of each agent. A theme-based anomaly detector further captures the evolving discussion focus in MAS dialogues, while a bi-level score fusion mechanism quantifies token-level contributions for explanation. Extensive experiments across diverse MAS topologies and attack scenarios demonstrate robust detection performance and strong interpretability of XG-Guard.",
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.MA"
      ],
      "published": "2025-12-21T13:46:36+00:00",
      "updated": "2025-12-21T13:46:36+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18733v1",
      "file": "papers/2512.18733v1.pdf"
    },
    {
      "arxiv_id": "2512.18613v1",
      "title": "Text2Graph VPR: A Text-to-Graph Expert System for Explainable Place Recognition in Changing Environments",
      "authors": [
        {
          "name": "Saeideh Yousefzadeh"
        },
        {
          "name": "Hamidreza Pourreza"
        }
      ],
      "abstract": "Visual Place Recognition (VPR) in long-term deployment requires reasoning beyond pixel similarity: systems must make transparent, interpretable decisions that remain robust under lighting, weather and seasonal change. We present Text2Graph VPR, an explainable semantic localization system that converts image sequences into textual scene descriptions, parses those descriptions into structured scene graphs, and reasons over the resulting graphs to identify places. Scene graphs capture objects, attributes and pairwise relations; we aggregate per-frame graphs into a compact place representation and perform retrieval with a dual-similarity mechanism that fuses learned Graph Attention Network (GAT) embeddings and a Shortest-Path (SP) kernel for structural matching. This hybrid design enables both learned semantic matching and topology-aware comparison, and -- critically -- produces human-readable intermediate representations that support diagnostic analysis and improve transparency in the decision process. We validate the system on Oxford RobotCar and MSLS (Amman/San Francisco) benchmarks and demonstrate robust retrieval under severe appearance shifts, along with zero-shot operation using human textual queries. The results illustrate that semantic, graph-based reasoning is a viable and interpretable alternative for place recognition, particularly suited to safety-sensitive and resource-constrained settings.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-21T06:16:20+00:00",
      "updated": "2025-12-21T06:16:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18613v1",
      "file": "papers/2512.18613v1.pdf"
    },
    {
      "arxiv_id": "2512.18607v1",
      "title": "The Interaction Bottleneck of Deep Neural Networks: Discovery, Proof, and Modulation",
      "authors": [
        {
          "name": "Huiqi Deng"
        },
        {
          "name": "Qihan Ren"
        },
        {
          "name": "Zhuofan Chen"
        },
        {
          "name": "Zhenyuan Cui"
        },
        {
          "name": "Wen Shen"
        },
        {
          "name": "Peng Zhang"
        },
        {
          "name": "Hongbin Pei"
        },
        {
          "name": "Quanshi Zhang"
        }
      ],
      "abstract": "Understanding what kinds of cooperative structures deep neural networks (DNNs) can represent remains a fundamental yet insufficiently understood problem. In this work, we treat interactions as the fundamental units of such structure and investigate a largely unexplored question: how DNNs encode interactions under different levels of contextual complexity, and how these microscopic interaction patterns shape macroscopic representation capacity. To quantify this complexity, we use multi-order interactions [57], where each order reflects the amount of contextual information required to evaluate the joint interaction utility of a variable pair. This formulation enables a stratified analysis of cooperative patterns learned by DNNs. Building on this formulation, we develop a comprehensive study of interaction structure in DNNs. (i) We empirically discover a universal interaction bottleneck: across architectures and tasks, DNNs easily learn low-order and high-order interactions but consistently under-represent mid-order ones. (ii) We theoretically explain this bottleneck by proving that mid-order interactions incur the highest contextual variability, yielding large gradient variance and making them intrinsically difficult to learn. (iii) We further modulate the bottleneck by introducing losses that steer models toward emphasizing interactions of selected orders. Finally, we connect microscopic interaction structures with macroscopic representational behavior: low-order-emphasized models exhibit stronger generalization and robustness, whereas high-order-emphasized models demonstrate greater structural modeling and fitting capability. Together, these results uncover an inherent representational bias in modern DNNs and establish interaction order as a powerful lens for interpreting and guiding deep representations.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-12-21T05:55:11+00:00",
      "updated": "2025-12-21T05:55:11+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18607v1",
      "file": "papers/2512.18607v1.pdf"
    },
    {
      "arxiv_id": "2512.18473v1",
      "title": "APC-GNN++: An Adaptive Patient-Centric GNN with Context-Aware Attention and Mini-Graph Explainability for Diabetes Classification",
      "authors": [
        {
          "name": "Khaled Berkani"
        }
      ],
      "abstract": "We propose APC-GNN++, an adaptive patient-centric Graph Neural Network for diabetes classification. Our model integrates context-aware edge attention, confidence-guided blending of node features and graph representations, and neighborhood consistency regularization to better capture clinically meaningful relationships between patients. To handle unseen patients, we introduce a mini-graph approach that leverages the nearest neighbors of the new patient, enabling real-time explainable predictions without retraining the global model. We evaluate APC-GNN++ on a real-world diabetes dataset collected from a regional hospital in Algeria and show that it outperforms traditional machine learning models (MLP, Random Forest, XGBoost) and a vanilla GCN, achieving higher test accuracy and macro F1- score. The analysis of node-level confidence scores further reveals how the model balances self-information and graph-based evidence across different patient groups, providing interpretable patient-centric insights. The system is also embedded in a Tkinter-based graphical user interface (GUI) for interactive use by healthcare professionals .",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-20T19:12:45+00:00",
      "updated": "2025-12-20T19:12:45+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18473v1",
      "file": "papers/2512.18473v1.pdf"
    },
    {
      "arxiv_id": "2512.18412v1",
      "title": "Few-Shot Learning of a Graph-Based Neural Network Model Without Backpropagation",
      "authors": [
        {
          "name": "Mykyta Lapin"
        },
        {
          "name": "Kostiantyn Bokhan"
        },
        {
          "name": "Yurii Parzhyn"
        }
      ],
      "abstract": "We propose a structural-graph approach to classifying contour images in a few-shot regime without using backpropagation. The core idea is to make structure the carrier of explanations: an image is encoded as an attributed graph (critical points and lines represented as nodes with geometric attributes), and generalization is achieved via the formation of concept attractors (class-level concept graphs). Purpose. To design and experimentally validate an architecture in which class concepts are formed from a handful of examples (5 - 6 per class) through structural and parametric reductions, providing transparent decisions and eliminating backpropagation. Methods. Contour vectorization is followed by constructing a bipartite graph (Point/Line as nodes) with normalized geometric attributes such as coordinates, length, angle, and direction; reductions include the elimination of unstable substructures or noise and the alignment of paths between critical points. Concepts are formed by iterative composition of samples, and classification is performed by selecting the best graph-to-concept match (using approximated GED). Results. On an MNIST subset with 5 - 6 base examples per class (single epoch), we obtain a consistent accuracy of around 82% with full traceability of decisions: misclassifications can be explained by explicit structural similarities. An indicative comparison with SVM, MLP, CNN, as well as metric and meta-learning baselines, is provided. The structural-graph scheme with concept attractors enables few-shot learning without backpropagation and offers built-in explanations through the explicit graph structure. Limitations concern the computational cost of GED and the quality of skeletonization; promising directions include classification-algorithm optimization, work with static scenes, and associative recognition.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-12-20T16:23:51+00:00",
      "updated": "2025-12-20T16:23:51+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18412v1",
      "file": "papers/2512.18412v1.pdf"
    },
    {
      "arxiv_id": "2512.18317v1",
      "title": "Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems",
      "authors": [
        {
          "name": "Vincent Bezold"
        },
        {
          "name": "Patrick Wagner"
        },
        {
          "name": "Jakob Hofmann"
        },
        {
          "name": "Marco Huber"
        },
        {
          "name": "Alexander Sauer"
        }
      ],
      "abstract": "This paper presents a trustworthy reinforcement learning approach for the control of industrial compressed air systems. We develop a framework that enables safe and energy-efficient operation under realistic boundary conditions and introduce a multi-level explainability pipeline combining input perturbation tests, gradient-based sensitivity analysis, and SHAP (SHapley Additive exPlanations) feature attribution. An empirical evaluation across multiple compressor configurations shows that the learned policy is physically plausible, anticipates future demand, and consistently respects system boundaries. Compared to the installed industrial controller, the proposed approach reduces unnecessary overpressure and achieves energy savings of approximately 4\\,\\% without relying on explicit physics models. The results further indicate that system pressure and forecast information dominate policy decisions, while compressor-level inputs play a secondary role. Overall, the combination of efficiency gains, predictive behavior, and transparent validation supports the trustworthy deployment of reinforcement learning in industrial energy systems.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "published": "2025-12-20T11:11:49+00:00",
      "updated": "2025-12-20T11:11:49+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18317v1",
      "file": "papers/2512.18317v1.pdf"
    },
    {
      "arxiv_id": "2512.18199v1",
      "title": "PROVEX: Enhancing SOC Analyst Trust with Explainable Provenance-Based IDS",
      "authors": [
        {
          "name": "Devang Dhanuka"
        },
        {
          "name": "Nidhi Rastogi"
        }
      ],
      "abstract": "Modern intrusion detection systems (IDS) leverage graph neural networks (GNNs) to detect malicious activity in system provenance data, but their decisions often remain a black box to analysts. This paper presents a comprehensive XAI framework designed to bridge the trust gap in Security Operations Centers (SOCs) by making graph-based detection transparent. We implement this framework on top of KAIROS, a state-of-the-art temporal graph-based IDS, though our design is applicable to any temporal graph-based detector with minimal adaptation. The complete codebase is available at https://github.com/devang1304/provex.git. We augment the detection pipeline with post-hoc explanations that highlight why an alert was triggered, identifying key causal subgraphs and events. We adapt three GNN explanation methods - GraphMask, GNNExplainer, and a variational temporal GNN explainer (VA-TGExplainer) - to the temporal provenance context. These tools output human-interpretable representations of anomalous behavior, including important edges and uncertainty estimates. Our contributions focus on the practical integration of these explainers, addressing challenges in memory management and reproducibility. We demonstrate our framework on the DARPA CADETS Engagement 3 dataset and show that it produces concise window-level explanations for detected attacks. Our evaluation reveals that the explainers preserve the TGNN's decisions with high fidelity, surfacing critical edges such as malicious file interactions and anomalous netflows. The average explanation overhead is 3-5 seconds per event. By providing insight into the model's reasoning, our framework aims to improve analyst trust and triage speed.",
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published": "2025-12-20T03:45:21+00:00",
      "updated": "2025-12-20T03:45:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18199v1",
      "file": "papers/2512.18199v1.pdf"
    },
    {
      "arxiv_id": "2512.18177v1",
      "title": "NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI",
      "authors": [
        {
          "name": "Midhat Urooj"
        },
        {
          "name": "Ayan Banerjee"
        },
        {
          "name": "Sandeep Gupta"
        }
      ],
      "abstract": "Accurate yet interpretable image-based diagnosis remains a central challenge in medical AI, particularly in settings characterized by limited data, subtle visual cues, and high-stakes clinical decision-making. Most existing vision models rely on purely data-driven learning and produce black-box predictions with limited interpretability and poor cross-domain generalization, hindering their real-world clinical adoption. We present NEURO-GUARD, a novel knowledge-guided vision framework that integrates Vision Transformers (ViTs) with language-driven reasoning to improve performance, transparency, and domain robustness. NEURO-GUARD employs a retrieval-augmented generation (RAG) mechanism for self-verification, in which a large language model (LLM) iteratively generates, evaluates, and refines feature-extraction code for medical images. By grounding this process in clinical guidelines and expert knowledge, the framework progressively enhances feature detection and classification beyond purely data-driven baselines. Extensive experiments on diabetic retinopathy classification across four benchmark datasets APTOS, EyePACS, Messidor-1, and Messidor-2 demonstrate that NEURO-GUARD improves accuracy by 6.2% over a ViT-only baseline (84.69% vs. 78.4%) and achieves a 5% gain in domain generalization. Additional evaluations on MRI-based seizure detection further confirm its cross-domain robustness, consistently outperforming existing methods.\n  Overall, NEURO-GUARD bridges symbolic medical reasoning with subsymbolic visual learning, enabling interpretable, knowledge-aware, and generalizable medical image diagnosis while achieving state-of-the-art performance across multiple datasets.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-12-20T02:32:15+00:00",
      "updated": "2025-12-20T02:32:15+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18177v1",
      "file": "papers/2512.18177v1.pdf"
    },
    {
      "arxiv_id": "2512.18092v1",
      "title": "Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability",
      "authors": [
        {
          "name": "Ge Yan"
        },
        {
          "name": "Tuomas Oikarinen"
        },
        {
          "name": "Tsui-Wei"
        },
        {
          "name": "Weng"
        }
      ],
      "abstract": "Neuron identification is a popular tool in mechanistic interpretability, aiming to uncover the human-interpretable concepts represented by individual neurons in deep networks. While algorithms such as Network Dissection and CLIP-Dissect achieve great empirical success, a rigorous theoretical foundation remains absent, which is crucial to enable trustworthy and reliable explanations. In this work, we observe that neuron identification can be viewed as the inverse process of machine learning, which allows us to derive guarantees for neuron explanations. Based on this insight, we present the first theoretical analysis of two fundamental challenges: (1) Faithfulness: whether the identified concept faithfully represents the neuron's underlying function and (2) Stability: whether the identification results are consistent across probing datasets. We derive generalization bounds for widely used similarity metrics (e.g. accuracy, AUROC, IoU) to guarantee faithfulness, and propose a bootstrap ensemble procedure that quantifies stability along with BE (Bootstrap Explanation) method to generate concept prediction sets with guaranteed coverage probability. Experiments on both synthetic and real data validate our theoretical results and demonstrate the practicality of our method, providing an important step toward trustworthy neuron identification.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-19T21:55:17+00:00",
      "updated": "2025-12-19T21:55:17+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18092v1",
      "file": "papers/2512.18092v1.pdf"
    },
    {
      "arxiv_id": "2512.18056v1",
      "title": "Probabilistic Digital Twins of Users: Latent Representation Learning with Statistically Validated Semantics",
      "authors": [
        {
          "name": "Daniel David"
        }
      ],
      "abstract": "Understanding user identity and behavior is central to applications such as personalization, recommendation, and decision support. Most existing approaches rely on deterministic embeddings or black-box predictive models, offering limited uncertainty quantification and little insight into what latent representations encode. We propose a probabilistic digital twin framework in which each user is modeled as a latent stochastic state that generates observed behavioral data. The digital twin is learned via amortized variational inference, enabling scalable posterior estimation while retaining a fully probabilistic interpretation. We instantiate this framework using a variational autoencoder (VAE) applied to a user-response dataset designed to capture stable aspects of user identity. Beyond standard reconstruction-based evaluation, we introduce a statistically grounded interpretation pipeline that links latent dimensions to observable behavioral patterns. By analyzing users at the extremes of each latent dimension and validating differences using nonparametric hypothesis tests and effect sizes, we demonstrate that specific dimensions correspond to interpretable traits such as opinion strength and decisiveness. Empirically, we find that user structure is predominantly continuous rather than discretely clustered, with weak but meaningful structure emerging along a small number of dominant latent axes. These results suggest that probabilistic digital twins can provide interpretable, uncertainty-aware representations that go beyond deterministic user embeddings.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.SI"
      ],
      "published": "2025-12-19T20:49:51+00:00",
      "updated": "2025-12-19T20:49:51+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.18056v1",
      "file": "papers/2512.18056v1.pdf"
    },
    {
      "arxiv_id": "2512.17689v1",
      "title": "Imputation Uncertainty in Interpretable Machine Learning Methods",
      "authors": [
        {
          "name": "Pegah Golchian"
        },
        {
          "name": "Marvin N. Wright"
        }
      ],
      "abstract": "In real data, missing values occur frequently, which affects the interpretation with interpretable machine learning (IML) methods. Recent work considers bias and shows that model explanations may differ between imputation methods, while ignoring additional imputation uncertainty and its influence on variance and confidence intervals. We therefore compare the effects of different imputation methods on the confidence interval coverage probabilities of the IML methods permutation feature importance, partial dependence plots and Shapley values. We show that single imputation leads to underestimation of variance and that, in most cases, only multiple imputation is close to nominal coverage.",
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "published": "2025-12-19T15:24:49+00:00",
      "updated": "2025-12-19T15:24:49+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17689v1",
      "file": "papers/2512.17689v1.pdf"
    },
    {
      "arxiv_id": "2512.17559v1",
      "title": "Towards Explainable Conversational AI for Early Diagnosis with Large Language Models",
      "authors": [
        {
          "name": "Maliha Tabassum"
        },
        {
          "name": "M Shamim Kaiser"
        }
      ],
      "abstract": "Healthcare systems around the world are grappling with issues like inefficient diagnostics, rising costs, and limited access to specialists. These problems often lead to delays in treatment and poor health outcomes. Most current AI and deep learning diagnostic systems are not very interactive or transparent, making them less effective in real-world, patient-centered environments. This research introduces a diagnostic chatbot powered by a Large Language Model (LLM), using GPT-4o, Retrieval-Augmented Generation, and explainable AI techniques. The chatbot engages patients in a dynamic conversation, helping to extract and normalize symptoms while prioritizing potential diagnoses through similarity matching and adaptive questioning. With Chain-of-Thought prompting, the system also offers more transparent reasoning behind its diagnoses. When tested against traditional machine learning models like Naive Bayes, Logistic Regression, SVM, Random Forest, and KNN, the LLM-based system delivered impressive results, achieving an accuracy of 90% and Top-3 accuracy of 100%. These findings offer a promising outlook for more transparent, interactive, and clinically relevant AI in healthcare.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-12-19T13:28:50+00:00",
      "updated": "2025-12-19T13:28:50+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17559v1",
      "file": "papers/2512.17559v1.pdf"
    },
    {
      "arxiv_id": "2512.17325v1",
      "title": "Task Schema and Binding: A Double Dissociation Study of In-Context Learning",
      "authors": [
        {
          "name": "Chaeha Kim"
        }
      ],
      "abstract": "We provide causal mechanistic validation that in-context learning (ICL) decomposes into two separable mechanisms: Task Schema (abstract task type recognition) and Binding (specific input-output associations). Through activation patching experiments across 9 models from 7 Transformer families plus Mamba (370M-13B parameters), we establish three key findings:\n  1. Double dissociation: Task Schema transfers at 100% via late MLP patching; Binding transfers at 62% via residual stream patching -- proving separable mechanisms\n  2. Prior-Schema trade-off: Schema reliance inversely correlates with prior knowledge (Spearman rho = -0.596, p < 0.001, N=28 task-model pairs)\n  3. Architecture generality: The mechanism operates across all tested architectures including the non-Transformer Mamba\n  These findings offer a mechanistic account of the ICL puzzle that contrasts with prior views treating ICL as a monolithic mechanism (whether retrieval-based, gradient descent-like, or purely Bayesian). By establishing that Schema and Binding are neurally dissociable -- not merely behavioral modes -- we provide causal evidence for dual-process theories of ICL. Models rely on Task Schema when prior knowledge is absent, but prior knowledge interferes through attentional mis-routing (72.7% recency bias) rather than direct output competition (0%). This explains why arbitrary mappings succeed (zero prior leads to full Schema reliance) while factual overrides fail -- and reveals that the true bottleneck is attentional, not output-level. Practical implications: Understanding these dual mechanisms enables more efficient prompt engineering -- reliable schema transfer reduces required demonstrations for novel tasks, while prior-aware design can mitigate the 38% binding failure rate in high-prior scenarios, improving ICL system reliability in production deployments.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2025-12-19T08:14:21+00:00",
      "updated": "2025-12-19T08:14:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17325v1",
      "file": "papers/2512.17325v1.pdf"
    },
    {
      "arxiv_id": "2512.17316v1",
      "title": "Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability",
      "authors": [
        {
          "name": "Michael Merry"
        },
        {
          "name": "Pat Riddle"
        },
        {
          "name": "Jim Warren"
        }
      ],
      "abstract": "Inherent explainability is the gold standard in Explainable Artificial Intelligence (XAI). However, there is not a consistent definition or test to demonstrate inherent explainability. Work to date either characterises explainability through metrics, or appeals to intuition - \"we know it when we see it\". We propose a globally applicable criterion for inherent explainability. The criterion uses graph theory for representing and decomposing models for structure-local explanation, and recomposing them into global explanations. We form the structure-local explanations as annotations, a verifiable hypothesis-evidence structure that allows for a range of explanatory methods to be used. This criterion matches existing intuitions on inherent explainability, and provides justifications why a large regression model may not be explainable but a sparse neural network could be. We differentiate explainable -- a model that allows for explanation -- and \\textit{explained} -- one that has a verified explanation. Finally, we provide a full explanation of PREDICT -- a Cox proportional hazards model of cardiovascular disease risk, which is in active clinical use in New Zealand. It follows that PREDICT is inherently explainable. This work provides structure to formalise other work on explainability, and allows regulators a flexible but rigorous test that can be used in compliance frameworks.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-12-19T07:59:36+00:00",
      "updated": "2025-12-19T07:59:36+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17316v1",
      "file": "papers/2512.17316v1.pdf"
    },
    {
      "arxiv_id": "2512.17255v1",
      "title": "From Priors to Predictions: Explaining and Visualizing Human Reasoning in a Graph Neural Network Framework",
      "authors": [
        {
          "name": "Quan Do"
        },
        {
          "name": "Caroline Ahn"
        },
        {
          "name": "Leah Bakst"
        },
        {
          "name": "Michael Pascale"
        },
        {
          "name": "Joseph T. McGuire"
        },
        {
          "name": "Chantal E. Stern"
        },
        {
          "name": "Michael E. Hasselmo"
        }
      ],
      "abstract": "Humans excel at solving novel reasoning problems from minimal exposure, guided by inductive biases, assumptions about which entities and relationships matter. Yet the computational form of these biases and their neural implementation remain poorly understood. We introduce a framework that combines Graph Theory and Graph Neural Networks (GNNs) to formalize inductive biases as explicit, manipulable priors over structure and abstraction. Using a human behavioral dataset adapted from the Abstraction and Reasoning Corpus (ARC), we show that differences in graph-based priors can explain individual differences in human solutions. Our method includes an optimization pipeline that searches over graph configurations, varying edge connectivity and node abstraction, and a visualization approach that identifies the computational graph, the subset of nodes and edges most critical to a model's prediction. Systematic ablation reveals how generalization depends on specific prior structures and internal processing, exposing why human like errors emerge from incorrect or incomplete priors. This work provides a principled, interpretable framework for modeling the representational assumptions and computational dynamics underlying generalization, offering new insights into human reasoning and a foundation for more human aligned AI systems.",
      "primary_category": "q-bio.NC",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "published": "2025-12-19T05:56:48+00:00",
      "updated": "2025-12-19T05:56:48+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17255v1",
      "file": "papers/2512.17255v1.pdf"
    },
    {
      "arxiv_id": "2512.17194v1",
      "title": "MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation",
      "authors": [
        {
          "name": "Shengwei Zhao"
        },
        {
          "name": "Jingwen Yao"
        },
        {
          "name": "Sitong Wei"
        },
        {
          "name": "Linhai Xu"
        },
        {
          "name": "Yuying Liu"
        },
        {
          "name": "Dong Zhang"
        },
        {
          "name": "Zhiqiang Tian"
        },
        {
          "name": "Shaoyi Du"
        }
      ],
      "abstract": "Multi-modal Retrieval-Augmented Generation (MMRAG) enables highly credible generation by integrating external multi-modal knowledge, thus demonstrating impressive performance in complex multi-modal scenarios. However, existing MMRAG methods fail to clarify the reasoning logic behind retrieval and response generation, which limits the explainability of the results. To address this gap, we propose to introduce reinforcement learning into multi-modal retrieval-augmented generation, enhancing the reasoning capabilities of multi-modal large language models through a two-stage reinforcement fine-tuning framework to achieve explainable multi-modal retrieval-augmented generation. Specifically, in the first stage, rule-based reinforcement fine-tuning is employed to perform coarse-grained point-wise ranking of multi-modal documents, effectively filtering out those that are significantly irrelevant. In the second stage, reasoning-based reinforcement fine-tuning is utilized to jointly optimize fine-grained list-wise ranking and answer generation, guiding multi-modal large language models to output explainable reasoning logic in the MMRAG process. Our method achieves state-of-the-art results on WebQA and MultimodalQA, two benchmark datasets for multi-modal retrieval-augmented generation, and its effectiveness is validated through comprehensive ablation experiments.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-12-19T03:19:54+00:00",
      "updated": "2025-12-19T03:19:54+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17194v1",
      "file": "papers/2512.17194v1.pdf"
    },
    {
      "arxiv_id": "2512.17172v1",
      "title": "PILAR: Personalizing Augmented Reality Interactions with LLM-based Human-Centric and Trustworthy Explanations for Daily Use Cases",
      "authors": [
        {
          "name": "Ripan Kumar Kundu"
        },
        {
          "name": "Istiak Ahmed"
        },
        {
          "name": "Khaza Anuarul Hoque"
        }
      ],
      "abstract": "Artificial intelligence (AI)-driven augmented reality (AR) systems are becoming increasingly integrated into daily life, and with this growth comes a greater need for explainability in real-time user interactions. Traditional explainable AI (XAI) methods, which often rely on feature-based or example-based explanations, struggle to deliver dynamic, context-specific, personalized, and human-centric insights for everyday AR users. These methods typically address separate explainability dimensions (e.g., when, what, how) with different explanation techniques, resulting in unrealistic and fragmented experiences for seamless AR interactions. To address this challenge, we propose PILAR, a novel framework that leverages a pre-trained large language model (LLM) to generate context-aware, personalized explanations, offering a more intuitive and trustworthy experience in real-time AI-powered AR systems. Unlike traditional methods, which rely on multiple techniques for different aspects of explanation, PILAR employs a unified LLM-based approach that dynamically adapts explanations to the user's needs, fostering greater trust and engagement. We implement the PILAR concept in a real-world AR application (e.g., personalized recipe recommendations), an open-source prototype that integrates real-time object detection, recipe recommendation, and LLM-based personalized explanations of the recommended recipes based on users' dietary preferences. We evaluate the effectiveness of PILAR through a user study with 16 participants performing AR-based recipe recommendation tasks, comparing an LLM-based explanation interface to a traditional template-based one. Results show that the LLM-based interface significantly enhances user performance and experience, with participants completing tasks 40% faster and reporting greater satisfaction, ease of use, and perceived transparency.",
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "published": "2025-12-19T02:19:38+00:00",
      "updated": "2025-12-19T02:19:38+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17172v1",
      "file": "papers/2512.17172v1.pdf"
    },
    {
      "arxiv_id": "2512.17121v1",
      "title": "The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining",
      "authors": [
        {
          "name": "Jasmine Vu"
        },
        {
          "name": "Shivanand Sheshappanavar"
        }
      ],
      "abstract": "Large vision-language models like CLIP are increasingly used in medical imaging tasks due to their ability to align images and text without the need for extensive labeled data. This makes them particularly useful for applications like image retrieval, report generation, and classification in clinical settings. A potential issue to this approach is that CLIP-based models often under perform when interpreting negated phrases, which is especially problematic in the context of medical diagnosing. In this study, we evaluate the Stanford AIMI CheXagent model on its ability to correctly retrieve chest X-ray images using prompts with and without negation. The goal of this project is to understand where this model fails and then use it as a base model to improve its retrieval accuracy by fine tuning methods outlined in previous work. Results from this study show improvement in handling of negation in the CLIP model with a slight decrease in accuracy of positive prompt evaluation. Alongside retrieval accuracy, we examined internal model behavior through token attribution, t-SNE projection, and attention-head ablation to better characterize how each fine tuning approach reshaped the text encoders representation of negated clinical language. Through this work, we hope to better understand the internal behavior of CLIP and improve its handling of negation using clinically relevant language for improving its reliability in medical AI devices.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-18T23:19:19+00:00",
      "updated": "2025-12-18T23:19:19+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17121v1",
      "file": "papers/2512.17121v1.pdf"
    },
    {
      "arxiv_id": "2512.17100v2",
      "title": "UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data",
      "authors": [
        {
          "name": "Justin Li"
        },
        {
          "name": "Efe Sencan"
        },
        {
          "name": "Jasper Zheng Duan"
        },
        {
          "name": "Vitus J. Leung"
        },
        {
          "name": "Stephen Tsaur"
        },
        {
          "name": "Ayse K. Coskun"
        }
      ],
      "abstract": "Machine learning models, particularly deep neural networks, have demonstrated strong performance in classifying complex time series data. However, their black-box nature limits trust and adoption, especially in high-stakes domains such as healthcare. To address this challenge, we introduce UniCoMTE, a model-agnostic framework for generating counterfactual explanations for multivariate time series classifiers. The framework identifies temporal features that most heavily influence a model's prediction by modifying the input sample and assessing its impact on the model's prediction. UniCoMTE is compatible with a wide range of model architectures and operates directly on raw time series inputs. In this study, we evaluate UniCoMTE's explanations on a time series ECG classifier. We quantify explanation quality by comparing our explanations' comprehensibility to comprehensibility of established techniques (LIME and SHAP) and assessing their generalizability to similar samples. Furthermore, clinical utility is assessed through a questionnaire completed by medical experts who review counterfactual explanations presented alongside original ECG samples. Results show that our approach produces concise, stable, and human-aligned explanations that outperform existing methods in both clarity and applicability. By linking model predictions to meaningful signal patterns, the framework advances the interpretability of deep learning models for real-world time series applications.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-12-18T21:56:08+00:00",
      "updated": "2025-12-22T02:23:31+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17100v2",
      "file": "papers/2512.17100v2.pdf"
    },
    {
      "arxiv_id": "2512.16921v1",
      "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
      "authors": [
        {
          "name": "Qihao Liu"
        },
        {
          "name": "Chengzhi Mao"
        },
        {
          "name": "Yaojie Liu"
        },
        {
          "name": "Alan Yuille"
        },
        {
          "name": "Wen-Sheng Chu"
        }
      ],
      "abstract": "Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-18T18:59:57+00:00",
      "updated": "2025-12-18T18:59:57+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16921v1",
      "file": "papers/2512.16921v1.pdf"
    },
    {
      "arxiv_id": "2512.16901v1",
      "title": "Impacts of Racial Bias in Historical Training Data for News AI",
      "authors": [
        {
          "name": "Rahul Bhargava"
        },
        {
          "name": "Malene Hornstrup Jespersen"
        },
        {
          "name": "Emily Boardman Ndulue"
        },
        {
          "name": "Vivica Dsouza"
        }
      ],
      "abstract": "AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning \"blacks\" thematic topic label. Through quantitative and qualitative means we investigate this label's use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the \"blacks\" label operates partially as a general \"racism detector\" across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "published": "2025-12-18T18:56:11+00:00",
      "updated": "2025-12-18T18:56:11+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16901v1",
      "file": "papers/2512.16901v1.pdf"
    },
    {
      "arxiv_id": "2512.16733v2",
      "title": "Discovering and Learning Probabilistic Models of Black-Box AI Capabilities",
      "authors": [
        {
          "name": "Daniel Bramblett"
        },
        {
          "name": "Rushang Karia"
        },
        {
          "name": "Adrian Ciotinga"
        },
        {
          "name": "Ruthvick Suresh"
        },
        {
          "name": "Pulkit Verma"
        },
        {
          "name": "YooJung Choi"
        },
        {
          "name": "Siddharth Srivastava"
        }
      ],
      "abstract": "Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-12-18T16:32:06+00:00",
      "updated": "2025-12-20T18:26:12+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16733v2",
      "file": "papers/2512.16733v2.pdf"
    },
    {
      "arxiv_id": "2512.16700v1",
      "title": "CLARiTy: A Vision Transformer for Multi-Label Classification and Weakly-Supervised Localization of Chest X-ray Pathologies",
      "authors": [
        {
          "name": "John M. Statheros"
        },
        {
          "name": "Hairong Wang"
        },
        {
          "name": "Richard Klein"
        }
      ],
      "abstract": "The interpretation of chest X-rays (CXRs) poses significant challenges, particularly in achieving accurate multi-label pathology classification and spatial localization. These tasks demand different levels of annotation granularity but are frequently constrained by the scarcity of region-level (dense) annotations. We introduce CLARiTy (Class Localizing and Attention Refining Image Transformer), a vision transformer-based model for joint multi-label classification and weakly-supervised localization of thoracic pathologies. CLARiTy employs multiple class-specific tokens to generate discriminative attention maps, and a SegmentCAM module for foreground segmentation and background suppression using explicit anatomical priors. Trained on image-level labels from the NIH ChestX-ray14 dataset, it leverages distillation from a ConvNeXtV2 teacher for efficiency. Evaluated on the official NIH split, the CLARiTy-S-16-512 (a configuration of CLARiTy), achieves competitive classification performance across 14 pathologies, and state-of-the-art weakly-supervised localization performance on 8 pathologies, outperforming prior methods by 50.7%. In particular, pronounced gains occur for small pathologies like nodules and masses. The lower-resolution variant of CLARiTy, CLARiTy-S-16-224, offers high efficiency while decisively surpassing baselines, thereby having the potential for use in low-resource settings. An ablation study confirms contributions of SegmentCAM, DINO pretraining, orthogonal class token loss, and attention pooling. CLARiTy advances beyond CNN-ViT hybrids by harnessing ViT self-attention for global context and class-specific localization, refined through convolutional background suppression for precise, noise-reduced heatmaps.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-18T16:04:55+00:00",
      "updated": "2025-12-18T16:04:55+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16700v1",
      "file": "papers/2512.16700v1.pdf"
    },
    {
      "arxiv_id": "2512.16614v1",
      "title": "Don't Guess, Escalate: Towards Explainable Uncertainty-Calibrated AI Forensic Agents",
      "authors": [
        {
          "name": "Giulia Boato"
        },
        {
          "name": "Andrea Montibeller"
        },
        {
          "name": "Edward Delp"
        },
        {
          "name": "Luisa Verdoliva"
        },
        {
          "name": "Daniele Miorandi"
        }
      ],
      "abstract": "AI is reshaping the landscape of multimedia forensics. We propose AI forensic agents: reliable orchestrators that select and combine forensic detectors, identify provenance and context, and provide uncertainty-aware assessments. We highlight pitfalls in current solutions and introduce a unified framework to improve the authenticity verification process.",
      "primary_category": "cs.MA",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-12-18T14:52:57+00:00",
      "updated": "2025-12-18T14:52:57+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16614v1",
      "file": "papers/2512.16614v1.pdf"
    },
    {
      "arxiv_id": "2512.16484v1",
      "title": "Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment",
      "authors": [
        {
          "name": "Yuan Li"
        },
        {
          "name": "Yahan Yu"
        },
        {
          "name": "Youyuan Lin"
        },
        {
          "name": "Yong-Hao Yang"
        },
        {
          "name": "Chenhui Chu"
        },
        {
          "name": "Shin'ya Nishida"
        }
      ],
      "abstract": "Humans assess image quality through a perception-reasoning cascade, integrating sensory cues with implicit reasoning to form self-consistent judgments. In this work, we investigate how a model can acquire both human-like and self-consistent reasoning capability for blind image quality assessment (BIQA). We first collect human evaluation data that capture several aspects of human perception-reasoning pipeline. Then, we adopt reinforcement learning, using human annotations as reward signals to guide the model toward human-like perception and reasoning. To enable the model to internalize self-consistent reasoning capability, we design a reward that drives the model to infer the image quality purely from self-generated descriptions. Empirically, our approach achieves score prediction performance comparable to state-of-the-art BIQA systems under general metrics, including Pearson and Spearman correlation coefficients. In addition to the rating score, we assess human-model alignment using ROUGE-1 to measure the similarity between model-generated and human perception-reasoning chains. On over 1,000 human-annotated samples, our model reaches a ROUGE-1 score of 0.512 (cf. 0.443 for baseline), indicating substantial coverage of human explanations and marking a step toward human-like interpretable reasoning in BIQA.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-18T12:52:37+00:00",
      "updated": "2025-12-18T12:52:37+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16484v1",
      "file": "papers/2512.16484v1.pdf"
    },
    {
      "arxiv_id": "2512.16468v1",
      "title": "Quantifying and Bridging the Fidelity Gap: A Decisive-Feature Approach to Comparing Synthetic and Real Imagery",
      "authors": [
        {
          "name": "Danial Safaei"
        },
        {
          "name": "Siddartha Khastgir"
        },
        {
          "name": "Mohsen Alirezaei"
        },
        {
          "name": "Jeroen Ploeg"
        },
        {
          "name": "Son Tong"
        },
        {
          "name": "Xingyu Zhao"
        }
      ],
      "abstract": "Virtual testing using synthetic data has become a cornerstone of autonomous vehicle (AV) safety assurance. Despite progress in improving visual realism through advanced simulators and generative AI, recent studies reveal that pixel-level fidelity alone does not ensure reliable transfer from simulation to the real world. What truly matters is whether the system-under-test (SUT) bases its decisions on the same causal evidence in both real and simulated environments - not just whether images \"look real\" to humans. This paper addresses the lack of such a behavior-grounded fidelity measure by introducing Decisive Feature Fidelity (DFF), a new SUT-specific metric that extends the existing fidelity spectrum to capture mechanism parity - the agreement in causal evidence underlying the SUT's decisions across domains. DFF leverages explainable-AI (XAI) methods to identify and compare the decisive features driving the SUT's outputs for matched real-synthetic pairs. We further propose practical estimators based on counterfactual explanations, along with a DFF-guided calibration scheme to enhance simulator fidelity. Experiments on 2126 matched KITTI-VirtualKITTI2 pairs demonstrate that DFF reveals discrepancies overlooked by conventional output-value fidelity. Furthermore, results show that DFF-guided calibration improves decisive-feature and input-level fidelity without sacrificing output value fidelity across diverse SUTs.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-12-18T12:39:13+00:00",
      "updated": "2025-12-18T12:39:13+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16468v1",
      "file": "papers/2512.16468v1.pdf"
    },
    {
      "arxiv_id": "2512.16271v1",
      "title": "Domain-Agnostic Causal-Aware Audio Transformer for Infant Cry Classification",
      "authors": [
        {
          "name": "Geofrey Owino"
        },
        {
          "name": "Bernard Shibwabo Kasamani"
        },
        {
          "name": "Ahmed M. Abdelmoniem"
        },
        {
          "name": "Edem Wornyo"
        }
      ],
      "abstract": "Accurate and interpretable classification of infant cry paralinguistics is essential for early detection of neonatal distress and clinical decision support. However, many existing deep learning methods rely on correlation-driven acoustic representations, which makes them vulnerable to noise, spurious cues, and domain shifts across recording environments. We propose DACH-TIC, a Domain-Agnostic Causal-Aware Hierarchical Audio Transformer for robust infant cry classification. The model integrates causal attention, hierarchical representation learning, multi-task supervision, and adversarial domain generalization within a unified framework.\n  DACH-TIC employs a structured transformer backbone with local token-level and global semantic encoders, augmented by causal attention masking and controlled perturbation training to approximate counterfactual acoustic variations. A domain-adversarial objective promotes environment-invariant representations, while multi-task learning jointly optimizes cry type recognition, distress intensity estimation, and causal relevance prediction. The model is evaluated on the Baby Chillanto and Donate-a-Cry datasets, with ESC-50 environmental noise overlays for domain augmentation.\n  Experimental results show that DACH-TIC outperforms state-of-the-art baselines, including HTS-AT and SE-ResNet Transformer, achieving improvements of 2.6 percent in accuracy and 2.2 points in macro-F1 score, alongside enhanced causal fidelity. The model generalizes effectively to unseen acoustic environments, with a domain performance gap of only 2.4 percent, demonstrating its suitability for real-world neonatal acoustic monitoring systems.",
      "primary_category": "cs.SD",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "published": "2025-12-18T07:40:44+00:00",
      "updated": "2025-12-18T07:40:44+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16271v1",
      "file": "papers/2512.16271v1.pdf"
    },
    {
      "arxiv_id": "2512.16251v2",
      "title": "Interpretable Deep Learning for Stock Returns: A Consensus-Bottleneck Asset Pricing Model",
      "authors": [
        {
          "name": "Bong-Gyu Jang"
        },
        {
          "name": "Younwoo Jeong"
        },
        {
          "name": "Changeun Kim"
        }
      ],
      "abstract": "We introduce the Consensus-Bottleneck Asset Pricing Model (CB-APM), a partially interpretable neural network that replicates the reasoning processes of sell-side analysts by capturing how dispersed investor beliefs are compressed into asset prices through a consensus formation process. By modeling this \"bottleneck\" to summarize firm- and macro-level information, CB-APM not only predicts future risk premiums of U.S. equities but also links belief aggregation to expected returns in a structurally interpretable manner. The model improves long-horizon return forecasts and outperforms standard deep learning approaches in both predictive accuracy and explanatory power. Comprehensive portfolio analyses show that CB-APM's out-of-sample predictions translate into economically meaningful payoffs, with monotonic return differentials and stable long-short performance across regularization settings. Empirically, CB-APM leverages consensus as a regularizer to amplify long-horizon predictability and yields interpretable consensus-based components that clarify how information is priced in returns. Moreover, regression and Gibbons-Ross-Shanken (GRS)-based pricing diagnostics reveal that the learned consensus representations capture priced variation only partially spanned by traditional factor models, demonstrating that CB-APM uncovers belief-driven structure in expected returns beyond the canonical factor space. Overall, CB-APM provides an interpretable and empirically grounded framework for understanding belief-driven return dynamics.",
      "primary_category": "q-fin.PR",
      "categories": [
        "q-fin.PR",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-18T07:05:25+00:00",
      "updated": "2025-12-23T02:11:19+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16251v2",
      "file": "papers/2512.16251v2.pdf"
    },
    {
      "arxiv_id": "2512.16037v1",
      "title": "Explainable AI in Big Data Fraud Detection",
      "authors": [
        {
          "name": "Ayush Jain"
        },
        {
          "name": "Rahul Kulkarni"
        },
        {
          "name": "Siyi Lin"
        }
      ],
      "abstract": "Big Data has become central to modern applications in finance, insurance, and cybersecurity, enabling machine learning systems to perform large-scale risk assessments and fraud detection. However, the increasing dependence on automated analytics introduces important concerns about transparency, regulatory compliance, and trust. This paper examines how explainable artificial intelligence (XAI) can be integrated into Big Data analytics pipelines for fraud detection and risk management. We review key Big Data characteristics and survey major analytical tools, including distributed storage systems, streaming platforms, and advanced fraud detection models such as anomaly detectors, graph-based approaches, and ensemble classifiers. We also present a structured review of widely used XAI methods, including LIME, SHAP, counterfactual explanations, and attention mechanisms, and analyze their strengths and limitations when deployed at scale. Based on these findings, we identify key research gaps related to scalability, real-time processing, and explainability for graph and temporal models. To address these challenges, we outline a conceptual framework that integrates scalable Big Data infrastructure with context-aware explanation mechanisms and human feedback. The paper concludes with open research directions in scalable XAI, privacy-aware explanations, and standardized evaluation methods for explainable fraud detection systems.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-17T23:40:54+00:00",
      "updated": "2025-12-17T23:40:54+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16037v1",
      "file": "papers/2512.16037v1.pdf"
    },
    {
      "arxiv_id": "2512.16022v1",
      "title": "Conversational Time Series Foundation Models: Towards Explainable and Effective Forecasting",
      "authors": [
        {
          "name": "Defu Cao"
        },
        {
          "name": "Michael Gee"
        },
        {
          "name": "Jinbo Liu"
        },
        {
          "name": "Hengxuan Wang"
        },
        {
          "name": "Wei Yang"
        },
        {
          "name": "Rui Wang"
        },
        {
          "name": "Yan Liu"
        }
      ],
      "abstract": "The proliferation of time series foundation models has created a landscape where no single method achieves consistent superiority, framing the central challenge not as finding the best model, but as orchestrating an optimal ensemble with interpretability. While Large Language Models (LLMs) offer powerful reasoning capabilities, their direct application to time series forecasting has proven ineffective. We address this gap by repositioning the LLM as an intelligent judge that evaluates, explains, and strategically coordinates an ensemble of foundation models. To overcome the LLM's inherent lack of domain-specific knowledge on time series, we introduce an R1-style finetuning process, guided by SHAP-based faithfulness scores, which teaches the model to interpret ensemble weights as meaningful causal statements about temporal dynamics. The trained agent then engages in iterative, multi-turn conversations to perform forward-looking assessments, provide causally-grounded explanations for its weighting decisions, and adaptively refine the optimization strategy. Validated on the GIFT-Eval benchmark on 23 datasets across 97 settings, our approach significantly outperforms leading time series foundation models on both CRPS and MASE metrics, establishing new state-of-the-art results.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-12-17T23:14:38+00:00",
      "updated": "2025-12-17T23:14:38+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16022v1",
      "file": "papers/2512.16022v1.pdf"
    },
    {
      "arxiv_id": "2512.15938v1",
      "title": "SALVE: Sparse Autoencoder-Latent Vector Editing for Mechanistic Control of Neural Networks",
      "authors": [
        {
          "name": "Vegard Flovik"
        }
      ],
      "abstract": "Deep neural networks achieve impressive performance but remain difficult to interpret and control. We present SALVE (Sparse Autoencoder-Latent Vector Editing), a unified \"discover, validate, and control\" framework that bridges mechanistic interpretability and model editing. Using an $\\ell_1$-regularized autoencoder, we learn a sparse, model-native feature basis without supervision. We validate these features with Grad-FAM, a feature-level saliency mapping method that visually grounds latent features in input data. Leveraging the autoencoder's structure, we perform precise and permanent weight-space interventions, enabling continuous modulation of both class-defining and cross-class features. We further derive a critical suppression threshold, $α_{crit}$, quantifying each class's reliance on its dominant feature, supporting fine-grained robustness diagnostics. Our approach is validated on both convolutional (ResNet-18) and transformer-based (ViT-B/16) models, demonstrating consistent, interpretable control over their behavior. This work contributes a principled methodology for turning feature discovery into actionable model edits, advancing the development of transparent and controllable AI systems.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-12-17T20:06:03+00:00",
      "updated": "2025-12-17T20:06:03+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15938v1",
      "file": "papers/2512.15938v1.pdf"
    },
    {
      "arxiv_id": "2512.15712v1",
      "title": "Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants",
      "authors": [
        {
          "name": "Vincent Huang"
        },
        {
          "name": "Dami Choi"
        },
        {
          "name": "Daniel D. Johnson"
        },
        {
          "name": "Sarah Schwettmann"
        },
        {
          "name": "Jacob Steinhardt"
        }
      ],
      "abstract": "Interpreting the internal activations of neural networks can produce more faithful explanations of their behavior, but is difficult due to the complex structure of activation space. Existing approaches to scalable interpretability use hand-designed agents that make and test hypotheses about how internal activations relate to external behavior. We propose to instead turn this task into an end-to-end training objective, by training interpretability assistants to accurately predict model behavior from activations through a communication bottleneck. Specifically, an encoder compresses activations to a sparse list of concepts, and a decoder reads this list and answers a natural language question about the model. We show how to pretrain this assistant on large unstructured data, then finetune it to answer questions. The resulting architecture, which we call a Predictive Concept Decoder, enjoys favorable scaling properties: the auto-interp score of the bottleneck concepts improves with data, as does the performance on downstream applications. Specifically, PCDs can detect jailbreaks, secret hints, and implanted latent concepts, and are able to accurately surface latent user attributes.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-12-17T18:59:48+00:00",
      "updated": "2025-12-17T18:59:48+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15712v1",
      "file": "papers/2512.15712v1.pdf"
    },
    {
      "arxiv_id": "2512.15674v1",
      "title": "Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers",
      "authors": [
        {
          "name": "Adam Karvonen"
        },
        {
          "name": "James Chua"
        },
        {
          "name": "Clément Dumas"
        },
        {
          "name": "Kit Fraser-Taliente"
        },
        {
          "name": "Subhash Kantamneni"
        },
        {
          "name": "Julian Minder"
        },
        {
          "name": "Euan Ong"
        },
        {
          "name": "Arnab Sen Sharma"
        },
        {
          "name": "Daniel Wen"
        },
        {
          "name": "Owain Evans"
        },
        {
          "name": "Samuel Marks"
        }
      ],
      "abstract": "Large language model (LLM) activations are notoriously difficult to understand, with most existing techniques using complex, specialized methods for interpreting them. Recent work has proposed a simpler approach known as LatentQA: training LLMs to directly accept LLM activations as inputs and answer arbitrary questions about them in natural language. However, prior work has focused on narrow task settings for both training and evaluation. In this paper, we instead take a generalist perspective. We evaluate LatentQA-trained models, which we call Activation Oracles (AOs), in far out-of-distribution settings and examine how performance scales with training data diversity. We find that AOs can recover information fine-tuned into a model (e.g., biographical knowledge or malign propensities) that does not appear in the input text, despite never being trained with activations from a fine-tuned model. Our main evaluations are four downstream tasks where we can compare to prior white- and black-box techniques. We find that even narrowly-trained LatentQA models can generalize well, and that adding additional training datasets (such as classification tasks and a self-supervised context prediction task) yields consistent further improvements. Overall, our best AOs match or exceed prior white-box baselines on all four tasks and are the best method on 3 out of 4. These results suggest that diversified training to answer natural-language queries imparts a general capability to verbalize information about LLM activations.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-17T18:26:28+00:00",
      "updated": "2025-12-17T18:26:28+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15674v1",
      "file": "papers/2512.15674v1.pdf"
    },
    {
      "arxiv_id": "2512.15663v1",
      "title": "Explaining the Reasoning of Large Language Models Using Attribution Graphs",
      "authors": [
        {
          "name": "Chase Walker"
        },
        {
          "name": "Rickard Ewetz"
        }
      ],
      "abstract": "Large language models (LLMs) exhibit remarkable capabilities, yet their reasoning remains opaque, raising safety and trust concerns. Attribution methods, which assign credit to input features, have proven effective for explaining the decision making of computer vision models. From these, context attributions have emerged as a promising approach for explaining the behavior of autoregressive LLMs. However, current context attributions produce incomplete explanations by directly relating generated tokens to the prompt, discarding inter-generational influence in the process. To overcome these shortcomings, we introduce the Context Attribution via Graph Explanations (CAGE) framework. CAGE introduces an attribution graph: a directed graph that quantifies how each generation is influenced by both the prompt and all prior generations. The graph is constructed to preserve two properties-causality and row stochasticity. The attribution graph allows context attributions to be computed by marginalizing intermediate contributions along paths in the graph. Across multiple models, datasets, metrics, and methods, CAGE improves context attribution faithfulness, achieving average gains of up to 40%.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-12-17T18:15:26+00:00",
      "updated": "2025-12-17T18:15:26+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15663v1",
      "file": "papers/2512.15663v1.pdf"
    },
    {
      "arxiv_id": "2512.15662v1",
      "title": "Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning",
      "authors": [
        {
          "name": "Jiaqi Xu"
        },
        {
          "name": "Cuiling Lan"
        },
        {
          "name": "Xuejin Chen"
        },
        {
          "name": "Yan LU"
        }
      ],
      "abstract": "Human beings solve complex problems through critical thinking, where reasoning and evaluation are intertwined to converge toward correct solutions. However, most existing large language models (LLMs) decouple reasoning from verification: they either generate reasoning without explicit self-checking or rely on external verifiers to detect errors post hoc. The former lacks immediate feedback, while the latter increases system complexity and hinders synchronized learning. Motivated by human critical thinking, we propose Stepwise Think-Critique (STC), a unified framework that interleaves reasoning and self-critique at each step within a single model. STC is trained with a hybrid reinforcement learning objective combining reasoning rewards and critique-consistency rewards to jointly optimize reasoning quality and self-evaluation. Experiments on mathematical reasoning benchmarks show that STC demonstrates strong critic-thinking capabilities and produces more interpretable reasoning traces, representing a step toward LLMs with built-in critical thinking.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-12-17T18:15:17+00:00",
      "updated": "2025-12-17T18:15:17+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15662v1",
      "file": "papers/2512.15662v1.pdf"
    },
    {
      "arxiv_id": "2512.15614v1",
      "title": "Behavior Tokens Speak Louder: Disentangled Explainable Recommendation with Behavior Vocabulary",
      "authors": [
        {
          "name": "Xinshun Feng"
        },
        {
          "name": "Mingzhe Liu"
        },
        {
          "name": "Yi Qiao"
        },
        {
          "name": "Tongyu Zhu"
        },
        {
          "name": "Leilei Sun"
        },
        {
          "name": "Shuai Wang"
        }
      ],
      "abstract": "Recent advances in explainable recommendations have explored the integration of language models to analyze natural language rationales for user-item interactions. Despite their potential, existing methods often rely on ID-based representations that obscure semantic meaning and impose structural constraints on language models, thereby limiting their applicability in open-ended scenarios. These challenges are intensified by the complex nature of real-world interactions, where diverse user intents are entangled and collaborative signals rarely align with linguistic semantics. To overcome these limitations, we propose BEAT, a unified and transferable framework that tokenizes user and item behaviors into discrete, interpretable sequences. We construct a behavior vocabulary via a vector-quantized autoencoding process that disentangles macro-level interests and micro-level intentions from graph-based representations. We then introduce multi-level semantic supervision to bridge the gap between behavioral signals and language space. A semantic alignment regularization mechanism is designed to embed behavior tokens directly into the input space of frozen language models. Experiments on three public datasets show that BEAT improves zero-shot recommendation performance while generating coherent and informative explanations. Further analysis demonstrates that our behavior tokens capture fine-grained semantics and offer a plug-and-play interface for integrating complex behavior patterns into large language models.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-17T17:24:24+00:00",
      "updated": "2025-12-17T17:24:24+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15614v1",
      "file": "papers/2512.15614v1.pdf"
    },
    {
      "arxiv_id": "2512.16950v1",
      "title": "Enhancing Tree Species Classification: Insights from YOLOv8 and Explainable AI Applied to TLS Point Cloud Projections",
      "authors": [
        {
          "name": "Adrian Straker"
        },
        {
          "name": "Paul Magdon"
        },
        {
          "name": "Marco Zullich"
        },
        {
          "name": "Maximilian Freudenberg"
        },
        {
          "name": "Christoph Kleinn"
        },
        {
          "name": "Johannes Breidenbach"
        },
        {
          "name": "Stefano Puliti"
        },
        {
          "name": "Nils Nölke"
        }
      ],
      "abstract": "Classifying tree species has been a core research area in forest remote sensing for decades. New sensors and classification approaches like TLS and deep learning achieve state-of-the art accuracy but their decision processes remain unclear. Methods such as Finer-CAM (Class Activation Mapping) can highlight features in TLS projections that contribute to the classification of a target species, yet are uncommon in similar looking contrastive tree species. We propose a novel method linking Finer-CAM explanations to segments of TLS projections representing structural tree features to systemically evaluate which features drive species discrimination. Using TLS data from 2,445 trees across seven European tree species, we trained and validated five YOLOv8 models with cross-validation, reaching a mean accuracy of 96% (SD = 0.24%). Analysis of 630 saliency maps shows the models primarily rely on crown features in TLS projections for species classification. While this result is pronounced in Silver Birch, European Beech, English oak, and Norway spruce, stem features contribute more frequently to the differentiation of European ash, Scots pine, and Douglas fir. Particularly representations of finer branches contribute to the decisions of the models. The models consider those tree species similar to each other which a human expert would also regard as similar. Furthermore, our results highlight the need for an improved understanding of the decision processes of tree species classification models to help reveal data set and model limitations, biases, and to build confidence in model predictions.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-17T12:09:41+00:00",
      "updated": "2025-12-17T12:09:41+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.16950v1",
      "file": "papers/2512.16950v1.pdf"
    },
    {
      "arxiv_id": "2512.19734v1",
      "title": "The Deleuzian Representation Hypothesis",
      "authors": [
        {
          "name": "Clément Cornet"
        },
        {
          "name": "Romaric Besançon"
        },
        {
          "name": "Hervé Le Borgne"
        }
      ],
      "abstract": "We propose an alternative to sparse autoencoders (SAEs) as a simple and effective unsupervised method for extracting interpretable concepts from neural networks. The core idea is to cluster differences in activations, which we formally justify within a discriminant analysis framework. To enhance the diversity of extracted concepts, we refine the approach by weighting the clustering using the skewness of activations. The method aligns with Deleuze's modern view of concepts as differences. We evaluate the approach across five models and three modalities (vision, language, and audio), measuring concept quality, diversity, and consistency. Our results show that the proposed method achieves concept quality surpassing prior unsupervised SAE variants while approaching supervised baselines, and that the extracted concepts enable steering of a model's inner representations, demonstrating their causal influence on downstream behavior.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-17T11:51:25+00:00",
      "updated": "2025-12-17T11:51:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19734v1",
      "file": "papers/2512.19734v1.pdf"
    },
    {
      "arxiv_id": "2512.15315v1",
      "title": "Automated Motion Artifact Check for MRI (AutoMAC-MRI): An Interpretable Framework for Motion Artifact Detection and Severity Assessment",
      "authors": [
        {
          "name": "Antony Jerald"
        },
        {
          "name": "Dattesh Shanbhag"
        },
        {
          "name": "Sudhanya Chatterjee"
        }
      ],
      "abstract": "Motion artifacts degrade MRI image quality and increase patient recalls. Existing automated quality assessment methods are largely limited to binary decisions and provide little interpretability. We introduce AutoMAC-MRI, an explainable framework for grading motion artifacts across heterogeneous MR contrasts and orientations. The approach uses supervised contrastive learning to learn a discriminative representation of motion severity. Within this feature space, we compute grade-specific affinity scores that quantify an image's proximity to each motion grade, thereby making grade assignments transparent and interpretable. We evaluate AutoMAC-MRI on more than 5000 expert-annotated brain MRI slices spanning multiple contrasts and views. Experiments assessing affinity scores against expert labels show that the scores align well with expert judgment, supporting their use as an interpretable measure of motion severity. By coupling accurate grade detection with per-grade affinity scoring, AutoMAC-MRI enables inline MRI quality control, with the potential to reduce unnecessary rescans and improve workflow efficiency.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-17T11:05:25+00:00",
      "updated": "2025-12-17T11:05:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15315v1",
      "file": "papers/2512.15315v1.pdf"
    },
    {
      "arxiv_id": "2512.15140v1",
      "title": "Generalization and Feature Attribution in Machine Learning Models for Crop Yield and Anomaly Prediction in Germany",
      "authors": [
        {
          "name": "Roland Baatz"
        }
      ],
      "abstract": "This study examines the generalization performance and interpretability of machine learning (ML) models used for predicting crop yield and yield anomalies in Germany's NUTS-3 regions. Using a high-quality, long-term dataset, the study systematically compares the evaluation and temporal validation behavior of ensemble tree-based models (XGBoost, Random Forest) and deep learning approaches (LSTM, TCN).\n  While all models perform well on spatially split, conventional test sets, their performance degrades substantially on temporally independent validation years, revealing persistent limitations in generalization. Notably, models with strong test-set accuracy, but weak temporal validation performance can still produce seemingly credible SHAP feature importance values. This exposes a critical vulnerability in post hoc explainability methods: interpretability may appear reliable even when the underlying model fails to generalize.\n  These findings underscore the need for validation-aware interpretation of ML predictions in agricultural and environmental systems. Feature importance should not be accepted at face value unless models are explicitly shown to generalize to unseen temporal and spatial conditions. The study advocates for domain-aware validation, hybrid modeling strategies, and more rigorous scrutiny of explainability methods in data-driven agriculture. Ultimately, this work addresses a growing challenge in environmental data science: how can we evaluate generalization robustly enough to trust model explanations?",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-17T07:01:47+00:00",
      "updated": "2025-12-17T07:01:47+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15140v1",
      "file": "papers/2512.15140v1.pdf"
    },
    {
      "arxiv_id": "2512.15134v1",
      "title": "From Isolation to Entanglement: When Do Interpretability Methods Identify and Disentangle Known Concepts?",
      "authors": [
        {
          "name": "Aaron Mueller"
        },
        {
          "name": "Andrew Lee"
        },
        {
          "name": "Shruti Joshi"
        },
        {
          "name": "Ekdeep Singh Lubana"
        },
        {
          "name": "Dhanya Sridhar"
        },
        {
          "name": "Patrik Reizinger"
        }
      ],
      "abstract": "A central goal of interpretability is to recover representations of causally relevant concepts from the activations of neural networks. The quality of these concept representations is typically evaluated in isolation, and under implicit independence assumptions that may not hold in practice. Thus, it is unclear whether common featurization methods - including sparse autoencoders (SAEs) and sparse probes - recover disentangled representations of these concepts. This study proposes a multi-concept evaluation setting where we control the correlations between textual concepts, such as sentiment, domain, and tense, and analyze performance under increasing correlations between them. We first evaluate the extent to which featurizers can learn disentangled representations of each concept under increasing correlational strengths. We observe a one-to-many relationship from concepts to features: features correspond to no more than one concept, but concepts are distributed across many features. Then, we perform steering experiments, measuring whether each concept is independently manipulable. Even when trained on uniform distributions of concepts, SAE features generally affect many concepts when steered, indicating that they are neither selective nor independent; nonetheless, features affect disjoint subspaces. These results suggest that correlational metrics for measuring disentanglement are generally not sufficient for establishing independence when steering, and that affecting disjoint subspaces is not sufficient for concept selectivity. These results underscore the importance of compositional evaluations in interpretability research.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-12-17T06:54:08+00:00",
      "updated": "2025-12-17T06:54:08+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15134v1",
      "file": "papers/2512.15134v1.pdf"
    },
    {
      "arxiv_id": "2512.14559v1",
      "title": "Counterfactual Explanations for Time Series Should be Human-Centered and Temporally Coherent in Interventions",
      "authors": [
        {
          "name": "Emmanuel C. Chukwu"
        },
        {
          "name": "Rianne M. Schouten"
        },
        {
          "name": "Monique Tabak"
        },
        {
          "name": "Mykola Pechenizkiy"
        }
      ],
      "abstract": "Counterfactual explanations are increasingly proposed as interpretable mechanisms to achieve algorithmic recourse. However, current counterfactual techniques for time series classification are predominantly designed with static data assumptions and focus on generating minimal input perturbations to flip model predictions. This paper argues that such approaches are fundamentally insufficient in clinical recommendation settings, where interventions unfold over time and must be causally plausible and temporally coherent. We advocate for a shift towards counterfactuals that reflect sustained, goal-directed interventions aligned with clinical reasoning and patient-specific dynamics. We identify critical gaps in existing methods that limit their practical applicability, specifically, temporal blind spots and the lack of user-centered considerations in both method design and evaluation metrics. To support our position, we conduct a robustness analysis of several state-of-the-art methods for time series and show that the generated counterfactuals are highly sensitive to stochastic noise. This finding highlights their limited reliability in real-world clinical settings, where minor measurement variations are inevitable. We conclude by calling for methods and evaluation frameworks that go beyond mere prediction changes without considering feasibility or actionability. We emphasize the need for actionable, purpose-driven interventions that are feasible in real-world contexts for the users of such applications.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-16T16:31:10+00:00",
      "updated": "2025-12-16T16:31:10+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14559v1",
      "file": "papers/2512.14559v1.pdf"
    },
    {
      "arxiv_id": "2512.14354v1",
      "title": "Enhancing Interpretability for Vision Models via Shapley Value Optimization",
      "authors": [
        {
          "name": "Kanglong Fan"
        },
        {
          "name": "Yunqiao Yang"
        },
        {
          "name": "Chen Ma"
        }
      ],
      "abstract": "Deep neural networks have demonstrated remarkable performance across various domains, yet their decision-making processes remain opaque. Although many explanation methods are dedicated to bringing the obscurity of DNNs to light, they exhibit significant limitations: post-hoc explanation methods often struggle to faithfully reflect model behaviors, while self-explaining neural networks sacrifice performance and compatibility due to their specialized architectural designs. To address these challenges, we propose a novel self-explaining framework that integrates Shapley value estimation as an auxiliary task during training, which achieves two key advancements: 1) a fair allocation of the model prediction scores to image patches, ensuring explanations inherently align with the model's decision logic, and 2) enhanced interpretability with minor structural modifications, preserving model performance and compatibility. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art interpretability.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-16T12:33:04+00:00",
      "updated": "2025-12-16T12:33:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14354v1",
      "file": "papers/2512.14354v1.pdf"
    },
    {
      "arxiv_id": "2512.14332v1",
      "title": "Step-Tagging: Toward controlling the generation of Language Reasoning Models through step monitoring",
      "authors": [
        {
          "name": "Yannis Belkhiter"
        },
        {
          "name": "Seshu Tirupathi"
        },
        {
          "name": "Giulio Zizzo"
        },
        {
          "name": "John D. Kelleher"
        }
      ],
      "abstract": "The field of Language Reasoning Models (LRMs) has been very active over the past few years with advances in training and inference techniques enabling LRMs to reason longer, and more accurately. However, a growing body of studies show that LRMs are still inefficient, over-generating verification and reflection steps. To address this challenge, we introduce the Step-Tagging framework, a lightweight sentence-classifier enabling real-time annotation of the type of reasoning steps that an LRM is generating. To monitor reasoning behaviors, we introduced ReasonType: a novel taxonomy of reasoning steps. Building on this framework, we demonstrated that online monitoring of the count of specific steps can produce effective interpretable early stopping criteria of LRM inferences. We evaluate the Step-tagging framework on three open-source reasoning models across standard benchmark datasets: MATH500, GSM8K, AIME and non-mathematical tasks (GPQA and MMLU-Pro). We achieve 20 to 50\\% token reduction while maintaining comparable accuracy to standard generation, with largest gains observed on more computation-heavy tasks. This work offers a novel way to increase control over the generation of LRMs, and a new tool to study behaviors of LRMs.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-16T12:01:16+00:00",
      "updated": "2025-12-16T12:01:16+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14332v1",
      "file": "papers/2512.14332v1.pdf"
    },
    {
      "arxiv_id": "2512.14263v1",
      "title": "Explainable Preference Learning: a Decision Tree-based Surrogate Model for Preferential Bayesian Optimization",
      "authors": [
        {
          "name": "Nick Leenders"
        },
        {
          "name": "Thomas Quadt"
        },
        {
          "name": "Boris Cule"
        },
        {
          "name": "Roy Lindelauf"
        },
        {
          "name": "Herman Monsuur"
        },
        {
          "name": "Joost van Oijen"
        },
        {
          "name": "Mark Voskuijl"
        }
      ],
      "abstract": "Current Preferential Bayesian Optimization methods rely on Gaussian Processes (GPs) as surrogate models. These models are hard to interpret, struggle with handling categorical data, and are computationally complex, limiting their real-world usability. In this paper, we introduce an inherently interpretable decision tree-based surrogate model capable of handling both categorical and continuous data, and scalable to large datasets. Extensive numerical experiments on eight increasingly spiky optimization functions show that our model outperforms GP-based alternatives on spiky functions and has only marginally lower performance for non-spiky functions. Moreover, we apply our model to the real-world Sushi dataset and show its ability to learn an individual's sushi preferences. Finally, we show some initial work on using historical preference data to speed up the optimization process for new unseen users.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "published": "2025-12-16T10:17:31+00:00",
      "updated": "2025-12-16T10:17:31+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14263v1",
      "file": "papers/2512.14263v1.pdf"
    },
    {
      "arxiv_id": "2512.15793v1",
      "title": "Explainable Ethical Assessment on Human Behaviors by Generating Conflicting Social Norms",
      "authors": [
        {
          "name": "Yuxi Sun"
        },
        {
          "name": "Wei Gao"
        },
        {
          "name": "Hongzhan Lin"
        },
        {
          "name": "Jing Ma"
        },
        {
          "name": "Wenxuan Zhang"
        }
      ],
      "abstract": "Human behaviors are often guided or constrained by social norms, which are defined as shared, commonsense rules. For example, underlying an action ``\\textit{report a witnessed crime}\" are social norms that inform our conduct, such as ``\\textit{It is expected to be brave to report crimes}''. Current AI systems that assess valence (i.e., support or oppose) of human actions by leveraging large-scale data training not grounded on explicit norms may be difficult to explain, and thus untrustworthy. Emulating human assessors by considering social norms can help AI models better understand and predict valence. While multiple norms come into play, conflicting norms can create tension and directly influence human behavior. For example, when deciding whether to ``\\textit{report a witnessed crime}'', one may balance \\textit{bravery} against \\textit{self-protection}. In this paper, we introduce \\textit{ClarityEthic}, a novel ethical assessment approach, to enhance valence prediction and explanation by generating conflicting social norms behind human actions, which strengthens the moral reasoning capabilities of language models by using a contrastive learning strategy. Extensive experiments demonstrate that our method outperforms strong baseline approaches, and human evaluations confirm that the generated social norms provide plausible explanations for the assessment of human behaviors.",
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-12-16T09:04:42+00:00",
      "updated": "2025-12-16T09:04:42+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15793v1",
      "file": "papers/2512.15793v1.pdf"
    },
    {
      "arxiv_id": "2512.14181v1",
      "title": "Towards Explainable Quantum AI: Informing the Encoder Selection of Quantum Neural Networks via Visualization",
      "authors": [
        {
          "name": "Shaolun Ruan"
        },
        {
          "name": "Feng Liang"
        },
        {
          "name": "Rohan Ramakrishna"
        },
        {
          "name": "Chao Ren"
        },
        {
          "name": "Rudai Yan"
        },
        {
          "name": "Qiang Guan"
        },
        {
          "name": "Jiannan Li"
        },
        {
          "name": "Yong Wang"
        }
      ],
      "abstract": "Quantum Neural Networks (QNNs) represent a promising fusion of quantum computing and neural network architectures, offering speed-ups and efficient processing of high-dimensional, entangled data. A crucial component of QNNs is the encoder, which maps classical input data into quantum states. However, choosing suitable encoders remains a significant challenge, largely due to the lack of systematic guidance and the trial-and-error nature of current approaches. This process is further impeded by two key challenges: (1) the difficulty in evaluating encoded quantum states prior to training, and (2) the lack of intuitive methods for analyzing an encoder's ability to effectively distinguish data features. To address these issues, we introduce a novel visualization tool, XQAI-Eyes, which enables QNN developers to compare classical data features with their corresponding encoded quantum states and to examine the mixed quantum states across different classes. By bridging classical and quantum perspectives, XQAI-Eyes facilitates a deeper understanding of how encoders influence QNN performance. Evaluations across diverse datasets and encoder designs demonstrate XQAI-Eyes's potential to support the exploration of the relationship between encoder design and QNN effectiveness, offering a holistic and transparent approach to optimizing quantum encoders. Moreover, domain experts used XQAI-Eyes to derive two key practices for quantum encoder selection, grounded in the principles of pattern preservation and feature mapping.",
      "primary_category": "quant-ph",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.HC"
      ],
      "published": "2025-12-16T08:21:47+00:00",
      "updated": "2025-12-16T08:21:47+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14181v1",
      "file": "papers/2512.14181v1.pdf"
    },
    {
      "arxiv_id": "2512.14121v2",
      "title": "SportsGPT: An LLM-driven Framework for Interpretable Sports Motion Assessment and Training Guidance",
      "authors": [
        {
          "name": "Wenbo Tian"
        },
        {
          "name": "Ruting Lin"
        },
        {
          "name": "Hongxian Zheng"
        },
        {
          "name": "Yaodong Yang"
        },
        {
          "name": "Geng Wu"
        },
        {
          "name": "Zihao Zhang"
        },
        {
          "name": "Zhang Zhang"
        }
      ],
      "abstract": "Existing intelligent sports analysis systems mainly focus on \"scoring and visualization,\" often lacking automatic performance diagnosis and interpretable training guidance. Recent advances in Large Language Models (LLMs) and motion analysis techniques provide new opportunities to address the above limitations. In this paper, we propose SportsGPT, an LLM-driven framework for interpretable sports motion assessment and training guidance, which establishes a closed loop from motion time-series input to professional training guidance. First, given a set of high-quality target models, we introduce MotionDTW, a two-stage time series alignment algorithm designed for accurate keyframe extraction from skeleton-based motion sequences. Subsequently, we design a Knowledge-based Interpretable Sports Motion Assessment Model (KISMAM) to obtain a set of interpretable assessment metrics (e.g., insufficient extension) by contrasting the keyframes with the target models. Finally, we propose SportsRAG, a RAG-based training guidance model built upon Qwen3. Leveraging a 6B-token knowledge base, it prompts the LLM to generate professional training guidance by retrieving domain-specific QA pairs. Experimental results demonstrate that MotionDTW significantly outperforms traditional methods with lower temporal error and higher IoU scores. Furthermore, ablation studies validate the KISMAM and SportsRAG, confirming that SportsGPT surpasses general LLMs in diagnostic accuracy and professionalism.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-16T06:05:55+00:00",
      "updated": "2025-12-19T06:44:37+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14121v2",
      "file": "papers/2512.14121v2.pdf"
    },
    {
      "arxiv_id": "2512.14102v1",
      "title": "Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries",
      "authors": [
        {
          "name": "Emanuele Mezzi"
        },
        {
          "name": "Gertjan Burghouts"
        },
        {
          "name": "Maarten Kruithof"
        }
      ],
      "abstract": "Text-to-image retrieval in remote sensing (RS) has advanced rapidly with the rise of large vision-language models (LVLMs) tailored for aerial and satellite imagery, culminating in remote sensing large vision-language models (RS-LVLMS). However, limited explainability and poor handling of complex spatial relations remain key challenges for real-world use. To address these issues, we introduce RUNE (Reasoning Using Neurosymbolic Entities), an approach that combines Large Language Models (LLMs) with neurosymbolic AI to retrieve images by reasoning over the compatibility between detected entities and First-Order Logic (FOL) expressions derived from text queries. Unlike RS-LVLMs that rely on implicit joint embeddings, RUNE performs explicit reasoning, enhancing performance and interpretability. For scalability, we propose a logic decomposition strategy that operates on conditioned subsets of detected entities, guaranteeing shorter execution time compared to neural approaches. Rather than using foundation models for end-to-end retrieval, we leverage them only to generate FOL expressions, delegating reasoning to a neurosymbolic inference module. For evaluation we repurpose the DOTA dataset, originally designed for object detection, by augmenting it with more complex queries than in existing benchmarks. We show the LLM's effectiveness in text-to-logic translation and compare RUNE with state-of-the-art RS-LVLMs, demonstrating superior performance. We introduce two metrics, Retrieval Robustness to Query Complexity (RRQC) and Retrieval Robustness to Image Uncertainty (RRIU), which evaluate performance relative to query complexity and image uncertainty. RUNE outperforms joint-embedding models in complex RS retrieval tasks, offering gains in performance, robustness, and explainability. We show RUNE's potential for real-world RS applications through a use case on post-flood satellite image retrieval.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR"
      ],
      "published": "2025-12-16T05:33:44+00:00",
      "updated": "2025-12-16T05:33:44+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14102v1",
      "file": "papers/2512.14102v1.pdf"
    },
    {
      "arxiv_id": "2512.14092v1",
      "title": "ProtoFlow: Interpretable and Robust Surgical Workflow Modeling with Learned Dynamic Scene Graph Prototypes",
      "authors": [
        {
          "name": "Felix Holm"
        },
        {
          "name": "Ghazal Ghazaei"
        },
        {
          "name": "Nassir Navab"
        }
      ],
      "abstract": "Purpose: Detailed surgical recognition is critical for advancing AI-assisted surgery, yet progress is hampered by high annotation costs, data scarcity, and a lack of interpretable models. While scene graphs offer a structured abstraction of surgical events, their full potential remains untapped. In this work, we introduce ProtoFlow, a novel framework that learns dynamic scene graph prototypes to model complex surgical workflows in an interpretable and robust manner.\n  Methods: ProtoFlow leverages a graph neural network (GNN) encoder-decoder architecture that combines self-supervised pretraining for rich representation learning with a prototype-based fine-tuning stage. This process discovers and refines core prototypes that encapsulate recurring, clinically meaningful patterns of surgical interaction, forming an explainable foundation for workflow analysis.\n  Results: We evaluate our approach on the fine-grained CAT-SG dataset. ProtoFlow not only outperforms standard GNN baselines in overall accuracy but also demonstrates exceptional robustness in limited-data, few-shot scenarios, maintaining strong performance when trained on as few as one surgical video. Our qualitative analyses further show that the learned prototypes successfully identify distinct surgical sub-techniques and provide clear, interpretable insights into workflow deviations and rare complications.\n  Conclusion: By uniting robust representation learning with inherent explainability, ProtoFlow represents a significant step toward developing more transparent, reliable, and data-efficient AI systems, accelerating their potential for clinical adoption in surgical training, real-time decision support, and workflow optimization.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-16T04:59:58+00:00",
      "updated": "2025-12-16T04:59:58+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14092v1",
      "file": "papers/2512.14092v1.pdf"
    },
    {
      "arxiv_id": "2512.13837v1",
      "title": "Explainable reinforcement learning from human feedback to improve alignment",
      "authors": [
        {
          "name": "Shicheng Liu"
        },
        {
          "name": "Siyuan Xu"
        },
        {
          "name": "Wenjie Qiu"
        },
        {
          "name": "Hangfan Zhang"
        },
        {
          "name": "Minghui Zhu"
        }
      ],
      "abstract": "A common and effective strategy for humans to improve an unsatisfactory outcome in daily life is to find a cause of this outcome and correct the cause. In this paper, we investigate whether this human improvement strategy can be applied to improving reinforcement learning from human feedback (RLHF) for alignment of language models (LMs). In particular, it is observed in the literature that LMs tuned by RLHF can still output unsatisfactory responses. This paper proposes a method to improve the unsatisfactory responses by correcting their causes. Our method has two parts. The first part proposes a post-hoc explanation method to explain why an unsatisfactory response is generated to a prompt by identifying the training data that lead to this response. We formulate this problem as a constrained combinatorial optimization problem where the objective is to find a set of training data closest to this prompt-response pair in a feature representation space, and the constraint is that the prompt-response pair can be decomposed as a convex combination of this set of training data in the feature space. We propose an efficient iterative data selection algorithm to solve this problem. The second part proposes an unlearning method that improves unsatisfactory responses to some prompts by unlearning the training data that lead to these unsatisfactory responses and, meanwhile, does not significantly degrade satisfactory responses to other prompts. Experimental results demonstrate that our algorithm can improve RLHF.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-15T19:18:35+00:00",
      "updated": "2025-12-15T19:18:35+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13837v1",
      "file": "papers/2512.13837v1.pdf"
    },
    {
      "arxiv_id": "2512.13806v1",
      "title": "EEG-D3: A Solution to the Hidden Overfitting Problem of Deep Learning Models",
      "authors": [
        {
          "name": "Siegfried Ludwig"
        },
        {
          "name": "Stylianos Bakas"
        },
        {
          "name": "Konstantinos Barmpas"
        },
        {
          "name": "Georgios Zoumpourlis"
        },
        {
          "name": "Dimitrios A. Adamos"
        },
        {
          "name": "Nikolaos Laskaris"
        },
        {
          "name": "Yannis Panagakis"
        },
        {
          "name": "Stefanos Zafeiriou"
        }
      ],
      "abstract": "Deep learning for decoding EEG signals has gained traction, with many claims to state-of-the-art accuracy. However, despite the convincing benchmark performance, successful translation to real applications is limited. The frequent disconnect between performance on controlled BCI benchmarks and its lack of generalisation to practical settings indicates hidden overfitting problems. We introduce Disentangled Decoding Decomposition (D3), a weakly supervised method for training deep learning models across EEG datasets. By predicting the place in the respective trial sequence from which the input window was sampled, EEG-D3 separates latent components of brain activity, akin to non-linear ICA. We utilise a novel model architecture with fully independent sub-networks for strict interpretability. We outline a feature interpretation paradigm to contrast the component activation profiles on different datasets and inspect the associated temporal and spatial filters. The proposed method reliably separates latent components of brain activity on motor imagery data. Training downstream classifiers on an appropriate subset of these components prevents hidden overfitting caused by task-correlated artefacts, which severely affects end-to-end classifiers. We further exploit the linearly separable latent space for effective few-shot learning on sleep stage classification. The ability to distinguish genuine components of brain activity from spurious features results in models that avoid the hidden overfitting problem and generalise well to real-world applications, while requiring only minimal labelled data. With interest to the neuroscience community, the proposed method gives researchers a tool to separate individual brain processes and potentially even uncover heretofore unknown dynamics.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.HC"
      ],
      "published": "2025-12-15T19:00:10+00:00",
      "updated": "2025-12-15T19:00:10+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13806v1",
      "file": "papers/2512.13806v1.pdf"
    },
    {
      "arxiv_id": "2512.13568v1",
      "title": "Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability",
      "authors": [
        {
          "name": "Leonard Bereska"
        },
        {
          "name": "Zoe Tzifa-Kratira"
        },
        {
          "name": "Reza Samavi"
        },
        {
          "name": "Efstratios Gavves"
        }
      ],
      "abstract": "Neural networks achieve remarkable performance through superposition: encoding multiple features as overlapping directions in activation space rather than dedicating individual neurons to each feature. This challenges interpretability, yet we lack principled methods to measure superposition. We present an information-theoretic framework measuring a neural representation's effective degrees of freedom. We apply Shannon entropy to sparse autoencoder activations to compute the number of effective features as the minimum neurons needed for interference-free encoding. Equivalently, this measures how many \"virtual neurons\" the network simulates through superposition. When networks encode more effective features than actual neurons, they must accept interference as the price of compression. Our metric strongly correlates with ground truth in toy models, detects minimal superposition in algorithmic tasks, and reveals systematic reduction under dropout. Layer-wise patterns mirror intrinsic dimensionality studies on Pythia-70M. The metric also captures developmental dynamics, detecting sharp feature consolidation during grokking. Surprisingly, adversarial training can increase effective features while improving robustness, contradicting the hypothesis that superposition causes vulnerability. Instead, the effect depends on task complexity and network capacity: simple tasks with ample capacity allow feature expansion (abundance regime), while complex tasks or limited capacity force reduction (scarcity regime). By defining superposition as lossy compression, this work enables principled measurement of how neural networks organize information under computational constraints, connecting superposition to adversarial robustness.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-12-15T17:25:39+00:00",
      "updated": "2025-12-15T17:25:39+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13568v1",
      "file": "papers/2512.13568v1.pdf"
    },
    {
      "arxiv_id": "2512.13442v1",
      "title": "XNNTab -- Interpretable Neural Networks for Tabular Data using Sparse Autoencoders",
      "authors": [
        {
          "name": "Khawla Elhadri"
        },
        {
          "name": "Jörg Schlötterer"
        },
        {
          "name": "Christin Seifert"
        }
      ],
      "abstract": "In data-driven applications relying on tabular data, where interpretability is key, machine learning models such as decision trees and linear regression are applied. Although neural networks can provide higher predictive performance, they are not used because of their blackbox nature. In this work, we present XNNTab, a neural architecture that combines the expressiveness of neural networks and interpretability. XNNTab first learns highly non-linear feature representations, which are decomposed into monosemantic features using a sparse autoencoder (SAE). These features are then assigned human-interpretable concepts, making the overall model prediction intrinsically interpretable. XNNTab outperforms interpretable predictive models, and achieves comparable performance to its non-interpretable counterparts.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-15T15:39:59+00:00",
      "updated": "2025-12-15T15:39:59+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13442v1",
      "file": "papers/2512.13442v1.pdf"
    },
    {
      "arxiv_id": "2512.15783v1",
      "title": "AI Epidemiology: achieving explainable AI through expert oversight patterns",
      "authors": [
        {
          "name": "Kit Tempest-Walters"
        }
      ],
      "abstract": "AI Epidemiology is a framework for governing and explaining advanced AI systems by applying population-level surveillance methods to AI outputs. The approach mirrors the way in which epidemiologists enable public health interventions through statistical evidence before molecular mechanisms are understood. This bypasses the problem of model complexity which plagues current interpretability methods (such as SHAP and mechanistic interpretability) at the scale of deployed models.\n  AI Epidemiology achieves this population-level surveillance by standardising capture of AI-expert interactions into structured assessment fields: risk level, alignment score, and accuracy score. These function as exposure variables which predict output failure through statistical associations, much like cholesterol and blood pressure act as exposure variables predicting cardiac events. Output-failure associations are subsequently validated against expert overrides and real-world outcomes.\n  The framework places zero burden on experts and provides automatic audit trails by passively tracking expert convergence and divergence with AI recommendations. Since it analyses outputs rather than internal model computations, it also provides governance continuity when institutions update models and switch vendors. Finally, by providing reliability scores and semantic assessments (e.g. 'this recommendation resembles 500 cases overridden by experts due to guideline violations'), it enables experts and institutions to detect unreliable AI outputs before they cause harm. This democratises AI oversight by enabling domain experts to govern AI systems without requiring machine learning expertise.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-15T11:29:05+00:00",
      "updated": "2025-12-15T11:29:05+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15783v1",
      "file": "papers/2512.15783v1.pdf"
    },
    {
      "arxiv_id": "2512.13144v1",
      "title": "Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models",
      "authors": [
        {
          "name": "Chun Kit Wong"
        },
        {
          "name": "Paraskevas Pegios"
        },
        {
          "name": "Nina Weng"
        },
        {
          "name": "Emilie Pi Fogtmann Sejer"
        },
        {
          "name": "Martin Grønnebæk Tolsgaard"
        },
        {
          "name": "Anders Nymark Christensen"
        },
        {
          "name": "Aasa Feragen"
        }
      ],
      "abstract": "Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG",
        "eess.IV"
      ],
      "published": "2025-12-15T09:52:46+00:00",
      "updated": "2025-12-15T09:52:46+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13144v1",
      "file": "papers/2512.13144v1.pdf"
    },
    {
      "arxiv_id": "2512.17945v1",
      "title": "What's the Price of Monotonicity? A Multi-Dataset Benchmark of Monotone-Constrained Gradient Boosting for Credit PD",
      "authors": [
        {
          "name": "Petr Koklev"
        }
      ],
      "abstract": "Financial institutions face a trade-off between predictive accuracy and interpretability when deploying machine learning models for credit risk. Monotonicity constraints align model behavior with domain knowledge, but their performance cost - the price of monotonicity - is not well quantified. This paper benchmarks monotone-constrained versus unconstrained gradient boosting models for credit probability of default across five public datasets and three libraries. We define the Price of Monotonicity (PoM) as the relative change in standard performance metrics when moving from unconstrained to constrained models, estimated via paired comparisons with bootstrap uncertainty. In our experiments, PoM in AUC ranges from essentially zero to about 2.9 percent: constraints are almost costless on large datasets (typically less than 0.2 percent, often indistinguishable from zero) and most costly on smaller datasets with extensive constraint coverage (around 2-3 percent). Thus, appropriately specified monotonicity constraints can often deliver interpretability with small accuracy losses, particularly in large-scale credit portfolios.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "q-fin.RM",
        "q-fin.ST"
      ],
      "published": "2025-12-14T22:18:05+00:00",
      "updated": "2025-12-14T22:18:05+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.17945v1",
      "file": "papers/2512.17945v1.pdf"
    },
    {
      "arxiv_id": "2512.13742v1",
      "title": "DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models",
      "authors": [
        {
          "name": "Md. Najib Hasan"
        },
        {
          "name": "Imran Ahmad"
        },
        {
          "name": "Sourav Basak Shuvo"
        },
        {
          "name": "Md. Mahadi Hasan Ankon"
        },
        {
          "name": "Sunanda Das"
        },
        {
          "name": "Nazmul Siddique"
        },
        {
          "name": "Hui Wang"
        }
      ],
      "abstract": "Medical image classifiers detect gastrointestinal diseases well, but they do not explain their decisions. Large language models can generate clinical text, yet they struggle with visual reasoning and often produce unstable or incorrect explanations. This leaves a gap between what a model sees and the type of reasoning a clinician expects. We introduce a framework that links image classification with structured clinical reasoning. A new hybrid model, MobileCoAtNet, is designed for endoscopic images and achieves high accuracy across eight stomach-related classes. Its outputs are then used to drive reasoning by several LLMs. To judge this reasoning, we build two expert-verified benchmarks covering causes, symptoms, treatment, lifestyle, and follow-up care. Thirty-two LLMs are evaluated against these gold standards. Strong classification improves the quality of their explanations, but none of the models reach human-level stability. Even the best LLMs change their reasoning when prompts vary. Our study shows that combining DL with LLMs can produce useful clinical narratives, but current LLMs remain unreliable for high-stakes medical decisions. The framework provides a clearer view of their limits and a path for building safer reasoning systems. The complete source code and datasets used in this study are available at https://github.com/souravbasakshuvo/DL3M.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-14T21:20:15+00:00",
      "updated": "2025-12-14T21:20:15+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.13742v1",
      "file": "papers/2512.13742v1.pdf"
    },
    {
      "arxiv_id": "2512.12777v1",
      "title": "State over Tokens: Characterizing the Role of Reasoning Tokens",
      "authors": [
        {
          "name": "Mosh Levy"
        },
        {
          "name": "Zohar Elyoseph"
        },
        {
          "name": "Shauli Ravfogel"
        },
        {
          "name": "Yoav Goldberg"
        }
      ],
      "abstract": "Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-14T17:30:34+00:00",
      "updated": "2025-12-14T17:30:34+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12777v1",
      "file": "papers/2512.12777v1.pdf"
    },
    {
      "arxiv_id": "2512.12605v1",
      "title": "Causal inference and model explainability tools for retail",
      "authors": [
        {
          "name": "Pranav Gupta"
        },
        {
          "name": "Nithin Surendran"
        }
      ],
      "abstract": "Most major retailers today have multiple divisions focused on various aspects, such as marketing, supply chain, online customer experience, store customer experience, employee productivity, and vendor fulfillment. They also regularly collect data corresponding to all these aspects as dashboards and weekly/monthly/quarterly reports. Although several machine learning and statistical techniques have been in place to analyze and predict key metrics, such models typically lack interpretability. Moreover, such techniques also do not allow the validation or discovery of causal links. In this paper, we aim to provide a recipe for applying model interpretability and causal inference for deriving sales insights. In this paper, we review the existing literature on causal inference and interpretability in the context of problems in e-commerce and retail, and apply them to a real-world dataset. We find that an inherently explainable model has a lower variance of SHAP values, and show that including multiple confounders through a double machine learning approach allows us to get the correct sign of causal effect.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-14T09:02:44+00:00",
      "updated": "2025-12-14T09:02:44+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12605v1",
      "file": "papers/2512.12605v1.pdf"
    },
    {
      "arxiv_id": "2512.12597v1",
      "title": "AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation",
      "authors": [
        {
          "name": "Miriam Horovicz"
        }
      ],
      "abstract": "LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-12-14T08:31:43+00:00",
      "updated": "2025-12-14T08:31:43+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12597v1",
      "file": "papers/2512.12597v1.pdf"
    },
    {
      "arxiv_id": "2512.12572v1",
      "title": "On the Accuracy of Newton Step and Influence Function Data Attributions",
      "authors": [
        {
          "name": "Ittai Rubinstein"
        },
        {
          "name": "Samuel B. Hopkins"
        }
      ],
      "abstract": "Data attribution aims to explain model predictions by estimating how they would change if certain training points were removed, and is used in a wide range of applications, from interpretability and credit assignment to unlearning and privacy.\n  Even in the relatively simple case of linear regressions, existing mathematical analyses of leading data attribution methods such as Influence Functions (IF) and single Newton Step (NS) remain limited in two key ways. First, they rely on global strong convexity assumptions which are often not satisfied in practice. Second, the resulting bounds scale very poorly with the number of parameters ($d$) and the number of samples removed ($k$). As a result, these analyses are not tight enough to answer fundamental questions such as \"what is the asymptotic scaling of the errors of each method?\" or \"which of these methods is more accurate for a given dataset?\"\n  In this paper, we introduce a new analysis of the NS and IF data attribution methods for convex learning problems. To the best of our knowledge, this is the first analysis of these questions that does not assume global strong convexity and also the first explanation of [KATL19] and [RH25a]'s observation that NS data attribution is often more accurate than IF. We prove that for sufficiently well-behaved logistic regression, our bounds are asymptotically tight up to poly-logarithmic factors, yielding scaling laws for the errors in the average-case sample removals.\n  \\[ \\mathbb{E}_{T \\subseteq [n],\\, |T| = k} \\bigl[ \\|\\hatθ_T - \\hatθ_T^{\\mathrm{NS}}\\|_2 \\bigr] = \\widetildeΘ\\!\\left(\\frac{k d}{n^2}\\right), \\qquad \\mathbb{E}_{T \\subseteq [n],\\, |T| = k} \\bigl[ \\|\\hatθ_T^{\\mathrm{NS}} - \\hatθ_T^{\\mathrm{IF}}\\|_2 \\bigr] = \\widetildeΘ\\!\\left( \\frac{(k + d)\\sqrt{k d}}{n^2} \\right). \\]",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2025-12-14T06:33:52+00:00",
      "updated": "2025-12-14T06:33:52+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12572v1",
      "file": "papers/2512.12572v1.pdf"
    },
    {
      "arxiv_id": "2512.12506v1",
      "title": "Explainable Artificial Intelligence for Economic Time Series: A Comprehensive Review and a Systematic Taxonomy of Methods and Concepts",
      "authors": [
        {
          "name": "Agustín García-García"
        },
        {
          "name": "Pablo Hidalgo"
        },
        {
          "name": "Julio E. Sandubete"
        }
      ],
      "abstract": "Explainable Artificial Intelligence (XAI) is increasingly required in computational economics, where machine-learning forecasters can outperform classical econometric models but remain difficult to audit and use for policy. This survey reviews and organizes the growing literature on XAI for economic time series, where autocorrelation, non-stationarity, seasonality, mixed frequencies, and regime shifts can make standard explanation techniques unreliable or economically implausible. We propose a taxonomy that classifies methods by (i) explanation mechanism: propagation-based approaches (e.g., Integrated Gradients, Layer-wise Relevance Propagation), perturbation and game-theoretic attribution (e.g., permutation importance, LIME, SHAP), and function-based global tools (e.g., Accumulated Local Effects); (ii) time-series compatibility, including preservation of temporal dependence, stability over time, and respect for data-generating constraints. We synthesize time-series-specific adaptations such as vector- and window-based formulations (e.g., Vector SHAP, WindowSHAP) that reduce lag fragmentation and computational cost while improving interpretability. We also connect explainability to causal inference and policy analysis through interventional attributions (Causal Shapley values) and constrained counterfactual reasoning. Finally, we discuss intrinsically interpretable architectures (notably attention-based transformers) and provide guidance for decision-grade applications such as nowcasting, stress testing, and regime monitoring, emphasizing attribution uncertainty and explanation dynamics as indicators of structural change.",
      "primary_category": "econ.GN",
      "categories": [
        "econ.GN",
        "cs.AI"
      ],
      "published": "2025-12-14T00:45:30+00:00",
      "updated": "2025-12-14T00:45:30+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12506v1",
      "file": "papers/2512.12506v1.pdf"
    },
    {
      "arxiv_id": "2512.12500v1",
      "title": "Explainable AI as a Double-Edged Sword in Dermatology: The Impact on Clinicians versus The Public",
      "authors": [
        {
          "name": "Xuhai Xu"
        },
        {
          "name": "Haoyu Hu"
        },
        {
          "name": "Haoran Zhang"
        },
        {
          "name": "Will Ke Wang"
        },
        {
          "name": "Reina Wang"
        },
        {
          "name": "Luis R. Soenksen"
        },
        {
          "name": "Omar Badri"
        },
        {
          "name": "Sheharbano Jafry"
        },
        {
          "name": "Elise Burger"
        },
        {
          "name": "Lotanna Nwandu"
        },
        {
          "name": "Apoorva Mehta"
        },
        {
          "name": "Erik P. Duhaime"
        },
        {
          "name": "Asif Qasim"
        },
        {
          "name": "Hause Lin"
        },
        {
          "name": "Janis Pereira"
        },
        {
          "name": "Jonathan Hershon"
        },
        {
          "name": "Paulius Mui"
        },
        {
          "name": "Alejandro A. Gru"
        },
        {
          "name": "Noémie Elhadad"
        },
        {
          "name": "Lena Mamykina"
        },
        {
          "name": "Matthew Groh"
        },
        {
          "name": "Philipp Tschandl"
        },
        {
          "name": "Roxana Daneshjou"
        },
        {
          "name": "Marzyeh Ghassemi"
        }
      ],
      "abstract": "Artificial intelligence (AI) is increasingly permeating healthcare, from physician assistants to consumer applications. Since AI algorithm's opacity challenges human interaction, explainable AI (XAI) addresses this by providing AI decision-making insight, but evidence suggests XAI can paradoxically induce over-reliance or bias. We present results from two large-scale experiments (623 lay people; 153 primary care physicians, PCPs) combining a fairness-based diagnosis AI model and different XAI explanations to examine how XAI assistance, particularly multimodal large language models (LLMs), influences diagnostic performance. AI assistance balanced across skin tones improved accuracy and reduced diagnostic disparities. However, LLM explanations yielded divergent effects: lay users showed higher automation bias - accuracy boosted when AI was correct, reduced when AI erred - while experienced PCPs remained resilient, benefiting irrespective of AI accuracy. Presenting AI suggestions first also led to worse outcomes when the AI was incorrect for both groups. These findings highlight XAI's varying impact based on expertise and timing, underscoring LLMs as a \"double-edged sword\" in medical AI and informing future human-AI collaborative system design.",
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "published": "2025-12-14T00:06:06+00:00",
      "updated": "2025-12-14T00:06:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12500v1",
      "file": "papers/2512.12500v1.pdf"
    },
    {
      "arxiv_id": "2512.12469v1",
      "title": "Sparse Concept Anchoring for Interpretable and Controllable Neural Representations",
      "authors": [
        {
          "name": "Sandy Fraser"
        },
        {
          "name": "Patryk Wielopolski"
        }
      ],
      "abstract": "We introduce Sparse Concept Anchoring, a method that biases latent space to position a targeted subset of concepts while allowing others to self-organize, using only minimal supervision (labels for <0.1% of examples per anchored concept). Training combines activation normalization, a separation regularizer, and anchor or subspace regularizers that attract rare labeled examples to predefined directions or axis-aligned subspaces. The anchored geometry enables two practical interventions: reversible behavioral steering that projects out a concept's latent component at inference, and permanent removal via targeted weight ablation of anchored dimensions. Experiments on structured autoencoders show selective attenuation of targeted concepts with negligible impact on orthogonal features, and complete elimination with reconstruction error approaching theoretical bounds. Sparse Concept Anchoring therefore provides a practical pathway to interpretable, steerable behavior in learned representations.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-13T21:43:17+00:00",
      "updated": "2025-12-13T21:43:17+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12469v1",
      "file": "papers/2512.12469v1.pdf"
    },
    {
      "arxiv_id": "2512.12448v1",
      "title": "Optimized Architectures for Kolmogorov-Arnold Networks",
      "authors": [
        {
          "name": "James Bagrow"
        },
        {
          "name": "Josh Bongard"
        }
      ],
      "abstract": "Efforts to improve Kolmogorov-Arnold networks (KANs) with architectural enhancements have been stymied by the complexity those enhancements bring, undermining the interpretability that makes KANs attractive in the first place. Here we study overprovisioned architectures combined with sparsification to learn compact, interpretable KANs without sacrificing accuracy. Crucially, we focus on differentiable sparsification, turning architecture search into an end-to-end optimization problem. Across function approximation benchmarks, dynamical systems forecasting, and real-world prediction tasks, we demonstrate competitive or superior accuracy while discovering substantially smaller models. Overprovisioning and sparsification are synergistic, with the combination outperforming either alone. The result is a principled path toward models that are both more expressive and more interpretable, addressing a key tension in scientific machine learning.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.NE",
        "physics.data-an",
        "stat.ML"
      ],
      "published": "2025-12-13T20:14:08+00:00",
      "updated": "2025-12-13T20:14:08+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12448v1",
      "file": "papers/2512.12448v1.pdf"
    },
    {
      "arxiv_id": "2512.12436v1",
      "title": "Rough Sets for Explainability of Spectral Graph Clustering",
      "authors": [
        {
          "name": "Bartłomiej Starosta"
        },
        {
          "name": "Sławomir T. Wierzchoń"
        },
        {
          "name": "Piotr Borkowski"
        },
        {
          "name": "Dariusz Czerski"
        },
        {
          "name": "Marcin Sydow"
        },
        {
          "name": "Eryk Laskowski"
        },
        {
          "name": "Mieczysław A. Kłopotek"
        }
      ],
      "abstract": "Graph Spectral Clustering methods (GSC) allow representing clusters of diverse shapes, densities, etc. However, the results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Furthermore, the presence of documents without clear content meaning and the stochastic nature of the clustering algorithms deteriorate explainability. This paper proposes an enhancement to the explanation methodology, proposed in an earlier research of our team. It allows us to overcome the latter problems by taking inspiration from rough set theory.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-12-13T19:29:04+00:00",
      "updated": "2025-12-13T19:29:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12436v1",
      "file": "papers/2512.12436v1.pdf"
    },
    {
      "arxiv_id": "2512.12109v2",
      "title": "A Neuro-Symbolic Framework for Accountability in Public-Sector AI",
      "authors": [
        {
          "name": "Allen Daniel Sunny"
        }
      ],
      "abstract": "Automated eligibility systems increasingly determine access to essential public benefits, but the explanations they generate often fail to reflect the legal rules that authorize those decisions. This thesis develops a legally grounded explainability framework that links system-generated decision justifications to the statutory constraints of CalFresh, California's Supplemental Nutrition Assistance Program. The framework combines a structured ontology of eligibility requirements derived from the state's Manual of Policies and Procedures (MPP), a rule extraction pipeline that expresses statutory logic in a verifiable formal representation, and a solver-based reasoning layer to evaluate whether the explanation aligns with governing law. Case evaluations demonstrate the framework's ability to detect legally inconsistent explanations, highlight violated eligibility rules, and support procedural accountability by making the basis of automated determinations traceable and contestable.",
      "primary_category": "cs.CY",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LO"
      ],
      "published": "2025-12-13T00:53:26+00:00",
      "updated": "2025-12-16T22:41:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12109v2",
      "file": "papers/2512.12109v2.pdf"
    },
    {
      "arxiv_id": "2512.12076v1",
      "title": "SigTime: Learning and Visually Explaining Time Series Signatures",
      "authors": [
        {
          "name": "Yu-Chia Huang"
        },
        {
          "name": "Juntong Chen"
        },
        {
          "name": "Dongyu Liu"
        },
        {
          "name": "Kwan-Liu Ma"
        }
      ],
      "abstract": "Understanding and distinguishing temporal patterns in time series data is essential for scientific discovery and decision-making. For example, in biomedical research, uncovering meaningful patterns in physiological signals can improve diagnosis, risk assessment, and patient outcomes. However, existing methods for time series pattern discovery face major challenges, including high computational complexity, limited interpretability, and difficulty in capturing meaningful temporal structures. To address these gaps, we introduce a novel learning framework that jointly trains two Transformer models using complementary time series representations: shapelet-based representations to capture localized temporal structures and traditional feature engineering to encode statistical properties. The learned shapelets serve as interpretable signatures that differentiate time series across classification labels. Additionally, we develop a visual analytics system -- SigTIme -- with coordinated views to facilitate exploration of time series signatures from multiple perspectives, aiding in useful insights generation. We quantitatively evaluate our learning framework on eight publicly available datasets and one proprietary clinical dataset. Additionally, we demonstrate the effectiveness of our system through two usage scenarios along with the domain experts: one involving public ECG data and the other focused on preterm labor analysis.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2025-12-12T22:47:34+00:00",
      "updated": "2025-12-12T22:47:34+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.12076v1",
      "file": "papers/2512.12076v1.pdf"
    },
    {
      "arxiv_id": "2512.11946v1",
      "title": "Data-Driven Global Sensitivity Analysis for Engineering Design Based on Individual Conditional Expectations",
      "authors": [
        {
          "name": "Pramudita Satria Palar"
        },
        {
          "name": "Paul Saves"
        },
        {
          "name": "Rommel G. Regis"
        },
        {
          "name": "Koji Shimoyama"
        },
        {
          "name": "Shigeru Obayashi"
        },
        {
          "name": "Nicolas Verstaevel"
        },
        {
          "name": "Joseph Morlier"
        }
      ],
      "abstract": "Explainable machine learning techniques have gained increasing attention in engineering applications, especially in aerospace design and analysis, where understanding how input variables influence data-driven models is essential. Partial Dependence Plots (PDPs) are widely used for interpreting black-box models by showing the average effect of an input variable on the prediction. However, their global sensitivity metric can be misleading when strong interactions are present, as averaging tends to obscure interaction effects. To address this limitation, we propose a global sensitivity metric based on Individual Conditional Expectation (ICE) curves. The method computes the expected feature importance across ICE curves, along with their standard deviation, to more effectively capture the influence of interactions. We provide a mathematical proof demonstrating that the PDP-based sensitivity is a lower bound of the proposed ICE-based metric under truncated orthogonal polynomial expansion. In addition, we introduce an ICE-based correlation value to quantify how interactions modify the relationship between inputs and the output. Comparative evaluations were performed on three cases: a 5-variable analytical function, a 5-variable wind-turbine fatigue problem, and a 9-variable airfoil aerodynamics case, where ICE-based sensitivity was benchmarked against PDP, SHapley Additive exPlanations (SHAP), and Sobol' indices. The results show that ICE-based feature importance provides richer insights than the traditional PDP-based approach, while visual interpretations from PDP, ICE, and SHAP complement one another by offering multiple perspectives.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2025-12-12T15:28:17+00:00",
      "updated": "2025-12-12T15:28:17+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.11946v1",
      "file": "papers/2512.11946v1.pdf"
    },
    {
      "arxiv_id": "2512.11616v1",
      "title": "A Fast Interpretable Fuzzy Tree Learner",
      "authors": [
        {
          "name": "Javier Fumanal-Idocin"
        },
        {
          "name": "Raquel Fernandez-Peralta"
        },
        {
          "name": "Javier Andreu-Perez"
        }
      ],
      "abstract": "Fuzzy rule-based systems have been mostly used in interpretable decision-making because of their interpretable linguistic rules. However, interpretability requires both sensible linguistic partitions and small rule-base sizes, which are not guaranteed by many existing fuzzy rule-mining algorithms. Evolutionary approaches can produce high-quality models but suffer from prohibitive computational costs, while neural-based methods like ANFIS have problems retaining linguistic interpretations. In this work, we propose an adaptation of classical tree-based splitting algorithms from crisp rules to fuzzy trees, combining the computational efficiency of greedy algoritms with the interpretability advantages of fuzzy logic. This approach achieves interpretable linguistic partitions and substantially improves running time compared to evolutionary-based approaches while maintaining competitive predictive performance. Our experiments on tabular classification benchmarks proof that our method achieves comparable accuracy to state-of-the-art fuzzy classifiers with significantly lower computational cost and produces more interpretable rule bases with constrained complexity. Code is available in: https://github.com/Fuminides/fuzzy_greedy_tree_public",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.SC"
      ],
      "published": "2025-12-12T14:51:07+00:00",
      "updated": "2025-12-12T14:51:07+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.11616v1",
      "file": "papers/2512.11616v1.pdf"
    },
    {
      "arxiv_id": "2512.11614v1",
      "title": "Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols",
      "authors": [
        {
          "name": "Björn Deiseroth"
        },
        {
          "name": "Max Henning Höth"
        },
        {
          "name": "Kristian Kersting"
        },
        {
          "name": "Letitia Parcalabescu"
        }
      ],
      "abstract": "Retrieval-augmented generation (RAG) models rely on retrieved evidence to guide large language model (LLM) generators, yet current systems treat retrieval as a weak heuristic rather than verifiable evidence. As a result, LLMs answer without support, hallucinate under incomplete or misleading context, and rely on spurious evidence. We introduce a training framework that treats the entire RAG pipeline -- both the retriever and the generator -- as an interactive proof system via an adaptation of the Merlin-Arthur (M/A) protocol. Arthur (the generator LLM) trains on questions of unkown provenance: Merlin provides helpful evidence, while Morgana injects adversarial, misleading context. Both use a linear-time XAI method to identify and modify the evidence most influential to Arthur. Consequently, Arthur learns to (i) answer when the context support the answer, (ii) reject when evidence is insufficient, and (iii) rely on the specific context spans that truly ground the answer. We further introduce a rigorous evaluation framework to disentangle explanation fidelity from baseline predictive errors. This allows us to introduce and measure the Explained Information Fraction (EIF), which normalizes M/A certified mutual-information guarantees relative to model capacity and imperfect benchmarks. Across three RAG datasets and two model families of varying sizes, M/A-trained LLMs show improved groundedness, completeness, soundness, and reject behavior, as well as reduced hallucinations -- without needing manually annotated unanswerable questions. The retriever likewise improves recall and MRR through automatically generated M/A hard positives and negatives. Our results demonstrate that autonomous interactive-proof-style supervision provides a principled and practical path toward reliable RAG systems that treat retrieved documents not as suggestions, but as verifiable evidence.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-12T14:50:38+00:00",
      "updated": "2025-12-12T14:50:38+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.11614v1",
      "file": "papers/2512.11614v1.pdf"
    },
    {
      "arxiv_id": "2512.11573v1",
      "title": "Visualizing token importance for black-box language models",
      "authors": [
        {
          "name": "Paulius Rauba"
        },
        {
          "name": "Qiyao Wei"
        },
        {
          "name": "Mihaela van der Schaar"
        }
      ],
      "abstract": "We consider the problem of auditing black-box large language models (LLMs) to ensure they behave reliably when deployed in production settings, particularly in high-stakes domains such as legal, medical, and regulatory compliance. Existing approaches for LLM auditing often focus on isolated aspects of model behavior, such as detecting specific biases or evaluating fairness. We are interested in a more general question -- can we understand how the outputs of black-box LLMs depend on each input token? There is a critical need to have such tools in real-world applications that rely on inaccessible API endpoints to language models. However, this is a highly non-trivial problem, as LLMs are stochastic functions (i.e. two outputs will be different by chance), while computing prompt-level gradients to approximate input sensitivity is infeasible. To address this, we propose Distribution-Based Sensitivity Analysis (DBSA), a lightweight model-agnostic procedure to evaluate the sensitivity of the output of a language model for each input token, without making any distributional assumptions about the LLM. DBSA is developed as a practical tool for practitioners, enabling quick, plug-and-play visual exploration of LLMs reliance on specific input tokens. Through illustrative examples, we demonstrate how DBSA can enable users to inspect LLM inputs and find sensitivities that may be overlooked by existing LLM interpretability methods.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-12-12T14:01:43+00:00",
      "updated": "2025-12-12T14:01:43+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.11573v1",
      "file": "papers/2512.11573v1.pdf"
    },
    {
      "arxiv_id": "2512.11433v1",
      "title": "Back to the Baseline: Examining Baseline Effects on Explainability Metrics",
      "authors": [
        {
          "name": "Agustin Martin Picard"
        },
        {
          "name": "Thibaut Boissin"
        },
        {
          "name": "Varshini Subhash"
        },
        {
          "name": "Rémi Cadène"
        },
        {
          "name": "Thomas Fel"
        }
      ],
      "abstract": "Attribution methods are among the most prevalent techniques in Explainable Artificial Intelligence (XAI) and are usually evaluated and compared using Fidelity metrics, with Insertion and Deletion being the most popular. These metrics rely on a baseline function to alter the pixels of the input image that the attribution map deems most important. In this work, we highlight a critical problem with these metrics: the choice of a given baseline will inevitably favour certain attribution methods over others. More concerningly, even a simple linear model with commonly used baselines contradicts itself by designating different optimal methods. A question then arises: which baseline should we use? We propose to study this problem through two desirable properties of a baseline: (i) that it removes information and (ii) that it does not produce overly out-of-distribution (OOD) images. We first show that none of the tested baselines satisfy both criteria, and there appears to be a trade-off among current baselines: either they remove information or they produce a sequence of OOD images. Finally, we introduce a novel baseline by leveraging recent work in feature visualisation to artificially produce a model-dependent baseline that removes information without being overly OOD, thus improving on the trade-off when compared to other existing baselines. Our code is available at https://github.com/deel-ai-papers/Back-to-the-Baseline",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-12-12T10:13:44+00:00",
      "updated": "2025-12-12T10:13:44+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.11433v1",
      "file": "papers/2512.11433v1.pdf"
    },
    {
      "arxiv_id": "2512.11412v1",
      "title": "Task-Specific Sparse Feature Masks for Molecular Toxicity Prediction with Chemical Language Models",
      "authors": [
        {
          "name": "Kwun Sy Lee"
        },
        {
          "name": "Jiawei Chen"
        },
        {
          "name": "Fuk Sheng Ford Chung"
        },
        {
          "name": "Tianyu Zhao"
        },
        {
          "name": "Zhenyuan Chen"
        },
        {
          "name": "Debby D. Wang"
        }
      ],
      "abstract": "Reliable in silico molecular toxicity prediction is a cornerstone of modern drug discovery, offering a scalable alternative to experimental screening. However, the black-box nature of state-of-the-art models remains a significant barrier to adoption, as high-stakes safety decisions demand verifiable structural insights alongside predictive performance. To address this, we propose a novel multi-task learning (MTL) framework designed to jointly enhance accuracy and interpretability. Our architecture integrates a shared chemical language model with task-specific attention modules. By imposing an L1 sparsity penalty on these modules, the framework is constrained to focus on a minimal set of salient molecular fragments for each distinct toxicity endpoint. The resulting framework is trained end-to-end and is readily adaptable to various transformer-based backbones. Evaluated on the ClinTox, SIDER, and Tox21 benchmark datasets, our approach consistently outperforms both single-task and standard MTL baselines. Crucially, the sparse attention weights provide chemically intuitive visualizations that reveal the specific fragments influencing predictions, thereby enhancing insight into the model's decision-making process.",
      "primary_category": "cs.CE",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "q-bio.BM"
      ],
      "published": "2025-12-12T09:41:04+00:00",
      "updated": "2025-12-12T09:41:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.11412v1",
      "file": "papers/2512.11412v1.pdf"
    },
    {
      "arxiv_id": "2512.15761v1",
      "title": "Machine Learning Framework for Thrombosis Risk Prediction in Rotary Blood Pumps",
      "authors": [
        {
          "name": "Christopher Blum"
        },
        {
          "name": "Michael Neidlin"
        }
      ],
      "abstract": "Thrombosis in rotary blood pumps arises from complex flow conditions that remain difficult to translate into reliable and interpretable risk predictions using existing computational models. This limitation reflects an incomplete understanding of how specific flow features contribute to thrombus initiation and growth. This study introduces an interpretable machine learning framework for spatial thrombosis assessment based directly on computational fluid dynamics-derived flow features. A logistic regression (LR) model combined with a structured feature-selection pipeline is used to derive a compact and physically interpretable feature set, including nonlinear feature combinations. The framework is trained using spatial risk patterns from a validated, macro-scale thrombosis model for two representative scenarios. The model reproduces the labeled risk distributions and identifies distinct sets of flow features associated with increased thrombosis risk. When applied to a centrifugal pump, despite training on a single axial pump operating point, the model predicts plausible thrombosis-prone regions. These results show that interpretable machine learning can link local flow features to thrombosis risk while remaining computationally efficient and mechanistically transparent. The low computational cost enables rapid thrombogenicity screening without repeated or costly simulations. The proposed framework complements physics-based thrombosis modeling and provides a methodological basis for integrating interpretable machine learning into CFD-driven thrombosis analysis and device design workflows.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "physics.flu-dyn"
      ],
      "published": "2025-12-12T07:50:56+00:00",
      "updated": "2025-12-12T07:50:56+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15761v1",
      "file": "papers/2512.15761v1.pdf"
    },
    {
      "arxiv_id": "2512.11298v1",
      "title": "SRLR: Symbolic Regression based Logic Recovery to Counter Programmable Logic Controller Attacks",
      "authors": [
        {
          "name": "Hao Zhou"
        },
        {
          "name": "Suman Sourav"
        },
        {
          "name": "Binbin Chen"
        },
        {
          "name": "Ke Yu"
        }
      ],
      "abstract": "Programmable Logic Controllers (PLCs) are critical components in Industrial Control Systems (ICSs). Their potential exposure to external world makes them susceptible to cyber-attacks. Existing detection methods against controller logic attacks use either specification-based or learnt models. However, specification-based models require experts' manual efforts or access to PLC's source code, while machine learning-based models often fall short of providing explanation for their decisions. We design SRLR -- a it Symbolic Regression based Logic Recovery} solution to identify the logic of a PLC based only on its inputs and outputs. The recovered logic is used to generate explainable rules for detecting controller logic attacks. SRLR enhances the latest deep symbolic regression methods using the following ICS-specific properties: (1) some important ICS control logic is best represented in frequency domain rather than time domain; (2) an ICS controller can operate in multiple modes, each using different logic, where mode switches usually do not happen frequently; (3) a robust controller usually filters out outlier inputs as ICS sensor data can be noisy; and (4) with the above factors captured, the degree of complexity of the formulas is reduced, making effective search possible. Thanks to these enhancements, SRLR consistently outperforms all existing methods in a variety of ICS settings that we evaluate. In terms of the recovery accuracy, SRLR's gain can be as high as 39% in some challenging environment. We also evaluate SRLR on a distribution grid containing hundreds of voltage regulators, demonstrating its stability in handling large-scale, complex systems with varied configurations.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-12T05:47:39+00:00",
      "updated": "2025-12-12T05:47:39+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.11298v1",
      "file": "papers/2512.11298v1.pdf"
    },
    {
      "arxiv_id": "2512.11263v2",
      "title": "Features Emerge as Discrete States: The First Application of SAEs to 3D Representations",
      "authors": [
        {
          "name": "Albert Miao"
        },
        {
          "name": "Chenliang Zhou"
        },
        {
          "name": "Jiawei Zhou"
        },
        {
          "name": "Cengiz Oztireli"
        }
      ],
      "abstract": "Sparse Autoencoders (SAEs) are a powerful dictionary learning technique for decomposing neural network activations, translating the hidden state into human ideas with high semantic value despite no external intervention or guidance. However, this technique has rarely been applied outside of the textual domain, limiting theoretical explorations of feature decomposition. We present the first application of SAEs to the 3D domain, analyzing the features used by a state-of-the-art 3D reconstruction VAE applied to 53k 3D models from the Objaverse dataset. We observe that the network encodes discrete rather than continuous features, leading to our key finding: such models approximate a discrete state space, driven by phase-like transitions from feature activations. Through this state transition framework, we address three otherwise unintuitive behaviors - the inclination of the reconstruction model towards positional encoding representations, the sigmoidal behavior of reconstruction loss from feature ablation, and the bimodality in the distribution of phase transition points. This final observation suggests the model redistributes the interference caused by superposition to prioritize the saliency of different features. Our work not only compiles and explains unexpected phenomena regarding feature decomposition, but also provides a framework to explain the model's feature learning dynamics. The code and dataset of encoded 3D objects will be available on release.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-12T03:54:45+00:00",
      "updated": "2025-12-15T23:25:31+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.11263v2",
      "file": "papers/2512.11263v2.pdf"
    },
    {
      "arxiv_id": "2512.15755v1",
      "title": "KAN-Matrix: Visualizing Nonlinear Pairwise and Multivariate Contributions for Physical Insight",
      "authors": [
        {
          "name": "Luis A. De la Fuente"
        },
        {
          "name": "Hernan A. Moreno"
        },
        {
          "name": "Laura V. Alvarez"
        },
        {
          "name": "Hoshin V. Gupta"
        }
      ],
      "abstract": "Interpreting complex datasets remains a major challenge for scientists, particularly due to high dimensionality and collinearity among variables. We introduce a novel application of Kolmogorov-Arnold Networks (KANs) to enhance interpretability and parsimony beyond what traditional correlation analyses offer. We present two interpretable, color-coded visualization tools: the Pairwise KAN Matrix (PKAN) and the Multivariate KAN Contribution Matrix (MKAN). PKAN characterizes nonlinear associations between pairs of variables, while MKAN serves as a nonlinear feature-ranking tool that quantifies the relative contributions of inputs in predicting a target variable. These tools support pre-processing (e.g., feature selection, redundancy analysis) and post-processing (e.g., model explanation, physical insights) in model development workflows. Through experimental comparisons, we demonstrate that PKAN and MKAN yield more robust and informative results than Pearson Correlation and Mutual Information. By capturing the strength and functional forms of relationships, these matrices facilitate the discovery of hidden physical patterns and promote domain-informed model development.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "physics.app-ph",
        "physics.data-an"
      ],
      "published": "2025-12-12T02:04:53+00:00",
      "updated": "2025-12-12T02:04:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.15755v1",
      "file": "papers/2512.15755v1.pdf"
    },
    {
      "arxiv_id": "2512.11108v1",
      "title": "Explanation Bias is a Product: Revealing the Hidden Lexical and Position Preferences in Post-Hoc Feature Attribution",
      "authors": [
        {
          "name": "Jonathan Kamp"
        },
        {
          "name": "Roos Bakker"
        },
        {
          "name": "Dominique Blok"
        }
      ],
      "abstract": "Good quality explanations strengthen the understanding of language models and data. Feature attribution methods, such as Integrated Gradient, are a type of post-hoc explainer that can provide token-level insights. However, explanations on the same input may vary greatly due to underlying biases of different methods. Users may be aware of this issue and mistrust their utility, while unaware users may trust them inadequately. In this work, we delve beyond the superficial inconsistencies between attribution methods, structuring their biases through a model- and method-agnostic framework of three evaluation metrics. We systematically assess both the lexical and position bias (what and where in the input) for two transformers; first, in a controlled, pseudo-random classification task on artificial data; then, in a semi-controlled causal relation detection task on natural data. We find that lexical and position biases are structurally unbalanced in our model comparison, with models that score high on one type score low on the other. We also find signs that methods producing anomalous explanations are more likely to be biased themselves.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-11T20:48:22+00:00",
      "updated": "2025-12-11T20:48:22+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.11108v1",
      "file": "papers/2512.11108v1.pdf"
    },
    {
      "arxiv_id": "2512.11081v1",
      "title": "Provable Recovery of Locally Important Signed Features and Interactions from Random Forest",
      "authors": [
        {
          "name": "Kata Vuk"
        },
        {
          "name": "Nicolas Alexander Ihlo"
        },
        {
          "name": "Merle Behr"
        }
      ],
      "abstract": "Feature and Interaction Importance (FII) methods are essential in supervised learning for assessing the relevance of input variables and their interactions in complex prediction models. In many domains, such as personalized medicine, local interpretations for individual predictions are often required, rather than global scores summarizing overall feature importance. Random Forests (RFs) are widely used in these settings, and existing interpretability methods typically exploit tree structures and split statistics to provide model-specific insights. However, theoretical understanding of local FII methods for RF remains limited, making it unclear how to interpret high importance scores for individual predictions. We propose a novel, local, model-specific FII method that identifies frequent co-occurrences of features along decision paths, combining global patterns with those observed on paths specific to a given test point. We prove that our method consistently recovers the true local signal features and their interactions under a Locally Spike Sparse (LSS) model and also identifies whether large or small feature values drive a prediction. We illustrate the usefulness of our method and theoretical results through simulation studies and a real-world data example.",
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "published": "2025-12-11T19:53:15+00:00",
      "updated": "2025-12-11T19:53:15+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.11081v1",
      "file": "papers/2512.11081v1.pdf"
    },
    {
      "arxiv_id": "2512.11067v1",
      "title": "KathDB: Explainable Multimodal Database Management System with Human-AI Collaboration",
      "authors": [
        {
          "name": "Guorui Xiao"
        },
        {
          "name": "Enhao Zhang"
        },
        {
          "name": "Nicole Sullivan"
        },
        {
          "name": "Will Hansen"
        },
        {
          "name": "Magdalena Balazinska"
        }
      ],
      "abstract": "Traditional DBMSs execute user- or application-provided SQL queries over relational data with strong semantic guarantees and advanced query optimization, but writing complex SQL is hard and focuses only on structured tables. Contemporary multimodal systems (which operate over relations but also text, images, and even videos) either expose low-level controls that force users to use (and possibly create) machine learning UDFs manually within SQL or offload execution entirely to black-box LLMs, sacrificing usability or explainability. We propose KathDB, a new system that combines relational semantics with the reasoning power of foundation models over multimodal data. Furthermore, KathDB includes human-AI interaction channels during query parsing, execution, and result explanation, such that users can iteratively obtain explainable answers across data modalities.",
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "published": "2025-12-11T19:36:23+00:00",
      "updated": "2025-12-11T19:36:23+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.11067v1",
      "file": "papers/2512.11067v1.pdf"
    },
    {
      "arxiv_id": "2512.10805v1",
      "title": "Interpretable and Steerable Concept Bottleneck Sparse Autoencoders",
      "authors": [
        {
          "name": "Akshay Kulkarni"
        },
        {
          "name": "Tsui-Wei Weng"
        },
        {
          "name": "Vivek Narayanaswamy"
        },
        {
          "name": "Shusen Liu"
        },
        {
          "name": "Wesam A. Sakla"
        },
        {
          "name": "Kowshik Thopalli"
        }
      ],
      "abstract": "Sparse autoencoders (SAEs) promise a unified approach for mechanistic interpretability, concept discovery, and model steering in LLMs and LVLMs. However, realizing this potential requires that the learned features be both interpretable and steerable. To that end, we introduce two new computationally inexpensive interpretability and steerability metrics and conduct a systematic analysis on LVLMs. Our analysis uncovers two observations; (i) a majority of SAE neurons exhibit either low interpretability or low steerability or both, rendering them ineffective for downstream use; and (ii) due to the unsupervised nature of SAEs, user-desired concepts are often absent in the learned dictionary, thus limiting their practical utility. To address these limitations, we propose Concept Bottleneck Sparse Autoencoders (CB-SAE) - a novel post-hoc framework that prunes low-utility neurons and augments the latent space with a lightweight concept bottleneck aligned to a user-defined concept set. The resulting CB-SAE improves interpretability by +32.1% and steerability by +14.5% across LVLMs and image generation tasks. We will make our code and model weights available.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "published": "2025-12-11T16:48:07+00:00",
      "updated": "2025-12-11T16:48:07+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.10805v1",
      "file": "papers/2512.10805v1.pdf"
    },
    {
      "arxiv_id": "2512.10745v1",
      "title": "PMB-NN: Physiology-Centred Hybrid AI for Personalized Hemodynamic Monitoring from Photoplethysmography",
      "authors": [
        {
          "name": "Yaowen Zhang"
        },
        {
          "name": "Libera Fresiello"
        },
        {
          "name": "Peter H. Veltink"
        },
        {
          "name": "Dirk W. Donker"
        },
        {
          "name": "Ying Wang"
        }
      ],
      "abstract": "Continuous monitoring of blood pressure (BP) and hemodynamic parameters such as peripheral resistance (R) and arterial compliance (C) are critical for early vascular dysfunction detection. While photoplethysmography (PPG) wearables has gained popularity, existing data-driven methods for BP estimation lack interpretability. We advanced our previously proposed physiology-centered hybrid AI method-Physiological Model-Based Neural Network (PMB-NN)-in blood pressure estimation, that unifies deep learning with a 2-element Windkessel based model parameterized by R and C acting as physics constraints. The PMB-NN model was trained in a subject-specific manner using PPG-derived timing features, while demographic information was used to infer an intermediate variable: cardiac output. We validated the model on 10 healthy adults performing static and cycling activities across two days for model's day-to-day robustness, benchmarked against deep learning (DL) models (FCNN, CNN-LSTM, Transformer) and standalone Windkessel based physiological model (PM). Validation was conducted on three perspectives: accuracy, interpretability and plausibility. PMB-NN achieved systolic BP accuracy (MAE: 7.2 mmHg) comparable to DL benchmarks, diastolic performance (MAE: 3.9 mmHg) lower than DL models. However, PMB-NN exhibited higher physiological plausibility than both DL baselines and PM, suggesting that the hybrid architecture unifies and enhances the respective merits of physiological principles and data-driven techniques. Beyond BP, PMB-NN identified R (ME: 0.15 mmHg$\\cdot$s/ml) and C (ME: -0.35 ml/mmHg) during training with accuracy similar to PM, demonstrating that the embedded physiological constraints confer interpretability to the hybrid AI framework. These results position PMB-NN as a balanced, physiologically grounded alternative to purely data-driven approaches for daily hemodynamic monitoring.",
      "primary_category": "physics.med-ph",
      "categories": [
        "physics.med-ph",
        "cs.LG"
      ],
      "published": "2025-12-11T15:32:50+00:00",
      "updated": "2025-12-11T15:32:50+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.10745v1",
      "file": "papers/2512.10745v1.pdf"
    },
    {
      "arxiv_id": "2512.10720v1",
      "title": "Beyond the Black Box: Identifiable Interpretation and Control in Generative Models via Causal Minimality",
      "authors": [
        {
          "name": "Lingjing Kong"
        },
        {
          "name": "Shaoan Xie"
        },
        {
          "name": "Guangyi Chen"
        },
        {
          "name": "Yuewen Sun"
        },
        {
          "name": "Xiangchen Song"
        },
        {
          "name": "Eric P. Xing"
        },
        {
          "name": "Kun Zhang"
        }
      ],
      "abstract": "Deep generative models, while revolutionizing fields like image and text generation, largely operate as opaque black boxes, hindering human understanding, control, and alignment. While methods like sparse autoencoders (SAEs) show remarkable empirical success, they often lack theoretical guarantees, risking subjective insights. Our primary objective is to establish a principled foundation for interpretable generative models. We demonstrate that the principle of causal minimality -- favoring the simplest causal explanation -- can endow the latent representations of diffusion vision and autoregressive language models with clear causal interpretation and robust, component-wise identifiable control. We introduce a novel theoretical framework for hierarchical selection models, where higher-level concepts emerge from the constrained composition of lower-level variables, better capturing the complex dependencies in data generation. Under theoretically derived minimality conditions (manifesting as sparsity or compression constraints), we show that learned representations can be equivalent to the true latent variables of the data-generating process. Empirically, applying these constraints to leading generative models allows us to extract their innate hierarchical concept graphs, offering fresh insights into their internal knowledge organization. Furthermore, these causally grounded concepts serve as levers for fine-grained model steering, paving the way for transparent, reliable systems.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-11T14:59:14+00:00",
      "updated": "2025-12-11T14:59:14+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.10720v1",
      "file": "papers/2512.10720v1.pdf"
    },
    {
      "arxiv_id": "2512.10659v2",
      "title": "DCFO: Density-Based Counterfactuals for Outliers - Additional Material",
      "authors": [
        {
          "name": "Tommaso Amico"
        },
        {
          "name": "Pernille Matthews"
        },
        {
          "name": "Lena Krieger"
        },
        {
          "name": "Arthur Zimek"
        },
        {
          "name": "Ira Assent"
        }
      ],
      "abstract": "Outlier detection identifies data points that significantly deviate from the majority of the data distribution. Explaining outliers is crucial for understanding the underlying factors that contribute to their detection, validating their significance, and identifying potential biases or errors. Effective explanations provide actionable insights, facilitating preventive measures to avoid similar outliers in the future. Counterfactual explanations clarify why specific data points are classified as outliers by identifying minimal changes required to alter their prediction. Although valuable, most existing counterfactual explanation methods overlook the unique challenges posed by outlier detection, and fail to target classical, widely adopted outlier detection algorithms. Local Outlier Factor (LOF) is one the most popular unsupervised outlier detection methods, quantifying outlierness through relative local density. Despite LOF's widespread use across diverse applications, it lacks interpretability. To address this limitation, we introduce Density-based Counterfactuals for Outliers (DCFO), a novel method specifically designed to generate counterfactual explanations for LOF. DCFO partitions the data space into regions where LOF behaves smoothly, enabling efficient gradient-based optimisation. Extensive experimental validation on 50 OpenML datasets demonstrates that DCFO consistently outperforms benchmarked competitors, offering superior proximity and validity of generated counterfactuals.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-11T14:04:52+00:00",
      "updated": "2025-12-18T15:12:09+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.10659v2",
      "file": "papers/2512.10659v2.pdf"
    },
    {
      "arxiv_id": "2512.10547v1",
      "title": "Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders",
      "authors": [
        {
          "name": "Qingsen Ma"
        },
        {
          "name": "Dianyun Wang"
        },
        {
          "name": "Jiaming Lyu"
        },
        {
          "name": "Yaoye Wang"
        },
        {
          "name": "Lechen Ning"
        },
        {
          "name": "Sujie Zhu"
        },
        {
          "name": "Zhenbo Xu"
        },
        {
          "name": "Liuyu Xiang"
        },
        {
          "name": "Huining Li"
        },
        {
          "name": "Huijia Wu"
        },
        {
          "name": "Zhaofeng He"
        }
      ],
      "abstract": "The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \\textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis uncovers a fundamental \\textbf{Key-Value Asymmetry}: while Key vectors serve as highly sparse routers dominated by a ``Semantic Elbow,'' deep Value vectors carry dense content payloads requiring a larger budget. Based on this structure, we introduce a Dual-Budget Strategy that selectively preserves the most informative semantic components while filtering representational noise. Experiments on Yi-6B, Mistral-7B, Qwen2.5-32B, and others show that our semantic reconstructions maintain perplexity and zero-shot performance comparable to the original models, effectively bridging the gap between mechanistic interpretability and faithful attention modeling.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-11T11:23:50+00:00",
      "updated": "2025-12-11T11:23:50+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.10547v1",
      "file": "papers/2512.10547v1.pdf"
    },
    {
      "arxiv_id": "2512.10308v1",
      "title": "An Interpretable AI Tool for SAVR vs TAVR in Low to Intermediate Risk Patients with Severe Aortic Stenosis",
      "authors": [
        {
          "name": "Vasiliki Stoumpou"
        },
        {
          "name": "Maciej Tysarowski"
        },
        {
          "name": "Talhat Azemi"
        },
        {
          "name": "Jawad Haider"
        },
        {
          "name": "Howard L. Haronian"
        },
        {
          "name": "Robert C. Hagberg"
        },
        {
          "name": "Dimitris Bertsimas"
        }
      ],
      "abstract": "Background. Treatment selection for low to intermediate risk patients with severe aortic stenosis between surgical (SAVR) and transcatheter (TAVR) aortic valve replacement remains variable in clinical practice, driven by patient heterogeneity and institutional preferences. While existing models predict postprocedural risk, there is a lack of interpretable, individualized treatment recommendations that directly optimize long-term outcomes.\n  Methods. We introduce an interpretable prescriptive framework that integrates prognostic matching, counterfactual outcome modeling, and an Optimal Policy Tree (OPT) to recommend the treatment minimizing expected 5-year mortality. Using data from Hartford Hospital and St. Vincent's Hospital, we emulate randomization via prognostic matching and sample weighting and estimate counterfactual mortality under both SAVR and TAVR. The policy model, trained on these counterfactual predictions, partitions patients into clinically coherent subgroups and prescribes the treatment associated with lower estimated risk.\n  Findings. If the OPT prescriptions are applied, counterfactual evaluation showed an estimated reduction in 5-year mortality of 20.3\\% in Hartford and 13.8\\% in St. Vincent's relative to real-life prescriptions, showing promising generalizability to unseen data from a different institution. The learned decision boundaries aligned with real-world outcomes and clinical observations.\n  Interpretation. Our interpretable prescriptive framework is, to the best of our knowledge, the first to provide transparent, data-driven recommendations for TAVR versus SAVR that improve estimated long-term outcomes both in an internal and external cohort, while remaining clinically grounded and contributing toward a more systematic and evidence-based approach to precision medicine in structural heart disease.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-11T05:54:22+00:00",
      "updated": "2025-12-11T05:54:22+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.10308v1",
      "file": "papers/2512.10308v1.pdf"
    },
    {
      "arxiv_id": "2512.10300v1",
      "title": "Investigating The Functional Roles of Attention Heads in Vision Language Models: Evidence for Reasoning Modules",
      "authors": [
        {
          "name": "Yanbei Jiang"
        },
        {
          "name": "Xueqi Ma"
        },
        {
          "name": "Shu Liu"
        },
        {
          "name": "Sarah Monazam Erfani"
        },
        {
          "name": "Tongliang Liu"
        },
        {
          "name": "James Bailey"
        },
        {
          "name": "Jey Han Lau"
        },
        {
          "name": "Krista A. Ehinger"
        }
      ],
      "abstract": "Despite excelling on multimodal benchmarks, vision-language models (VLMs) largely remain a black box. In this paper, we propose a novel interpretability framework to systematically analyze the internal mechanisms of VLMs, focusing on the functional roles of attention heads in multimodal reasoning. To this end, we introduce CogVision, a dataset that decomposes complex multimodal questions into step-by-step subquestions designed to simulate human reasoning through a chain-of-thought paradigm, with each subquestion associated with specific receptive or cognitive functions such as high-level visual reception and inference. Using a probing-based methodology, we identify attention heads that specialize in these functions and characterize them as functional heads. Our analysis across diverse VLM families reveals that these functional heads are universally sparse, vary in number and distribution across functions, and mediate interactions and hierarchical organization. Furthermore, intervention experiments demonstrate their critical role in multimodal reasoning: removing functional heads leads to performance degradation, while emphasizing them enhances accuracy. These findings provide new insights into the cognitive organization of VLMs and suggest promising directions for designing models with more human-aligned perceptual and reasoning abilities.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-12-11T05:42:53+00:00",
      "updated": "2025-12-11T05:42:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.10300v1",
      "file": "papers/2512.10300v1.pdf"
    },
    {
      "arxiv_id": "2512.11909v1",
      "title": "Causal Strengths and Leaky Beliefs: Interpreting LLM Reasoning via Noisy-OR Causal Bayes Nets",
      "authors": [
        {
          "name": "Hanna Dettki"
        }
      ],
      "abstract": "The nature of intelligence in both humans and machines is a longstanding question. While there is no universally accepted definition, the ability to reason causally is often regarded as a pivotal aspect of intelligence (Lake et al., 2017). Evaluating causal reasoning in LLMs and humans on the same tasks provides hence a more comprehensive understanding of their respective strengths and weaknesses. Our study asks: (Q1) Are LLMs aligned with humans given the \\emph{same} reasoning tasks? (Q2) Do LLMs and humans reason consistently at the task level? (Q3) Do they have distinct reasoning signatures?\n  We answer these by evaluating 20+ LLMs on eleven semantically meaningful causal tasks formalized by a collider graph ($C_1\\!\\to\\!E\\!\\leftarrow\\!C_2$ ) under \\emph{Direct} (one-shot number as response = probability judgment of query node being one and \\emph{Chain of Thought} (CoT; think first, then provide answer).\n  Judgments are modeled with a leaky noisy-OR causal Bayes net (CBN) whose parameters $θ=(b,m_1,m_2,p(C)) \\in [0,1]$ include a shared prior $p(C)$;\n  we select the winning model via AIC between a 3-parameter symmetric causal strength ($m_1{=}m_2$) and 4-parameter asymmetric ($m_1{\\neq}m_2$) variant.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-12-10T21:58:16+00:00",
      "updated": "2025-12-10T21:58:16+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.11909v1",
      "file": "papers/2512.11909v1.pdf"
    },
    {
      "arxiv_id": "2512.10098v1",
      "title": "MedXAI: A Retrieval-Augmented and Self-Verifying Framework for Knowledge-Guided Medical Image Analysis",
      "authors": [
        {
          "name": "Midhat Urooj"
        },
        {
          "name": "Ayan Banerjee"
        },
        {
          "name": "Farhat Shaikh"
        },
        {
          "name": "Kuntal Thakur"
        },
        {
          "name": "Sandeep Gupta"
        }
      ],
      "abstract": "Accurate and interpretable image-based diagnosis remains a fundamental challenge in medical AI, particularly under domain shifts and rare-class conditions. Deep learning models often struggle with real-world distribution changes, exhibit bias against infrequent pathologies, and lack the transparency required for deployment in safety-critical clinical environments. We introduce MedXAI (An Explainable Framework for Medical Imaging Classification), a unified expert knowledge based framework that integrates deep vision models with clinician-derived expert knowledge to improve generalization, reduce rare-class bias, and provide human-understandable explanations by localizing the relevant diagnostic features rather than relying on technical post-hoc methods (e.g., Saliency Maps, LIME). We evaluate MedXAI across heterogeneous modalities on two challenging tasks: (i) Seizure Onset Zone localization from resting-state fMRI, and (ii) Diabetic Retinopathy grading. Ex periments on ten multicenter datasets show consistent gains, including a 3% improvement in cross-domain generalization and a 10% improvmnet in F1 score of rare class, substantially outperforming strong deep learning baselines. Ablations confirm that the symbolic components act as effective clinical priors and regularizers, improving robustness under distribution shift. MedXAI delivers clinically aligned explanations while achieving superior in-domain and cross-domain performance, particularly for rare diseases in multimodal medical AI.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-12-10T21:40:04+00:00",
      "updated": "2025-12-10T21:40:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.10098v1",
      "file": "papers/2512.10098v1.pdf"
    },
    {
      "arxiv_id": "2512.10092v1",
      "title": "Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit",
      "authors": [
        {
          "name": "Nick Jiang"
        },
        {
          "name": "Xiaoqing Sun"
        },
        {
          "name": "Lisa Dunlap"
        },
        {
          "name": "Lewis Smith"
        },
        {
          "name": "Neel Nanda"
        }
      ],
      "abstract": "Analyzing large-scale text corpora is a core challenge in machine learning, crucial for tasks like identifying undesirable model behaviors or biases in training data. Current methods often rely on costly LLM-based techniques (e.g. annotating dataset differences) or dense embedding models (e.g. for clustering), which lack control over the properties of interest. We propose using sparse autoencoders (SAEs) to create SAE embeddings: representations whose dimensions map to interpretable concepts. Through four data analysis tasks, we show that SAE embeddings are more cost-effective and reliable than LLMs and more controllable than dense embeddings. Using the large hypothesis space of SAEs, we can uncover insights such as (1) semantic differences between datasets and (2) unexpected concept correlations in documents. For instance, by comparing model responses, we find that Grok-4 clarifies ambiguities more often than nine other frontier models. Relative to LLMs, SAE embeddings uncover bigger differences at 2-8x lower cost and identify biases more reliably. Additionally, SAE embeddings are controllable: by filtering concepts, we can (3) cluster documents along axes of interest and (4) outperform dense embeddings on property-based retrieval. Using SAE embeddings, we study model behavior with two case studies: investigating how OpenAI model behavior has changed over time and finding \"trigger\" phrases learned by Tulu-3 (Lambert et al., 2024) from its training data. These results position SAEs as a versatile tool for unstructured data analysis and highlight the neglected importance of interpreting models through their data.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-10T21:26:24+00:00",
      "updated": "2025-12-10T21:26:24+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.10092v1",
      "file": "papers/2512.10092v1.pdf"
    },
    {
      "arxiv_id": "2512.10065v1",
      "title": "Linear socio-demographic representations emerge in Large Language Models from indirect cues",
      "authors": [
        {
          "name": "Paul Bouchaud"
        },
        {
          "name": "Pedro Ramaciotti"
        }
      ],
      "abstract": "We investigate how LLMs encode sociodemographic attributes of human conversational partners inferred from indirect cues such as names and occupations. We show that LLMs develop linear representations of user demographics within activation space, wherein stereotypically associated attributes are encoded along interpretable geometric directions. We first probe residual streams across layers of four open transformer-based LLMs (Magistral 24B, Qwen3 14B, GPT-OSS 20B, OLMo2-1B) prompted with explicit demographic disclosure. We show that the same probes predict demographics from implicit cues: names activate census-aligned gender and race representations, while occupations trigger representations correlated with real-world workforce statistics. These linear representations allow us to explain demographic inferences implicitly formed by LLMs during conversation. We demonstrate that these implicit demographic representations actively shape downstream behavior, such as career recommendations. Our study further highlights that models that pass bias benchmark tests may still harbor and leverage implicit biases, with implications for fairness when applied at scale.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "published": "2025-12-10T20:36:36+00:00",
      "updated": "2025-12-10T20:36:36+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.10065v1",
      "file": "papers/2512.10065v1.pdf"
    },
    {
      "arxiv_id": "2512.11000v1",
      "title": "Unambiguous Representations in Neural Networks: An Information-Theoretic Approach to Intentionality",
      "authors": [
        {
          "name": "Francesco Lässig"
        }
      ],
      "abstract": "Representations pervade our daily experience, from letters representing sounds to bit strings encoding digital files. While such representations require externally defined decoders to convey meaning, conscious experience appears fundamentally different: a neural state corresponding to perceiving a red square cannot alternatively encode the experience of a green square. This intrinsic property of consciousness suggests that conscious representations must be unambiguous in a way that conventional representations are not. We formalize this intuition using information theory, defining representational ambiguity as the conditional entropy H(I|R) over possible interpretations I given a representation R. Through experiments on neural networks trained to classify MNIST digits, we demonstrate that relational structures in network connectivity can unambiguously encode representational content. Using both learned decoders and direct geometric matching, we achieve perfect (100%) accuracy for dropout-trained networks and 38% for standard backpropagation in identifying output neuron class identity, despite identical task performance, demonstrating that representational ambiguity can arise orthogonally to behavioral accuracy. We further show that spatial position information of input neurons can be decoded from network connectivity with R2 up to 0.844. These results provide a quantitative method for measuring representational ambiguity in neural systems and demonstrate that neural networks can exhibit the low-ambiguity representations posited as necessary (though not sufficient) by theoretical accounts of consciousness.",
      "primary_category": "q-bio.NC",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.NE"
      ],
      "published": "2025-12-10T19:00:34+00:00",
      "updated": "2025-12-10T19:00:34+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.11000v1",
      "file": "papers/2512.11000v1.pdf"
    },
    {
      "arxiv_id": "2512.09912v1",
      "title": "Supervised learning pays attention",
      "authors": [
        {
          "name": "Erin Craig"
        },
        {
          "name": "Robert Tibshirani"
        }
      ],
      "abstract": "In-context learning with attention enables large neural networks to make context-specific predictions by selectively focusing on relevant examples. Here, we adapt this idea to supervised learning procedures such as lasso regression and gradient boosting, for tabular data. Our goals are to (1) flexibly fit personalized models for each prediction point and (2) retain model simplicity and interpretability.\n  Our method fits a local model for each test observation by weighting the training data according to attention, a supervised similarity measure that emphasizes features and interactions that are predictive of the outcome. Attention weighting allows the method to adapt to heterogeneous data in a data-driven way, without requiring cluster or similarity pre-specification. Further, our approach is uniquely interpretable: for each test observation, we identify which features are most predictive and which training observations are most relevant. We then show how to use attention weighting for time series and spatial data, and we present a method for adapting pretrained tree-based models to distributional shift using attention-weighted residual corrections. Across real and simulated datasets, attention weighting improves predictive performance while preserving interpretability, and theory shows that attention-weighting linear models attain lower mean squared error than the standard linear model under mixture-of-models data-generating processes with known subgroup structure.",
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-10T18:43:46+00:00",
      "updated": "2025-12-10T18:43:46+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.09912v1",
      "file": "papers/2512.09912v1.pdf"
    },
    {
      "arxiv_id": "2512.09909v1",
      "title": "STACHE: Local Black-Box Explanations for Reinforcement Learning Policies",
      "authors": [
        {
          "name": "Andrew Elashkin"
        },
        {
          "name": "Orna Grumberg"
        }
      ],
      "abstract": "Reinforcement learning agents often behave unexpectedly in sparse-reward or safety-critical environments, creating a strong need for reliable debugging and verification tools. In this paper, we propose STACHE, a comprehensive framework for generating local, black-box explanations for an agent's specific action within discrete Markov games. Our method produces a Composite Explanation consisting of two complementary components: (1) a Robustness Region, the connected neighborhood of states where the agent's action remains invariant, and (2) Minimal Counterfactuals, the smallest state perturbations required to alter that decision. By exploiting the structure of factored state spaces, we introduce an exact, search-based algorithm that circumvents the fidelity gaps of surrogate models. Empirical validation on Gymnasium environments demonstrates that our framework not only explains policy actions, but also effectively captures the evolution of policy logic during training - from erratic, unstable behavior to optimized, robust strategies - providing actionable insights into agent sensitivity and decision boundaries.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-12-10T18:37:28+00:00",
      "updated": "2025-12-10T18:37:28+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.09909v1",
      "file": "papers/2512.09909v1.pdf"
    },
    {
      "arxiv_id": "2512.09730v1",
      "title": "Interpreto: An Explainability Library for Transformers",
      "authors": [
        {
          "name": "Antonin Poché"
        },
        {
          "name": "Thomas Mullor"
        },
        {
          "name": "Gabriele Sarti"
        },
        {
          "name": "Frédéric Boisnard"
        },
        {
          "name": "Corentin Friedrich"
        },
        {
          "name": "Charlotte Claye"
        },
        {
          "name": "François Hoofd"
        },
        {
          "name": "Raphael Bernas"
        },
        {
          "name": "Céline Hudelot"
        },
        {
          "name": "Fanny Jourdan"
        }
      ],
      "abstract": "Interpreto is a Python library for post-hoc explainability of text HuggingFace models, from early BERT variants to LLMs. It provides two complementary families of methods: attributions and concept-based explanations. The library connects recent research to practical tooling for data scientists, aiming to make explanations accessible to end users. It includes documentation, examples, and tutorials.\n  Interpreto supports both classification and generation models through a unified API. A key differentiator is its concept-based functionality, which goes beyond feature-level attributions and is uncommon in existing libraries.\n  The library is open source; install via pip install interpreto. Code and documentation are available at https://github.com/FOR-sight-ai/interpreto.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-12-10T15:12:09+00:00",
      "updated": "2025-12-10T15:12:09+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.09730v1",
      "file": "papers/2512.09730v1.pdf"
    },
    {
      "arxiv_id": "2512.14731v1",
      "title": "Semantic Geometry for policy-constrained interpretation",
      "authors": [
        {
          "name": "Nikit Phadke"
        }
      ],
      "abstract": "We present a geometric framework for policy-constrained semantic interpretation that provably prevents hallucinated commitments in high-stakes domains. Semantic meaning is represented as direction on a unit sphere, evidence is modeled as sets of witness vectors, and admissible interpretations correspond to spherical convex regions. Policy constraints are introduced as explicit priors defined over the same manifold, separated from evidence geometry. Interpretation reduces to constrained optimization over admissible regions, with refusal emerging as a topologically necessary outcome under contradiction or policy exclusion. We connect this framework to information theory, Bayesian inference, and sheaf-theoretic semantics, proving that our complexity bounds are information-theoretically optimal. Empirical validation on large scale regulated financial data demonstrates zero hallucinated approvals across multiple policy regimes-the first such result at scale.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-12-10T10:10:53+00:00",
      "updated": "2025-12-10T10:10:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14731v1",
      "file": "papers/2512.14731v1.pdf"
    },
    {
      "arxiv_id": "2512.09477v1",
      "title": "Color encoding in Latent Space of Stable Diffusion Models",
      "authors": [
        {
          "name": "Guillem Arias"
        },
        {
          "name": "Ariadna Solà"
        },
        {
          "name": "Martí Armengod"
        },
        {
          "name": "Maria Vanrell"
        }
      ],
      "abstract": "Recent advances in diffusion-based generative models have achieved remarkable visual fidelity, yet a detailed understanding of how specific perceptual attributes - such as color and shape - are internally represented remains limited. This work explores how color is encoded in a generative model through a systematic analysis of the latent representations in Stable Diffusion. Through controlled synthetic datasets, principal component analysis (PCA) and similarity metrics, we reveal that color information is encoded along circular, opponent axes predominantly captured in latent channels c_3 and c_4, whereas intensity and shape are primarily represented in channels c_1 and c_2. Our findings indicate that the latent space of Stable Diffusion exhibits an interpretable structure aligned with a efficient coding representation. These insights provide a foundation for future work in model understanding, editing applications, and the design of more disentangled generative frameworks.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-10T09:54:03+00:00",
      "updated": "2025-12-10T09:54:03+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.09477v1",
      "file": "papers/2512.09477v1.pdf"
    },
    {
      "arxiv_id": "2512.09340v1",
      "title": "Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration",
      "authors": [
        {
          "name": "Chethana Prasad Kabgere"
        }
      ],
      "abstract": "Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural networks, focusing on low-resolution, perceptually degraded stimuli. Drawing from computational cognitive science, cognitive architectures, and connectionist-symbolic hybrid models, we contrast human strategies such as analogical reasoning, shape-based recognition, and confidence modulation with AI's feature-based processing. Grounded in Marr's tri-level hypothesis, Simon's bounded rationality, and Thagard's frameworks of representation and emotion, we analyze participant responses in relation to Grad-CAM visualizations of model attention. Human behavior is further interpreted through cognitive principles modeled in ACT-R and Soar, revealing layered and heuristic decision strategies under uncertainty. Our findings highlight key parallels and divergences between biological and artificial systems in representation, inference, and confidence calibration. The analysis motivates future neuro-symbolic architectures that unify structured symbolic reasoning with connectionist representations. Such architectures, informed by principles of embodiment, explainability, and cognitive alignment, offer a path toward AI systems that are not only performant but also interpretable and cognitively grounded.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-12-10T05:58:12+00:00",
      "updated": "2025-12-10T05:58:12+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.09340v1",
      "file": "papers/2512.09340v1.pdf"
    },
    {
      "arxiv_id": "2512.09251v1",
      "title": "GLACIA: Instance-Aware Positional Reasoning for Glacial Lake Segmentation via Multimodal Large Language Model",
      "authors": [
        {
          "name": "Lalit Maurya"
        },
        {
          "name": "Saurabh Kaushik"
        },
        {
          "name": "Beth Tellman"
        }
      ],
      "abstract": "Glacial lake monitoring bears great significance in mitigating the anticipated risk of Glacial Lake Outburst Floods. However, existing segmentation methods based on convolutional neural networks (CNNs) and Vision Transformers (ViTs), remain constrained to pixel-level predictions, lacking high-level global scene semantics and human-interpretable reasoning. To address this, we introduce GLACIA (\\textbf{G}lacial \\textbf{LA}ke segmentation with \\textbf{C}ontextual \\textbf{I}nstance \\textbf{A}wareness), the first framework that integrates large language models with segmentation capabilities to produce both accurate segmentation masks and corresponding spatial reasoning outputs. We construct the Glacial Lake Position Reasoning (GLake-Pos) dataset pipeline, which provides diverse, spatially grounded question-answer pairs designed to overcome the lack of instance-aware positional reasoning data in remote sensing. Comparative evaluation demonstrate that GLACIA (mIoU: 87.30) surpasses state-of-the-art method based on CNNs (mIoU: 78.55 - 79.01), ViTs (mIoU: 69.27 - 81.75), Geo-foundation models (mIoU: 76.37 - 87.10), and reasoning based segmentation methods (mIoU: 60.12 - 75.66). Our approach enables intuitive disaster preparedness and informed policy-making in the context of rapidly changing glacial environments by facilitating natural language interaction, thereby supporting more efficient and interpretable decision-making. The code is released on https://github.com/lalitmaurya47/GLACIA",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-10T02:11:48+00:00",
      "updated": "2025-12-10T02:11:48+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.09251v1",
      "file": "papers/2512.09251v1.pdf"
    },
    {
      "arxiv_id": "2512.09152v1",
      "title": "Understanding temperature tuning in energy-based models",
      "authors": [
        {
          "name": "Peter W Fields"
        },
        {
          "name": "Vudtiwat Ngampruetikorn"
        },
        {
          "name": "David J Schwab"
        },
        {
          "name": "Stephanie E Palmer"
        }
      ],
      "abstract": "Generative models of complex systems often require post-hoc parameter adjustments to produce useful outputs. For example, energy-based models for protein design are sampled at an artificially low ''temperature'' to generate novel, functional sequences. This temperature tuning is a common yet poorly understood heuristic used across machine learning contexts to control the trade-off between generative fidelity and diversity. Here, we develop an interpretable, physically motivated framework to explain this phenomenon. We demonstrate that in systems with a large ''energy gap'' - separating a small fraction of meaningful states from a vast space of unrealistic states - learning from sparse data causes models to systematically overestimate high-energy state probabilities, a bias that lowering the sampling temperature corrects. More generally, we characterize how the optimal sampling temperature depends on the interplay between data size and the system's underlying energy landscape. Crucially, our results show that lowering the sampling temperature is not always desirable; we identify the conditions where \\emph{raising} it results in better generative performance. Our framework thus casts post-hoc temperature tuning as a diagnostic tool that reveals properties of the true data distribution and the limits of the learned model.",
      "primary_category": "q-bio.QM",
      "categories": [
        "q-bio.QM",
        "cs.LG"
      ],
      "published": "2025-12-09T22:06:30+00:00",
      "updated": "2025-12-09T22:06:30+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.09152v1",
      "file": "papers/2512.09152v1.pdf"
    },
    {
      "arxiv_id": "2512.09148v1",
      "title": "Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment",
      "authors": [
        {
          "name": "Shanghao Li"
        },
        {
          "name": "Jinda Han"
        },
        {
          "name": "Yibo Wang"
        },
        {
          "name": "Yuanjie Zhu"
        },
        {
          "name": "Zihe Song"
        },
        {
          "name": "Langzhou He"
        },
        {
          "name": "Kenan Kamel A Alghythee"
        },
        {
          "name": "Philip S. Yu"
        }
      ],
      "abstract": "Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However, LLMs struggle to interpret the relational and topological information in these inputs, resulting in hallucinations that are inconsistent with the retrieved knowledge. To analyze how LLMs attend to and retain structured knowledge during generation, we propose two lightweight interpretability metrics: Path Reliance Degree (PRD), which measures over-reliance on shortest-path triples, and Semantic Alignment Score (SAS), which assesses how well the model's internal representations align with the retrieved knowledge. Through empirical analysis on a knowledge-based QA task, we identify failure patterns associated with over-reliance on salient paths and weak semantic grounding, as indicated by high PRD and low SAS scores. We further develop a lightweight post-hoc hallucination detector, Graph Grounding and Alignment (GGA), which outperforms strong semantic and confidence-based baselines across AUC and F1. By grounding hallucination analysis in mechanistic interpretability, our work offers insights into how structural limitations in LLMs contribute to hallucinations, informing the design of more reliable GraphRAG systems in the future.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-09T21:52:50+00:00",
      "updated": "2025-12-09T21:52:50+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.09148v1",
      "file": "papers/2512.09148v1.pdf"
    },
    {
      "arxiv_id": "2512.09103v1",
      "title": "Natural Geometry of Robust Data Attribution: From Convex Models to Deep Networks",
      "authors": [
        {
          "name": "Shihao Li"
        },
        {
          "name": "Jiachen Li"
        },
        {
          "name": "Dongmei Chen"
        }
      ],
      "abstract": "Data attribution methods identify which training examples are responsible for a model's predictions, but their sensitivity to distributional perturbations undermines practical reliability. We present a unified framework for certified robust attribution that extends from convex models to deep networks. For convex settings, we derive Wasserstein-Robust Influence Functions (W-RIF) with provable coverage guarantees. For deep networks, we demonstrate that Euclidean certification is rendered vacuous by spectral amplification -- a mechanism where the inherent ill-conditioning of deep representations inflates Lipschitz bounds by over $10{,}000\\times$. This explains why standard TRAK scores, while accurate point estimates, are geometrically fragile: naive Euclidean robustness analysis yields 0\\% certification. Our key contribution is the Natural Wasserstein metric, which measures perturbations in the geometry induced by the model's own feature covariance. This eliminates spectral amplification, reducing worst-case sensitivity by $76\\times$ and stabilizing attribution estimates. On CIFAR-10 with ResNet-18, Natural W-TRAK certifies 68.7\\% of ranking pairs compared to 0\\% for Euclidean baselines -- to our knowledge, the first non-vacuous certified bounds for neural network attribution. Furthermore, we prove that the Self-Influence term arising from our analysis equals the Lipschitz constant governing attribution stability, providing theoretical grounding for leverage-based anomaly detection. Empirically, Self-Influence achieves 0.970 AUROC for label noise detection, identifying 94.1\\% of corrupted labels by examining just the top 20\\% of training data.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "published": "2025-12-09T20:40:27+00:00",
      "updated": "2025-12-09T20:40:27+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.09103v1",
      "file": "papers/2512.09103v1.pdf"
    },
    {
      "arxiv_id": "2512.08892v1",
      "title": "Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders",
      "authors": [
        {
          "name": "Guangzhi Xiong"
        },
        {
          "name": "Zhenghao He"
        },
        {
          "name": "Bohan Liu"
        },
        {
          "name": "Sanchit Sinha"
        },
        {
          "name": "Aidong Zhang"
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-09T18:33:22+00:00",
      "updated": "2025-12-09T18:33:22+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.08892v1",
      "file": "papers/2512.08892v1.pdf"
    },
    {
      "arxiv_id": "2512.08885v1",
      "title": "Explainable Anomaly Detection for Industrial IoT Data Streams",
      "authors": [
        {
          "name": "Ana Rita Paupério"
        },
        {
          "name": "Diogo Risca"
        },
        {
          "name": "Afonso Lourenço"
        },
        {
          "name": "Goreti Marreiros"
        },
        {
          "name": "Ricardo Martins"
        }
      ],
      "abstract": "Industrial maintenance is being transformed by the Internet of Things and edge computing, generating continuous data streams that demand real-time, adaptive decision-making under limited computational resources. While data stream mining (DSM) addresses this challenge, most methods assume fully supervised settings, yet in practice, ground-truth labels are often delayed or unavailable. This paper presents a collaborative DSM framework that integrates unsupervised anomaly detection with interactive, human-in-the-loop learning to support maintenance decisions. We employ an online Isolation Forest and enhance interpretability using incremental Partial Dependence Plots and a feature importance score, derived from deviations of Individual Conditional Expectation curves from a fading average, enabling users to dynamically reassess feature relevance and adjust anomaly thresholds. We describe the real-time implementation and provide initial results for fault detection in a Jacquard loom unit. Ongoing work targets continuous monitoring to predict and explain imminent bearing failures.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-09T18:20:35+00:00",
      "updated": "2025-12-09T18:20:35+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.08885v1",
      "file": "papers/2512.08885v1.pdf"
    },
    {
      "arxiv_id": "2512.08445v1",
      "title": "Uncertainty-Aware Subset Selection for Robust Visual Explainability under Distribution Shifts",
      "authors": [
        {
          "name": "Madhav Gupta"
        },
        {
          "name": "Vishak Prasad C"
        },
        {
          "name": "Ganesh Ramakrishnan"
        }
      ],
      "abstract": "Subset selection-based methods are widely used to explain deep vision models: they attribute predictions by highlighting the most influential image regions and support object-level explanations. While these methods perform well in in-distribution (ID) settings, their behavior under out-of-distribution (OOD) conditions remains poorly understood. Through extensive experiments across multiple ID-OOD sets, we find that reliability of the existing subset based methods degrades markedly, yielding redundant, unstable, and uncertainty-sensitive explanations. To address these shortcomings, we introduce a framework that combines submodular subset selection with layer-wise, gradient-based uncertainty estimation to improve robustness and fidelity without requiring additional training or auxiliary models. Our approach estimates uncertainty via adaptive weight perturbations and uses these estimates to guide submodular optimization, ensuring diverse and informative subset selection. Empirical evaluations show that, beyond mitigating the weaknesses of existing methods under OOD scenarios, our framework also yields improvements in ID settings. These findings highlight limitations of current subset-based approaches and demonstrate how uncertainty-driven optimization can enhance attribution and object-level interpretability, paving the way for more transparent and trustworthy AI in real-world vision applications.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-12-09T10:19:07+00:00",
      "updated": "2025-12-09T10:19:07+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.08445v1",
      "file": "papers/2512.08445v1.pdf"
    },
    {
      "arxiv_id": "2512.08344v1",
      "title": "Enhancing Explainability of Graph Neural Networks Through Conceptual and Structural Analyses and Their Extensions",
      "authors": [
        {
          "name": "Tien Cuong Bui"
        }
      ],
      "abstract": "Graph Neural Networks (GNNs) have become a powerful tool for modeling and analyzing data with graph structures. The wide adoption in numerous applications underscores the value of these models. However, the complexity of these methods often impedes understanding their decision-making processes. Current Explainable AI (XAI) methods struggle to untangle the intricate relationships and interactions within graphs. Several methods have tried to bridge this gap via a post-hoc approach or self-interpretable design. Most of them focus on graph structure analysis to determine essential patterns that correlate with prediction outcomes. While post-hoc explanation methods are adaptable, they require extra computational resources and may be less reliable due to limited access to the model's internal workings. Conversely, Interpretable models can provide immediate explanations, but their generalizability to different scenarios remains a major concern. To address these shortcomings, this thesis seeks to develop a novel XAI framework tailored for graph-based machine learning. The proposed framework aims to offer adaptable, computationally efficient explanations for GNNs, moving beyond individual feature analysis to capture how graph structure influences predictions.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.IT",
        "cs.LG"
      ],
      "published": "2025-12-09T08:13:31+00:00",
      "updated": "2025-12-09T08:13:31+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.08344v1",
      "file": "papers/2512.08344v1.pdf"
    },
    {
      "arxiv_id": "2512.08329v1",
      "title": "Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models",
      "authors": [
        {
          "name": "Michael R. Martin"
        },
        {
          "name": "Garrick Chan"
        },
        {
          "name": "Kwan-Liu Ma"
        }
      ],
      "abstract": "Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-09T07:55:11+00:00",
      "updated": "2025-12-09T07:55:11+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.08329v1",
      "file": "papers/2512.08329v1.pdf"
    },
    {
      "arxiv_id": "2512.14719v1",
      "title": "Hybrid Attribution Priors for Explainable and Robust Model Training",
      "authors": [
        {
          "name": "Zhuoran Zhang"
        },
        {
          "name": "Feng Zhang"
        },
        {
          "name": "Shangyuan Li"
        },
        {
          "name": "Yang Shi"
        },
        {
          "name": "Yuanxing Zhang"
        },
        {
          "name": "Wei Chen"
        },
        {
          "name": "Tengjiao Wang"
        },
        {
          "name": "Kam-Fai Wong"
        }
      ],
      "abstract": "Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-12-09T07:52:47+00:00",
      "updated": "2025-12-09T07:52:47+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14719v1",
      "file": "papers/2512.14719v1.pdf"
    },
    {
      "arxiv_id": "2512.08261v1",
      "title": "Beyond Traditional Diagnostics: Transforming Patient-Side Information into Predictive Insights with Knowledge Graphs and Prototypes",
      "authors": [
        {
          "name": "Yibowen Zhao"
        },
        {
          "name": "Yinan Zhang"
        },
        {
          "name": "Zhixiang Su"
        },
        {
          "name": "Lizhen Cui"
        },
        {
          "name": "Chunyan Miao"
        }
      ],
      "abstract": "Predicting diseases solely from patient-side information, such as demographics and self-reported symptoms, has attracted significant research attention due to its potential to enhance patient awareness, facilitate early healthcare engagement, and improve healthcare system efficiency. However, existing approaches encounter critical challenges, including imbalanced disease distributions and a lack of interpretability, resulting in biased or unreliable predictions. To address these issues, we propose the Knowledge graph-enhanced, Prototype-aware, and Interpretable (KPI) framework. KPI systematically integrates structured and trusted medical knowledge into a unified disease knowledge graph, constructs clinically meaningful disease prototypes, and employs contrastive learning to enhance predictive accuracy, which is particularly important for long-tailed diseases. Additionally, KPI utilizes large language models (LLMs) to generate patient-specific, medically relevant explanations, thereby improving interpretability and reliability. Extensive experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art methods in predictive accuracy and provides clinically valid explanations that closely align with patient narratives, highlighting its practical value for patient-centered healthcare delivery.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-12-09T05:37:54+00:00",
      "updated": "2025-12-09T05:37:54+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.08261v1",
      "file": "papers/2512.08261v1.pdf"
    },
    {
      "arxiv_id": "2512.08077v1",
      "title": "Unveiling Latent Knowledge in Chemistry Language Models through Sparse Autoencoders",
      "authors": [
        {
          "name": "Jaron Cohen"
        },
        {
          "name": "Alexander G. Hasson"
        },
        {
          "name": "Sara Tanovic"
        }
      ],
      "abstract": "Since the advent of machine learning, interpretability has remained a persistent challenge, becoming increasingly urgent as generative models support high-stakes applications in drug and material discovery. Recent advances in large language model (LLM) architectures have yielded chemistry language models (CLMs) with impressive capabilities in molecular property prediction and molecular generation. However, how these models internally represent chemical knowledge remains poorly understood. In this work, we extend sparse autoencoder techniques to uncover and examine interpretable features within CLMs. Applying our methodology to the Foundation Models for Materials (FM4M) SMI-TED chemistry foundation model, we extract semantically meaningful latent features and analyse their activation patterns across diverse molecular datasets. Our findings reveal that these models encode a rich landscape of chemical concepts. We identify correlations between specific latent features and distinct domains of chemical knowledge, including structural motifs, physicochemical properties, and pharmacological drug classes. Our approach provides a generalisable framework for uncovering latent knowledge in chemistry-focused AI systems. This work has implications for both foundational understanding and practical deployment; with the potential to accelerate computational chemistry research.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "physics.chem-ph"
      ],
      "published": "2025-12-08T22:20:01+00:00",
      "updated": "2025-12-08T22:20:01+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.08077v1",
      "file": "papers/2512.08077v1.pdf"
    },
    {
      "arxiv_id": "2512.08063v1",
      "title": "Deep Kernel Aalen-Johansen Estimator: An Interpretable and Flexible Neural Net Framework for Competing Risks",
      "authors": [
        {
          "name": "Xiaobin Shen"
        },
        {
          "name": "George H. Chen"
        }
      ],
      "abstract": "We propose an interpretable deep competing risks model called the Deep Kernel Aalen-Johansen (DKAJ) estimator, which generalizes the classical Aalen-Johansen nonparametric estimate of cumulative incidence functions (CIFs). Each data point (e.g., patient) is represented as a weighted combination of clusters. If a data point has nonzero weight only for one cluster, then its predicted CIFs correspond to those of the classical Aalen-Johansen estimator restricted to data points from that cluster. These weights come from an automatically learned kernel function that measures how similar any two data points are. On four standard competing risks datasets, we show that DKAJ is competitive with state-of-the-art baselines while being able to provide visualizations to assist model interpretation.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-08T21:55:13+00:00",
      "updated": "2025-12-08T21:55:13+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.08063v1",
      "file": "papers/2512.08063v1.pdf"
    },
    {
      "arxiv_id": "2512.07988v2",
      "title": "HOLE: Homological Observation of Latent Embeddings for Neural Network Interpretability",
      "authors": [
        {
          "name": "Sudhanva Manjunath Athreya"
        },
        {
          "name": "Paul Rosen"
        }
      ],
      "abstract": "Deep learning models have achieved remarkable success across various domains, yet their learned representations and decision-making processes remain largely opaque and hard to interpret. This work introduces HOLE (Homological Observation of Latent Embeddings), a method for analyzing and interpreting deep neural networks through persistent homology. HOLE extracts topological features from neural activations and presents them using a suite of visualization techniques, including Sankey diagrams, heatmaps, dendrograms, and blob graphs. These tools facilitate the examination of representation structure and quality across layers. We evaluate HOLE on standard datasets using a range of discriminative models, focusing on representation quality, interpretability across layers, and robustness to input perturbations and model compression. The results indicate that topological analysis reveals patterns associated with class separation, feature disentanglement, and model robustness, providing a complementary perspective for understanding and improving deep learning systems.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.GR",
        "cs.HC"
      ],
      "published": "2025-12-08T19:20:05+00:00",
      "updated": "2025-12-10T02:59:33+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.07988v2",
      "file": "papers/2512.07988v2.pdf"
    },
    {
      "arxiv_id": "2512.07981v1",
      "title": "CIP-Net: Continual Interpretable Prototype-based Network",
      "authors": [
        {
          "name": "Federico Di Valerio"
        },
        {
          "name": "Michela Proietti"
        },
        {
          "name": "Alessio Ragno"
        },
        {
          "name": "Roberto Capobianco"
        }
      ],
      "abstract": "Continual learning constrains models to learn new tasks over time without forgetting what they have already learned. A key challenge in this setting is catastrophic forgetting, where learning new information causes the model to lose its performance on previous tasks. Recently, explainable AI has been proposed as a promising way to better understand and reduce forgetting. In particular, self-explainable models are useful because they generate explanations during prediction, which can help preserve knowledge. However, most existing explainable approaches use post-hoc explanations or require additional memory for each new task, resulting in limited scalability. In this work, we introduce CIP-Net, an exemplar-free self-explainable prototype-based model designed for continual learning. CIP-Net avoids storing past examples and maintains a simple architecture, while still providing useful explanations and strong performance. We demonstrate that CIPNet achieves state-of-the-art performances compared to previous exemplar-free and self-explainable methods in both task- and class-incremental settings, while bearing significantly lower memory-related overhead. This makes it a practical and interpretable solution for continual learning.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "published": "2025-12-08T19:13:19+00:00",
      "updated": "2025-12-08T19:13:19+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.07981v1",
      "file": "papers/2512.07981v1.pdf"
    },
    {
      "arxiv_id": "2512.07961v1",
      "title": "Towards symbolic regression for interpretable clinical decision scores",
      "authors": [
        {
          "name": "Guilherme Seidyo Imai Aldeia"
        },
        {
          "name": "Joseph D. Romano"
        },
        {
          "name": "Fabricio Olivetti de Franca"
        },
        {
          "name": "Daniel S. Herman"
        },
        {
          "name": "William G. La Cava"
        }
      ],
      "abstract": "Medical decision-making makes frequent use of algorithms that combine risk equations with rules, providing clear and standardized treatment pathways. Symbolic regression (SR) traditionally limits its search space to continuous function forms and their parameters, making it difficult to model this decision-making. However, due to its ability to derive data-driven, interpretable models, SR holds promise for developing data-driven clinical risk scores. To that end we introduce Brush, an SR algorithm that combines decision-tree-like splitting algorithms with non-linear constant optimization, allowing for seamless integration of rule-based logic into symbolic regression and classification models. Brush achieves Pareto-optimal performance on SRBench, and was applied to recapitulate two widely used clinical scoring systems, achieving high accuracy and interpretable models. Compared to decision trees, random forests, and other SR methods, Brush achieves comparable or superior predictive performance while producing simpler models.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.NE"
      ],
      "published": "2025-12-08T19:00:41+00:00",
      "updated": "2025-12-08T19:00:41+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.07961v1",
      "file": "papers/2512.07961v1.pdf"
    },
    {
      "arxiv_id": "2512.07578v1",
      "title": "$φ$-test: Global Feature Selection and Inference for Shapley Additive Explanations",
      "authors": [
        {
          "name": "Dongseok Kim"
        },
        {
          "name": "Hyoungsun Choi"
        },
        {
          "name": "Mohamed Jismy Aashik Rasool"
        },
        {
          "name": "Gisung Oh"
        }
      ],
      "abstract": "We propose $φ$-test, a global feature-selection and significance procedure for black-box predictors that combines Shapley attributions with selective inference. Given a trained model and an evaluation dataset, $φ$-test performs SHAP-guided screening and fits a linear surrogate on the screened features via a selection rule with a tractable selective-inference form. For each retained feature, it outputs a Shapley-based global score, a surrogate coefficient, and post-selection $p$-values and confidence intervals in a global feature-importance table. Experiments on real tabular regression tasks with tree-based and neural backbones suggest that $φ$-test can retain much of the predictive ability of the original model while using only a few features and producing feature sets that remain fairly stable across resamples and backbone classes. In these settings, $φ$-test acts as a practical global explanation layer linking Shapley-based importance summaries with classical statistical inference.",
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "published": "2025-12-08T14:14:01+00:00",
      "updated": "2025-12-08T14:14:01+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.07578v1",
      "file": "papers/2512.07578v1.pdf"
    },
    {
      "arxiv_id": "2512.07462v2",
      "title": "Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics",
      "authors": [
        {
          "name": "Trung-Kiet Huynh"
        },
        {
          "name": "Duy-Minh Dao-Sy"
        },
        {
          "name": "Thanh-Bang Cao"
        },
        {
          "name": "Phong-Hao Le"
        },
        {
          "name": "Hong-Dan Nguyen"
        },
        {
          "name": "Phu-Quy Nguyen-Lam"
        },
        {
          "name": "Minh-Luan Nguyen-Vo"
        },
        {
          "name": "Hong-Phat Pham"
        },
        {
          "name": "Phu-Hoa Pham"
        },
        {
          "name": "Thien-Kim Than"
        },
        {
          "name": "Chi-Nguyen Tran"
        },
        {
          "name": "Huy Tran"
        },
        {
          "name": "Gia-Thoai Tran-Le"
        },
        {
          "name": "Alessio Buscemi"
        },
        {
          "name": "Le Hong Trang"
        },
        {
          "name": "The Anh Han"
        }
      ],
      "abstract": "As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.",
      "primary_category": "cs.MA",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT",
        "cs.LG",
        "math.DS"
      ],
      "published": "2025-12-08T11:40:03+00:00",
      "updated": "2025-12-11T20:32:28+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.07462v2",
      "file": "papers/2512.07462v2.pdf"
    },
    {
      "arxiv_id": "2512.07450v1",
      "title": "Forget and Explain: Transparent Verification of GNN Unlearning",
      "authors": [
        {
          "name": "Imran Ahsan"
        },
        {
          "name": "Hyunwook Yu"
        },
        {
          "name": "Jinsung Kim"
        },
        {
          "name": "Mucheol Kim"
        }
      ],
      "abstract": "Graph neural networks (GNNs) are increasingly used to model complex patterns in graph-structured data. However, enabling them to \"forget\" designated information remains challenging, especially under privacy regulations such as the GDPR. Existing unlearning methods largely optimize for efficiency and scalability, yet they offer little transparency, and the black-box nature of GNNs makes it difficult to verify whether forgetting has truly occurred. We propose an explainability-driven verifier for GNN unlearning that snapshots the model before and after deletion, using attribution shifts and localized structural changes (for example, graph edit distance) as transparent evidence. The verifier uses five explainability metrics: residual attribution, heatmap shift, explainability score deviation, graph edit distance, and a diagnostic graph rule shift. We evaluate two backbones (GCN, GAT) and four unlearning strategies (Retrain, GraphEditor, GNNDelete, IDEA) across five benchmarks (Cora, Citeseer, Pubmed, Coauthor-CS, Coauthor-Physics). Results show that Retrain and GNNDelete achieve near-complete forgetting, GraphEditor provides partial erasure, and IDEA leaves residual signals. These explanation deltas provide the primary, human-readable evidence of forgetting; we also report membership-inference ROC-AUC as a complementary, graph-wide privacy signal.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-12-08T11:25:19+00:00",
      "updated": "2025-12-08T11:25:19+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.07450v1",
      "file": "papers/2512.07450v1.pdf"
    },
    {
      "arxiv_id": "2512.07355v1",
      "title": "A Geometric Unification of Concept Learning with Concept Cones",
      "authors": [
        {
          "name": "Alexandre Rocchi--Henry"
        },
        {
          "name": "Thomas Fel"
        },
        {
          "name": "Gianni Franchi"
        }
      ],
      "abstract": "Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\\footnote{We adopt the terminology of \\citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-12-08T09:51:46+00:00",
      "updated": "2025-12-08T09:51:46+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.07355v1",
      "file": "papers/2512.07355v1.pdf"
    },
    {
      "arxiv_id": "2512.07224v1",
      "title": "Clinical Interpretability of Deep Learning Segmentation Through Shapley-Derived Agreement and Uncertainty Metrics",
      "authors": [
        {
          "name": "Tianyi Ren"
        },
        {
          "name": "Daniel Low"
        },
        {
          "name": "Pittra Jaengprajak"
        },
        {
          "name": "Juampablo Heras Rivera"
        },
        {
          "name": "Jacob Ruzevick"
        },
        {
          "name": "Mehmet Kurt"
        }
      ],
      "abstract": "Segmentation is the identification of anatomical regions of interest, such as organs, tissue, and lesions, serving as a fundamental task in computer-aided diagnosis in medical imaging. Although deep learning models have achieved remarkable performance in medical image segmentation, the need for explainability remains critical for ensuring their acceptance and integration in clinical practice, despite the growing research attention in this area. Our approach explored the use of contrast-level Shapley values, a systematic perturbation of model inputs to assess feature importance. While other studies have investigated gradient-based techniques through identifying influential regions in imaging inputs, Shapley values offer a broader, clinically aligned approach, explaining how model performance is fairly attributed to certain imaging contrasts over others. Using the BraTS 2024 dataset, we generated rankings for Shapley values for four MRI contrasts across four model architectures. Two metrics were proposed from the Shapley ranking: agreement between model and ``clinician\" imaging ranking, and uncertainty quantified through Shapley ranking variance across cross-validation folds. Higher-performing cases (Dice \\textgreater0.6) showed significantly greater agreement with clinical rankings. Increased Shapley ranking variance correlated with decreased performance (U-Net: $r=-0.581$). These metrics provide clinically interpretable proxies for model reliability, helping clinicians better understand state-of-the-art segmentation models.",
      "primary_category": "eess.IV",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-12-08T07:06:58+00:00",
      "updated": "2025-12-08T07:06:58+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.07224v1",
      "file": "papers/2512.07224v1.pdf"
    },
    {
      "arxiv_id": "2512.14709v1",
      "title": "Attention as Binding: A Vector-Symbolic Perspective on Transformer Reasoning",
      "authors": [
        {
          "name": "Sahil Rajesh Dhayalkar"
        }
      ],
      "abstract": "Transformer-based language models display impressive reasoning-like behavior, yet remain brittle on tasks that require stable symbolic manipulation. This paper develops a unified perspective on these phenomena by interpreting self-attention and residual streams as implementing an approximate Vector Symbolic Architecture (VSA). In this view, queries and keys define role spaces, values encode fillers, attention weights perform soft unbinding, and residual connections realize superposition of many bound structures. We use this algebraic lens to relate transformer internals to chain-of-thought traces, program-based reasoning, and memory-augmented tool use, and to explain characteristic failure modes such as variable confusion and inconsistency across logically related prompts. Building on this perspective, we propose VSA-inspired architectural biases, including explicit binding/unbinding heads and hyperdimensional memory layers, and training objectives that promote role-filler separation and robust superposition. Finally, we outline metrics for measuring \"VSA-likeness\" and logical compositionality, and pose theoretical and architectural open problems. Overall, the paper argues that viewing attention as soft vector-symbolic computation offers a principled route toward more interpretable and logically reliable reasoning systems.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-08T05:38:24+00:00",
      "updated": "2025-12-08T05:38:24+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.14709v1",
      "file": "papers/2512.14709v1.pdf"
    },
    {
      "arxiv_id": "2512.07178v1",
      "title": "ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation",
      "authors": [
        {
          "name": "Latifa Dwiyanti"
        },
        {
          "name": "Sergio Ryan Wibisono"
        },
        {
          "name": "Hidetaka Nambo"
        }
      ],
      "abstract": "Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "published": "2025-12-08T05:18:15+00:00",
      "updated": "2025-12-08T05:18:15+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.07178v1",
      "file": "papers/2512.07178v1.pdf"
    },
    {
      "arxiv_id": "2512.07081v1",
      "title": "ClinNoteAgents: An LLM Multi-Agent System for Predicting and Interpreting Heart Failure 30-Day Readmission from Clinical Notes",
      "authors": [
        {
          "name": "Rongjia Zhou"
        },
        {
          "name": "Chengzhuo Li"
        },
        {
          "name": "Carl Yang"
        },
        {
          "name": "Jiaying Lu"
        }
      ],
      "abstract": "Heart failure (HF) is one of the leading causes of rehospitalization among older adults in the United States. Although clinical notes contain rich, detailed patient information and make up a large portion of electronic health records (EHRs), they remain underutilized for HF readmission risk analysis. Traditional computational models for HF readmission often rely on expert-crafted rules, medical thesauri, and ontologies to interpret clinical notes, which are typically written under time pressure and may contain misspellings, abbreviations, and domain-specific jargon. We present ClinNoteAgents, an LLM-based multi-agent framework that transforms free-text clinical notes into (1) structured representations of clinical and social risk factors for association analysis and (2) clinician-style abstractions for HF 30-day readmission prediction. We evaluate ClinNoteAgents on 3,544 notes from 2,065 patients (readmission rate=35.16%), demonstrating strong performance in extracting risk factors from free-text, identifying key contributing factors, and predicting readmission risk. By reducing reliance on structured fields and minimizing manual annotation and model training, ClinNoteAgents provides a scalable and interpretable approach to note-based HF readmission risk modeling in data-limited healthcare systems.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-12-08T01:32:14+00:00",
      "updated": "2025-12-08T01:32:14+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.07081v1",
      "file": "papers/2512.07081v1.pdf"
    },
    {
      "arxiv_id": "2512.07021v1",
      "title": "Transferring Clinical Knowledge into ECGs Representation",
      "authors": [
        {
          "name": "Jose Geraldo Fernandes"
        },
        {
          "name": "Luiz Facury de Souza"
        },
        {
          "name": "Pedro Robles Dutenhefner"
        },
        {
          "name": "Gisele L. Pappa"
        },
        {
          "name": "Wagner Meira"
        }
      ],
      "abstract": "Deep learning models have shown high accuracy in classifying electrocardiograms (ECGs), but their black box nature hinders clinical adoption due to a lack of trust and interpretability. To address this, we propose a novel three-stage training paradigm that transfers knowledge from multimodal clinical data (laboratory exams, vitals, biometrics) into a powerful, yet unimodal, ECG encoder. We employ a self-supervised, joint-embedding pre-training stage to create an ECG representation that is enriched with contextual clinical information, while only requiring the ECG signal at inference time. Furthermore, as an indirect way to explain the model's output we train it to also predict associated laboratory abnormalities directly from the ECG embedding. Evaluated on the MIMIC-IV-ECG dataset, our model outperforms a standard signal-only baseline in multi-label diagnosis classification and successfully bridges a substantial portion of the performance gap to a fully multimodal model that requires all data at inference. Our work demonstrates a practical and effective method for creating more accurate and trustworthy ECG classification models. By converting abstract predictions into physiologically grounded \\emph{explanations}, our approach offers a promising path toward the safer integration of AI into clinical workflows.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-12-07T22:19:24+00:00",
      "updated": "2025-12-07T22:19:24+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.07021v1",
      "file": "papers/2512.07021v1.pdf"
    },
    {
      "arxiv_id": "2512.07010v2",
      "title": "Always Keep Your Promises: DynamicLRP, A Model-Agnostic Solution To Layer-Wise Relevance Propagation",
      "authors": [
        {
          "name": "Kevin Lee"
        },
        {
          "name": "Pablo Millan Arias"
        }
      ],
      "abstract": "Layer-wise Relevance Propagation (LRP) provides principled attribution for neural networks through conservation properties and foundations in Deep Taylor Decomposition. However, existing implementations operate at the module level, requiring architecture-specific propagation rules and model modifications. These limit the generality of target model and sustainability of implementations as architectures evolve. We introduce DynamicLRP, a model-agnostic LRP framework operating at the tensor operation level. By decomposing attribution to individual operations within computation graphs and introducing a novel mechanism for deferred activation resolution, named the Promise System, our approach achieves true architecture agnosticity while maintaining LRP's theoretical guarantees. This design operates independently of backpropagation machinery, requiring no model modification, enabling side-by-side execution with gradient backpropagation. Being based on computation graphs, this method is theoretically extensible to other deep learning libraries that support auto-differentiation. We demonstrate faithfulness matching or exceeding specialized implementations (1.77 vs 1.69 ABPC on VGG, equivalent performance on ViT, 93.70% and 95.06% top-1 attribution accuracy for explaining RoBERTa-large and Flan-T5-large answers on SQuADv2, respectively) while maintaining practical efficiency on models with 100M-1B parameters. We achieved 99.92% node coverage across 31,465 computation graph nodes from 15 diverse architectures, including state-space models (Mamba), audio transformers (Whisper), and multimodal systems (DePlot) without any model-specific code with rules for 47 fundamental operations implemented. Our operation-level decomposition and Promise System establish a sustainable, extensible foundation for LRP across evolving architectures. All code is available at https://github.com/keeinlev/dynamicLRP .",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-07T21:19:04+00:00",
      "updated": "2025-12-20T06:54:10+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.07010v2",
      "file": "papers/2512.07010v2.pdf"
    },
    {
      "arxiv_id": "2512.06917v1",
      "title": "Know your Trajectory -- Trustworthy Reinforcement Learning deployment through Importance-Based Trajectory Analysis",
      "authors": [
        {
          "name": "Clifford F"
        },
        {
          "name": "Devika Jay"
        },
        {
          "name": "Abhishek Sarkar"
        },
        {
          "name": "Satheesh K Perepu"
        },
        {
          "name": "Santhosh G S"
        },
        {
          "name": "Kaushik Dey"
        },
        {
          "name": "Balaraman Ravindran"
        }
      ],
      "abstract": "As Reinforcement Learning (RL) agents are increasingly deployed in real-world applications, ensuring their behavior is transparent and trustworthy is paramount. A key component of trust is explainability, yet much of the work in Explainable RL (XRL) focuses on local, single-step decisions. This paper addresses the critical need for explaining an agent's long-term behavior through trajectory-level analysis. We introduce a novel framework that ranks entire trajectories by defining and aggregating a new state-importance metric. This metric combines the classic Q-value difference with a \"radical term\" that captures the agent's affinity to reach its goal, providing a more nuanced measure of state criticality. We demonstrate that our method successfully identifies optimal trajectories from a heterogeneous collection of agent experiences. Furthermore, by generating counterfactual rollouts from critical states within these trajectories, we show that the agent's chosen path is robustly superior to alternatives, thereby providing a powerful \"Why this, and not that?\" explanation. Our experiments in standard OpenAI Gym environments validate that our proposed importance metric is more effective at identifying optimal behaviors compared to classic approaches, offering a significant step towards trustworthy autonomous systems.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-12-07T16:52:08+00:00",
      "updated": "2025-12-07T16:52:08+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.06917v1",
      "file": "papers/2512.06917v1.pdf"
    },
    {
      "arxiv_id": "2512.06814v1",
      "title": "CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation",
      "authors": [
        {
          "name": "Dibyanayan Bandyopadhyay"
        },
        {
          "name": "Soham Bhattacharjee"
        },
        {
          "name": "Mohammed Hasanuzzaman"
        },
        {
          "name": "Asif Ekbal"
        }
      ],
      "abstract": "Multimodal classifiers function as opaque black box models. While several techniques exist to interpret their predictions, very few of them are as intuitive and accessible as natural language explanations (NLEs). To build trust, such explanations must faithfully capture the classifier's internal decision making behavior, a property known as faithfulness. In this paper, we propose CAuSE (Causal Abstraction under Simulated Explanations), a novel framework to generate faithful NLEs for any pretrained multimodal classifier. We demonstrate that CAuSE generalizes across datasets and models through extensive empirical evaluations. Theoretically, we show that CAuSE, trained via interchange intervention, forms a causal abstraction of the underlying classifier. We further validate this through a redesigned metric for measuring causal faithfulness in multimodal settings. CAuSE surpasses other methods on this metric, with qualitative analysis reinforcing its advantages. We perform detailed error analysis to pinpoint the failure cases of CAuSE. For replicability, we make the codes available at https://github.com/newcodevelop/CAuSE",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-12-07T12:15:21+00:00",
      "updated": "2025-12-07T12:15:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.06814v1",
      "file": "papers/2512.06814v1.pdf"
    },
    {
      "arxiv_id": "2512.01020v1",
      "title": "Evaluating Legal Reasoning Traces with Legal Issue Tree Rubrics",
      "authors": [
        {
          "name": "Jinu Lee"
        },
        {
          "name": "Kyoung-Woon On"
        },
        {
          "name": "Simeng Han"
        },
        {
          "name": "Arman Cohan"
        },
        {
          "name": "Julia Hockenmaier"
        }
      ],
      "abstract": "Evaluating the quality of LLM-generated reasoning traces in expert domains (e.g., law) is essential for ensuring credibility and explainability, yet remains challenging due to the inherent complexity of such reasoning tasks. We introduce LEGIT (LEGal Issue Trees), a novel large-scale (24K instances) expert-level legal reasoning dataset with an emphasis on reasoning trace evaluation. We convert court judgments into hierarchical trees of opposing parties' arguments and the court's conclusions, which serve as rubrics for evaluating the issue coverage and correctness of the reasoning traces. We verify the reliability of these rubrics via human expert annotations and comparison with coarse, less informative rubrics. Using the LEGIT dataset, we show that (1) LLMs' legal reasoning ability is seriously affected by both legal issue coverage and correctness, and that (2) retrieval-augmented generation (RAG) and RL with rubrics bring complementary benefits for legal reasoning abilities, where RAG improves overall reasoning capability, whereas RL improves correctness albeit with reduced coverage.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-11-30T18:32:43+00:00",
      "updated": "2025-11-30T18:32:43+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.01020v1",
      "file": "papers/2512.01020v1.pdf"
    },
    {
      "arxiv_id": "2512.00938v1",
      "title": "DeformAr: Rethinking NER Evaluation through Component Analysis and Visual Analytics",
      "authors": [
        {
          "name": "Ahmed Mustafa Younes"
        }
      ],
      "abstract": "Transformer models have significantly advanced Natural Language Processing (NLP), demonstrating strong performance in English. However, their effectiveness in Arabic, particularly for Named Entity Recognition (NER), remains limited, even with larger pre-trained models. This performance gap stems from multiple factors, including tokenisation, dataset quality, and annotation inconsistencies. Existing studies often analyze these issues in isolation, failing to capture their joint effect on system behaviour and performance.\n  We introduce DeformAr (Debugging and Evaluation Framework for Transformer-based NER Systems), a novel framework designed to investigate and explain the performance discrepancy between Arabic and English NER systems. DeformAr integrates a data extraction library and an interactive dashboard, supporting two modes of evaluation: cross-component analysis and behavioural analysis. The framework divides each language into dataset and model components to examine their interactions.\n  The analysis proceeds in two stages. First, cross-component analysis provides systematic diagnostic measures across data and model subcomponents, addressing the \"what,\" \"how,\" and \"why\" behind observed discrepancies. The second stage applies behavioural analysis by combining interpretability techniques with token-level metrics, interactive visualisations, and representation space analysis. These stages enable a component-aware diagnostic process that detects model behaviours and explains them by linking them to underlying representational patterns and data factors. DeformAr is the first Arabic-specific, component-based interpretability tool, offering a crucial resource for advancing model analysis in under-resourced languages.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-11-30T15:39:28+00:00",
      "updated": "2025-11-30T15:39:28+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00938v1",
      "file": "papers/2512.00938v1.pdf"
    },
    {
      "arxiv_id": "2512.00852v1",
      "title": "One Swallow Does Not Make a Summer: Understanding Semantic Structures in Embedding Spaces",
      "authors": [
        {
          "name": "Yandong Sun"
        },
        {
          "name": "Qiang Huang"
        },
        {
          "name": "Ziwei Xu"
        },
        {
          "name": "Yiqun Sun"
        },
        {
          "name": "Yixuan Tang"
        },
        {
          "name": "Anthony K. H. Tung"
        }
      ],
      "abstract": "Embedding spaces are fundamental to modern AI, translating raw data into high-dimensional vectors that encode rich semantic relationships. Yet, their internal structures remain opaque, with existing approaches often sacrificing semantic coherence for structural regularity or incurring high computational overhead to improve interpretability. To address these challenges, we introduce the Semantic Field Subspace (SFS), a geometry-preserving, context-aware representation that captures local semantic neighborhoods within the embedding space. We also propose SAFARI (SemAntic Field subspAce deteRmInation), an unsupervised, modality-agnostic algorithm that uncovers hierarchical semantic structures using a novel metric called Semantic Shift, which quantifies how semantics evolve as SFSes evolve. To ensure scalability, we develop an efficient approximation of Semantic Shift that replaces costly SVD computations, achieving a 15~30x speedup with average errors below 0.01. Extensive evaluations across six real-world text and image datasets show that SFSes outperform standard classifiers not only in classification but also in nuanced tasks such as political bias detection, while SAFARI consistently reveals interpretable and generalizable semantic hierarchies. This work presents a unified framework for structuring, analyzing, and scaling semantic understanding in embedding spaces.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-11-30T11:48:00+00:00",
      "updated": "2025-11-30T11:48:00+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00852v1",
      "file": "papers/2512.00852v1.pdf"
    },
    {
      "arxiv_id": "2512.10966v1",
      "title": "Multimodal Fusion of Regional Brain Experts for Interpretable Alzheimer's Disease Diagnosis",
      "authors": [
        {
          "name": "Farica Zhuang"
        },
        {
          "name": "Dinara Aliyeva"
        },
        {
          "name": "Shu Yang"
        },
        {
          "name": "Zixuan Wen"
        },
        {
          "name": "Duy Duong-Tran"
        },
        {
          "name": "Christos Davatzikos"
        },
        {
          "name": "Tianlong Chen"
        },
        {
          "name": "Song Wang"
        },
        {
          "name": "Li Shen"
        }
      ],
      "abstract": "Accurate and early diagnosis of Alzheimer's disease (AD) can benefit from integrating complementary information from multiple modalities, mirroring clinical practice. However, conventional fusion approaches often rely on simple concatenation of features, which cannot adaptively balance the contributions of biomarkers such as amyloid PET and MRI across brain regions. In this work, we propose MREF-AD, a Multimodal Regional Expert Fusion model for AD diagnosis. It is a Mixture-of-Experts (MoE) framework that models meso-scale brain regions in each modality as an independent expert and employs two-level gating networks to learn subject-specific fusion weights. Beyond improving diagnostic performance, MREF-AD provides modality- and region-level insight into how structural and molecular imaging jointly contribute to disease diagnosis. Using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), MREF-AD achieves state-of-the-art performance over baselines while providing enhanced interpretability of brain region-specific biomarker relevance, underscoring its utility as a general framework for adaptive and interpretable multimodal fusion in neuroimaging.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "published": "2025-11-30T02:12:12+00:00",
      "updated": "2025-11-30T02:12:12+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.10966v1",
      "file": "papers/2512.10966v1.pdf"
    },
    {
      "arxiv_id": "2512.00663v1",
      "title": "Graphing the Truth: Structured Visualizations for Automated Hallucination Detection in LLMs",
      "authors": [
        {
          "name": "Tanmay Agrawal"
        }
      ],
      "abstract": "Large Language Models have rapidly advanced in their ability to interpret and generate natural language. In enterprise settings, they are frequently augmented with closed-source domain knowledge to deliver more contextually informed responses. However, operational constraints such as limited context windows and inconsistencies between pre-training data and supplied knowledge often lead to hallucinations, some of which appear highly credible and escape routine human review. Current mitigation strategies either depend on costly, large-scale gold-standard Q\\&A curation or rely on secondary model verification, neither of which offers deterministic assurance. This paper introduces a framework that organizes proprietary knowledge and model-generated content into interactive visual knowledge graphs. The objective is to provide end users with a clear, intuitive view of potential hallucination zones by linking model assertions to underlying sources of truth and indicating confidence levels. Through this visual interface, users can diagnose inconsistencies, identify weak reasoning chains, and supply corrective feedback. The resulting human-in-the-loop workflow creates a structured feedback loop that can enhance model reliability and continuously improve response quality.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-11-29T23:09:15+00:00",
      "updated": "2025-11-29T23:09:15+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00663v1",
      "file": "papers/2512.00663v1.pdf"
    },
    {
      "arxiv_id": "2512.00528v1",
      "title": "Pushing the Boundaries of Interpretability: Incremental Enhancements to the Explainable Boosting Machine",
      "authors": [
        {
          "name": "Isara Liyanage"
        },
        {
          "name": "Uthayasanker Thayasivam"
        }
      ],
      "abstract": "The widespread adoption of complex machine learning models in high-stakes domains has brought the \"black-box\" problem to the forefront of responsible AI research. This paper aims at addressing this issue by improving the Explainable Boosting Machine (EBM), a state-of-the-art glassbox model that delivers both high accuracy and complete transparency. The paper outlines three distinct enhancement methodologies: targeted hyperparameter optimization with Bayesian methods, the implementation of a custom multi-objective function for fairness for hyperparameter optimization, and a novel self-supervised pre-training pipeline for cold-start scenarios. All three methodologies are evaluated across standard benchmark datasets, including the Adult Income, Credit Card Fraud Detection, and UCI Heart Disease datasets. The analysis indicates that while the tuning process yielded marginal improvements in the primary ROC AUC metric, it led to a subtle but important shift in the model's decision-making behavior, demonstrating the value of a multi-faceted evaluation beyond a single performance score. This work is positioned as a critical step toward developing machine learning systems that are not only accurate but also robust, equitable, and transparent, meeting the growing demands of regulatory and ethical compliance.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-11-29T15:46:13+00:00",
      "updated": "2025-11-29T15:46:13+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00528v1",
      "file": "papers/2512.00528v1.pdf"
    },
    {
      "arxiv_id": "2512.00456v1",
      "title": "CausalAffect: Causal Discovery for Facial Affective Understanding",
      "authors": [
        {
          "name": "Guanyu Hu"
        },
        {
          "name": "Tangzheng Lian"
        },
        {
          "name": "Dimitrios Kollias"
        },
        {
          "name": "Oya Celiktutan"
        },
        {
          "name": "Xinyu Yang"
        }
      ],
      "abstract": "Understanding human affect from facial behavior requires not only accurate recognition but also structured reasoning over the latent dependencies that drive muscle activations and their expressive outcomes. Although Action Units (AUs) have long served as the foundation of affective computing, existing approaches rarely address how to infer psychologically plausible causal relations between AUs and expressions directly from data. We propose CausalAffect, the first framework for causal graph discovery in facial affect analysis. CausalAffect models AU-AU and AU-Expression dependencies through a two-level polarity and direction aware causal hierarchy that integrates population-level regularities with sample-adaptive structures. A feature-level counterfactual intervention mechanism further enforces true causal effects while suppressing spurious correlations. Crucially, our approach requires neither jointly annotated datasets nor handcrafted causal priors, yet it recovers causal structures consistent with established psychological theories while revealing novel inhibitory and previously uncharacterized dependencies. Extensive experiments across six benchmarks demonstrate that CausalAffect advances the state of the art in both AU detection and expression recognition, establishing a principled connection between causal discovery and interpretable facial behavior. All trained models and source code will be released upon acceptance.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-11-29T12:07:33+00:00",
      "updated": "2025-11-29T12:07:33+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00456v1",
      "file": "papers/2512.00456v1.pdf"
    },
    {
      "arxiv_id": "2512.00311v1",
      "title": "Tracing Mathematical Proficiency Through Problem-Solving Processes",
      "authors": [
        {
          "name": "Jungyang Park"
        },
        {
          "name": "Suho Kang"
        },
        {
          "name": "Jaewoo Park"
        },
        {
          "name": "Jaehong Kim"
        },
        {
          "name": "Jaewoo Shin"
        },
        {
          "name": "Seonjoon Park"
        },
        {
          "name": "Youngjae Yu"
        }
      ],
      "abstract": "Knowledge Tracing (KT) aims to model student's knowledge state and predict future performance to enable personalized learning in Intelligent Tutoring Systems. However, traditional KT methods face fundamental limitations in explainability, as they rely solely on the response correctness, neglecting the rich information embedded in students' problem-solving processes. To address this gap, we propose Knowledge Tracing Leveraging Problem-Solving Process (KT-PSP), which incorporates students' problem-solving processes to capture the multidimensional aspects of mathematical proficiency. We also introduce KT-PSP-25, a new dataset specifically designed for the KT-PSP. Building on this, we present StatusKT, a KT framework that employs a teacher-student-teacher three-stage LLM pipeline to extract students' MP as intermediate signals. In this pipeline, the teacher LLM first extracts problem-specific proficiency indicators, then a student LLM generates responses based on the student's solution process, and a teacher LLM evaluates these responses to determine mastery of each indicator. The experimental results on KT-PSP-25 demonstrate that StatusKT improves the prediction performance of existing KT methods. Moreover, StatusKT provides interpretable explanations for its predictions by explicitly modeling students' mathematical proficiency.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "published": "2025-11-29T04:12:06+00:00",
      "updated": "2025-11-29T04:12:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00311v1",
      "file": "papers/2512.00311v1.pdf"
    },
    {
      "arxiv_id": "2512.00260v1",
      "title": "Scalable and Interpretable Scientific Discovery via Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KAN)",
      "authors": [
        {
          "name": "Y. Sungtaek Ju"
        }
      ],
      "abstract": "Kolmogorov-Arnold Networks (KANs) offer a promising alternative to Multi-Layer Perceptron (MLP) by placing learnable univariate functions on network edges, enhancing interpretability. However, standard KANs lack probabilistic outputs, limiting their utility in applications requiring uncertainty quantification. While recent Gaussian Process (GP) extensions to KANs address this, they utilize exact inference methods that scale cubically with data size N, restricting their application to smaller datasets. We introduce the Sparse Variational GP-KAN (SVGP-KAN), an architecture that integrates sparse variational inference with the KAN topology. By employing $M$ inducing points and analytic moment matching, our method reduces computational complexity from $O(N^3)$ to $O(NM^2)$ or linear in sample size, enabling the application of probabilistic KANs to larger scientific datasets. Furthermore, we demonstrate that integrating a permutation-based importance analysis enables the network to function as a framework for structural identification, identifying relevant inputs and classifying functional relationships.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2025-11-29T00:48:55+00:00",
      "updated": "2025-11-29T00:48:55+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00260v1",
      "file": "papers/2512.00260v1.pdf"
    },
    {
      "arxiv_id": "2512.00194v1",
      "title": "AutocleanEEG ICVision: Automated ICA Artifact Classification Using Vision-Language AI",
      "authors": [
        {
          "name": "Zag ElSayed"
        },
        {
          "name": "Grace Westerkamp"
        },
        {
          "name": "Gavin Gammoh"
        },
        {
          "name": "Yanchen Liu"
        },
        {
          "name": "Peyton Siekierski"
        },
        {
          "name": "Craig Erickson"
        },
        {
          "name": "Ernest Pedapati"
        }
      ],
      "abstract": "We introduce EEG Autoclean Vision Language AI (ICVision) a first-of-its-kind system that emulates expert-level EEG ICA component classification through AI-agent vision and natural language reasoning. Unlike conventional classifiers such as ICLabel, which rely on handcrafted features, ICVision directly interprets ICA dashboard visualizations topography, time series, power spectra, and ERP plots, using a multimodal large language model (GPT-4 Vision). This allows the AI to see and explain EEG components the way trained neurologists do, making it the first scientific implementation of AI-agent visual cognition in neurophysiology. ICVision classifies each component into one of six canonical categories (brain, eye, heart, muscle, channel noise, and other noise), returning both a confidence score and a human-like explanation. Evaluated on 3,168 ICA components from 124 EEG datasets, ICVision achieved k = 0.677 agreement with expert consensus, surpassing MNE ICLabel, while also preserving clinically relevant brain signals in ambiguous cases. Over 97% of its outputs were rated as interpretable and actionable by expert reviewers. As a core module of the open-source EEG Autoclean platform, ICVision signals a paradigm shift in scientific AI, where models do not just classify, but see, reason, and communicate. It opens the door to globally scalable, explainable, and reproducible EEG workflows, marking the emergence of AI agents capable of expert-level visual decision-making in brain science and beyond.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG",
        "eess.IV",
        "q-bio.QM"
      ],
      "published": "2025-11-28T20:19:34+00:00",
      "updated": "2025-11-28T20:19:34+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00194v1",
      "file": "papers/2512.00194v1.pdf"
    },
    {
      "arxiv_id": "2512.00164v1",
      "title": "Faster Verified Explanations for Neural Networks",
      "authors": [
        {
          "name": "Alessandro De Palma"
        },
        {
          "name": "Greta Dolcetti"
        },
        {
          "name": "Caterina Urban"
        }
      ],
      "abstract": "Verified explanations are a theoretically-principled way to explain the decisions taken by neural networks, which are otherwise black-box in nature. However, these techniques face significant scalability challenges, as they require multiple calls to neural network verifiers, each of them with an exponential worst-case complexity. We present FaVeX, a novel algorithm to compute verified explanations. FaVeX accelerates the computation by dynamically combining batch and sequential processing of input features, and by reusing information from previous queries, both when proving invariances with respect to certain input features, and when searching for feature assignments altering the prediction. Furthermore, we present a novel and hierarchical definition of verified explanations, termed verifier-optimal robust explanations, that explicitly factors the incompleteness of network verifiers within the explanation. Our comprehensive experimental evaluation demonstrates the superior scalability of both FaVeX, and of verifier-optimal robust explanations, which together can produce meaningful formal explanation on networks with hundreds of thousands of non-linear activations.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.PL"
      ],
      "published": "2025-11-28T19:05:39+00:00",
      "updated": "2025-11-28T19:05:39+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00164v1",
      "file": "papers/2512.00164v1.pdf"
    },
    {
      "arxiv_id": "2512.00163v1",
      "title": "Measuring What LLMs Think They Do: SHAP Faithfulness and Deployability on Financial Tabular Classification",
      "authors": [
        {
          "name": "Saeed AlMarri"
        },
        {
          "name": "Mathieu Ravaut"
        },
        {
          "name": "Kristof Juhasz"
        },
        {
          "name": "Gautier Marti"
        },
        {
          "name": "Hamdan Al Ahbabi"
        },
        {
          "name": "Ibrahim Elfadel"
        }
      ],
      "abstract": "Large Language Models (LLMs) have attracted significant attention for classification tasks, offering a flexible alternative to trusted classical machine learning models like LightGBM through zero-shot prompting. However, their reliability for structured tabular data remains unclear, particularly in high stakes applications like financial risk assessment. Our study systematically evaluates LLMs and generates their SHAP values on financial classification tasks. Our analysis shows a divergence between LLMs self-explanation of feature impact and their SHAP values, as well as notable differences between LLMs and LightGBM SHAP values. These findings highlight the limitations of LLMs as standalone classifiers for structured financial modeling, but also instill optimism that improved explainability mechanisms coupled with few-shot prompting will make LLMs usable in risk-sensitive domains.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2025-11-28T19:04:25+00:00",
      "updated": "2025-11-28T19:04:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.00163v1",
      "file": "papers/2512.00163v1.pdf"
    },
    {
      "arxiv_id": "2511.23387v1",
      "title": "Hierarchical AI-Meteorologist: LLM-Agent System for Multi-Scale and Explainable Weather Forecast Reporting",
      "authors": [
        {
          "name": "Daniil Sukhorukov"
        },
        {
          "name": "Andrei Zakharov"
        },
        {
          "name": "Nikita Glazkov"
        },
        {
          "name": "Katsiaryna Yanchanka"
        },
        {
          "name": "Vladimir Kirilin"
        },
        {
          "name": "Maxim Dubovitsky"
        },
        {
          "name": "Roman Sultimov"
        },
        {
          "name": "Yuri Maksimov"
        },
        {
          "name": "Ilya Makarov"
        }
      ],
      "abstract": "We present the Hierarchical AI-Meteorologist, an LLM-agent system that generates explainable weather reports using a hierarchical forecast reasoning and weather keyword generation. Unlike standard approaches that treat forecasts as flat time series, our framework performs multi-scale reasoning across hourly, 6-hour, and daily aggregations to capture both short-term dynamics and long-term trends. Its core reasoning agent converts structured meteorological inputs into coherent narratives while simultaneously extracting a few keywords effectively summarizing the dominant meteorological events. These keywords serve as semantic anchors for validating consistency, temporal coherence and factual alignment of the generated reports. Using OpenWeather and Meteostat data, we demonstrate that hierarchical context and keyword-based validation substantially improve interpretability and robustness of LLM-generated weather narratives, offering a reproducible framework for semantic evaluation of automated meteorological reporting and advancing agent-based scientific reasoning.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-11-28T17:27:06+00:00",
      "updated": "2025-11-28T17:27:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.23387v1",
      "file": "papers/2511.23387v1.pdf"
    },
    {
      "arxiv_id": "2511.23335v1",
      "title": "Towards Improving Interpretability of Language Model Generation through a Structured Knowledge Discovery Approach",
      "authors": [
        {
          "name": "Shuqi Liu"
        },
        {
          "name": "Han Wu"
        },
        {
          "name": "Guanzhi Deng"
        },
        {
          "name": "Jianshu Chen"
        },
        {
          "name": "Xiaoyang Wang"
        },
        {
          "name": "Linqi Song"
        }
      ],
      "abstract": "Knowledge-enhanced text generation aims to enhance the quality of generated text by utilizing internal or external knowledge sources. While language models have demonstrated impressive capabilities in generating coherent and fluent text, the lack of interpretability presents a substantial obstacle. The limited interpretability of generated text significantly impacts its practical usability, particularly in knowledge-enhanced text generation tasks that necessitate reliability and explainability. Existing methods often employ domain-specific knowledge retrievers that are tailored to specific data characteristics, limiting their generalizability to diverse data types and tasks. To overcome this limitation, we directly leverage the two-tier architecture of structured knowledge, consisting of high-level entities and low-level knowledge triples, to design our task-agnostic structured knowledge hunter. Specifically, we employ a local-global interaction scheme for structured knowledge representation learning and a hierarchical transformer-based pointer network as the backbone for selecting relevant knowledge triples and entities. By combining the strong generative ability of language models with the high faithfulness of the knowledge hunter, our model achieves high interpretability, enabling users to comprehend the model output generation process. Furthermore, we empirically demonstrate the effectiveness of our model in both internal knowledge-enhanced table-to-text generation on the RotoWireFG dataset and external knowledge-enhanced dialogue response generation on the KdConv dataset. Our task-agnostic model outperforms state-of-the-art methods and corresponding language models, setting new standards on the benchmark.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-11-28T16:43:46+00:00",
      "updated": "2025-11-28T16:43:46+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.23335v1",
      "file": "papers/2511.23335v1.pdf"
    },
    {
      "arxiv_id": "2511.23239v1",
      "title": "Towards Understanding Transformers in Learning Random Walks",
      "authors": [
        {
          "name": "Wei Shi"
        },
        {
          "name": "Yuan Cao"
        }
      ],
      "abstract": "Transformers have proven highly effective across various applications, especially in handling sequential data such as natural languages and time series. However, transformer models often lack clear interpretability, and the success of transformers has not been well understood in theory. In this paper, we study the capability and interpretability of transformers in learning a family of classic statistical models, namely random walks on circles. We theoretically demonstrate that, after training with gradient descent, a one-layer transformer model can achieve optimal accuracy in predicting random walks. Importantly, our analysis reveals that the trained model is interpretable: the trained softmax attention serves as a token selector, focusing on the direct parent state; subsequently, the value matrix executes a one-step probability transition to predict the location of the next state based on this parent state. We also show that certain edge cases not covered by our theory are indeed failure cases, demonstrating that our theoretical conditions are tight. By investigating these success and failure cases, it is revealed that gradient descent with small initialization may fail or struggle to converge to a good solution in certain simple tasks even beyond random walks. Experiments are conducted to support our theoretical findings.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2025-11-28T14:48:28+00:00",
      "updated": "2025-11-28T14:48:28+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.23239v1",
      "file": "papers/2511.23239v1.pdf"
    },
    {
      "arxiv_id": "2511.23158v1",
      "title": "REVEAL: Reasoning-enhanced Forensic Evidence Analysis for Explainable AI-generated Image Detection",
      "authors": [
        {
          "name": "Huangsen Cao"
        },
        {
          "name": "Qin Mei"
        },
        {
          "name": "Zhiheng Li"
        },
        {
          "name": "Yuxi Li"
        },
        {
          "name": "Ying Zhang"
        },
        {
          "name": "Chen Li"
        },
        {
          "name": "Zhimeng Zhang"
        },
        {
          "name": "Xin Ding"
        },
        {
          "name": "Yongwei Wang"
        },
        {
          "name": "Jing Lyu"
        },
        {
          "name": "Fei Wu"
        }
      ],
      "abstract": "With the rapid advancement of generative models, visually realistic AI-generated images have become increasingly difficult to distinguish from authentic ones, posing severe threats to social trust and information integrity. Consequently, there is an urgent need for efficient and truly explainable image forensic methods. Recent detection paradigms have shifted towards explainable forensics. However, state-of-the-art approaches primarily rely on post-hoc rationalizations or visual discrimination, lacking a verifiable chain of evidence. This reliance on surface-level pattern matching limits the generation of causally grounded explanations and often results in poor generalization. To bridge this critical gap, we introduce \\textbf{REVEAL-Bench}, the first reasoning-enhanced multimodal benchmark for AI-generated image detection that is explicitly structured around a chain-of-evidence derived from multiple lightweight expert models, then records step-by-step reasoning traces and evidential justifications. Building upon this dataset, we propose \\textbf{REVEAL} (\\underline{R}easoning-\\underline{e}nhanced Forensic E\\underline{v}id\\underline{e}nce \\underline{A}na\\underline{l}ysis), an effective and explainable forensic framework that integrates detection with a novel expert-grounded reinforcement learning. Our reward mechanism is specially tailored to jointly optimize detection accuracy, explanation fidelity, and logical coherence grounded in explicit forensic evidence, enabling REVEAL to produce fine-grained, interpretable, and verifiable reasoning chains alongside its detection outcomes. Extensive experimental results demonstrate that REVEAL significantly enhances detection accuracy, explanation fidelity, and robust cross-model generalization, benchmarking a new state of the art for explainable image forensics.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-11-28T13:11:08+00:00",
      "updated": "2025-11-28T13:11:08+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.23158v1",
      "file": "papers/2511.23158v1.pdf"
    },
    {
      "arxiv_id": "2511.23036v1",
      "title": "Delta-XAI: A Unified Framework for Explaining Prediction Changes in Online Time Series Monitoring",
      "authors": [
        {
          "name": "Changhun Kim"
        },
        {
          "name": "Yechan Mun"
        },
        {
          "name": "Hyeongwon Jang"
        },
        {
          "name": "Eunseo Lee"
        },
        {
          "name": "Sangchul Hahn"
        },
        {
          "name": "Eunho Yang"
        }
      ],
      "abstract": "Explaining online time series monitoring models is crucial across sensitive domains such as healthcare and finance, where temporal and contextual prediction dynamics underpin critical decisions. While recent XAI methods have improved the explainability of time series models, they mostly analyze each time step independently, overlooking temporal dependencies. This results in further challenges: explaining prediction changes is non-trivial, methods fail to leverage online dynamics, and evaluation remains difficult. To address these challenges, we propose Delta-XAI, which adapts 14 existing XAI methods through a wrapper function and introduces a principled evaluation suite for the online setting, assessing diverse aspects, such as faithfulness, sufficiency, and coherence. Experiments reveal that classical gradient-based methods, such as Integrated Gradients (IG), can outperform recent approaches when adapted for temporal analysis. Building on this, we propose Shifted Window Integrated Gradients (SWING), which incorporates past observations in the integration path to systematically capture temporal dependencies and mitigate out-of-distribution effects. Extensive experiments consistently demonstrate the effectiveness of SWING across diverse settings with respect to diverse metrics. Our code is publicly available at https://anonymous.4open.science/r/Delta-XAI.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-11-28T09:57:44+00:00",
      "updated": "2025-11-28T09:57:44+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.23036v1",
      "file": "papers/2511.23036v1.pdf"
    },
    {
      "arxiv_id": "2512.08956v1",
      "title": "DW-KNN: A Transparent Local Classifier Integrating Distance Consistency and Neighbor Reliability",
      "authors": [
        {
          "name": "Kumarjit Pathak"
        },
        {
          "name": "Karthik K"
        },
        {
          "name": "Sachin Madan"
        },
        {
          "name": "Jitin Kapila"
        }
      ],
      "abstract": "K-Nearest Neighbors (KNN) is one of the most used ML classifiers. However, if we observe closely, standard distance-weighted KNN and relative variants assume all 'k' neighbors are equally reliable. In heterogeneous feature space, this becomes a limitation that hinders reliability in predicting true levels of the observation.\n  We propose DW-KNN (Double Weighted KNN), a transparent and robust variant that integrates exponential distance with neighbor validity. This enables instance-level interpretability, suppresses noisy or mislabeled samples, and reduces hyperparameter sensitivity.\n  Comprehensive evaluation on 9 data-sets helps to demonstrate that DW-KNN achieves 0.8988 accuracy on average. It ranks 2nd among six methods and within 0.2% of the best-performing Ensemble KNN. It also exhibits the lowest cross-validation variance (0.0156), indicating reliable prediction stability. Statistical significance test confirmed ($p < 0.001$) improvement over compactness weighted KNN (+4.09\\%) and Kernel weighted KNN (+1.13\\%). The method provides a simple yet effective alternative to complex adaptive schemes, particularly valuable for high-stakes applications requiring explainable predictions.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2025-11-28T09:26:45+00:00",
      "updated": "2025-11-28T09:26:45+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.08956v1",
      "file": "papers/2512.08956v1.pdf"
    },
    {
      "arxiv_id": "2511.22998v1",
      "title": "TIM-PRM: Verifying multimodal reasoning with Tool-Integrated PRM",
      "authors": [
        {
          "name": "Peng Kuang"
        },
        {
          "name": "Xiangxiang Wang"
        },
        {
          "name": "Wentao Liu"
        },
        {
          "name": "Jian Dong"
        },
        {
          "name": "Kaidi Xu"
        },
        {
          "name": "Haohan Wang"
        }
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved impressive performances in mathematical reasoning, yet they remain vulnerable to visual hallucinations and logical inconsistencies that standard outcome-based supervision fails to mitigate. While Process Reward Models (PRMs) promise step-by-step verification, current approaches typically operate as scalar scorers or generative critics that suffer from sycophancy, blindly validating the flawed hypotheses rather than grounding them in visual reality. To bridge this gap, we introduce TIM-PRM (Tool-Integrated Multimodal PRM), a novel agentic framework that transforms verification from a passive classification task into an active, tool-augmented investigation. TIM-PRM is trained to explicitly plan verification strategies and utilizes a mechanism of Independent Question Asking to query evidence via external tools, effectively decoupling verification from the reasoning context to eliminate confirmation bias. We instantiate this method by curating a high-quality dataset of tool-integrated verification trajectories. Extensive experiments on VisualProcessBench demonstrate that our 8B parameter model surpasses existing open-source multimodal PRMs, significantly outperforming much larger models like Qwen2.5-72B and InternVL-78B, while offering interpretable insights into the verification process.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-11-28T09:01:38+00:00",
      "updated": "2025-11-28T09:01:38+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.22998v1",
      "file": "papers/2511.22998v1.pdf"
    },
    {
      "arxiv_id": "2512.02057v1",
      "title": "Opening the Black Box: An Explainable, Few-shot AI4E Framework Informed by Physics and Expert Knowledge for Materials Engineering",
      "authors": [
        {
          "name": "Haoxiang Zhang"
        },
        {
          "name": "Ruihao Yuan"
        },
        {
          "name": "Lihui Zhang"
        },
        {
          "name": "Yushi Luo"
        },
        {
          "name": "Qiang Zhang"
        },
        {
          "name": "Pan Ding"
        },
        {
          "name": "Xiaodong Ren"
        },
        {
          "name": "Weijie Xing"
        },
        {
          "name": "Niu Gao"
        },
        {
          "name": "Jishan Chen"
        },
        {
          "name": "Chubo Zhang"
        }
      ],
      "abstract": "The industrial adoption of Artificial Intelligence for Engineering (AI4E) faces two fundamental bottlenecks: scarce high-quality data and the lack of interpretability in black-box models-particularly critical in safety-sensitive sectors like aerospace. We present an explainable, few-shot AI4E framework that is systematically informed by physics and expert knowledge throughout its architecture. Starting from only 32 experimental samples in an aerial K439B superalloy castings repair welding case, we first augment physically plausible synthetic data through a three-stage protocol: differentiated noise injection calibrated to process variabilities, enforcement of hard physical constraints, and preservation of inter-parameter relationships. We then employ a nested optimization strategy for constitutive model discovery, where symbolic regression explores equation structures while differential evolution optimizes parameters, followed by intensive parameter refinement using hybrid global-local optimization. The resulting interpretable constitutive equation achieves 88% accuracy in predicting hot-cracking tendency. This equation not only provides quantitative predictions but also delivers explicit physical insight, revealing how thermal, geometric, and metallurgical mechanisms couple to drive cracking-thereby advancing engineers' cognitive understanding of the process. Furthermore, the constitutive equation serves as a multi-functional tool for process optimization and high-fidelity virtual data generation, enabling accuracy improvements in other data-driven models. Our approach provides a general blueprint for developing trustworthy AI systems that embed engineering domain knowledge directly into their architecture, enabling reliable adoption in high-stakes industrial applications where data is limited but physical understanding is available.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-11-28T06:50:29+00:00",
      "updated": "2025-11-28T06:50:29+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.02057v1",
      "file": "papers/2512.02057v1.pdf"
    },
    {
      "arxiv_id": "2511.22866v1",
      "title": "ARM-Explainer -- Explaining and improving graph neural network predictions for the maximum clique problem using node features and association rule mining",
      "authors": [
        {
          "name": "Bharat Sharman"
        },
        {
          "name": "Elkafi Hassini"
        }
      ],
      "abstract": "Numerous graph neural network (GNN)-based algorithms have been proposed to solve graph-based combinatorial optimization problems (COPs), but methods to explain their predictions remain largely undeveloped. We introduce ARM-Explainer, a post-hoc, model-level explainer based on association rule mining, and demonstrate it on the predictions of the hybrid geometric scattering (HGS) GNN for the maximum clique problem (MCP), a canonical NP-hard graph-based COP. The eight most explanatory association rules discovered by ARM-Explainer achieve high median lift and confidence values of 2.42 and 0.49, respectively, on test instances from the TWITTER and BHOSLIB-DIMACS benchmark datasets. ARM-Explainer identifies the most important node features, together with their value ranges, that influence the GNN's predictions on these datasets. Furthermore, augmenting the GNN with informative node features substantially improves its performance on the MCP, increasing the median largest-found clique size by 22% (from 29.5 to 36) on large graphs from the BHOSLIB-DIMACS dataset.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.OC"
      ],
      "published": "2025-11-28T03:54:31+00:00",
      "updated": "2025-11-28T03:54:31+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.22866v1",
      "file": "papers/2511.22866v1.pdf"
    },
    {
      "arxiv_id": "2511.22420v1",
      "title": "MATCH: Engineering Transparent and Controllable Conversational XAI Systems through Composable Building Blocks",
      "authors": [
        {
          "name": "Sebe Vanbrabant"
        },
        {
          "name": "Gustavo Rovelo Ruiz"
        },
        {
          "name": "Davy Vanacken"
        }
      ],
      "abstract": "While the increased integration of AI technologies into interactive systems enables them to solve an increasing number of tasks, the black-box problem of AI models continues to spread throughout the interactive system as a whole. Explainable AI (XAI) techniques can make AI models more accessible by employing post-hoc methods or transitioning to inherently interpretable models. While this makes individual AI models clearer, the overarching system architecture remains opaque. This challenge not only pertains to standard XAI techniques but also to human examination and conversational XAI approaches that need access to model internals to interpret them correctly and completely. To this end, we propose conceptually representing such interactive systems as sequences of structural building blocks. These include the AI models themselves, as well as control mechanisms grounded in literature. The structural building blocks can then be explained through complementary explanatory building blocks, such as established XAI techniques like LIME and SHAP. The flow and APIs of the structural building blocks form an unambiguous overview of the underlying system, serving as a communication basis for both human and automated agents, thus aligning human and machine interpretability of the embedded AI models. In this paper, we present our flow-based approach and a selection of building blocks as MATCH: a framework for engineering Multi-Agent Transparent and Controllable Human-centered systems. This research contributes to the field of (conversational) XAI by facilitating the integration of interpretability into existing interactive systems.",
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "published": "2025-11-27T12:58:04+00:00",
      "updated": "2025-11-27T12:58:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.22420v1",
      "file": "papers/2511.22420v1.pdf"
    },
    {
      "arxiv_id": "2512.03072v2",
      "title": "Beyond the Black Box: A Cognitive Architecture for Explainable and Aligned AI",
      "authors": [
        {
          "name": "Hu Keyi"
        }
      ],
      "abstract": "Current AI paradigms, as \"architects of experience,\" face fundamental challenges in explainability and value alignment. This paper introduces \"Weight-Calculatism,\" a novel cognitive architecture grounded in first principles, and demonstrates its potential as a viable pathway toward Artificial General Intelligence (AGI). The architecture deconstructs cognition into indivisible Logical Atoms and two fundamental operations: Pointing and Comparison. Decision-making is formalized through an interpretable Weight-Calculation model (Weight = Benefit * Probability), where all values are traceable to an auditable set of Initial Weights. This atomic decomposition enables radical explainability, intrinsic generality for novel situations, and traceable value alignment. We detail its implementation via a graph-algorithm-based computational engine and a global workspace workflow, supported by a preliminary code implementation and scenario validation. Results indicate that the architecture achieves transparent, human-like reasoning and robust learning in unprecedented scenarios, establishing a practical and theoretical foundation for building trustworthy and aligned AGI.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "published": "2025-11-27T12:42:54+00:00",
      "updated": "2025-12-08T14:27:36+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.03072v2",
      "file": "papers/2512.03072v2.pdf"
    },
    {
      "arxiv_id": "2511.22402v1",
      "title": "Mapping Clinical Doubt: Locating Linguistic Uncertainty in LLMs",
      "authors": [
        {
          "name": "Srivarshinee Sridhar"
        },
        {
          "name": "Raghav Kaushik Ravi"
        },
        {
          "name": "Kripabandhu Ghosh"
        }
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used in clinical settings, where sensitivity to linguistic uncertainty can influence diagnostic interpretation and decision-making. Yet little is known about where such epistemic cues are internally represented within these models. Distinct from uncertainty quantification, which measures output confidence, this work examines input-side representational sensitivity to linguistic uncertainty in medical text. We curate a contrastive dataset of clinical statements varying in epistemic modality (e.g., 'is consistent with' vs. 'may be consistent with') and propose Model Sensitivity to Uncertainty (MSU), a layerwise probing metric that quantifies activation-level shifts induced by uncertainty cues. Our results show that LLMs exhibit structured, depth-dependent sensitivity to clinical uncertainty, suggesting that epistemic information is progressively encoded in deeper layers. These findings reveal how linguistic uncertainty is internally represented in LLMs, offering insight into their interpretability and epistemic reliability.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-11-27T12:26:06+00:00",
      "updated": "2025-11-27T12:26:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.22402v1",
      "file": "papers/2511.22402v1.pdf"
    },
    {
      "arxiv_id": "2512.07875v1",
      "title": "Softly Symbolifying Kolmogorov-Arnold Networks",
      "authors": [
        {
          "name": "James Bagrow"
        },
        {
          "name": "Josh Bongard"
        }
      ],
      "abstract": "Kolmogorov-Arnold Networks (KANs) offer a promising path toward interpretable machine learning: their learnable activations can be studied individually, while collectively fitting complex data accurately. In practice, however, trained activations often lack symbolic fidelity, learning pathological decompositions with no meaningful correspondence to interpretable forms. We propose Softly Symbolified Kolmogorov-Arnold Networks (S2KAN), which integrate symbolic primitives directly into training. Each activation draws from a dictionary of symbolic and dense terms, with learnable gates that sparsify the representation. Crucially, this sparsification is differentiable, enabling end-to-end optimization, and is guided by a principled Minimum Description Length objective. When symbolic terms suffice, S2KAN discovers interpretable forms; when they do not, it gracefully degrades to dense splines. We demonstrate competitive or superior accuracy with substantially smaller models across symbolic benchmarks, dynamical systems forecasting, and real-world prediction tasks, and observe evidence of emergent self-sparsification even without regularization pressure.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.NE",
        "physics.data-an",
        "stat.ML"
      ],
      "published": "2025-11-27T07:03:38+00:00",
      "updated": "2025-11-27T07:03:38+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.07875v1",
      "file": "papers/2512.07875v1.pdf"
    },
    {
      "arxiv_id": "2511.22150v2",
      "title": "From Topology to Retrieval: Decoding Embedding Spaces with Unified Signatures",
      "authors": [
        {
          "name": "Florian Rottach"
        },
        {
          "name": "William Rudman"
        },
        {
          "name": "Bastian Rieck"
        },
        {
          "name": "Harrisen Scells"
        },
        {
          "name": "Carsten Eickhoff"
        }
      ],
      "abstract": "Studying how embeddings are organized in space not only enhances model interpretability but also uncovers factors that drive downstream task performance. In this paper, we present a comprehensive analysis of topological and geometric measures across a wide set of text embedding models and datasets. We find a high degree of redundancy among these measures and observe that individual metrics often fail to sufficiently differentiate embedding spaces. Building on these insights, we introduce Unified Topological Signatures (UTS), a holistic framework for characterizing embedding spaces. We show that UTS can predict model-specific properties and reveal similarities driven by model architecture. Further, we demonstrate the utility of our method by linking topological structure to ranking effectiveness and accurately predicting document retrievability. We find that a holistic, multi-attribute perspective is essential to understanding and leveraging the geometry of text embeddings.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.IR"
      ],
      "published": "2025-11-27T06:37:45+00:00",
      "updated": "2025-12-01T06:39:02+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.22150v2",
      "file": "papers/2511.22150v2.pdf"
    },
    {
      "arxiv_id": "2511.22018v1",
      "title": "MedEyes: Learning Dynamic Visual Focus for Medical Progressive Diagnosis",
      "authors": [
        {
          "name": "Chunzheng Zhu"
        },
        {
          "name": "Yangfang Lin"
        },
        {
          "name": "Shen Chen"
        },
        {
          "name": "Yijun Wang"
        },
        {
          "name": "Jianxin Lin"
        }
      ],
      "abstract": "Accurate medical diagnosis often involves progressive visual focusing and iterative reasoning, characteristics commonly observed in clinical workflows. While recent vision-language models demonstrate promising chain-of-thought (CoT) reasoning capabilities via reinforcement learning with verifiable rewards (RLVR), their purely on-policy learning paradigm tends to reinforce superficially coherent but clinically inaccurate reasoning paths. We propose MedEyes, a novel reinforcement learning framework that dynamically models clinician-style diagnostic reasoning by progressively attending to and interpreting relevant medical image regions. By incorporating off-policy expert guidance, MedEyes converts expert visual search trajectories into structured external behavioral signals, guiding the model toward clinically aligned visual reasoning. We design the Gaze-guided Reasoning Navigator (GRN) to emulate the diagnostic process through a dual-mode exploration strategy, scanning for systematic abnormality localization and drilling for detailed regional analysis. To balance expert imitation and autonomous discovery, we introduce the Confidence Value Sampler (CVS), which employs nucleus sampling and adaptive termination to create diverse yet credible exploration paths. Finally, the dual-stream GRPO optimization framework decouples on-policy and off-policy learning signals, mitigating reward assimilation and entropy collapse. Experiments demonstrate that MedEyes achieves an average performance improvement of +8.5\\% across multiple medical VQA benchmarks, validating MedEyes's potential in building interpretable medical AI systems.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-11-27T01:47:43+00:00",
      "updated": "2025-11-27T01:47:43+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.22018v1",
      "file": "papers/2511.22018v1.pdf"
    },
    {
      "arxiv_id": "2511.21952v1",
      "title": "ABLE: Using Adversarial Pairs to Construct Local Models for Explaining Model Predictions",
      "authors": [
        {
          "name": "Krishna Khadka"
        },
        {
          "name": "Sunny Shree"
        },
        {
          "name": "Pujan Budhathoki"
        },
        {
          "name": "Yu Lei"
        },
        {
          "name": "Raghu Kacker"
        },
        {
          "name": "D. Richard Kuhn"
        }
      ],
      "abstract": "Machine learning models are increasingly used in critical applications but are mostly \"black boxes\" due to their lack of transparency. Local explanation approaches, such as LIME, address this issue by approximating the behavior of complex models near a test instance using simple, interpretable models. However, these approaches often suffer from instability and poor local fidelity. In this paper, we propose a novel approach called Adversarially Bracketed Local Explanation (ABLE) to address these limitations. Our approach first generates a set of neighborhood points near the test instance, x_test, by adding bounded Gaussian noise. For each neighborhood point D, we apply an adversarial attack to generate an adversarial point A with minimal perturbation that results in a different label than D. A second adversarial attack is then performed on A to generate a point A' that has the same label as D (and thus different than A). The points A and A' form an adversarial pair that brackets the local decision boundary for x_test. We then train a linear model on these adversarial pairs to approximate the local decision boundary. Experimental results on six UCI benchmark datasets across three deep neural network architectures demonstrate that our approach achieves higher stability and fidelity than the state-of-the-art.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-11-26T22:20:28+00:00",
      "updated": "2025-11-26T22:20:28+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.21952v1",
      "file": "papers/2511.21952v1.pdf"
    },
    {
      "arxiv_id": "2511.21931v2",
      "title": "Does the Model Say What the Data Says? A Simple Heuristic for Model Data Alignment",
      "authors": [
        {
          "name": "Henry Salgado"
        },
        {
          "name": "Meagan R. Kendall"
        },
        {
          "name": "Martine Ceberio"
        }
      ],
      "abstract": "In this work, we propose a simple and computationally efficient framework for evaluating whether machine learning models align with the structure of the data they learn from; that is, whether the model says what the data says. Unlike existing interpretability methods that focus exclusively on explaining model behavior, our approach establishes a baseline derived directly from the data itself. Drawing inspiration from Rubin's Potential Outcomes Framework, we quantify how strongly each feature separates the two outcome groups in a binary classification task, moving beyond traditional descriptive statistics to estimate each feature's effect on the outcome. By comparing these data-derived feature rankings with model-based explanations, we provide practitioners with an interpretable and model-agnostic method for assessing model-data alignment.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-11-26T21:44:55+00:00",
      "updated": "2025-12-05T21:53:03+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.21931v2",
      "file": "papers/2511.21931v2.pdf"
    },
    {
      "arxiv_id": "2511.21594v1",
      "title": "Visualizing LLM Latent Space Geometry Through Dimensionality Reduction",
      "authors": [
        {
          "name": "Alex Ning"
        },
        {
          "name": "Vainateya Rangaraju"
        }
      ],
      "abstract": "Large language models (LLMs) achieve state-of-the-art results across many natural language tasks, but their internal mechanisms remain difficult to interpret. In this work, we extract, process, and visualize latent state geometries in Transformer-based language models through dimensionality reduction. We capture layerwise activations at multiple points within Transformer blocks and enable systematic analysis through Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We demonstrate experiments on GPT-2 and LLaMa models, where we uncover interesting geometric patterns in latent space. Notably, we identify a clear separation between attention and MLP component outputs across intermediate layers, a pattern not documented in prior work to our knowledge. We also characterize the high norm of latent states at the initial sequence position and visualize the layerwise evolution of latent states. Additionally, we demonstrate the high-dimensional helical structure of GPT-2's positional embeddings, the sequence-wise geometric patterns in LLaMa, and experiment with repeating token sequences. We aim to support systematic analysis of Transformer internals with the goal of enabling further reproducible interpretability research. We make our code available at https://github.com/Vainateya/Feature_Geometry_Visualization.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-11-26T17:11:39+00:00",
      "updated": "2025-11-26T17:11:39+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.21594v1",
      "file": "papers/2511.21594v1.pdf"
    },
    {
      "arxiv_id": "2511.21517v1",
      "title": "Voice, Bias, and Coreference: An Interpretability Study of Gender in Speech Translation",
      "authors": [
        {
          "name": "Lina Conti"
        },
        {
          "name": "Dennis Fucci"
        },
        {
          "name": "Marco Gaido"
        },
        {
          "name": "Matteo Negri"
        },
        {
          "name": "Guillaume Wisniewski"
        },
        {
          "name": "Luisa Bentivogli"
        }
      ],
      "abstract": "Unlike text, speech conveys information about the speaker, such as gender, through acoustic cues like pitch. This gives rise to modality-specific bias concerns. For example, in speech translation (ST), when translating from languages with notional gender, such as English, into languages where gender-ambiguous terms referring to the speaker are assigned grammatical gender, the speaker's vocal characteristics may play a role in gender assignment. This risks misgendering speakers, whether through masculine defaults or vocal-based assumptions. Yet, how ST models make these decisions remains poorly understood. We investigate the mechanisms ST models use to assign gender to speaker-referring terms across three language pairs (en-es/fr/it), examining how training data patterns, internal language model (ILM) biases, and acoustic information interact. We find that models do not simply replicate term-specific gender associations from training data, but learn broader patterns of masculine prevalence. While the ILM exhibits strong masculine bias, models can override these preferences based on acoustic input. Using contrastive feature attribution on spectrograms, we reveal that the model with higher gender accuracy relies on a previously unknown mechanism: using first-person pronouns to link gendered terms back to the speaker, accessing gender information distributed across the frequency spectrum rather than concentrated in pitch.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-11-26T15:48:04+00:00",
      "updated": "2025-11-26T15:48:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.21517v1",
      "file": "papers/2511.21517v1.pdf"
    },
    {
      "arxiv_id": "2511.21514v1",
      "title": "Mechanistic Interpretability for Transformer-based Time Series Classification",
      "authors": [
        {
          "name": "Matīss Kalnāre"
        },
        {
          "name": "Sofoklis Kitharidis"
        },
        {
          "name": "Thomas Bäck"
        },
        {
          "name": "Niki van Stein"
        }
      ],
      "abstract": "Transformer-based models have become state-of-the-art tools in various machine learning tasks, including time series classification, yet their complexity makes understanding their internal decision-making challenging. Existing explainability methods often focus on input-output attributions, leaving the internal mechanisms largely opaque. This paper addresses this gap by adapting various Mechanistic Interpretability techniques; activation patching, attention saliency, and sparse autoencoders, from NLP to transformer architectures designed explicitly for time series classification. We systematically probe the internal causal roles of individual attention heads and timesteps, revealing causal structures within these models. Through experimentation on a benchmark time series dataset, we construct causal graphs illustrating how information propagates internally, highlighting key attention heads and temporal positions driving correct classifications. Additionally, we demonstrate the potential of sparse autoencoders for uncovering interpretable latent features. Our findings provide both methodological contributions to transformer interpretability and novel insights into the functional mechanics underlying transformer performance in time series classification tasks.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-11-26T15:46:29+00:00",
      "updated": "2025-11-26T15:46:29+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.21514v1",
      "file": "papers/2511.21514v1.pdf"
    },
    {
      "arxiv_id": "2511.21109v1",
      "title": "Interpretable Fair Clustering",
      "authors": [
        {
          "name": "Mudi Jiang"
        },
        {
          "name": "Jiahui Zhou"
        },
        {
          "name": "Xinying Liu"
        },
        {
          "name": "Zengyou He"
        },
        {
          "name": "Zhikui Chen"
        }
      ],
      "abstract": "Fair clustering has gained increasing attention in recent years, especially in applications involving socially sensitive attributes. However, existing fair clustering methods often lack interpretability, limiting their applicability in high-stakes scenarios where understanding the rationale behind clustering decisions is essential. In this work, we address this limitation by proposing an interpretable and fair clustering framework, which integrates fairness constraints into the structure of decision trees. Our approach constructs interpretable decision trees that partition the data while ensuring fair treatment across protected groups. To further enhance the practicality of our framework, we also introduce a variant that requires no fairness hyperparameter tuning, achieved through post-pruning a tree constructed without fairness constraints. Extensive experiments on both real-world and synthetic datasets demonstrate that our method not only delivers competitive clustering performance and improved fairness, but also offers additional advantages such as interpretability and the ability to handle multiple sensitive attributes. These strengths enable our method to perform robustly under complex fairness constraints, opening new possibilities for equitable and transparent clustering.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-11-26T06:52:25+00:00",
      "updated": "2025-11-26T06:52:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.21109v1",
      "file": "papers/2511.21109v1.pdf"
    },
    {
      "arxiv_id": "2511.21033v1",
      "title": "Towards Trustworthy Legal AI through LLM Agents and Formal Reasoning",
      "authors": [
        {
          "name": "Linze Chen"
        },
        {
          "name": "Yufan Cai"
        },
        {
          "name": "Zhe Hou"
        },
        {
          "name": "Jinsong Dong"
        }
      ],
      "abstract": "The rationality of law manifests in two forms: substantive rationality, which concerns the fairness or moral desirability of outcomes, and formal rationality, which requires legal decisions to follow explicitly stated, general, and logically coherent rules. Existing LLM-based systems excel at surface-level text analysis but lack the guarantees required for principled jurisprudence. We introduce L4M, a novel framework that combines adversarial LLM agents with SMT-solver-backed proofs to unite the interpretive flexibility of natural language with the rigor of symbolic verification. The pipeline consists of three phases: (1) Statute Formalization, where domain-specific prompts convert legal provisions into logical formulae; (2) Dual Fact and Statute Extraction, in which prosecutor- and defense-aligned LLMs independently map case narratives to fact tuples and statutes, ensuring role isolation; and (3) Solver-Centric Adjudication, where an autoformalizer compiles both parties' arguments into logic constraints, and unsat cores trigger iterative self-critique until a satisfiable formula is achieved, which is then verbalized by a Judge-LLM into a transparent verdict and optimized sentence. Experimental results on public benchmarks show that our system surpasses advanced LLMs including GPT-o4-mini, DeepSeek-V3, and Claude 4 as well as state-of-the-art Legal AI baselines, while providing rigorous and explainable symbolic justifications.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-11-26T04:05:06+00:00",
      "updated": "2025-11-26T04:05:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.21033v1",
      "file": "papers/2511.21033v1.pdf"
    },
    {
      "arxiv_id": "2511.20931v1",
      "title": "Open Vocabulary Compositional Explanations for Neuron Alignment",
      "authors": [
        {
          "name": "Biagio La Rosa"
        },
        {
          "name": "Leilani H. Gilpin"
        }
      ],
      "abstract": "Neurons are the fundamental building blocks of deep neural networks, and their interconnections allow AI to achieve unprecedented results. Motivated by the goal of understanding how neurons encode information, compositional explanations leverage logical relationships between concepts to express the spatial alignment between neuron activations and human knowledge. However, these explanations rely on human-annotated datasets, restricting their applicability to specific domains and predefined concepts. This paper addresses this limitation by introducing a framework for the vision domain that allows users to probe neurons for arbitrary concepts and datasets. Specifically, the framework leverages masks generated by open vocabulary semantic segmentation to compute open vocabulary compositional explanations. The proposed framework consists of three steps: specifying arbitrary concepts, generating semantic segmentation masks using open vocabulary models, and deriving compositional explanations from these masks. The paper compares the proposed framework with previous methods for computing compositional explanations both in terms of quantitative metrics and human interpretability, analyzes the differences in explanations when shifting from human-annotated data to model-annotated data, and showcases the additional capabilities provided by the framework in terms of flexibility of the explanations with respect to the tasks and properties of interest.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-11-25T23:45:37+00:00",
      "updated": "2025-11-25T23:45:37+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.20931v1",
      "file": "papers/2511.20931v1.pdf"
    },
    {
      "arxiv_id": "2511.20798v2",
      "title": "Physics Steering: Causal Control of Cross-Domain Concepts in a Physics Foundation Model",
      "authors": [
        {
          "name": "Rio Alexa Fear"
        },
        {
          "name": "Payel Mukhopadhyay"
        },
        {
          "name": "Michael McCabe"
        },
        {
          "name": "Alberto Bietti"
        },
        {
          "name": "Miles Cranmer"
        }
      ],
      "abstract": "Recent advances in mechanistic interpretability have revealed that large language models (LLMs) develop internal representations corresponding not only to concrete entities but also distinct, human-understandable abstract concepts and behaviour. Moreover, these hidden features can be directly manipulated to steer model behaviour. However, it remains an open question whether this phenomenon is unique to models trained on inherently structured data (ie. language, images) or if it is a general property of foundation models. In this work, we investigate the internal representations of a large physics-focused foundation model. Inspired by recent work identifying single directions in activation space for complex behaviours in LLMs, we extract activation vectors from the model during forward passes over simulation datasets for different physical regimes. We then compute \"delta\" representations between the two regimes. These delta tensors act as concept directions in activation space, encoding specific physical features. By injecting these concept directions back into the model during inference, we can steer its predictions, demonstrating causal control over physical behaviours, such as inducing or removing some particular physical feature from a simulation. These results suggest that scientific foundation models learn generalised representations of physical principles. They do not merely rely on superficial correlations and patterns in the simulations. Our findings open new avenues for understanding and controlling scientific foundation models and has implications for AI-enabled scientific discovery.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.comp-ph"
      ],
      "published": "2025-11-25T19:40:22+00:00",
      "updated": "2025-11-28T04:04:02+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.20798v2",
      "file": "papers/2511.20798v2.pdf"
    },
    {
      "arxiv_id": "2511.20779v1",
      "title": "CHiQPM: Calibrated Hierarchical Interpretable Image Classification",
      "authors": [
        {
          "name": "Thomas Norrenbrock"
        },
        {
          "name": "Timo Kaiser"
        },
        {
          "name": "Sovan Biswas"
        },
        {
          "name": "Neslihan Kose"
        },
        {
          "name": "Ramesh Manuvinakurike"
        },
        {
          "name": "Bodo Rosenhahn"
        }
      ],
      "abstract": "Globally interpretable models are a promising approach for trustworthy AI in safety-critical domains. Alongside global explanations, detailed local explanations are a crucial complement to effectively support human experts during inference. This work proposes the Calibrated Hierarchical QPM (CHiQPM) which offers uniquely comprehensive global and local interpretability, paving the way for human-AI complementarity. CHiQPM achieves superior global interpretability by contrastively explaining the majority of classes and offers novel hierarchical explanations that are more similar to how humans reason and can be traversed to offer a built-in interpretable Conformal prediction (CP) method. Our comprehensive evaluation shows that CHiQPM achieves state-of-the-art accuracy as a point predictor, maintaining 99% accuracy of non-interpretable models. This demonstrates a substantial improvement, where interpretability is incorporated without sacrificing overall accuracy. Furthermore, its calibrated set prediction is competitively efficient to other CP methods, while providing interpretable predictions of coherent sets along its hierarchical explanation.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV",
        "cs.HC"
      ],
      "published": "2025-11-25T19:16:31+00:00",
      "updated": "2025-11-25T19:16:31+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.20779v1",
      "file": "papers/2511.20779v1.pdf"
    },
    {
      "arxiv_id": "2511.20591v2",
      "title": "Attention Trajectories as a Diagnostic Axis for Deep Reinforcement Learning",
      "authors": [
        {
          "name": "Charlotte Beylier"
        },
        {
          "name": "Hannah Selder"
        },
        {
          "name": "Arthur Fleig"
        },
        {
          "name": "Simon M. Hofmann"
        },
        {
          "name": "Nico Scherf"
        }
      ],
      "abstract": "While deep reinforcement learning agents demonstrate high performance across domains, their internal decision processes remain difficult to interpret when evaluated only through performance metrics. In particular, it is poorly understood which input features agents rely on, how these dependencies evolve during training, and how they relate to behavior. We introduce a scientific methodology for analyzing the learning process through quantitative analysis of saliency. This approach aggregates saliency information at the object and modality level into hierarchical attention profiles, quantifying how agents allocate attention over time, thereby forming attention trajectories throughout training. Applied to Atari benchmarks, custom Pong environments, and muscle-actuated biomechanical user simulations in visuomotor interactive tasks, this methodology uncovers algorithm-specific attention biases, reveals unintended reward-driven strategies, and diagnoses overfitting to redundant sensory channels. These patterns correspond to measurable behavioral differences, demonstrating empirical links between attention profiles, learning dynamics, and agent behavior. To assess robustness of the attention profiles, we validate our findings across multiple saliency methods and environments. The results establish attention trajectories as a promising diagnostic axis for tracing how feature reliance develops during training and for identifying biases and vulnerabilities invisible to performance metrics alone.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-11-25T18:20:42+00:00",
      "updated": "2025-11-27T08:35:55+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.20591v2",
      "file": "papers/2511.20591v2.pdf"
    },
    {
      "arxiv_id": "2511.20586v3",
      "title": "PaTAS: A Framework for Trust Propagation in Neural Networks Using Subjective Logic",
      "authors": [
        {
          "name": "Koffi Ismael Ouattara"
        },
        {
          "name": "Ioannis Krontiris"
        },
        {
          "name": "Theo Dimitrakos"
        },
        {
          "name": "Dennis Eisermann"
        },
        {
          "name": "Houda Labiod"
        },
        {
          "name": "Frank Kargl"
        }
      ],
      "abstract": "Trustworthiness has become a key requirement for the deployment of artificial intelligence systems in safety-critical applications. Conventional evaluation metrics, such as accuracy and precision, fail to appropriately capture uncertainty or the reliability of model predictions, particularly under adversarial or degraded conditions. This paper introduces the Parallel Trust Assessment System (PaTAS), a framework for modeling and propagating trust in neural networks using Subjective Logic (SL). PaTAS operates in parallel with standard neural computation through Trust Nodes and Trust Functions that propagate input, parameter, and activation trust across the network. The framework defines a Parameter Trust Update mechanism to refine parameter reliability during training and an Inference-Path Trust Assessment (IPTA) method to compute instance-specific trust at inference. Experiments on real-world and adversarial datasets demonstrate that PaTAS produces interpretable, symmetric, and convergent trust estimates that complement accuracy and expose reliability gaps in poisoned, biased, or uncertain data scenarios. The results show that PaTAS effectively distinguishes between benign and adversarial inputs and identifies cases where model confidence diverges from actual reliability. By enabling transparent and quantifiable trust reasoning within neural architectures, PaTAS provides a foundation for evaluating model reliability across the AI lifecycle.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-11-25T18:15:36+00:00",
      "updated": "2025-12-11T07:35:44+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.20586v3",
      "file": "papers/2511.20586v3.pdf"
    },
    {
      "arxiv_id": "2511.21767v1",
      "title": "LAYER: A Quantitative Explainable AI Framework for Decoding Tissue-Layer Drivers of Myofascial Low Back Pain",
      "authors": [
        {
          "name": "Zixue Zeng"
        },
        {
          "name": "Anthony M. Perti"
        },
        {
          "name": "Tong Yu"
        },
        {
          "name": "Grant Kokenberger"
        },
        {
          "name": "Hao-En Lu"
        },
        {
          "name": "Jing Wang"
        },
        {
          "name": "Xin Meng"
        },
        {
          "name": "Zhiyu Sheng"
        },
        {
          "name": "Maryam Satarpour"
        },
        {
          "name": "John M. Cormack"
        },
        {
          "name": "Allison C. Bean"
        },
        {
          "name": "Ryan P. Nussbaum"
        },
        {
          "name": "Emily Landis-Walkenhorst"
        },
        {
          "name": "Kang Kim"
        },
        {
          "name": "Ajay D. Wasan"
        },
        {
          "name": "Jiantao Pu"
        }
      ],
      "abstract": "Myofascial pain (MP) is a leading cause of chronic low back pain, yet its tissue-level drivers remain poorly defined and lack reliable image biomarkers. Existing studies focus predominantly on muscle while neglecting fascia, fat, and other soft tissues that play integral biomechanical roles. We developed an anatomically grounded explainable artificial intelligence (AI) framework, LAYER (Layer-wise Analysis for Yielding Explainable Relevance Tissue), that analyses six tissue layers in three-dimensional (3D) ultrasound and quantifies their contribution to MP prediction. By utilizing the largest multi-model 3D ultrasound cohort consisting of over 4,000 scans, LAYER reveals that non-muscle tissues contribute substantially to pain prediction. In B-mode imaging, the deep fascial membrane (DFM) showed the highest saliency (0.420), while in combined B-mode and shear-wave images, the collective saliency of non-muscle layers (0.316) nearly matches that of muscle (0.317), challenging the conventional muscle-centric paradigm in MP research and potentially affecting the therapy methods. LAYER establishes a quantitative, interpretable framework for linking layer-specific anatomy to pain physiology, uncovering new tissue targets and providing a generalizable approach for explainable analysis of soft-tissue imaging.",
      "primary_category": "eess.IV",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "q-bio.TO"
      ],
      "published": "2025-11-25T15:47:43+00:00",
      "updated": "2025-11-25T15:47:43+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.21767v1",
      "file": "papers/2511.21767v1.pdf"
    },
    {
      "arxiv_id": "2511.20273v1",
      "title": "Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits",
      "authors": [
        {
          "name": "Areeb Ahmad"
        },
        {
          "name": "Abhinav Joshi"
        },
        {
          "name": "Ashutosh Modi"
        }
      ],
      "abstract": "Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-11-25T12:59:15+00:00",
      "updated": "2025-11-25T12:59:15+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.20273v1",
      "file": "papers/2511.20273v1.pdf"
    },
    {
      "arxiv_id": "2511.20257v1",
      "title": "Interpretable Air Pollution Forecasting by Physics-Guided Spatiotemporal Decoupling",
      "authors": [
        {
          "name": "Zhiguo Zhang"
        },
        {
          "name": "Xiaoliang Ma"
        },
        {
          "name": "Daniel Schlesinger"
        }
      ],
      "abstract": "Accurate and interpretable air pollution forecasting is crucial for public health, but most models face a trade-off between performance and interpretability. This study proposes a physics-guided, interpretable-by-design spatiotemporal learning framework. The model decomposes the spatiotemporal behavior of air pollutant concentrations into two transparent, additive modules. The first is a physics-guided transport kernel with directed weights conditioned on wind and geography (advection). The second is an explainable attention mechanism that learns local responses and attributes future concentrations to specific historical lags and exogenous drivers. Evaluated on a comprehensive dataset from the Stockholm region, our model consistently outperforms state-of-the-art baselines across multiple forecasting horizons. Our model's integration of high predictive performance and spatiotemporal interpretability provides a more reliable foundation for operational air-quality management in real-world applications.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-11-25T12:36:27+00:00",
      "updated": "2025-11-25T12:36:27+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.20257v1",
      "file": "papers/2511.20257v1.pdf"
    },
    {
      "arxiv_id": "2511.20236v2",
      "title": "Actionable and diverse counterfactual explanations incorporating domain knowledge and causal constraints",
      "authors": [
        {
          "name": "Szymon Bobek"
        },
        {
          "name": "Łukasz Bałec"
        },
        {
          "name": "Grzegorz J. Nalepa"
        }
      ],
      "abstract": "Counterfactual explanations enhance the actionable interpretability of machine learning models by identifying the minimal changes required to achieve a desired outcome of the model. However, existing methods often ignore the complex dependencies in real-world datasets, leading to unrealistic or impractical modifications. Motivated by cybersecurity applications in the email marketing domain, we propose a method for generating Diverse, Actionable, and kNowledge-Constrained Explanations (DANCE), which incorporates feature dependencies and causal constraints to ensure plausibility and real-world feasibility of counterfactuals. Our method learns linear and nonlinear constraints from data or integrates expert-provided dependency graphs, ensuring counterfactuals are plausible and actionable. By maintaining consistency with feature relationships, the method produces explanations that align with real-world constraints. Additionally, it balances plausibility, diversity, and sparsity, effectively addressing key limitations in existing algorithms. The work is developed based on a real-life case study with Freshmail, the largest email marketing company in Poland and supported by a joint R&D project Sendguard. Furthermore, we provide an extensive evaluation using 140 public datasets, which highlights its ability to generate meaningful, domain-relevant counterfactuals that outperform other existing approaches based on widely used metrics. The source code for reproduction of the results can be found in a GitHub repository we provide.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-11-25T12:09:36+00:00",
      "updated": "2025-11-28T07:20:29+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.20236v2",
      "file": "papers/2511.20236v2.pdf"
    },
    {
      "arxiv_id": "2511.20088v1",
      "title": "Explainable Visual Anomaly Detection via Concept Bottleneck Models",
      "authors": [
        {
          "name": "Arianna Stropeni"
        },
        {
          "name": "Valentina Zaccaria"
        },
        {
          "name": "Francesco Borsatti"
        },
        {
          "name": "Davide Dalle Pezze"
        },
        {
          "name": "Manuel Barusco"
        },
        {
          "name": "Gian Antonio Susto"
        }
      ],
      "abstract": "In recent years, Visual Anomaly Detection (VAD) has gained significant attention due to its ability to identify anomalous images using only normal images during training. Many VAD models work without supervision but are still able to provide visual explanations by highlighting the anomalous regions within an image. However, although these visual explanations can be helpful, they lack a direct and semantically meaningful interpretation for users. To address this limitation, we propose extending Concept Bottleneck Models (CBMs) to the VAD setting. By learning meaningful concepts, the network can provide human-interpretable descriptions of anomalies, offering a novel and more insightful way to explain them. Our contributions are threefold: (i) we develop a Concept Dataset to support research on CBMs for VAD; (ii) we improve the CBM architecture to generate both concept-based and visual explanations, bridging semantic and localization interpretability; and (iii) we introduce a pipeline for synthesizing artificial anomalies, preserving the VAD paradigm of minimizing dependence on rare anomalous samples. Our approach, Concept-Aware Visual Anomaly Detection (CONVAD), achieves performance comparable to classic VAD methods while providing richer, concept-driven explanations that enhance interpretability and trust in VAD systems.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-11-25T09:03:30+00:00",
      "updated": "2025-11-25T09:03:30+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.20088v1",
      "file": "papers/2511.20088v1.pdf"
    },
    {
      "arxiv_id": "2511.19845v1",
      "title": "SX-GeoTree: Self-eXplaining Geospatial Regression Tree Incorporating the Spatial Similarity of Feature Attributions",
      "authors": [
        {
          "name": "Chaogui Kang"
        },
        {
          "name": "Lijian Luo"
        },
        {
          "name": "Qingfeng Guan"
        },
        {
          "name": "Yu Liu"
        }
      ],
      "abstract": "Decision trees remain central for tabular prediction but struggle with (i) capturing spatial dependence and (ii) producing locally stable (robust) explanations. We present SX-GeoTree, a self-explaining geospatial regression tree that integrates three coupled objectives during recursive splitting: impurity reduction (MSE), spatial residual control (global Moran's I), and explanation robustness via modularity maximization on a consensus similarity network formed from (a) geographically weighted regression (GWR) coefficient distances (stimulus-response similarity) and (b) SHAP attribution distances (explanatory similarity). We recast local Lipschitz continuity of feature attributions as a network community preservation problem, enabling scalable enforcement of spatially coherent explanations without per-sample neighborhood searches. Experiments on two exemplar tasks (county-level GDP in Fujian, n=83; point-wise housing prices in Seattle, n=21,613) show SX-GeoTree maintains competitive predictive accuracy (within 0.01 $R^{2}$ of decision trees) while improving residual spatial evenness and doubling attribution consensus (modularity: Fujian 0.19 vs 0.09; Seattle 0.10 vs 0.05). Ablation confirms Moran's I and modularity terms are complementary; removing either degrades both spatial residual structure and explanation stability. The framework demonstrates how spatial similarity - extended beyond geometric proximity through GWR-derived local relationships - can be embedded in interpretable models, advancing trustworthy geospatial machine learning and offering a transferable template for domain-aware explainability.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CY",
        "stat.ML"
      ],
      "published": "2025-11-25T02:23:04+00:00",
      "updated": "2025-11-25T02:23:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.19845v1",
      "file": "papers/2511.19845v1.pdf"
    },
    {
      "arxiv_id": "2511.19726v1",
      "title": "An Adaptive, Data-Integrated Agent-Based Modeling Framework for Explainable and Contestable Policy Design",
      "authors": [
        {
          "name": "Roberto Garrone"
        }
      ],
      "abstract": "Multi-agent systems often operate under feedback, adaptation, and non-stationarity, yet many simulation studies retain static decision rules and fixed control parameters. This paper introduces a general adaptive multi-agent learning framework that integrates: (i) four dynamic regimes distinguishing static versus adaptive agents and fixed versus adaptive system parameters; (ii) information-theoretic diagnostics (entropy rate, statistical complexity, and predictive information) to assess predictability and structure; (iii) structural causal models for explicit intervention semantics; (iv) procedures for generating agent-level priors from aggregate or sample data; and (v) unsupervised methods for identifying emergent behavioral regimes. The framework offers a domain-neutral architecture for analyzing how learning agents and adaptive controls jointly shape system trajectories, enabling systematic comparison of stability, performance, and interpretability across non-equilibrium, oscillatory, or drifting dynamics. Mathematical definitions, computational operators, and an experimental design template are provided, yielding a structured methodology for developing explainable and contestable multi-agent decision processes.",
      "primary_category": "cs.MA",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "published": "2025-11-24T21:41:45+00:00",
      "updated": "2025-11-24T21:41:45+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.19726v1",
      "file": "papers/2511.19726v1.pdf"
    },
    {
      "arxiv_id": "2511.19654v1",
      "title": "Accuracy and Efficiency Trade-Offs in LLM-Based Malware Detection and Explanation: A Comparative Study of Parameter Tuning vs. Full Fine-Tuning",
      "authors": [
        {
          "name": "Stephen C. Gravereaux"
        },
        {
          "name": "Sheikh Rabiul Islam"
        }
      ],
      "abstract": "This study examines whether Low-Rank Adaptation (LoRA) fine-tuned Large Language Models (LLMs) can approximate the performance of fully fine-tuned models in generating human-interpretable decisions and explanations for malware classification. Achieving trustworthy malware detection, particularly when LLMs are involved, remains a significant challenge. We developed an evaluation framework using Bilingual Evaluation Understudy (BLEU), Recall-Oriented Understudy for Gisting Evaluation (ROUGE), and Semantic Similarity Metrics to benchmark explanation quality across five LoRA configurations and a fully fine-tuned baseline. Results indicate that full fine-tuning achieves the highest overall scores, with BLEU and ROUGE improvements of up to 10% over LoRA variants. However, mid-range LoRA models deliver competitive performance exceeding full fine-tuning on two metrics while reducing model size by approximately 81% and training time by over 80% on a LoRA model with 15.5% trainable parameters. These findings demonstrate that LoRA offers a practical balance of interpretability and resource efficiency, enabling deployment in resource-constrained environments without sacrificing explanation quality. By providing feature-driven natural language explanations for malware classifications, this approach enhances transparency, analyst confidence, and operational scalability in malware detection systems.",
      "primary_category": "cs.CR",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "published": "2025-11-24T19:37:13+00:00",
      "updated": "2025-11-24T19:37:13+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.19654v1",
      "file": "papers/2511.19654v1.pdf"
    },
    {
      "arxiv_id": "2511.19265v1",
      "title": "Unboxing the Black Box: Mechanistic Interpretability for Algorithmic Understanding of Neural Networks",
      "authors": [
        {
          "name": "Bianka Kowalska"
        },
        {
          "name": "Halina Kwaśnicka"
        }
      ],
      "abstract": "The black box nature of deep neural networks poses a significant challenge for the deployment of transparent and trustworthy artificial intelligence (AI) systems. With the growing presence of AI in society, it becomes increasingly important to develop methods that can explain and interpret the decisions made by these systems. To address this, mechanistic interpretability (MI) emerged as a promising and distinctive research program within the broader field of explainable artificial intelligence (XAI). MI is the process of studying the inner computations of neural networks and translating them into human-understandable algorithms. It encompasses reverse engineering techniques aimed at uncovering the computational algorithms implemented by neural networks. In this article, we propose a unified taxonomy of MI approaches and provide a detailed analysis of key techniques, illustrated with concrete examples and pseudo-code. We contextualize MI within the broader interpretability landscape, comparing its goals, methods, and insights to other strands of XAI. Additionally, we trace the development of MI as a research area, highlighting its conceptual roots and the accelerating pace of recent work. We argue that MI holds significant potential to support a more scientific understanding of machine learning systems -- treating models not only as tools for solving tasks, but also as systems to be studied and understood. We hope to invite new researchers into the field of mechanistic interpretability.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-11-24T16:16:49+00:00",
      "updated": "2025-11-24T16:16:49+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.19265v1",
      "file": "papers/2511.19265v1.pdf"
    },
    {
      "arxiv_id": "2511.19264v1",
      "title": "Interpreting GFlowNets for Drug Discovery: Extracting Actionable Insights for Medicinal Chemistry",
      "authors": [
        {
          "name": "Amirtha Varshini A S"
        },
        {
          "name": "Duminda S. Ranasinghe"
        },
        {
          "name": "Hok Hei Tam"
        }
      ],
      "abstract": "Generative Flow Networks, or GFlowNets, offer a promising framework for molecular design, but their internal decision policies remain opaque. This limits adoption in drug discovery, where chemists require clear and interpretable rationales for proposed structures. We present an interpretability framework for SynFlowNet, a GFlowNet trained on documented chemical reactions and purchasable starting materials that generates both molecules and the synthetic routes that produce them. Our approach integrates three complementary components. Gradient based saliency combined with counterfactual perturbations identifies which atomic environments influence reward and how structural edits change molecular outcomes. Sparse autoencoders reveal axis aligned latent factors that correspond to physicochemical properties such as polarity, lipophilicity, and molecular size. Motif probes show that functional groups including aromatic rings and halogens are explicitly encoded and linearly decodable from the internal embeddings. Together, these results expose the chemical logic inside SynFlowNet and provide actionable and mechanistic insight that supports transparent and controllable molecular design.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "published": "2025-11-24T16:16:18+00:00",
      "updated": "2025-11-24T16:16:18+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.19264v1",
      "file": "papers/2511.19264v1.pdf"
    },
    {
      "arxiv_id": "2511.19220v2",
      "title": "Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering",
      "authors": [
        {
          "name": "Federico Felizzi"
        },
        {
          "name": "Olivia Riccomi"
        },
        {
          "name": "Michele Ferramola"
        },
        {
          "name": "Francesco Andrea Causio"
        },
        {
          "name": "Manuel Del Medico"
        },
        {
          "name": "Vittorio De Vita"
        },
        {
          "name": "Lorenzo De Mori"
        },
        {
          "name": "Alessandra Piscitelli"
        },
        {
          "name": "Pietro Eric Risuleo"
        },
        {
          "name": "Bianca Destro Castaniti"
        },
        {
          "name": "Antonio Cristiano"
        },
        {
          "name": "Alessia Longo"
        },
        {
          "name": "Luigi De Angelis"
        },
        {
          "name": "Mariapia Vassalli"
        },
        {
          "name": "Marcello Di Pumpo"
        }
      ],
      "abstract": "Large vision language models (VLMs) have achieved impressive performance on medical visual question answering benchmarks, yet their reliance on visual information remains unclear. We investigate whether frontier VLMs demonstrate genuine visual grounding when answering Italian medical questions by testing four state-of-the-art models: Claude Sonnet 4.5, GPT-4o, GPT-5-mini, and Gemini 2.0 flash exp. Using 60 questions from the EuropeMedQA Italian dataset that explicitly require image interpretation, we substitute correct medical images with blank placeholders to test whether models truly integrate visual and textual information. Our results reveal striking variability in visual dependency: GPT-4o shows the strongest visual grounding with a 27.9pp accuracy drop (83.2% [74.6%, 91.7%] to 55.3% [44.1%, 66.6%]), while GPT-5-mini, Gemini, and Claude maintain high accuracy with modest drops of 8.5pp, 2.4pp, and 5.6pp respectively. Analysis of model-generated reasoning reveals confident explanations for fabricated visual interpretations across all models, suggesting varying degrees of reliance on textual shortcuts versus genuine visual analysis. These findings highlight critical differences in model robustness and the need for rigorous evaluation before clinical deployment.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-11-24T15:26:58+00:00",
      "updated": "2025-11-26T20:23:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.19220v2",
      "file": "papers/2511.19220v2.pdf"
    },
    {
      "arxiv_id": "2511.19557v1",
      "title": "Think First, Assign Next (ThiFAN-VQA): A Two-stage Chain-of-Thought Framework for Post-Disaster Damage Assessment",
      "authors": [
        {
          "name": "Ehsan Karimi"
        },
        {
          "name": "Nhut Le"
        },
        {
          "name": "Maryam Rahnemoonfar"
        }
      ],
      "abstract": "Timely and accurate assessment of damages following natural disasters is essential for effective emergency response and recovery. Recent AI-based frameworks have been developed to analyze large volumes of aerial imagery collected by Unmanned Aerial Vehicles, providing actionable insights rapidly. However, creating and annotating data for training these models is costly and time-consuming, resulting in datasets that are limited in size and diversity. Furthermore, most existing approaches rely on traditional classification-based frameworks with fixed answer spaces, restricting their ability to provide new information without additional data collection or model retraining. Using pre-trained generative models built on in-context learning (ICL) allows for flexible and open-ended answer spaces. However, these models often generate hallucinated outputs or produce generic responses that lack domain-specific relevance. To address these limitations, we propose ThiFAN-VQA, a two-stage reasoning-based framework for visual question answering (VQA) in disaster scenarios. ThiFAN-VQA first generates structured reasoning traces using chain-of-thought (CoT) prompting and ICL to enable interpretable reasoning under limited supervision. A subsequent answer selection module evaluates the generated responses and assigns the most coherent and contextually accurate answer, effectively improve the model performance. By integrating a custom information retrieval system, domain-specific prompting, and reasoning-guided answer selection, ThiFAN-VQA bridges the gap between zero-shot and supervised methods, combining flexibility with consistency. Experiments on FloodNet and RescueNet-VQA, UAV-based datasets from flood- and hurricane-affected regions, demonstrate that ThiFAN-VQA achieves superior accuracy, interpretability, and adaptability for real-world post-disaster damage assessment tasks.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-11-24T14:32:07+00:00",
      "updated": "2025-11-24T14:32:07+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.19557v1",
      "file": "papers/2511.19557v1.pdf"
    },
    {
      "arxiv_id": "2511.19155v1",
      "title": "EEG-VLM: A Hierarchical Vision-Language Model with Multi-Level Feature Alignment and Visually Enhanced Language-Guided Reasoning for EEG Image-Based Sleep Stage Prediction",
      "authors": [
        {
          "name": "Xihe Qiu"
        },
        {
          "name": "Gengchen Ma"
        },
        {
          "name": "Haoyu Wang"
        },
        {
          "name": "Chen Zhan"
        },
        {
          "name": "Xiaoyu Tan"
        },
        {
          "name": "Shuo Li"
        }
      ],
      "abstract": "Sleep stage classification based on electroencephalography (EEG) is fundamental for assessing sleep quality and diagnosing sleep-related disorders. However, most traditional machine learning methods rely heavily on prior knowledge and handcrafted features, while existing deep learning models still struggle to jointly capture fine-grained time-frequency patterns and achieve clinical interpretability. Recently, vision-language models (VLMs) have made significant progress in the medical domain, yet their performance remains constrained when applied to physiological waveform data, especially EEG signals, due to their limited visual understanding and insufficient reasoning capability. To address these challenges, we propose EEG-VLM, a hierarchical vision-language framework that integrates multi-level feature alignment with visually enhanced language-guided reasoning for interpretable EEG-based sleep stage classification. Specifically, a specialized visual enhancement module constructs high-level visual tokens from intermediate-layer features to extract rich semantic representations of EEG images. These tokens are further aligned with low-level CLIP features through a multi-level alignment mechanism, enhancing the VLM's image-processing capability. In addition, a Chain-of-Thought (CoT) reasoning strategy decomposes complex medical inference into interpretable logical steps, effectively simulating expert-like decision-making. Experimental results demonstrate that the proposed method significantly improves both the accuracy and interpretability of VLMs in EEG-based sleep stage classification, showing promising potential for automated and explainable EEG analysis in clinical settings.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-11-24T14:23:42+00:00",
      "updated": "2025-11-24T14:23:42+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.19155v1",
      "file": "papers/2511.19155v1.pdf"
    },
    {
      "arxiv_id": "2511.19150v1",
      "title": "Feature Ranking in Credit-Risk with Qudit-Based Networks",
      "authors": [
        {
          "name": "Georgios Maragkopoulos"
        },
        {
          "name": "Lazaros Chavatzoglou"
        },
        {
          "name": "Aikaterini Mandilara"
        },
        {
          "name": "Dimitris Syvridis"
        }
      ],
      "abstract": "In finance, predictive models must balance accuracy and interpretability, particularly in credit risk assessment, where model decisions carry material consequences. We present a quantum neural network (QNN) based on a single qudit, in which both data features and trainable parameters are co-encoded within a unified unitary evolution generated by the full Lie algebra. This design explores the entire Hilbert space while enabling interpretability through the magnitudes of the learned coefficients. We benchmark our model on a real-world, imbalanced credit-risk dataset from Taiwan. The proposed QNN consistently outperforms LR and reaches the results of random forest models in macro-F1 score while preserving a transparent correspondence between learned parameters and input feature importance. To quantify the interpretability of the proposed model, we introduce two complementary metrics: (i) the edit distance between the model's feature ranking and that of LR, and (ii) a feature-poisoning test where selected features are replaced with noise. Results indicate that the proposed quantum model achieves competitive performance while offering a tractable path toward interpretable quantum learning.",
      "primary_category": "quant-ph",
      "categories": [
        "quant-ph",
        "cs.LG"
      ],
      "published": "2025-11-24T14:15:57+00:00",
      "updated": "2025-11-24T14:15:57+00:00",
      "pdf_url": "https://arxiv.org/pdf/2511.19150v1",
      "file": "papers/2511.19150v1.pdf"
    }
  ]
}