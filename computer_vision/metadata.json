{
  "corpus": "computer_vision",
  "source": "arxiv",
  "search_query": "cat:cs.CV AND (vision OR detection OR segmentation OR recognition OR image)",
  "curated_at": "2025-12-26T19:46:02.796256+00:00",
  "total_papers": 200,
  "papers_evaluated": 237,
  "acceptance_rate": 0.8438818565400844,
  "papers": [
    {
      "arxiv_id": "2512.21338v1",
      "title": "HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming",
      "authors": [
        {
          "name": "Haonan Qiu"
        },
        {
          "name": "Shikun Liu"
        },
        {
          "name": "Zijian Zhou"
        },
        {
          "name": "Zhaochong An"
        },
        {
          "name": "Weiming Ren"
        },
        {
          "name": "Zhiheng Liu"
        },
        {
          "name": "Jonas Schult"
        },
        {
          "name": "Sen He"
        },
        {
          "name": "Shoufa Chen"
        },
        {
          "name": "Yuren Cong"
        },
        {
          "name": "Tao Xiang"
        },
        {
          "name": "Ziwei Liu"
        },
        {
          "name": "Juan-Manuel Perez-Rua"
        }
      ],
      "abstract": "High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T18:59:58+00:00",
      "updated": "2025-12-24T18:59:58+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21338v1",
      "file": "papers/2512.21338v1.pdf"
    },
    {
      "arxiv_id": "2512.21337v1",
      "title": "Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models",
      "authors": [
        {
          "name": "Li-Zhong Szu-Tu"
        },
        {
          "name": "Ting-Lin Wu"
        },
        {
          "name": "Chia-Jui Chang"
        },
        {
          "name": "He Syu"
        },
        {
          "name": "Yu-Lun Liu"
        }
      ],
      "abstract": "We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T18:59:54+00:00",
      "updated": "2025-12-24T18:59:54+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21337v1",
      "file": "papers/2512.21337v1.pdf"
    },
    {
      "arxiv_id": "2512.21334v1",
      "title": "Streaming Video Instruction Tuning",
      "authors": [
        {
          "name": "Jiaer Xia"
        },
        {
          "name": "Peixian Chen"
        },
        {
          "name": "Mengdan Zhang"
        },
        {
          "name": "Xing Sun"
        },
        {
          "name": "Kaiyang Zhou"
        }
      ],
      "abstract": "We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T18:59:36+00:00",
      "updated": "2025-12-24T18:59:36+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21334v1",
      "file": "papers/2512.21334v1.pdf"
    },
    {
      "arxiv_id": "2512.21333v1",
      "title": "Fast SAM2 with Text-Driven Token Pruning",
      "authors": [
        {
          "name": "Avilasha Mandal"
        },
        {
          "name": "Chaoning Zhang"
        },
        {
          "name": "Fachrina Dewi Puspitasari"
        },
        {
          "name": "Xudong Wang"
        },
        {
          "name": "Jiaquan Zhang"
        },
        {
          "name": "Caiyan Qin"
        },
        {
          "name": "Guoqing Wang"
        },
        {
          "name": "Yang Yang"
        },
        {
          "name": "Heng Tao Shen"
        }
      ],
      "abstract": "Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T18:59:05+00:00",
      "updated": "2025-12-24T18:59:05+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21333v1",
      "file": "papers/2512.21333v1.pdf"
    },
    {
      "arxiv_id": "2512.21331v1",
      "title": "TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning",
      "authors": [
        {
          "name": "Varun Belagali"
        },
        {
          "name": "Saarthak Kapse"
        },
        {
          "name": "Pierre Marza"
        },
        {
          "name": "Srijan Das"
        },
        {
          "name": "Zilinghan Li"
        },
        {
          "name": "Sofiène Boutaj"
        },
        {
          "name": "Pushpak Pati"
        },
        {
          "name": "Srikar Yellapragada"
        },
        {
          "name": "Tarak Nath Nandi"
        },
        {
          "name": "Ravi K Madduri"
        },
        {
          "name": "Joel Saltz"
        },
        {
          "name": "Prateek Prasanna"
        },
        {
          "name": "Stergios Christodoulidis Maria Vakalopoulou"
        },
        {
          "name": "Dimitris Samaras"
        }
      ],
      "abstract": "The interpretation of small tiles in large whole slide images (WSI) often needs a larger image context. We introduce TICON, a transformer-based tile representation contextualizer that produces rich, contextualized embeddings for ''any'' application in computational pathology. Standard tile encoder-based pipelines, which extract embeddings of tiles stripped from their context, fail to model the rich slide-level information essential for both local and global tasks. Furthermore, different tile-encoders excel at different downstream tasks. Therefore, a unified model is needed to contextualize embeddings derived from ''any'' tile-level foundation model. TICON addresses this need with a single, shared encoder, pretrained using a masked modeling objective to simultaneously unify and contextualize representations from diverse tile-level pathology foundation models. Our experiments demonstrate that TICON-contextualized embeddings significantly improve performance across many different tasks, establishing new state-of-the-art results on tile-level benchmarks (i.e., HEST-Bench, THUNDER, CATCH) and slide-level benchmarks (i.e., Patho-Bench). Finally, we pretrain an aggregator on TICON to form a slide-level foundation model, using only 11K WSIs, outperforming SoTA slide-level foundation models pretrained with up to 350K WSIs.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T18:58:16+00:00",
      "updated": "2025-12-24T18:58:16+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21331v1",
      "file": "papers/2512.21331v1.pdf"
    },
    {
      "arxiv_id": "2512.21287v1",
      "title": "Post-Processing Mask-Based Table Segmentation for Structural Coordinate Extraction",
      "authors": [
        {
          "name": "Suren Bandara"
        }
      ],
      "abstract": "Structured data extraction from tables plays a crucial role in document image analysis for scanned documents and digital archives. Although many methods have been proposed to detect table structures and extract cell contents, accurately identifying table segment boundaries (rows and columns) remains challenging, particularly in low-resolution or noisy images. In many real-world scenarios, table data are incomplete or degraded, limiting the adaptability of transformer-based methods to noisy inputs. Mask-based edge detection techniques have shown greater robustness under such conditions, as their sensitivity can be adjusted through threshold tuning; however, existing approaches typically apply masks directly to images, leading to noise sensitivity, resolution loss, or high computational cost. This paper proposes a novel multi-scale signal-processing method for detecting table edges from table masks. Row and column transitions are modeled as one-dimensional signals and processed using Gaussian convolution with progressively increasing variances, followed by statistical thresholding to suppress noise while preserving stable structural edges. Detected signal peaks are mapped back to image coordinates to obtain accurate segment boundaries. Experimental results show that applying the proposed approach to column edge detection improves Cell-Aware Segmentation Accuracy (CASA) a layout-aware metric evaluating both textual correctness and correct cell placement from 67% to 76% on the PubLayNet-1M benchmark when using TableNet with PyTesseract OCR. The method is robust to resolution variations through zero-padding and scaling strategies and produces optimized structured tabular outputs suitable for downstream analysis.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T17:10:37+00:00",
      "updated": "2025-12-24T17:10:37+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21287v1",
      "file": "papers/2512.21287v1.pdf"
    },
    {
      "arxiv_id": "2512.21284v1",
      "title": "Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential",
      "authors": [
        {
          "name": "Shihao Zou"
        },
        {
          "name": "Jingjing Li"
        },
        {
          "name": "Wei Ji"
        },
        {
          "name": "Jincai Huang"
        },
        {
          "name": "Kai Wang"
        },
        {
          "name": "Guo Dan"
        },
        {
          "name": "Weixin Si"
        },
        {
          "name": "Yi Pan"
        }
      ],
      "abstract": "Modern surgical systems increasingly rely on intelligent scene understanding to provide timely situational awareness for enhanced intra-operative safety. Within this pipeline, surgical scene segmentation plays a central role in accurately perceiving operative events. Although recent deep learning models, particularly large-scale foundation models, achieve remarkable segmentation accuracy, their substantial computational demands and power consumption hinder real-time deployment in resource-constrained surgical environments. To address this limitation, we explore the emerging SNN as a promising paradigm for highly efficient surgical intelligence. However, their performance is still constrained by the scarcity of labeled surgical data and the inherently sparse nature of surgical video representations. To this end, we propose \\textit{SpikeSurgSeg}, the first spike-driven video Transformer framework tailored for surgical scene segmentation with real-time potential on non-GPU platforms. To address the limited availability of surgical annotations, we introduce a surgical-scene masked autoencoding pretraining strategy for SNNs that enables robust spatiotemporal representation learning via layer-wise tube masking. Building on this pretrained backbone, we further adopt a lightweight spike-driven segmentation head that produces temporally consistent predictions while preserving the low-latency characteristics of SNNs. Extensive experiments on EndoVis18 and our in-house SurgBleed dataset demonstrate that SpikeSurgSeg achieves mIoU comparable to SOTA ANN-based models while reducing inference latency by at least $8\\times$. Notably, it delivers over $20\\times$ acceleration relative to most foundation-model baselines, underscoring its potential for time-critical surgical scene segmentation.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T17:05:09+00:00",
      "updated": "2025-12-24T17:05:09+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21284v1",
      "file": "papers/2512.21284v1.pdf"
    },
    {
      "arxiv_id": "2512.21276v1",
      "title": "GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation",
      "authors": [
        {
          "name": "Snehal Singh Tomar"
        },
        {
          "name": "Alexandros Graikos"
        },
        {
          "name": "Arjun Krishna"
        },
        {
          "name": "Dimitris Samaras"
        },
        {
          "name": "Klaus Mueller"
        }
      ],
      "abstract": "Modern deep learning methods typically treat image sequences as large tensors of sequentially stacked frames. However, is this straightforward representation ideal given the current state-of-the-art (SoTA)? In this work, we address this question in the context of generative models and aim to devise a more effective way of modeling image sequence data. Observing the inefficiencies and bottlenecks of current SoTA image sequence generation methods, we showcase that rather than working with large tensors, we can improve the generation process by factorizing it into first generating the coarse sequence at low resolution and then refining the individual frames at high resolution. We train a generative model solely on grid images comprising subsampled frames. Yet, we learn to generate image sequences, using the strong self-attention mechanism of the Diffusion Transformer (DiT) to capture correlations between frames. In effect, our formulation extends a 2D image generator to operate as a low-resolution 3D image-sequence generator without introducing any architectural modifications. Subsequently, we super-resolve each frame individually to add the sequence-independent high-resolution details. This approach offers several advantages and can overcome key limitations of the SoTA in this domain. Compared to existing image sequence generation models, our method achieves superior synthesis quality and improved coherence across sequences. It also delivers high-fidelity generation of arbitrary-length sequences and increased efficiency in inference time and training data usage. Furthermore, our straightforward formulation enables our method to generalize effectively across diverse data domains, which typically require additional priors and supervision to model in a generative context. Our method consistently outperforms SoTA in quality and inference speed (at least twice-as-fast) across datasets.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T16:46:04+00:00",
      "updated": "2025-12-24T16:46:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21276v1",
      "file": "papers/2512.21276v1.pdf"
    },
    {
      "arxiv_id": "2512.21268v1",
      "title": "ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision",
      "authors": [
        {
          "name": "Weiqi Li"
        },
        {
          "name": "Zehao Zhang"
        },
        {
          "name": "Liang Lin"
        },
        {
          "name": "Guangrun Wang"
        }
      ],
      "abstract": "Controllability is a fundamental requirement in video synthesis, where accurate alignment with conditioning signals is essential. Existing classifier-free guidance methods typically achieve conditioning indirectly by modeling the joint distribution of data and conditions, which often results in limited controllability over the specified conditions. Classifier-based guidance enforces conditions through an external classifier, but the model may exploit this mechanism to raise the classifier score without genuinely satisfying the intended condition, resulting in adversarial artifacts and limited effective controllability. In this paper, we propose Attention-Conditional Diffusion (ACD), a novel framework for direct conditional control in video diffusion models via attention supervision. By aligning the model's attention maps with external control signals, ACD achieves better controllability. To support this, we introduce a sparse 3D-aware object layout as an efficient conditioning signal, along with a dedicated Layout ControlNet and an automated annotation pipeline for scalable layout integration. Extensive experiments on benchmark video generation datasets demonstrate that ACD delivers superior alignment with conditioning inputs while preserving temporal coherence and visual fidelity, establishing an effective paradigm for conditional video synthesis.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T16:24:18+00:00",
      "updated": "2025-12-24T16:24:18+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21268v1",
      "file": "papers/2512.21268v1.pdf"
    },
    {
      "arxiv_id": "2512.21264v1",
      "title": "AnyAD: Unified Any-Modality Anomaly Detection in Incomplete Multi-Sequence MRI",
      "authors": [
        {
          "name": "Changwei Wu"
        },
        {
          "name": "Yifei Chen"
        },
        {
          "name": "Yuxin Du"
        },
        {
          "name": "Mingxuan Liu"
        },
        {
          "name": "Jinying Zong"
        },
        {
          "name": "Beining Wu"
        },
        {
          "name": "Jie Dong"
        },
        {
          "name": "Feiwei Qin"
        },
        {
          "name": "Yunkang Cao"
        },
        {
          "name": "Qiyuan Tian"
        }
      ],
      "abstract": "Reliable anomaly detection in brain MRI remains challenging due to the scarcity of annotated abnormal cases and the frequent absence of key imaging modalities in real clinical workflows. Existing single-class or multi-class anomaly detection (AD) models typically rely on fixed modality configurations, require repetitive training, or fail to generalize to unseen modality combinations, limiting their clinical scalability. In this work, we present a unified Any-Modality AD framework that performs robust anomaly detection and localization under arbitrary MRI modality availability. The framework integrates a dual-pathway DINOv2 encoder with a feature distribution alignment mechanism that statistically aligns incomplete-modality features with full-modality representations, enabling stable inference even with severe modality dropout. To further enhance semantic consistency, we introduce an Intrinsic Normal Prototypes (INPs) extractor and an INP-guided decoder that reconstruct only normal anatomical patterns while naturally amplifying abnormal deviations. Through randomized modality masking and indirect feature completion during training, the model learns to adapt to all modality configurations without re-training. Extensive experiments on BraTS2018, MU-Glioma-Post, and Pretreat-MetsToBrain-Masks demonstrate that our approach consistently surpasses state-of-the-art industrial and medical AD baselines across 7 modality combinations, achieving superior generalization. This study establishes a scalable paradigm for multimodal medical AD under real-world, imperfect modality conditions. Our source code is available at https://github.com/wuchangw/AnyAD.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T16:16:09+00:00",
      "updated": "2025-12-24T16:16:09+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21264v1",
      "file": "papers/2512.21264v1.pdf"
    },
    {
      "arxiv_id": "2512.21252v1",
      "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation",
      "authors": [
        {
          "name": "Jiawei Liu"
        },
        {
          "name": "Junqiao Li"
        },
        {
          "name": "Jiangfan Deng"
        },
        {
          "name": "Gen Li"
        },
        {
          "name": "Siyu Zhou"
        },
        {
          "name": "Zetao Fang"
        },
        {
          "name": "Shanshan Lao"
        },
        {
          "name": "Zengde Deng"
        },
        {
          "name": "Jianing Zhu"
        },
        {
          "name": "Tingting Ma"
        },
        {
          "name": "Jiayi Li"
        },
        {
          "name": "Yunqiu Wang"
        },
        {
          "name": "Qian He"
        },
        {
          "name": "Xinglong Wu"
        }
      ],
      "abstract": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T16:00:15+00:00",
      "updated": "2025-12-24T16:00:15+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21252v1",
      "file": "papers/2512.21252v1.pdf"
    },
    {
      "arxiv_id": "2512.21221v1",
      "title": "Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval",
      "authors": [
        {
          "name": "Dao Sy Duy Minh"
        },
        {
          "name": "Huynh Trung Kiet"
        },
        {
          "name": "Nguyen Lam Phu Quy"
        },
        {
          "name": "Phu-Hoa Pham"
        },
        {
          "name": "Tran Chi Nguyen"
        }
      ],
      "abstract": "Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-24T15:02:33+00:00",
      "updated": "2025-12-24T15:02:33+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21221v1",
      "file": "papers/2512.21221v1.pdf"
    },
    {
      "arxiv_id": "2512.21218v1",
      "title": "Latent Implicit Visual Reasoning",
      "authors": [
        {
          "name": "Kelvin Li"
        },
        {
          "name": "Chuyi Shang"
        },
        {
          "name": "Leonid Karlinsky"
        },
        {
          "name": "Rogerio Feris"
        },
        {
          "name": "Trevor Darrell"
        },
        {
          "name": "Roei Herzig"
        }
      ],
      "abstract": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T14:59:49+00:00",
      "updated": "2025-12-24T14:59:49+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21218v1",
      "file": "papers/2512.21218v1.pdf"
    },
    {
      "arxiv_id": "2512.21209v1",
      "title": "Human Motion Estimation with Everyday Wearables",
      "authors": [
        {
          "name": "Siqi Zhu"
        },
        {
          "name": "Yixuan Li"
        },
        {
          "name": "Junfu Li"
        },
        {
          "name": "Qi Wu"
        },
        {
          "name": "Zan Wang"
        },
        {
          "name": "Haozhe Ma"
        },
        {
          "name": "Wei Liang"
        }
      ],
      "abstract": "While on-body device-based human motion estimation is crucial for applications such as XR interaction, existing methods often suffer from poor wearability, expensive hardware, and cumbersome calibration, which hinder their adoption in daily life. To address these challenges, we present EveryWear, a lightweight and practical human motion capture approach based entirely on everyday wearables: a smartphone, smartwatch, earbuds, and smart glasses equipped with one forward-facing and two downward-facing cameras, requiring no explicit calibration before use. We introduce Ego-Elec, a 9-hour real-world dataset covering 56 daily activities across 17 diverse indoor and outdoor environments, with ground-truth 3D annotations provided by the motion capture (MoCap), to facilitate robust research and benchmarking in this direction. Our approach employs a multimodal teacher-student framework that integrates visual cues from egocentric cameras with inertial signals from consumer devices. By training directly on real-world data rather than synthetic data, our model effectively eliminates the sim-to-real gap that constrains prior work. Experiments demonstrate that our method outperforms baseline models, validating its effectiveness for practical full-body motion estimation.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T14:44:51+00:00",
      "updated": "2025-12-24T14:44:51+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21209v1",
      "file": "papers/2512.21209v1.pdf"
    },
    {
      "arxiv_id": "2512.21194v1",
      "title": "VisRes Bench: On Evaluating the Visual Reasoning Capabilities of VLMs",
      "authors": [
        {
          "name": "Brigitta Malagurski Törtei"
        },
        {
          "name": "Yasser Dahou"
        },
        {
          "name": "Ngoc Dung Huynh"
        },
        {
          "name": "Wamiq Reyaz Para"
        },
        {
          "name": "Phúc H. Lê Khac"
        },
        {
          "name": "Ankit Singh"
        },
        {
          "name": "Sofian Chaybouti"
        },
        {
          "name": "Sanath Narayan"
        }
      ],
      "abstract": "Vision-Language Models (VLMs) have achieved remarkable progress across tasks such as visual question answering and image captioning. Yet, the extent to which these models perform visual reasoning as opposed to relying on linguistic priors remains unclear. To address this, we introduce VisRes Bench, a benchmark designed to study visual reasoning in naturalistic settings without contextual language supervision. Analyzing model behavior across three levels of complexity, we uncover clear limitations in perceptual and relational visual reasoning capacities. VisRes isolates distinct reasoning abilities across its levels. Level 1 probes perceptual completion and global image matching under perturbations such as blur, texture changes, occlusion, and rotation; Level 2 tests rule-based inference over a single attribute (e.g., color, count, orientation); and Level 3 targets compositional reasoning that requires integrating multiple visual attributes. Across more than 19,000 controlled task images, we find that state-of-the-art VLMs perform near random under subtle perceptual perturbations, revealing limited abstraction beyond pattern recognition. We conclude by discussing how VisRes provides a unified framework for advancing abstract visual reasoning in multimodal research.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T14:18:38+00:00",
      "updated": "2025-12-24T14:18:38+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21194v1",
      "file": "papers/2512.21194v1.pdf"
    },
    {
      "arxiv_id": "2512.21185v1",
      "title": "UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement",
      "authors": [
        {
          "name": "Tanghui Jia"
        },
        {
          "name": "Dongyu Yan"
        },
        {
          "name": "Dehao Hao"
        },
        {
          "name": "Yang Li"
        },
        {
          "name": "Kaiyi Zhang"
        },
        {
          "name": "Xianyi He"
        },
        {
          "name": "Lanjiong Li"
        },
        {
          "name": "Jinnan Chen"
        },
        {
          "name": "Lutao Jiang"
        },
        {
          "name": "Qishen Yin"
        },
        {
          "name": "Long Quan"
        },
        {
          "name": "Ying-Cong Chen"
        },
        {
          "name": "Li Yuan"
        }
      ],
      "abstract": "In this report, we introduce UltraShape 1.0, a scalable 3D diffusion framework for high-fidelity 3D geometry generation. The proposed approach adopts a two-stage generation pipeline: a coarse global structure is first synthesized and then refined to produce detailed, high-quality geometry. To support reliable 3D generation, we develop a comprehensive data processing pipeline that includes a novel watertight processing method and high-quality data filtering. This pipeline improves the geometric quality of publicly available 3D datasets by removing low-quality samples, filling holes, and thickening thin structures, while preserving fine-grained geometric details. To enable fine-grained geometry refinement, we decouple spatial localization from geometric detail synthesis in the diffusion process. We achieve this by performing voxel-based refinement at fixed spatial locations, where voxel queries derived from coarse geometry provide explicit positional anchors encoded via RoPE, allowing the diffusion model to focus on synthesizing local geometric details within a reduced, structured solution space. Our model is trained exclusively on publicly available 3D datasets, achieving strong geometric quality despite limited training resources. Extensive evaluations demonstrate that UltraShape 1.0 performs competitively with existing open-source methods in both data processing quality and geometry generation. All code and trained models will be released to support future research.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "published": "2025-12-24T14:08:38+00:00",
      "updated": "2025-12-24T14:08:38+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21185v1",
      "file": "papers/2512.21185v1.pdf"
    },
    {
      "arxiv_id": "2512.21180v1",
      "title": "Equivariant Multiscale Learned Invertible Reconstruction for Cone Beam CT: From Simulated to Real Data",
      "authors": [
        {
          "name": "Nikita Moriakov"
        },
        {
          "name": "Efstratios Gavves"
        },
        {
          "name": "Jonathan H. Mason"
        },
        {
          "name": "Carmen Seller-Oria"
        },
        {
          "name": "Jonas Teuwen"
        },
        {
          "name": "Jan-Jakob Sonke"
        }
      ],
      "abstract": "Cone Beam CT (CBCT) is an important imaging modality nowadays, however lower image quality of CBCT compared to more conventional Computed Tomography (CT) remains a limiting factor in CBCT applications. Deep learning reconstruction methods are a promising alternative to classical analytical and iterative reconstruction methods, but applying such methods to CBCT is often difficult due to the lack of ground truth data, memory limitations and the need for fast inference at clinically-relevant resolutions. In this work we propose LIRE++, an end-to-end rotationally-equivariant multiscale learned invertible primal-dual scheme for fast and memory-efficient CBCT reconstruction. Memory optimizations and multiscale reconstruction allow for fast training and inference, while rotational equivariance improves parameter efficiency. LIRE++ was trained on simulated projection data from a fast quasi-Monte Carlo CBCT projection simulator that we developed as well. Evaluated on synthetic data, LIRE++ gave an average improvement of 1 dB in Peak Signal-to-Noise Ratio over alternative deep learning baselines. On real clinical data, LIRE++ improved the average Mean Absolute Error between the reconstruction and the corresponding planning CT by 10 Hounsfield Units with respect to current proprietary state-of-the-art hybrid deep-learning/iterative method.",
      "primary_category": "physics.med-ph",
      "categories": [
        "physics.med-ph",
        "cs.CV"
      ],
      "published": "2025-12-24T13:59:43+00:00",
      "updated": "2025-12-24T13:59:43+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21180v1",
      "file": "papers/2512.21180v1.pdf"
    },
    {
      "arxiv_id": "2512.21174v1",
      "title": "A Turn Toward Better Alignment: Few-Shot Generative Adaptation with Equivariant Feature Rotation",
      "authors": [
        {
          "name": "Chenghao Xu"
        },
        {
          "name": "Qi Liu"
        },
        {
          "name": "Jiexi Yan"
        },
        {
          "name": "Muli Yang"
        },
        {
          "name": "Cheng Deng"
        }
      ],
      "abstract": "Few-shot image generation aims to effectively adapt a source generative model to a target domain using very few training images. Most existing approaches introduce consistency constraints-typically through instance-level or distribution-level loss functions-to directly align the distribution patterns of source and target domains within their respective latent spaces. However, these strategies often fall short: overly strict constraints can amplify the negative effects of the domain gap, leading to distorted or uninformative content, while overly relaxed constraints may fail to leverage the source domain effectively. This limitation primarily stems from the inherent discrepancy in the underlying distribution structures of the source and target domains. The scarcity of target samples further compounds this issue by hindering accurate estimation of the target domain's distribution. To overcome these limitations, we propose Equivariant Feature Rotation (EFR), a novel adaptation strategy that aligns source and target domains at two complementary levels within a self-rotated proxy feature space. Specifically, we perform adaptive rotations within a parameterized Lie Group to transform both source and target features into an equivariant proxy space, where alignment is conducted. These learnable rotation matrices serve to bridge the domain gap by preserving intra-domain structural information without distortion, while the alignment optimization facilitates effective knowledge transfer from the source to the target domain. Comprehensive experiments on a variety of commonly used datasets demonstrate that our method significantly enhances the generative performance within the targeted domain.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T13:48:22+00:00",
      "updated": "2025-12-24T13:48:22+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21174v1",
      "file": "papers/2512.21174v1.pdf"
    },
    {
      "arxiv_id": "2512.21150v1",
      "title": "ORCA: Object Recognition and Comprehension for Archiving Marine Species",
      "authors": [
        {
          "name": "Yuk-Kwan Wong"
        },
        {
          "name": "Haixin Liang"
        },
        {
          "name": "Zeyu Ma"
        },
        {
          "name": "Yiwei Chen"
        },
        {
          "name": "Ziqiang Zheng"
        },
        {
          "name": "Rinaldi Gotama"
        },
        {
          "name": "Pascal Sebastian"
        },
        {
          "name": "Lauren D. Sparks"
        },
        {
          "name": "Sai-Kit Yeung"
        }
      ],
      "abstract": "Marine visual understanding is essential for monitoring and protecting marine ecosystems, enabling automatic and scalable biological surveys. However, progress is hindered by limited training data and the lack of a systematic task formulation that aligns domain-specific marine challenges with well-defined computer vision tasks, thereby limiting effective model application. To address this gap, we present ORCA, a multi-modal benchmark for marine research comprising 14,647 images from 478 species, with 42,217 bounding box annotations and 22,321 expert-verified instance captions. The dataset provides fine-grained visual and textual annotations that capture morphology-oriented attributes across diverse marine species. To catalyze methodological advances, we evaluate 18 state-of-the-art models on three tasks: object detection (closed-set and open-vocabulary), instance captioning, and visual grounding. Results highlight key challenges, including species diversity, morphological overlap, and specialized domain demands, underscoring the difficulty of marine understanding. ORCA thus establishes a comprehensive benchmark to advance research in marine domain. Project Page: http://orca.hkustvgd.com/.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T12:36:57+00:00",
      "updated": "2025-12-24T12:36:57+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21150v1",
      "file": "papers/2512.21150v1.pdf"
    },
    {
      "arxiv_id": "2512.21135v1",
      "title": "TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation",
      "authors": [
        {
          "name": "Gaoren Lin"
        },
        {
          "name": "Huangxuan Zhao"
        },
        {
          "name": "Yuan Xiong"
        },
        {
          "name": "Lefei Zhang"
        },
        {
          "name": "Bo Du"
        },
        {
          "name": "Wentao Zhu"
        }
      ],
      "abstract": "Text-guided medical segmentation enhances segmentation accuracy by utilizing clinical reports as auxiliary information. However, existing methods typically rely on unaligned image and text encoders, which necessitate complex interaction modules for multimodal fusion. While CLIP provides a pre-aligned multimodal feature space, its direct application to medical imaging is limited by three main issues: insufficient preservation of fine-grained anatomical structures, inadequate modeling of complex clinical descriptions, and domain-specific semantic misalignment. To tackle these challenges, we propose TGC-Net, a CLIP-based framework focusing on parameter-efficient, task-specific adaptations. Specifically, it incorporates a Semantic-Structural Synergy Encoder (SSE) that augments CLIP's ViT with a CNN branch for multi-scale structural refinement, a Domain-Augmented Text Encoder (DATE) that injects large-language-model-derived medical knowledge, and a Vision-Language Calibration Module (VLCM) that refines cross-modal correspondence in a unified feature space. Experiments on five datasets across chest X-ray and thoracic CT modalities demonstrate that TGC-Net achieves state-of-the-art performance with substantially fewer trainable parameters, including notable Dice gains on challenging benchmarks.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-24T12:06:26+00:00",
      "updated": "2025-12-24T12:06:26+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21135v1",
      "file": "papers/2512.21135v1.pdf"
    },
    {
      "arxiv_id": "2512.21126v1",
      "title": "MarineEval: Assessing the Marine Intelligence of Vision-Language Models",
      "authors": [
        {
          "name": "YuK-Kwan Wong"
        },
        {
          "name": "Tuan-An To"
        },
        {
          "name": "Jipeng Zhang"
        },
        {
          "name": "Ziqiang Zheng"
        },
        {
          "name": "Sai-Kit Yeung"
        }
      ],
      "abstract": "We have witnessed promising progress led by large language models (LLMs) and further vision language models (VLMs) in handling various queries as a general-purpose assistant. VLMs, as a bridge to connect the visual world and language corpus, receive both visual content and various text-only user instructions to generate corresponding responses. Though great success has been achieved by VLMs in various fields, in this work, we ask whether the existing VLMs can act as domain experts, accurately answering marine questions, which require significant domain expertise and address special domain challenges/requirements. To comprehensively evaluate the effectiveness and explore the boundary of existing VLMs, we construct the first large-scale marine VLM dataset and benchmark called MarineEval, with 2,000 image-based question-answering pairs. During our dataset construction, we ensure the diversity and coverage of the constructed data: 7 task dimensions and 20 capacity dimensions. The domain requirements are specially integrated into the data construction and further verified by the corresponding marine domain experts. We comprehensively benchmark 17 existing VLMs on our MarineEval and also investigate the limitations of existing models in answering marine research questions. The experimental results reveal that existing VLMs cannot effectively answer the domain-specific questions, and there is still a large room for further performance improvements. We hope our new benchmark and observations will facilitate future research. Project Page: http://marineeval.hkustvgd.com/",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.DB"
      ],
      "published": "2025-12-24T11:57:50+00:00",
      "updated": "2025-12-24T11:57:50+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21126v1",
      "file": "papers/2512.21126v1.pdf"
    },
    {
      "arxiv_id": "2512.21118v1",
      "title": "STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting",
      "authors": [
        {
          "name": "Shi Quan Foo"
        },
        {
          "name": "Chi-Ho Wong"
        },
        {
          "name": "Zhihan Gao"
        },
        {
          "name": "Dit-Yan Yeung"
        },
        {
          "name": "Ka-Hing Wong"
        },
        {
          "name": "Wai-Kin Wong"
        }
      ],
      "abstract": "Precipitation nowcasting is a critical spatio-temporal prediction task for society to prevent severe damage owing to extreme weather events. Despite the advances in this field, the complex and stochastic nature of this task still poses challenges to existing approaches. Specifically, deterministic models tend to produce blurry predictions while generative models often struggle with poor accuracy. In this paper, we present a simple yet effective model architecture termed STLDM, a diffusion-based model that learns the latent representation from end to end alongside both the Variational Autoencoder and the conditioning network. STLDM decomposes this task into two stages: a deterministic forecasting stage handled by the conditioning network, and an enhancement stage performed by the latent diffusion model. Experimental results on multiple radar datasets demonstrate that STLDM achieves superior performance compared to the state of the art, while also improving inference efficiency. The code is available in https://github.com/sqfoo/stldm_official.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-12-24T11:34:44+00:00",
      "updated": "2025-12-24T11:34:44+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21118v1",
      "file": "papers/2512.21118v1.pdf"
    },
    {
      "arxiv_id": "2512.21104v1",
      "title": "FreeInpaint: Tuning-free Prompt Alignment and Visual Rationality Enhancement in Image Inpainting",
      "authors": [
        {
          "name": "Chao Gong"
        },
        {
          "name": "Dong Li"
        },
        {
          "name": "Yingwei Pan"
        },
        {
          "name": "Jingjing Chen"
        },
        {
          "name": "Ting Yao"
        },
        {
          "name": "Tao Mei"
        }
      ],
      "abstract": "Text-guided image inpainting endeavors to generate new content within specified regions of images using textual prompts from users. The primary challenge is to accurately align the inpainted areas with the user-provided prompts while maintaining a high degree of visual fidelity. While existing inpainting methods have produced visually convincing results by leveraging the pre-trained text-to-image diffusion models, they still struggle to uphold both prompt alignment and visual rationality simultaneously. In this work, we introduce FreeInpaint, a plug-and-play tuning-free approach that directly optimizes the diffusion latents on the fly during inference to improve the faithfulness of the generated images. Technically, we introduce a prior-guided noise optimization method that steers model attention towards valid inpainting regions by optimizing the initial noise. Furthermore, we meticulously design a composite guidance objective tailored specifically for the inpainting task. This objective efficiently directs the denoising process, enhancing prompt alignment and visual rationality by optimizing intermediate latents at each step. Through extensive experiments involving various inpainting diffusion models and evaluation metrics, we demonstrate the effectiveness and robustness of our proposed FreeInpaint.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T11:06:26+00:00",
      "updated": "2025-12-24T11:06:26+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21104v1",
      "file": "papers/2512.21104v1.pdf"
    },
    {
      "arxiv_id": "2512.21095v1",
      "title": "UniRec-0.1B: Unified Text and Formula Recognition with 0.1B Parameters",
      "authors": [
        {
          "name": "Yongkun Du"
        },
        {
          "name": "Zhineng Chen"
        },
        {
          "name": "Yazhen Xie"
        },
        {
          "name": "Weikang Baiand Hao Feng"
        },
        {
          "name": "Wei Shi"
        },
        {
          "name": "Yuchen Su"
        },
        {
          "name": "Can Huang"
        },
        {
          "name": "Yu-Gang Jiang"
        }
      ],
      "abstract": "Text and formulas constitute the core informational components of many documents. Accurately and efficiently recognizing both is crucial for developing robust and generalizable document parsing systems. Recently, vision-language models (VLMs) have achieved impressive unified recognition of text and formulas. However, they are large-sized and computationally demanding, restricting their usage in many applications. In this paper, we propose UniRec-0.1B, a unified recognition model with only 0.1B parameters. It is capable of performing text and formula recognition at multiple levels, including characters, words, lines, paragraphs, and documents. To implement this task, we first establish UniRec40M, a large-scale dataset comprises 40 million text, formula and their mix samples, enabling the training of a powerful yet lightweight model. Secondly, we identify two challenges when building such a lightweight but unified expert model. They are: structural variability across hierarchies and semantic entanglement between textual and formulaic content. To tackle these, we introduce a hierarchical supervision training that explicitly guides structural comprehension, and a semantic-decoupled tokenizer that separates text and formula representations. Finally, we develop a comprehensive evaluation benchmark covering Chinese and English documents from multiple domains and with multiple levels. Experimental results on this and public benchmarks demonstrate that UniRec-0.1B outperforms both general-purpose VLMs and leading document parsing expert models, while achieving a 2-9$\\times$ speedup, validating its effectiveness and efficiency. Codebase and Dataset: https://github.com/Topdu/OpenOCR.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T10:35:21+00:00",
      "updated": "2025-12-24T10:35:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21095v1",
      "file": "papers/2512.21095v1.pdf"
    },
    {
      "arxiv_id": "2512.21094v1",
      "title": "T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation",
      "authors": [
        {
          "name": "Zhe Cao"
        },
        {
          "name": "Tao Wang"
        },
        {
          "name": "Jiaming Wang"
        },
        {
          "name": "Yanghai Wang"
        },
        {
          "name": "Yuanxing Zhang"
        },
        {
          "name": "Jialu Chen"
        },
        {
          "name": "Miao Deng"
        },
        {
          "name": "Jiahao Wang"
        },
        {
          "name": "Yubin Guo"
        },
        {
          "name": "Chenxi Liao"
        },
        {
          "name": "Yize Zhang"
        },
        {
          "name": "Zhaoxiang Zhang"
        },
        {
          "name": "Jiaheng Liu"
        }
      ],
      "abstract": "Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction following, and perceptual realism under complex prompts. To address this limitation, we present T2AV-Compass, a unified benchmark for comprehensive evaluation of T2AV systems, consisting of 500 diverse and complex prompts constructed via a taxonomy-driven pipeline to ensure semantic richness and physical plausibility. Besides, T2AV-Compass introduces a dual-level evaluation framework that integrates objective signal-level metrics for video quality, audio quality, and cross-modal alignment with a subjective MLLM-as-a-Judge protocol for instruction following and realism assessment. Extensive evaluation of 11 representative T2AVsystems reveals that even the strongest models fall substantially short of human-level realism and cross-modal consistency, with persistent failures in audio realism, fine-grained synchronization, instruction following, etc. These results indicate significant improvement room for future models and highlight the value of T2AV-Compass as a challenging and diagnostic testbed for advancing text-to-audio-video generation.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T10:30:35+00:00",
      "updated": "2025-12-24T10:30:35+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21094v1",
      "file": "papers/2512.21094v1.pdf"
    },
    {
      "arxiv_id": "2512.21083v1",
      "title": "Hierarchical Modeling Approach to Fast and Accurate Table Recognition",
      "authors": [
        {
          "name": "Takaya Kawakatsu"
        }
      ],
      "abstract": "The extraction and use of diverse knowledge from numerous documents is a pressing challenge in intelligent information retrieval. Documents contain elements that require different recognition methods. Table recognition typically consists of three subtasks, namely table structure, cell position and cell content recognition. Recent models have achieved excellent recognition with a combination of multi-task learning, local attention, and mutual learning. However, their effectiveness has not been fully explained, and they require a long period of time for inference. This paper presents a novel multi-task model that utilizes non-causal attention to capture the entire table structure, and a parallel inference algorithm for faster cell content inference. The superiority is demonstrated both visually and statistically on two large public datasets.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-12-24T09:58:30+00:00",
      "updated": "2025-12-24T09:58:30+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21083v1",
      "file": "papers/2512.21083v1.pdf"
    },
    {
      "arxiv_id": "2512.21078v1",
      "title": "UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer",
      "authors": [
        {
          "name": "Tianchen Deng"
        },
        {
          "name": "Xun Chen"
        },
        {
          "name": "Ziming Li"
        },
        {
          "name": "Hongming Shen"
        },
        {
          "name": "Danwei Wang"
        },
        {
          "name": "Javier Civera"
        },
        {
          "name": "Hesheng Wang"
        }
      ],
      "abstract": "Visual Place Recognition (VPR) has been traditionally formulated as a single-image retrieval task. Using multiple views offers clear advantages, yet this setting remains relatively underexplored and existing methods often struggle to generalize across diverse environments. In this work we introduce UniPR-3D, the first VPR architecture that effectively integrates information from multiple views. UniPR-3D builds on a VGGT backbone capable of encoding multi-view 3D representations, which we adapt by designing feature aggregators and fine-tune for the place recognition task. To construct our descriptor, we jointly leverage the 3D tokens and intermediate 2D tokens produced by VGGT. Based on their distinct characteristics, we design dedicated aggregation modules for 2D and 3D features, allowing our descriptor to capture fine-grained texture cues while also reasoning across viewpoints. To further enhance generalization, we incorporate both single- and multi-frame aggregation schemes, along with a variable-length sequence retrieval strategy. Our experiments show that UniPR-3D sets a new state of the art, outperforming both single- and multi-view baselines and highlighting the effectiveness of geometry-grounded tokens for VPR. Our code and models will be made publicly available on Github https://github.com/dtc111111/UniPR-3D.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T09:55:16+00:00",
      "updated": "2025-12-24T09:55:16+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21078v1",
      "file": "papers/2512.21078v1.pdf"
    },
    {
      "arxiv_id": "2512.21065v1",
      "title": "Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation",
      "authors": [
        {
          "name": "Zebin Jiang"
        },
        {
          "name": "Tianle Jin"
        },
        {
          "name": "Xiangtong Yao"
        },
        {
          "name": "Alois Knoll"
        },
        {
          "name": "Hu Cao"
        }
      ],
      "abstract": "Grasping is one of the most fundamental challenging capabilities in robotic manipulation, especially in unstructured, cluttered, and semantically diverse environments. Recent researches have increasingly explored language-guided manipulation, where robots not only perceive the scene but also interpret task-relevant natural language instructions. However, existing language-conditioned grasping methods typically rely on shallow fusion strategies, leading to limited semantic grounding and weak alignment between linguistic intent and visual grasp reasoning.In this work, we propose Language-Guided Grasp Detection (LGGD) with a coarse-to-fine learning paradigm for robotic manipulation. LGGD leverages CLIP-based visual and textual embeddings within a hierarchical cross-modal fusion pipeline, progressively injecting linguistic cues into the visual feature reconstruction process. This design enables fine-grained visual-semantic alignment and improves the feasibility of the predicted grasps with respect to task instructions. In addition, we introduce a language-conditioned dynamic convolution head (LDCH) that mixes multiple convolution experts based on sentence-level features, enabling instruction-adaptive coarse mask and grasp predictions. A final refinement module further enhances grasp consistency and robustness in complex scenes.Experiments on the OCID-VLG and Grasp-Anything++ datasets show that LGGD surpasses existing language-guided grasping methods, exhibiting strong generalization to unseen objects and diverse language queries. Moreover, deployment on a real robotic platform demonstrates the practical effectiveness of our approach in executing accurate, instruction-conditioned grasp actions. The code will be released publicly upon acceptance.",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published": "2025-12-24T09:16:42+00:00",
      "updated": "2025-12-24T09:16:42+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21065v1",
      "file": "papers/2512.21065v1.pdf"
    },
    {
      "arxiv_id": "2512.21064v1",
      "title": "Multimodal Skeleton-Based Action Representation Learning via Decomposition and Composition",
      "authors": [
        {
          "name": "Hongsong Wang"
        },
        {
          "name": "Heng Fei"
        },
        {
          "name": "Bingxuan Dai"
        },
        {
          "name": "Jie Gui"
        }
      ],
      "abstract": "Multimodal human action understanding is a significant problem in computer vision, with the central challenge being the effective utilization of the complementarity among diverse modalities while maintaining model efficiency. However, most existing methods rely on simple late fusion to enhance performance, which results in substantial computational overhead. Although early fusion with a shared backbone for all modalities is efficient, it struggles to achieve excellent performance. To address the dilemma of balancing efficiency and effectiveness, we introduce a self-supervised multimodal skeleton-based action representation learning framework, named Decomposition and Composition. The Decomposition strategy meticulously decomposes the fused multimodal features into distinct unimodal features, subsequently aligning them with their respective ground truth unimodal counterparts. On the other hand, the Composition strategy integrates multiple unimodal features, leveraging them as self-supervised guidance to enhance the learning of multimodal representations. Extensive experiments on the NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD II datasets demonstrate that the proposed method strikes an excellent balance between computational cost and model performance.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T09:10:04+00:00",
      "updated": "2025-12-24T09:10:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21064v1",
      "file": "papers/2512.21064v1.pdf"
    },
    {
      "arxiv_id": "2512.21058v1",
      "title": "Beyond Pixel Simulation: Pathology Image Generation via Diagnostic Semantic Tokens and Prototype Control",
      "authors": [
        {
          "name": "Minghao Han"
        },
        {
          "name": "YiChen Liu"
        },
        {
          "name": "Yizhou Liu"
        },
        {
          "name": "Zizhi Chen"
        },
        {
          "name": "Jingqun Tang"
        },
        {
          "name": "Xuecheng Wu"
        },
        {
          "name": "Dingkang Yang"
        },
        {
          "name": "Lihua Zhang"
        }
      ],
      "abstract": "In computational pathology, understanding and generation have evolved along disparate paths: advanced understanding models already exhibit diagnostic-level competence, whereas generative models largely simulate pixels. Progress remains hindered by three coupled factors: the scarcity of large, high-quality image-text corpora; the lack of precise, fine-grained semantic control, which forces reliance on non-semantic cues; and terminological heterogeneity, where diverse phrasings for the same diagnostic concept impede reliable text conditioning. We introduce UniPath, a semantics-driven pathology image generation framework that leverages mature diagnostic understanding to enable controllable generation. UniPath implements Multi-Stream Control: a Raw-Text stream; a High-Level Semantics stream that uses learnable queries to a frozen pathology MLLM to distill paraphrase-robust Diagnostic Semantic Tokens and to expand prompts into diagnosis-aware attribute bundles; and a Prototype stream that affords component-level morphological control via a prototype bank. On the data front, we curate a 2.65M image-text corpus and a finely annotated, high-quality 68K subset to alleviate data scarcity. For a comprehensive assessment, we establish a four-tier evaluation hierarchy tailored to pathology. Extensive experiments demonstrate UniPath's SOTA performance, including a Patho-FID of 80.9 (51% better than the second-best) and fine-grained semantic control achieving 98.7% of the real-image. The meticulously curated datasets, complete source code, and pre-trained model weights developed in this study will be made openly accessible to the public.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T08:52:08+00:00",
      "updated": "2025-12-24T08:52:08+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21058v1",
      "file": "papers/2512.21058v1.pdf"
    },
    {
      "arxiv_id": "2512.21054v1",
      "title": "DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors",
      "authors": [
        {
          "name": "Kaustubh Kundu"
        },
        {
          "name": "Hrishav Bakul Barua"
        },
        {
          "name": "Lucy Robertson-Bell"
        },
        {
          "name": "Zhixi Cai"
        },
        {
          "name": "Kalin Stefanov"
        }
      ],
      "abstract": "The trend in sign language generation is centered around data-driven generative methods that require vast amounts of precise 2D and 3D human pose data to achieve an acceptable generation quality. However, currently, most sign language datasets are video-based and limited to automatically reconstructed 2D human poses (i.e., keypoints) and lack accurate 3D information. Furthermore, existing state-of-the-art for automatic 3D human pose estimation from sign language videos is prone to self-occlusion, noise, and motion blur effects, resulting in poor reconstruction quality. In response to this, we introduce DexAvatar, a novel framework to reconstruct bio-mechanically accurate fine-grained hand articulations and body movements from in-the-wild monocular sign language videos, guided by learned 3D hand and body priors. DexAvatar achieves strong performance in the SGNify motion capture dataset, the only benchmark available for this task, reaching an improvement of 35.11% in the estimation of body and hand poses compared to the state-of-the-art. The official website of this work is: https://github.com/kaustesseract/DexAvatar.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "published": "2025-12-24T08:44:58+00:00",
      "updated": "2025-12-24T08:44:58+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21054v1",
      "file": "papers/2512.21054v1.pdf"
    },
    {
      "arxiv_id": "2512.21053v1",
      "title": "Optical Flow-Guided 6DoF Object Pose Tracking with an Event Camera",
      "authors": [
        {
          "name": "Zibin Liu"
        },
        {
          "name": "Banglei Guan"
        },
        {
          "name": "Yang Shang"
        },
        {
          "name": "Shunkun Liang"
        },
        {
          "name": "Zhenbao Yu"
        },
        {
          "name": "Qifeng Yu"
        }
      ],
      "abstract": "Object pose tracking is one of the pivotal technologies in multimedia, attracting ever-growing attention in recent years. Existing methods employing traditional cameras encounter numerous challenges such as motion blur, sensor noise, partial occlusion, and changing lighting conditions. The emerging bio-inspired sensors, particularly event cameras, possess advantages such as high dynamic range and low latency, which hold the potential to address the aforementioned challenges. In this work, we present an optical flow-guided 6DoF object pose tracking method with an event camera. A 2D-3D hybrid feature extraction strategy is firstly utilized to detect corners and edges from events and object models, which characterizes object motion precisely. Then, we search for the optical flow of corners by maximizing the event-associated probability within a spatio-temporal window, and establish the correlation between corners and edges guided by optical flow. Furthermore, by minimizing the distances between corners and edges, the 6DoF object pose is iteratively optimized to achieve continuous pose tracking. Experimental results of both simulated and real events demonstrate that our methods outperform event-based state-of-the-art methods in terms of both accuracy and robustness.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T08:40:57+00:00",
      "updated": "2025-12-24T08:40:57+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21053v1",
      "file": "papers/2512.21053v1.pdf"
    },
    {
      "arxiv_id": "2512.21038v1",
      "title": "Next-Scale Prediction: A Self-Supervised Approach for Real-World Image Denoising",
      "authors": [
        {
          "name": "Yiwen Shan"
        },
        {
          "name": "Haiyu Zhao"
        },
        {
          "name": "Peng Hu"
        },
        {
          "name": "Xi Peng"
        },
        {
          "name": "Yuanbiao Gou"
        }
      ],
      "abstract": "Self-supervised real-world image denoising remains a fundamental challenge, arising from the antagonistic trade-off between decorrelating spatially structured noise and preserving high-frequency details. Existing blind-spot network (BSN) methods rely on pixel-shuffle downsampling (PD) to decorrelate noise, but aggressive downsampling fragments fine structures, while milder downsampling fails to remove correlated noise. To address this, we introduce Next-Scale Prediction (NSP), a novel self-supervised paradigm that decouples noise decorrelation from detail preservation. NSP constructs cross-scale training pairs, where BSN takes low-resolution, fully decorrelated sub-images as input to predict high-resolution targets that retain fine details. As a by-product, NSP naturally supports super-resolution of noisy images without retraining or modification. Extensive experiments demonstrate that NSP achieves state-of-the-art self-supervised denoising performance on real-world benchmarks, significantly alleviating the long-standing conflict between noise decorrelation and detail preservation.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T08:06:17+00:00",
      "updated": "2025-12-24T08:06:17+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21038v1",
      "file": "papers/2512.21038v1.pdf"
    },
    {
      "arxiv_id": "2512.21032v1",
      "title": "Multi-Attribute guided Thermal Face Image Translation based on Latent Diffusion Model",
      "authors": [
        {
          "name": "Mingshu Cai"
        },
        {
          "name": "Osamu Yoshie"
        },
        {
          "name": "Yuya Ieiri"
        }
      ],
      "abstract": "Modern surveillance systems increasingly rely on multi-wavelength sensors and deep neural networks to recognize faces in infrared images captured at night. However, most facial recognition models are trained on visible light datasets, leading to substantial performance degradation on infrared inputs due to significant domain shifts. Early feature-based methods for infrared face recognition proved ineffective, prompting researchers to adopt generative approaches that convert infrared images into visible light images for improved recognition. This paradigm, known as Heterogeneous Face Recognition (HFR), faces challenges such as model and modality discrepancies, leading to distortion and feature loss in generated images. To address these limitations, this paper introduces a novel latent diffusion-based model designed to generate high-quality visible face images from thermal inputs while preserving critical identity features. A multi-attribute classifier is incorporated to extract key facial attributes from visible images, mitigating feature loss during infrared-to-visible image restoration. Additionally, we propose the Self-attn Mamba module, which enhances global modeling of cross-modal features and significantly improves inference speed. Experimental results on two benchmark datasets demonstrate the superiority of our approach, achieving state-of-the-art performance in both image quality and identity preservation.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T07:55:54+00:00",
      "updated": "2025-12-24T07:55:54+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21032v1",
      "file": "papers/2512.21032v1.pdf"
    },
    {
      "arxiv_id": "2512.21019v1",
      "title": "Efficient and Robust Video Defense Framework against 3D-field Personalized Talking Face",
      "authors": [
        {
          "name": "Rui-qing Sun"
        },
        {
          "name": "Xingshan Yao"
        },
        {
          "name": "Tian Lan"
        },
        {
          "name": "Hui-Yang Zhao"
        },
        {
          "name": "Jia-Ling Shi"
        },
        {
          "name": "Chen-Hao Cui"
        },
        {
          "name": "Zhijing Wu"
        },
        {
          "name": "Chen Yang"
        },
        {
          "name": "Xian-Ling Mao"
        }
      ],
      "abstract": "State-of-the-art 3D-field video-referenced Talking Face Generation (TFG) methods synthesize high-fidelity personalized talking-face videos in real time by modeling 3D geometry and appearance from reference portrait video. This capability raises significant privacy concerns regarding malicious misuse of personal portraits. However, no efficient defense framework exists to protect such videos against 3D-field TFG methods. While image-based defenses could apply per-frame 2D perturbations, they incur prohibitive computational costs, severe video quality degradation, failing to disrupt 3D information for video protection. To address this, we propose a novel and efficient video defense framework against 3D-field TFG methods, which protects portrait video by perturbing the 3D information acquisition process while maintain high-fidelity video quality. Specifically, our method introduces: (1) a similarity-guided parameter sharing mechanism for computational efficiency, and (2) a multi-scale dual-domain attention module to jointly optimize spatial-frequency perturbations. Extensive experiments demonstrate that our proposed framework exhibits strong defense capability and achieves a 47x acceleration over the fastest baseline while maintaining high fidelity. Moreover, it remains robust against scaling operations and state-of-the-art purification attacks, and the effectiveness of our design choices is further validated through ablation studies. Our project is available at https://github.com/Richen7418/VDF.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T07:26:06+00:00",
      "updated": "2025-12-24T07:26:06+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21019v1",
      "file": "papers/2512.21019v1.pdf"
    },
    {
      "arxiv_id": "2512.21015v1",
      "title": "FluencyVE: Marrying Temporal-Aware Mamba with Bypass Attention for Video Editing",
      "authors": [
        {
          "name": "Mingshu Cai"
        },
        {
          "name": "Yixuan Li"
        },
        {
          "name": "Osamu Yoshie"
        },
        {
          "name": "Yuya Ieiri"
        }
      ],
      "abstract": "Large-scale text-to-image diffusion models have achieved unprecedented success in image generation and editing. However, extending this success to video editing remains challenging. Recent video editing efforts have adapted pretrained text-to-image models by adding temporal attention mechanisms to handle video tasks. Unfortunately, these methods continue to suffer from temporal inconsistency issues and high computational overheads. In this study, we propose FluencyVE, which is a simple yet effective one-shot video editing approach. FluencyVE integrates the linear time-series module, Mamba, into a video editing model based on pretrained Stable Diffusion models, replacing the temporal attention layer. This enables global frame-level attention while reducing the computational costs. In addition, we employ low-rank approximation matrices to replace the query and key weight matrices in the causal attention, and use a weighted averaging technique during training to update the attention scores. This approach significantly preserves the generative power of the text-to-image model while effectively reducing the computational burden. Experiments and analyses demonstrate promising results in editing various attributes, subjects, and locations in real-world videos.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T07:21:59+00:00",
      "updated": "2025-12-24T07:21:59+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21015v1",
      "file": "papers/2512.21015v1.pdf"
    },
    {
      "arxiv_id": "2512.21011v1",
      "title": "Granular-ball Guided Masking: Structure-aware Data Augmentation",
      "authors": [
        {
          "name": "Shuyin Xia"
        },
        {
          "name": "Fan Chen"
        },
        {
          "name": "Dawei Dai"
        },
        {
          "name": "Meng Yang"
        },
        {
          "name": "Junwei Han"
        },
        {
          "name": "Xinbo Gao"
        },
        {
          "name": "Guoyin Wang"
        }
      ],
      "abstract": "Deep learning models have achieved remarkable success in computer vision, but they still rely heavily on large-scale labeled data and tend to overfit when data are limited or distributions shift. Data augmentation, particularly mask-based information dropping, can enhance robustness by forcing models to explore complementary cues; however, existing approaches often lack structural awareness and may discard essential semantics. We propose Granular-ball Guided Masking (GBGM), a structure-aware augmentation strategy guided by Granular-ball Computing (GBC). GBGM adaptively preserves semantically rich, structurally important regions while suppressing redundant areas through a coarse-to-fine hierarchical masking process, producing augmentations that are both representative and discriminative. Extensive experiments on multiple benchmarks demonstrate consistent improvements in classification accuracy and masked image reconstruction, confirming the effectiveness and broad applicability of the proposed method. Simple and model-agnostic, it integrates seamlessly into CNNs and Vision Transformers and provides a new paradigm for structure-aware data augmentation.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T07:15:33+00:00",
      "updated": "2025-12-24T07:15:33+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21011v1",
      "file": "papers/2512.21011v1.pdf"
    },
    {
      "arxiv_id": "2512.21004v1",
      "title": "Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations",
      "authors": [
        {
          "name": "Jinghan Li"
        },
        {
          "name": "Yang Jin"
        },
        {
          "name": "Hao Jiang"
        },
        {
          "name": "Yadong Mu"
        },
        {
          "name": "Yang Song"
        },
        {
          "name": "Kun Xu"
        }
      ],
      "abstract": "Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which often disregards the temporal information essential for video analysis. The few existing autoregressive visual pretraining methods suffer from issues such as inaccurate semantic localization and poor generation quality, leading to poor semantics. In this work, we propose NExT-Vid, a novel autoregressive visual generative pretraining framework that utilizes masked next-frame prediction to jointly model images and videos. NExT-Vid introduces a context-isolated autoregressive predictor to decouple semantic representation from target decoding, and a conditioned flow-matching decoder to enhance generation quality and diversity. Through context-isolated flow-matching pretraining, our approach achieves strong representations. Extensive experiments on large-scale pretrained models demonstrate that our proposed method consistently outperforms previous generative pretraining methods for visual representation learning via attentive probing in downstream classification.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T07:07:08+00:00",
      "updated": "2025-12-24T07:07:08+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21004v1",
      "file": "papers/2512.21004v1.pdf"
    },
    {
      "arxiv_id": "2512.21003v1",
      "title": "MVInverse: Feed-forward Multi-view Inverse Rendering in Seconds",
      "authors": [
        {
          "name": "Xiangzuo Wu"
        },
        {
          "name": "Chengwei Ren"
        },
        {
          "name": "Jun Zhou"
        },
        {
          "name": "Xiu Li"
        },
        {
          "name": "Yuan Liu"
        }
      ],
      "abstract": "Multi-view inverse rendering aims to recover geometry, materials, and illumination consistently across multiple viewpoints. When applied to multi-view images, existing single-view approaches often ignore cross-view relationships, leading to inconsistent results. In contrast, multi-view optimization methods rely on slow differentiable rendering and per-scene refinement, making them computationally expensive and hard to scale. To address these limitations, we introduce a feed-forward multi-view inverse rendering framework that directly predicts spatially varying albedo, metallic, roughness, diffuse shading, and surface normals from sequences of RGB images. By alternating attention across views, our model captures both intra-view long-range lighting interactions and inter-view material consistency, enabling coherent scene-level reasoning within a single forward pass. Due to the scarcity of real-world training data, models trained on existing synthetic datasets often struggle to generalize to real-world scenes. To overcome this limitation, we propose a consistency-based finetuning strategy that leverages unlabeled real-world videos to enhance both multi-view coherence and robustness under in-the-wild conditions. Extensive experiments on benchmark datasets demonstrate that our method achieves state-of-the-art performance in terms of multi-view consistency, material and normal estimation quality, and generalization to real-world imagery.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T06:59:29+00:00",
      "updated": "2025-12-24T06:59:29+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.21003v1",
      "file": "papers/2512.21003v1.pdf"
    },
    {
      "arxiv_id": "2512.20988v1",
      "title": "PUFM++: Point Cloud Upsampling via Enhanced Flow Matching",
      "authors": [
        {
          "name": "Zhi-Song Liu"
        },
        {
          "name": "Chenhang He"
        },
        {
          "name": "Roland Maier"
        },
        {
          "name": "Andreas Rupp"
        }
      ],
      "abstract": "Recent advances in generative modeling have demonstrated strong promise for high-quality point cloud upsampling. In this work, we present PUFM++, an enhanced flow-matching framework for reconstructing dense and accurate point clouds from sparse, noisy, and partial observations. PUFM++ improves flow matching along three key axes: (i) geometric fidelity, (ii) robustness to imperfect input, and (iii) consistency with downstream surface-based tasks. We introduce a two-stage flow-matching strategy that first learns a direct, straight-path flow from sparse inputs to dense targets, and then refines it using noise-perturbed samples to approximate the terminal marginal distribution better. To accelerate and stabilize inference, we propose a data-driven adaptive time scheduler that improves sampling efficiency based on interpolation behavior. We further impose on-manifold constraints during sampling to ensure that generated points remain aligned with the underlying surface. Finally, we incorporate a recurrent interface network~(RIN) to strengthen hierarchical feature interactions and boost reconstruction quality. Extensive experiments on synthetic benchmarks and real-world scans show that PUFM++ sets a new state of the art in point cloud upsampling, delivering superior visual fidelity and quantitative accuracy across a wide range of tasks. Code and pretrained models are publicly available at https://github.com/Holmes-Alan/Enhanced_PUFM.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T06:30:42+00:00",
      "updated": "2025-12-24T06:30:42+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20988v1",
      "file": "papers/2512.20988v1.pdf"
    },
    {
      "arxiv_id": "2512.20980v1",
      "title": "X-ray Insights Unleashed: Pioneering the Enhancement of Multi-Label Long-Tail Data",
      "authors": [
        {
          "name": "Xinquan Yang"
        },
        {
          "name": "Jinheng Xie"
        },
        {
          "name": "Yawen Huang"
        },
        {
          "name": "Yuexiang Li"
        },
        {
          "name": "Huimin Huang"
        },
        {
          "name": "Hao Zheng"
        },
        {
          "name": "Xian Wu"
        },
        {
          "name": "Yefeng Zheng"
        },
        {
          "name": "Linlin Shen"
        }
      ],
      "abstract": "Long-tailed pulmonary anomalies in chest radiography present formidable diagnostic challenges. Despite the recent strides in diffusion-based methods for enhancing the representation of tailed lesions, the paucity of rare lesion exemplars curtails the generative capabilities of these approaches, thereby leaving the diagnostic precision less than optimal. In this paper, we propose a novel data synthesis pipeline designed to augment tail lesions utilizing a copious supply of conventional normal X-rays. Specifically, a sufficient quantity of normal samples is amassed to train a diffusion model capable of generating normal X-ray images. This pre-trained diffusion model is subsequently utilized to inpaint the head lesions present in the diseased X-rays, thereby preserving the tail classes as augmented training data. Additionally, we propose the integration of a Large Language Model Knowledge Guidance (LKG) module alongside a Progressive Incremental Learning (PIL) strategy to stabilize the inpainting fine-tuning process. Comprehensive evaluations conducted on the public lung datasets MIMIC and CheXpert demonstrate that the proposed method sets a new benchmark in performance.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T06:14:55+00:00",
      "updated": "2025-12-24T06:14:55+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20980v1",
      "file": "papers/2512.20980v1.pdf"
    },
    {
      "arxiv_id": "2512.20976v1",
      "title": "XGrid-Mapping: Explicit Implicit Hybrid Grid Submaps for Efficient Incremental Neural LiDAR Mapping",
      "authors": [
        {
          "name": "Zeqing Song"
        },
        {
          "name": "Zhongmiao Yan"
        },
        {
          "name": "Junyuan Deng"
        },
        {
          "name": "Songpengcheng Xia"
        },
        {
          "name": "Xiang Mu"
        },
        {
          "name": "Jingyi Xu"
        },
        {
          "name": "Qi Wu"
        },
        {
          "name": "Ling Pei"
        }
      ],
      "abstract": "Large-scale incremental mapping is fundamental to the development of robust and reliable autonomous systems, as it underpins incremental environmental understanding with sequential inputs for navigation and decision-making. LiDAR is widely used for this purpose due to its accuracy and robustness. Recently, neural LiDAR mapping has shown impressive performance; however, most approaches rely on dense implicit representations and underutilize geometric structure, while existing voxel-guided methods struggle to achieve real-time performance. To address these challenges, we propose XGrid-Mapping, a hybrid grid framework that jointly exploits explicit and implicit representations for efficient neural LiDAR mapping. Specifically, the strategy combines a sparse grid, providing geometric priors and structural guidance, with an implicit dense grid that enriches scene representation. By coupling the VDB structure with a submap-based organization, the framework reduces computational load and enables efficient incremental mapping on a large scale. To mitigate discontinuities across submaps, we introduce a distillation-based overlap alignment strategy, in which preceding submaps supervise subsequent ones to ensure consistency in overlapping regions. To further enhance robustness and sampling efficiency, we incorporate a dynamic removal module. Extensive experiments show that our approach delivers superior mapping quality while overcoming the efficiency limitations of voxel-guided methods, thereby outperforming existing state-of-the-art mapping methods.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T06:08:50+00:00",
      "updated": "2025-12-24T06:08:50+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20976v1",
      "file": "papers/2512.20976v1.pdf"
    },
    {
      "arxiv_id": "2512.20975v1",
      "title": "SPOT!: Map-Guided LLM Agent for Unsupervised Multi-CCTV Dynamic Object Tracking",
      "authors": [
        {
          "name": "Yujin Noh"
        },
        {
          "name": "Inho Jake Park"
        },
        {
          "name": "Chigon Hwang"
        }
      ],
      "abstract": "CCTV-based vehicle tracking systems face structural limitations in continuously connecting the trajectories of the same vehicle across multiple camera environments. In particular, blind spots occur due to the intervals between CCTVs and limited Fields of View (FOV), which leads to object ID switching and trajectory loss, thereby reducing the reliability of real-time path prediction. This paper proposes SPOT (Spatial Prediction Over Trajectories), a map-guided LLM agent capable of tracking vehicles even in blind spots of multi-CCTV environments without prior training. The proposed method represents road structures (Waypoints) and CCTV placement information as documents based on 2D spatial coordinates and organizes them through chunking techniques to enable real-time querying and inference. Furthermore, it transforms the vehicle's position into the actual world coordinate system using the relative position and FOV information of objects observed in CCTV images. By combining map spatial information with the vehicle's moving direction, speed, and driving patterns, a beam search is performed at the intersection level to derive candidate CCTV locations where the vehicle is most likely to enter after the blind spot. Experimental results based on the CARLA simulator in a virtual city environment confirmed that the proposed method accurately predicts the next appearing CCTV even in blind spot sections, maintaining continuous vehicle trajectories more effectively than existing techniques.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T06:04:58+00:00",
      "updated": "2025-12-24T06:04:58+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20975v1",
      "file": "papers/2512.20975v1.pdf"
    },
    {
      "arxiv_id": "2512.20963v1",
      "title": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
      "authors": [
        {
          "name": "Zekai Zhang"
        },
        {
          "name": "Xiao Li"
        },
        {
          "name": "Xiang Li"
        },
        {
          "name": "Lianghe Shi"
        },
        {
          "name": "Meng Wu"
        },
        {
          "name": "Molei Tao"
        },
        {
          "name": "Qing Qu"
        }
      ],
      "abstract": "Diffusion models excel at generating high-quality, diverse samples, yet they risk memorizing training data when overfit to the training objective. We analyze the distinctions between memorization and generalization in diffusion models through the lens of representation learning. By investigating a two-layer ReLU denoising autoencoder (DAE), we prove that (i) memorization corresponds to the model storing raw training samples in the learned weights for encoding and decoding, yielding localized \"spiky\" representations, whereas (ii) generalization arises when the model captures local data statistics, producing \"balanced\" representations. Furthermore, we validate these theoretical findings on real-world unconditional and text-to-image diffusion models, demonstrating that the same representation structures emerge in deep generative models with significant practical implications. Building on these insights, we propose a representation-based method for detecting memorization and a training-free editing technique that allows precise control via representation steering. Together, our results highlight that learning good representations is central to novel and meaningful generative modeling.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "published": "2025-12-24T05:40:40+00:00",
      "updated": "2025-12-24T05:40:40+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20963v1",
      "file": "papers/2512.20963v1.pdf"
    },
    {
      "arxiv_id": "2512.20937v1",
      "title": "Beyond Artifacts: Real-Centric Envelope Modeling for Reliable AI-Generated Image Detection",
      "authors": [
        {
          "name": "Ruiqi Liu"
        },
        {
          "name": "Yi Han"
        },
        {
          "name": "Zhengbo Zhang"
        },
        {
          "name": "Liwei Yao"
        },
        {
          "name": "Zhiyuan Yan"
        },
        {
          "name": "Jialiang Shen"
        },
        {
          "name": "ZhiJin Chen"
        },
        {
          "name": "Boyi Sun"
        },
        {
          "name": "Lubin Weng"
        },
        {
          "name": "Jing Dong"
        },
        {
          "name": "Yan Wang"
        },
        {
          "name": "Shu Wu"
        }
      ],
      "abstract": "The rapid progress of generative models has intensified the need for reliable and robust detection under real-world conditions. However, existing detectors often overfit to generator-specific artifacts and remain highly sensitive to real-world degradations. As generative architectures evolve and images undergo multi-round cross-platform sharing and post-processing (chain degradations), these artifact cues become obsolete and harder to detect. To address this, we propose Real-centric Envelope Modeling (REM), a new paradigm that shifts detection from learning generator artifacts to modeling the robust distribution of real images. REM introduces feature-level perturbations in self-reconstruction to generate near-real samples, and employs an envelope estimator with cross-domain consistency to learn a boundary enclosing the real image manifold. We further build RealChain, a comprehensive benchmark covering both open-source and commercial generators with simulated real-world degradation. Across eight benchmark evaluations, REM achieves an average improvement of 7.5% over state-of-the-art methods, and notably maintains exceptional generalization on the severely degraded RealChain benchmark, establishing a solid foundation for synthetic image detection under real-world conditions. The code and the RealChain benchmark will be made publicly available upon acceptance of the paper.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T04:41:04+00:00",
      "updated": "2025-12-24T04:41:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20937v1",
      "file": "papers/2512.20937v1.pdf"
    },
    {
      "arxiv_id": "2512.20936v1",
      "title": "Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation",
      "authors": [
        {
          "name": "Hongxing Fan"
        },
        {
          "name": "Shuyu Zhao"
        },
        {
          "name": "Jiayang Ao"
        },
        {
          "name": "Lu Sheng"
        }
      ],
      "abstract": "Amodal completion, the task of inferring invisible object parts, faces significant challenges in maintaining semantic consistency and structural integrity. Prior progressive approaches are inherently limited by inference instability and error accumulation. To tackle these limitations, we present a Collaborative Multi-Agent Reasoning Framework that explicitly decouples Semantic Planning from Visual Synthesis. By employing specialized agents for upfront reasoning, our method generates a structured, explicit plan before pixel generation, enabling visually and semantically coherent single-pass synthesis. We integrate this framework with two critical mechanisms: (1) a self-correcting Verification Agent that employs Chain-of-Thought reasoning to rectify visible region segmentation and identify residual occluders strictly within the Semantic Planning phase, and (2) a Diverse Hypothesis Generator that addresses the ambiguity of invisible regions by offering diverse, plausible semantic interpretations, surpassing the limited pixel-level variations of standard random seed sampling. Furthermore, addressing the limitations of traditional metrics in assessing inferred invisible content, we introduce the MAC-Score (MLLM Amodal Completion Score), a novel human-aligned evaluation metric. Validated against human judgment and ground truth, these metrics establish a robust standard for assessing structural completeness and semantic consistency with visible context. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods across multiple datasets. Our project is available at: https://fanhongxing.github.io/remac-page.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T04:39:45+00:00",
      "updated": "2025-12-24T04:39:45+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20936v1",
      "file": "papers/2512.20936v1.pdf"
    },
    {
      "arxiv_id": "2512.20934v1",
      "title": "Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning",
      "authors": [
        {
          "name": "Shengguang Wu"
        },
        {
          "name": "Xiaohan Wang"
        },
        {
          "name": "Yuhui Zhang"
        },
        {
          "name": "Hao Zhu"
        },
        {
          "name": "Serena Yeung-Levy"
        }
      ],
      "abstract": "Spatial reasoning in 3D scenes requires precise geometric calculations that challenge vision-language models. Visual programming addresses this by decomposing problems into steps calling specialized tools, yet existing methods rely on either fixed toolsets or speculative tool induction before solving problems, resulting in suboptimal programs and poor utilization of induced tools. We present Transductive Visual Programming (TVP), a novel framework that builds new tools from its own experience rather than speculation. TVP first solves problems using basic tools while accumulating experiential solutions into an Example Library, then abstracts recurring patterns from these programs into reusable higher-level tools for an evolving Tool Library. This allows TVP to tackle new problems with increasingly powerful tools learned from experience. On Omni3D-Bench, TVP achieves state-of-the-art performance, outperforming GPT-4o by 22% and the previous best visual programming system by 11%. Our transductively learned tools are used 5x more frequently as core program dependency than inductively created ones, demonstrating more effective tool discovery and reuse. The evolved tools also show strong generalization to unseen spatial tasks, achieving superior performance on benchmarks from SpatialScore-Hard collection without any testset-specific modification. Our work establishes experience-driven transductive tool creation as a powerful paradigm for building self-evolving visual programming agents that effectively tackle challenging spatial reasoning tasks. We release our code at https://transductive-visualprogram.github.io/.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "published": "2025-12-24T04:30:21+00:00",
      "updated": "2025-12-24T04:30:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20934v1",
      "file": "papers/2512.20934v1.pdf"
    },
    {
      "arxiv_id": "2512.20927v1",
      "title": "Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting",
      "authors": [
        {
          "name": "Yoonwoo Jeong"
        },
        {
          "name": "Cheng Sun"
        },
        {
          "name": "Frank Wang"
        },
        {
          "name": "Minsu Cho"
        },
        {
          "name": "Jaesung Choe"
        }
      ],
      "abstract": "Recent advancements in computer vision have successfully extended Open-vocabulary segmentation (OVS) to the 3D domain by leveraging 3D Gaussian Splatting (3D-GS). Despite this progress, efficiently rendering the high-dimensional features required for open-vocabulary queries poses a significant challenge. Existing methods employ codebooks or feature compression, causing information loss, thereby degrading segmentation quality. To address this limitation, we introduce Quantile Rendering (Q-Render), a novel rendering strategy for 3D Gaussians that efficiently handles high-dimensional features while maintaining high fidelity. Unlike conventional volume rendering, which densely samples all 3D Gaussians intersecting each ray, Q-Render sparsely samples only those with dominant influence along the ray. By integrating Q-Render into a generalizable 3D neural network, we also propose Gaussian Splatting Network (GS-Net), which predicts Gaussian features in a generalizable manner. Extensive experiments on ScanNet and LeRF demonstrate that our framework outperforms state-of-the-art methods, while enabling real-time rendering with an approximate ~43.7x speedup on 512-D feature maps. Code will be made publicly available.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T04:16:18+00:00",
      "updated": "2025-12-24T04:16:18+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20927v1",
      "file": "papers/2512.20927v1.pdf"
    },
    {
      "arxiv_id": "2512.20921v1",
      "title": "Self-supervised Multiplex Consensus Mamba for General Image Fusion",
      "authors": [
        {
          "name": "Yingying Wang"
        },
        {
          "name": "Rongjin Zhuang"
        },
        {
          "name": "Hui Zheng"
        },
        {
          "name": "Xuanhua He"
        },
        {
          "name": "Ke Cao"
        },
        {
          "name": "Xiaotong Tu"
        },
        {
          "name": "Xinghao Ding"
        }
      ],
      "abstract": "Image fusion integrates complementary information from different modalities to generate high-quality fused images, thereby enhancing downstream tasks such as object detection and semantic segmentation. Unlike task-specific techniques that primarily focus on consolidating inter-modal information, general image fusion needs to address a wide range of tasks while improving performance without increasing complexity. To achieve this, we propose SMC-Mamba, a Self-supervised Multiplex Consensus Mamba framework for general image fusion. Specifically, the Modality-Agnostic Feature Enhancement (MAFE) module preserves fine details through adaptive gating and enhances global representations via spatial-channel and frequency-rotational scanning. The Multiplex Consensus Cross-modal Mamba (MCCM) module enables dynamic collaboration among experts, reaching a consensus to efficiently integrate complementary information from multiple modalities. The cross-modal scanning within MCCM further strengthens feature interactions across modalities, facilitating seamless integration of critical information from both sources. Additionally, we introduce a Bi-level Self-supervised Contrastive Learning Loss (BSCL), which preserves high-frequency information without increasing computational overhead while simultaneously boosting performance in downstream tasks. Extensive experiments demonstrate that our approach outperforms state-of-the-art (SOTA) image fusion algorithms in tasks such as infrared-visible, medical, multi-focus, and multi-exposure fusion, as well as downstream visual tasks.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T03:57:21+00:00",
      "updated": "2025-12-24T03:57:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20921v1",
      "file": "papers/2512.20921v1.pdf"
    },
    {
      "arxiv_id": "2512.20907v1",
      "title": "PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding",
      "authors": [
        {
          "name": "Seongmin Jung"
        },
        {
          "name": "Seongho Choi"
        },
        {
          "name": "Gunwoo Jeon"
        },
        {
          "name": "Minsu Cho"
        },
        {
          "name": "Jongwoo Lim"
        }
      ],
      "abstract": "3D Visual Grounding (3DVG) is a critical bridge from vision-language perception to robotics, requiring both language understanding and 3D scene reasoning. Traditional supervised models leverage explicit 3D geometry but exhibit limited generalization, owing to the scarcity of 3D vision-language datasets and the limited reasoning capabilities compared to modern vision-language models (VLMs). We propose PanoGrounder, a generalizable 3DVG framework that couples multi-modal panoramic representation with pretrained 2D VLMs for strong vision-language reasoning. Panoramic renderings, augmented with 3D semantic and geometric features, serve as an intermediate representation between 2D and 3D, and offer two major benefits: (i) they can be directly fed to VLMs with minimal adaptation and (ii) they retain long-range object-to-object relations thanks to their 360-degree field of view. We devise a three-stage pipeline that places a compact set of panoramic viewpoints considering the scene layout and geometry, grounds a text query on each panoramic rendering with a VLM, and fuses per-view predictions into a single 3D bounding box via lifting. Our approach achieves state-of-the-art results on ScanRefer and Nr3D, and demonstrates superior generalization to unseen 3D datasets and text rephrasings.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T03:18:51+00:00",
      "updated": "2025-12-24T03:18:51+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20907v1",
      "file": "papers/2512.20907v1.pdf"
    },
    {
      "arxiv_id": "2512.20901v1",
      "title": "Benchmarking and Enhancing VLM for Compressed Image Understanding",
      "authors": [
        {
          "name": "Zifu Zhang"
        },
        {
          "name": "Tongda Xu"
        },
        {
          "name": "Siqi Li"
        },
        {
          "name": "Shengxi Li"
        },
        {
          "name": "Yue Zhang"
        },
        {
          "name": "Mai Xu"
        },
        {
          "name": "Yan Wang"
        }
      ],
      "abstract": "With the rapid development of Vision-Language Models (VLMs) and the growing demand for their applications, efficient compression of the image inputs has become increasingly important. Existing VLMs predominantly digest and understand high-bitrate compressed images, while their ability to interpret low-bitrate compressed images has yet to be explored by far. In this paper, we introduce the first comprehensive benchmark to evaluate the ability of VLM against compressed images, varying existing widely used image codecs and diverse set of tasks, encompassing over one million compressed images in our benchmark. Next, we analyse the source of performance gap, by categorising the gap from a) the information loss during compression and b) generalisation failure of VLM. We visualize these gaps with concrete examples and identify that for compressed images, only the generalization gap can be mitigated. Finally, we propose a universal VLM adaptor to enhance model performance on images compressed by existing codecs. Consequently, we demonstrate that a single adaptor can improve VLM performance across images with varying codecs and bitrates by 10%-30%. We believe that our benchmark and enhancement method provide valuable insights and contribute toward bridging the gap between VLMs and compressed images.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T02:59:01+00:00",
      "updated": "2025-12-24T02:59:01+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20901v1",
      "file": "papers/2512.20901v1.pdf"
    },
    {
      "arxiv_id": "2512.20898v1",
      "title": "DGSAN: Dual-Graph Spatiotemporal Attention Network for Pulmonary Nodule Malignancy Prediction",
      "authors": [
        {
          "name": "Xiao Yu"
        },
        {
          "name": "Zhaojie Fang"
        },
        {
          "name": "Guanyu Zhou"
        },
        {
          "name": "Yin Shen"
        },
        {
          "name": "Huoling Luo"
        },
        {
          "name": "Ye Li"
        },
        {
          "name": "Ahmed Elazab"
        },
        {
          "name": "Xiang Wan"
        },
        {
          "name": "Ruiquan Ge"
        },
        {
          "name": "Changmiao Wang"
        }
      ],
      "abstract": "Lung cancer continues to be the leading cause of cancer-related deaths globally. Early detection and diagnosis of pulmonary nodules are essential for improving patient survival rates. Although previous research has integrated multimodal and multi-temporal information, outperforming single modality and single time point, the fusion methods are limited to inefficient vector concatenation and simple mutual attention, highlighting the need for more effective multimodal information fusion. To address these challenges, we introduce a Dual-Graph Spatiotemporal Attention Network, which leverages temporal variations and multimodal data to enhance the accuracy of predictions. Our methodology involves developing a Global-Local Feature Encoder to better capture the local, global, and fused characteristics of pulmonary nodules. Additionally, a Dual-Graph Construction method organizes multimodal features into inter-modal and intra-modal graphs. Furthermore, a Hierarchical Cross-Modal Graph Fusion Module is introduced to refine feature integration. We also compiled a novel multimodal dataset named the NLST-cmst dataset as a comprehensive source of support for related research. Our extensive experiments, conducted on both the NLST-cmst and curated CSTL-derived datasets, demonstrate that our DGSAN significantly outperforms state-of-the-art methods in classifying pulmonary nodules with exceptional computational efficiency.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-24T02:47:22+00:00",
      "updated": "2025-12-24T02:47:22+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20898v1",
      "file": "papers/2512.20898v1.pdf"
    },
    {
      "arxiv_id": "2512.20892v1",
      "title": "Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification",
      "authors": [
        {
          "name": "Tingfeng Xian"
        },
        {
          "name": "Wenlve Zhou"
        },
        {
          "name": "Zhiheng Zhou"
        },
        {
          "name": "Zhelin Li"
        }
      ],
      "abstract": "Cross-Modality Ship Re-Identification (CMS Re-ID) is critical for achieving all-day and all-weather maritime target tracking, yet it is fundamentally challenged by significant modality discrepancies. Mainstream solutions typically rely on explicit modality alignment strategies; however, this paradigm heavily depends on constructing large-scale paired datasets for pre-training. To address this, grounded in the Platonic Representation Hypothesis, we explore the potential of Vision Foundation Models (VFMs) in bridging modality gaps. Recognizing the suboptimal performance of existing generic Parameter-Efficient Fine-Tuning (PEFT) methods that operate within the weight space, particularly on limited-capacity models, we shift the optimization perspective to the feature space and propose a novel PEFT strategy termed Domain Representation Injection (DRI). Specifically, while keeping the VFM fully frozen to maximize the preservation of general knowledge, we design a lightweight, learnable Offset Encoder to extract domain-specific representations rich in modality and identity attributes from raw inputs. Guided by the contextual information of intermediate features at different layers, a Modulator adaptively transforms these representations. Subsequently, they are injected into the intermediate layers via additive fusion, dynamically reshaping the feature distribution to adapt to the downstream task without altering the VFM's pre-trained weights. Extensive experimental results demonstrate the superiority of our method, achieving State-of-the-Art (SOTA) performance with minimal trainable parameters. For instance, on the HOSS-ReID dataset, we attain 57.9\\% and 60.5\\% mAP using only 1.54M and 7.05M parameters, respectively. The code is available at https://github.com/TingfengXian/DRI.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-24T02:30:23+00:00",
      "updated": "2025-12-24T02:30:23+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20892v1",
      "file": "papers/2512.20892v1.pdf"
    },
    {
      "arxiv_id": "2512.20871v1",
      "title": "NeRV360: Neural Representation for 360-Degree Videos with a Viewport Decoder",
      "authors": [
        {
          "name": "Daichi Arai"
        },
        {
          "name": "Kyohei Unno"
        },
        {
          "name": "Yasuko Sugito"
        },
        {
          "name": "Yuichi Kusakabe"
        }
      ],
      "abstract": "Implicit neural representations for videos (NeRV) have shown strong potential for video compression. However, applying NeRV to high-resolution 360-degree videos causes high memory usage and slow decoding, making real-time applications impractical. We propose NeRV360, an end-to-end framework that decodes only the user-selected viewport instead of reconstructing the entire panoramic frame. Unlike conventional pipelines, NeRV360 integrates viewport extraction into decoding and introduces a spatial-temporal affine transform module for conditional decoding based on viewpoint and time. Experiments on 6K-resolution videos show that NeRV360 achieves a 7-fold reduction in memory consumption and a 2.5-fold increase in decoding speed compared to HNeRV, a representative prior work, while delivering better image quality in terms of objective metrics.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.MM",
        "eess.IV"
      ],
      "published": "2025-12-24T01:21:25+00:00",
      "updated": "2025-12-24T01:21:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20871v1",
      "file": "papers/2512.20871v1.pdf"
    },
    {
      "arxiv_id": "2512.20866v1",
      "title": "Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images",
      "authors": [
        {
          "name": "Haotian Lv"
        },
        {
          "name": "Chao Li"
        },
        {
          "name": "Jiangbo Dai"
        },
        {
          "name": "Yuhui Zhang"
        },
        {
          "name": "Zepeng Fan"
        },
        {
          "name": "Yiqiu Tan"
        },
        {
          "name": "Dawei Wang"
        },
        {
          "name": "Binglei Xie"
        }
      ],
      "abstract": "To address the issues of weak correlation between multi-view features, low recognition accuracy of small-scale targets, and insufficient robustness in complex scenarios in underground pipeline detection using 3D GPR, this paper proposes a 3D pipeline intelligent detection framework. First, based on a B/C/D-Scan three-view joint analysis strategy, a three-dimensional pipeline three-view feature evaluation method is established by cross-validating forward simulation results obtained using FDTD methods with actual measurement data. Second, the DCO-YOLO framework is proposed, which integrates DySample, CGLU, and OutlookAttention cross-dimensional correlation mechanisms into the original YOLOv11 algorithm, significantly improving the small-scale pipeline edge feature extraction capability. Furthermore, a 3D-DIoU spatial feature matching algorithm is proposed, which integrates three-dimensional geometric constraints and center distance penalty terms to achieve automated association of multi-view annotations. The three-view fusion strategy resolves inherent ambiguities in single-view detection. Experiments based on real urban underground pipeline data show that the proposed method achieves accuracy, recall, and mean average precision of 96.2%, 93.3%, and 96.7%, respectively, in complex multi-pipeline scenarios, which are 2.0%, 2.1%, and 0.9% higher than the baseline model. Ablation experiments validated the synergistic optimization effect of the dynamic feature enhancement module and Grad-CAM++ heatmap visualization demonstrated that the improved model significantly enhanced its ability to focus on pipeline geometric features. This study integrates deep learning optimization strategies with the physical characteristics of 3D GPR, offering an efficient and reliable novel technical framework for the intelligent recognition and localization of underground pipelines.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-24T00:50:27+00:00",
      "updated": "2025-12-24T00:50:27+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20866v1",
      "file": "papers/2512.20866v1.pdf"
    },
    {
      "arxiv_id": "2512.20839v1",
      "title": "Input-Adaptive Visual Preprocessing for Efficient Fast Vision-Language Model Inference",
      "authors": [
        {
          "name": "Putu Indah Githa Cahyani"
        },
        {
          "name": "Komang David Dananjaya Suartana"
        },
        {
          "name": "Novanto Yudistira"
        }
      ],
      "abstract": "Vision-Language Models (VLMs) have demonstrated strong performance on multimodal reasoning tasks, but their deployment remains challenging due to high inference latency and computational cost, particularly when processing high-resolution visual inputs. While recent architectures such as FastVLM improve efficiency through optimized vision encoders, existing pipelines still rely on static visual preprocessing, leading to redundant computation for visually simple inputs. In this work, we propose an adaptive visual preprocessing method that dynamically adjusts input resolution and spatial coverage based on image content characteristics. The proposed approach combines content-aware image analysis, adaptive resolution selection, and content-aware cropping to reduce visual redundancy prior to vision encoding. Importantly, the method is integrated with FastVLM without modifying its architecture or requiring retraining. We evaluate the proposed method on a subset of the DocVQA dataset in an inference-only setting, focusing on efficiency-oriented metrics. Experimental results show that adaptive preprocessing reduces per-image inference time by over 50\\%, lowers mean full generation time, and achieves a consistent reduction of more than 55\\% in visual token count compared to the baseline pipeline. These findings demonstrate that input-aware preprocessing is an effective and lightweight strategy for improving deployment-oriented efficiency of vision-language models. To facilitate reproducibility, our implementation is provided as a fork of the FastVLM repository, incorporating the files for the proposed method, and is available at https://github.com/kmdavidds/mlfastlm.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T23:30:56+00:00",
      "updated": "2025-12-23T23:30:56+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20839v1",
      "file": "papers/2512.20839v1.pdf"
    },
    {
      "arxiv_id": "2512.20833v1",
      "title": "CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images",
      "authors": [
        {
          "name": "Vidit Agrawal"
        },
        {
          "name": "John Peters"
        },
        {
          "name": "Tyler N. Thompson"
        },
        {
          "name": "Mohammad Vali Sanian"
        },
        {
          "name": "Chau Pham"
        },
        {
          "name": "Nikita Moshkov"
        },
        {
          "name": "Arshad Kazi"
        },
        {
          "name": "Aditya Pillai"
        },
        {
          "name": "Jack Freeman"
        },
        {
          "name": "Byunguk Kang"
        },
        {
          "name": "Samouil L. Farhi"
        },
        {
          "name": "Ernest Fraenkel"
        },
        {
          "name": "Ron Stewart"
        },
        {
          "name": "Lassi Paavolainen"
        },
        {
          "name": "Bryan A. Plummer"
        },
        {
          "name": "Juan C. Caicedo"
        }
      ],
      "abstract": "Quantifying cell morphology using images and machine learning has proven to be a powerful tool to study the response of cells to treatments. However, models used to quantify cellular morphology are typically trained with a single microscopy imaging type. This results in specialized models that cannot be reused across biological studies because the technical specifications do not match (e.g., different number of channels), or because the target experimental conditions are out of distribution. Here, we present CHAMMI-75, an open access dataset of heterogeneous, multi-channel microscopy images from 75 diverse biological studies. We curated this resource from publicly available sources to investigate cellular morphology models that are channel-adaptive and can process any microscopy image type. Our experiments show that training with CHAMMI-75 can improve performance in multi-channel bioimaging tasks primarily because of its high diversity in microscopy modalities. This work paves the way to create the next generation of cellular morphology models for biological studies.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-12-23T23:15:10+00:00",
      "updated": "2025-12-23T23:15:10+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20833v1",
      "file": "papers/2512.20833v1.pdf"
    },
    {
      "arxiv_id": "2512.20815v1",
      "title": "Learning to Sense for Driving: Joint Optics-Sensor-Model Co-Design for Semantic Segmentation",
      "authors": [
        {
          "name": "Reeshad Khan amd John Gauch"
        }
      ],
      "abstract": "Traditional autonomous driving pipelines decouple camera design from downstream perception, relying on fixed optics and handcrafted ISPs that prioritize human viewable imagery rather than machine semantics. This separation discards information during demosaicing, denoising, or quantization, while forcing models to adapt to sensor artifacts. We present a task-driven co-design framework that unifies optics, sensor modeling, and lightweight semantic segmentation networks into a single end-to-end RAW-to-task pipeline. Building on DeepLens[19], our system integrates realistic cellphone-scale lens models, learnable color filter arrays, Poisson-Gaussian noise processes, and quantization, all optimized directly for segmentation objectives. Evaluations on KITTI-360 show consistent mIoU improvements over fixed pipelines, with optics modeling and CFA learning providing the largest gains, especially for thin or low-light-sensitive classes. Importantly, these robustness gains are achieved with a compact ~1M-parameter model running at ~28 FPS, demonstrating edge deployability. Visual and quantitative analyses further highlight how co-designed sensors adapt acquisition to semantic structure, sharpening boundaries and maintaining accuracy under blur, noise, and low bit-depth. Together, these findings establish full-stack co-optimization of optics, sensors, and networks as a principled path toward efficient, reliable, and deployable perception in autonomous systems.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T22:28:30+00:00",
      "updated": "2025-12-23T22:28:30+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20815v1",
      "file": "papers/2512.20815v1.pdf"
    },
    {
      "arxiv_id": "2512.20783v1",
      "title": "NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts",
      "authors": [
        {
          "name": "Raja Mallina"
        },
        {
          "name": "Bryar Shareef"
        }
      ],
      "abstract": "Breast ultrasound (BUS) segmentation provides lesion boundaries essential for computer-aided diagnosis and treatment planning. While promptable methods can improve segmentation performance and tumor delineation when text or spatial prompts are available, many public BUS datasets lack reliable metadata or reports, constraining training to small multimodal subsets and reducing robustness. We propose NullBUS, a multimodal mixed-supervision framework that learns from images with and without prompts in a single model. To handle missing text, we introduce nullable prompts, implemented as learnable null embeddings with presence masks, enabling fallback to image-only evidence when metadata are absent and the use of text when present. Evaluated on a unified pool of three public BUS datasets, NullBUS achieves a mean IoU of 0.8568 and a mean Dice of 0.9103, demonstrating state-of-the-art performance under mixed prompt availability.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-23T21:30:05+00:00",
      "updated": "2025-12-23T21:30:05+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20783v1",
      "file": "papers/2512.20783v1.pdf"
    },
    {
      "arxiv_id": "2512.20770v1",
      "title": "OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective",
      "authors": [
        {
          "name": "Markus Gross"
        },
        {
          "name": "Sai B. Matha"
        },
        {
          "name": "Aya Fahmy"
        },
        {
          "name": "Rui Song"
        },
        {
          "name": "Daniel Cremers"
        },
        {
          "name": "Henri Meess"
        }
      ],
      "abstract": "Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T21:14:55+00:00",
      "updated": "2025-12-23T21:14:55+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20770v1",
      "file": "papers/2512.20770v1.pdf"
    },
    {
      "arxiv_id": "2512.20746v1",
      "title": "TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection",
      "authors": [
        {
          "name": "Tony Tran"
        },
        {
          "name": "Bin Hu"
        }
      ],
      "abstract": "This paper addresses trash detection on the TACO dataset under strict TinyML constraints using an iterative hardware-aware neural architecture search framework targeting edge and IoT devices. The proposed method constructs a Once-for-All-style ResDets supernet and performs iterative evolutionary search that alternates between backbone and neck/head optimization, supported by a population passthrough mechanism and an accuracy predictor to reduce search cost and improve stability. This framework yields a family of deployment-ready detectors, termed TrashDets. On a five-class TACO subset (paper, plastic, bottle, can, cigarette), the strongest variant, TrashDet-l, achieves 19.5 mAP50 with 30.5M parameters, improving accuracy by up to 3.6 mAP50 over prior detectors while using substantially fewer parameters. The TrashDet family spans 1.2M to 30.5M parameters with mAP50 values between 11.4 and 19.5, providing scalable detector options for diverse TinyML deployment budgets on resource-constrained hardware. On the MAX78002 microcontroller with the TrashNet dataset, two specialized variants, TrashDet-ResNet and TrashDet-MBNet, jointly dominate the ai87-fpndetector baseline, with TrashDet-ResNet achieving 7525~$μ$J energy per inference at 26.7 ms latency and 37.45 FPS, and TrashDet-MBNet improving mAP50 by 10.2%; together they reduce energy consumption by up to 88%, latency by up to 78%, and average power by up to 53% compared to existing TinyML detectors.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-12-23T20:00:34+00:00",
      "updated": "2025-12-23T20:00:34+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20746v1",
      "file": "papers/2512.20746v1.pdf"
    },
    {
      "arxiv_id": "2512.20735v1",
      "title": "VL4Gaze: Unleashing Vision-Language Models for Gaze Following",
      "authors": [
        {
          "name": "Shijing Wang"
        },
        {
          "name": "Chaoqun Cui"
        },
        {
          "name": "Yaping Huang"
        },
        {
          "name": "Hyung Jin Chang"
        },
        {
          "name": "Yihua Cheng"
        }
      ],
      "abstract": "Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T19:47:11+00:00",
      "updated": "2025-12-23T19:47:11+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20735v1",
      "file": "papers/2512.20735v1.pdf"
    },
    {
      "arxiv_id": "2512.20619v2",
      "title": "SemanticGen: Video Generation in Semantic Space",
      "authors": [
        {
          "name": "Jianhong Bai"
        },
        {
          "name": "Xiaoshi Wu"
        },
        {
          "name": "Xintao Wang"
        },
        {
          "name": "Xiao Fu"
        },
        {
          "name": "Yuanxing Zhang"
        },
        {
          "name": "Qinghe Wang"
        },
        {
          "name": "Xiaoyu Shi"
        },
        {
          "name": "Menghan Xia"
        },
        {
          "name": "Zuozhu Liu"
        },
        {
          "name": "Haoji Hu"
        },
        {
          "name": "Pengfei Wan"
        },
        {
          "name": "Kun Gai"
        }
      ],
      "abstract": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T18:59:56+00:00",
      "updated": "2025-12-24T11:39:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20619v2",
      "file": "papers/2512.20619v2.pdf"
    },
    {
      "arxiv_id": "2512.20618v1",
      "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
      "authors": [
        {
          "name": "Runtao Liu"
        },
        {
          "name": "Ziyi Liu"
        },
        {
          "name": "Jiaqi Tang"
        },
        {
          "name": "Yue Ma"
        },
        {
          "name": "Renjie Pi"
        },
        {
          "name": "Jipeng Zhang"
        },
        {
          "name": "Qifeng Chen"
        }
      ],
      "abstract": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MA"
      ],
      "published": "2025-12-23T18:59:49+00:00",
      "updated": "2025-12-23T18:59:49+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20618v1",
      "file": "papers/2512.20618v1.pdf"
    },
    {
      "arxiv_id": "2512.20617v1",
      "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
      "authors": [
        {
          "name": "Yuxi Xiao"
        },
        {
          "name": "Longfei Li"
        },
        {
          "name": "Shen Yan"
        },
        {
          "name": "Xinhang Liu"
        },
        {
          "name": "Sida Peng"
        },
        {
          "name": "Yunchao Wei"
        },
        {
          "name": "Xiaowei Zhou"
        },
        {
          "name": "Bingyi Kang"
        }
      ],
      "abstract": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T18:59:46+00:00",
      "updated": "2025-12-23T18:59:46+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20617v1",
      "file": "papers/2512.20617v1.pdf"
    },
    {
      "arxiv_id": "2512.20615v1",
      "title": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
      "authors": [
        {
          "name": "Xuanhua He"
        },
        {
          "name": "Tianyu Yang"
        },
        {
          "name": "Ke Cao"
        },
        {
          "name": "Ruiqi Wu"
        },
        {
          "name": "Cheng Meng"
        },
        {
          "name": "Yong Zhang"
        },
        {
          "name": "Zhuoliang Kang"
        },
        {
          "name": "Xiaoming Wei"
        },
        {
          "name": "Qifeng Chen"
        }
      ],
      "abstract": "Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T18:59:16+00:00",
      "updated": "2025-12-23T18:59:16+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20615v1",
      "file": "papers/2512.20615v1.pdf"
    },
    {
      "arxiv_id": "2512.20606v1",
      "title": "Repurposing Video Diffusion Transformers for Robust Point Tracking",
      "authors": [
        {
          "name": "Soowon Son"
        },
        {
          "name": "Honggyu An"
        },
        {
          "name": "Chaehyun Kim"
        },
        {
          "name": "Hyunah Ko"
        },
        {
          "name": "Jisu Nam"
        },
        {
          "name": "Dahyun Chung"
        },
        {
          "name": "Siyoon Jin"
        },
        {
          "name": "Jung Yi"
        },
        {
          "name": "Jaewon Min"
        },
        {
          "name": "Junhwa Hur"
        },
        {
          "name": "Seungryong Kim"
        }
      ],
      "abstract": "Point tracking aims to localize corresponding points across video frames, serving as a fundamental task for 4D reconstruction, robotics, and video editing. Existing methods commonly rely on shallow convolutional backbones such as ResNet that process frames independently, lacking temporal coherence and producing unreliable matching costs under challenging conditions. Through systematic analysis, we find that video Diffusion Transformers (DiTs), pre-trained on large-scale real-world videos with spatio-temporal attention, inherently exhibit strong point tracking capability and robustly handle dynamic motions and frequent occlusions. We propose DiTracker, which adapts video DiTs through: (1) query-key attention matching, (2) lightweight LoRA tuning, and (3) cost fusion with a ResNet backbone. Despite training with 8 times smaller batch size, DiTracker achieves state-of-the-art performance on challenging ITTO benchmark and matches or outperforms state-of-the-art models on TAP-Vid benchmarks. Our work validates video DiT features as an effective and efficient foundation for point tracking.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T18:54:10+00:00",
      "updated": "2025-12-23T18:54:10+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20606v1",
      "file": "papers/2512.20606v1.pdf"
    },
    {
      "arxiv_id": "2512.20561v1",
      "title": "FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models",
      "authors": [
        {
          "name": "Kaitong Cai"
        },
        {
          "name": "Jusheng Zhang"
        },
        {
          "name": "Jing Yang"
        },
        {
          "name": "Yijia Fan"
        },
        {
          "name": "Pengtao Xie"
        },
        {
          "name": "Jian Wang"
        },
        {
          "name": "Keze Wang"
        }
      ],
      "abstract": "Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.\n  We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying on noisy attention weights, FlashVLM computes an explicit cross modal similarity between projected image tokens and normalized text embeddings in the language model space. This extrinsic relevance is fused with intrinsic visual saliency using log domain weighting and temperature controlled sharpening. In addition, a diversity preserving partition retains a minimal yet representative set of background tokens to maintain global context.\n  Under identical token budgets and evaluation protocols, FlashVLM achieves beyond lossless compression, slightly surpassing the unpruned baseline while pruning up to 77.8 percent of visual tokens on LLaVA 1.5, and maintaining 92.8 percent accuracy even under 94.4 percent compression. Extensive experiments on 14 image and video benchmarks demonstrate that FlashVLM delivers state of the art efficiency performance trade offs while maintaining strong robustness and generalization across mainstream VLMs.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T18:05:43+00:00",
      "updated": "2025-12-23T18:05:43+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20561v1",
      "file": "papers/2512.20561v1.pdf"
    },
    {
      "arxiv_id": "2512.20557v1",
      "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models",
      "authors": [
        {
          "name": "Shengchao Zhou"
        },
        {
          "name": "Yuxin Chen"
        },
        {
          "name": "Yuying Ge"
        },
        {
          "name": "Wei Huang"
        },
        {
          "name": "Jiehong Lin"
        },
        {
          "name": "Ying Shan"
        },
        {
          "name": "Xiaojuan Qi"
        }
      ],
      "abstract": "Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T17:56:36+00:00",
      "updated": "2025-12-23T17:56:36+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20557v1",
      "file": "papers/2512.20557v1.pdf"
    },
    {
      "arxiv_id": "2512.20556v1",
      "title": "Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios",
      "authors": [
        {
          "name": "Mingwei Tang"
        },
        {
          "name": "Jiahao Nie"
        },
        {
          "name": "Guang Yang"
        },
        {
          "name": "Ziqing Cui"
        },
        {
          "name": "Jie Li"
        }
      ],
      "abstract": "Image fusion aims to synthesize a single high-quality image from a pair of inputs captured under challenging conditions, such as differing exposure levels or focal depths. A core challenge lies in effectively handling disparities in dynamic range and focus depth between the inputs. With the advent of vision-language models, recent methods incorporate textual descriptions as auxiliary guidance to enhance fusion quality. However, simply incorporating coarse-grained descriptions hampers the understanding of fine-grained details and poses challenges for precise cross-modal alignment. To address these limitations, we propose Multi-grained Text-guided Image Fusion (MTIF), a novel fusion paradigm with three key designs. First, it introduces multi-grained textual descriptions that separately capture fine details, structural cues, and semantic content, guiding image fusion through a hierarchical cross-modal modulation module. Second, it involves supervision signals at each granularity to facilitate alignment between visual and textual features and enhance the utility of auxiliary text. Third, it adopts a saliency-driven enrichment module to augment training data with dense semantic content, further strengthening the cross-modal modulation and alignment. Extensive experiments show that MTIF consistently outperforms previous methods on both multi-exposure and multi-focus image fusion tasks.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T17:55:35+00:00",
      "updated": "2025-12-23T17:55:35+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20556v1",
      "file": "papers/2512.20556v1.pdf"
    },
    {
      "arxiv_id": "2512.20538v1",
      "title": "AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment",
      "authors": [
        {
          "name": "Anna Šárová Mikeštíková"
        },
        {
          "name": "Médéric Fourmy"
        },
        {
          "name": "Martin Cífka"
        },
        {
          "name": "Josef Sivic"
        },
        {
          "name": "Vladimir Petrik"
        }
      ],
      "abstract": "Single-view RGB model-based object pose estimation methods achieve strong generalization but are fundamentally limited by depth ambiguity, clutter, and occlusions. Multi-view pose estimation methods have the potential to solve these issues, but existing works rely on precise single-view pose estimates or lack generalization to unseen objects. We address these challenges via the following three contributions. First, we introduce AlignPose, a 6D object pose estimation method that aggregates information from multiple extrinsically calibrated RGB views and does not require any object-specific training or symmetry annotation. Second, the key component of this approach is a new multi-view feature-metric refinement specifically designed for object pose. It optimizes a single, consistent world-frame object pose minimizing the feature discrepancy between on-the-fly rendered object features and observed image features across all views simultaneously. Third, we report extensive experiments on four datasets (YCB-V, T-LESS, ITODD-MV, HouseCat6D) using the BOP benchmark evaluation and show that AlignPose outperforms other published methods, especially on challenging industrial datasets where multiple views are readily available in practice.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T17:29:08+00:00",
      "updated": "2025-12-23T17:29:08+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20538v1",
      "file": "papers/2512.20538v1.pdf"
    },
    {
      "arxiv_id": "2512.20531v1",
      "title": "SirenPose: Dynamic Scene Reconstruction via Geometric Supervision",
      "authors": [
        {
          "name": "Kaitong Cai"
        },
        {
          "name": "Jensen Zhang"
        },
        {
          "name": "Jing Yang"
        },
        {
          "name": "Keze Wang"
        }
      ],
      "abstract": "We introduce SirenPose, a geometry-aware loss formulation that integrates the periodic activation properties of sinusoidal representation networks with keypoint-based geometric supervision, enabling accurate and temporally consistent reconstruction of dynamic 3D scenes from monocular videos. Existing approaches often struggle with motion fidelity and spatiotemporal coherence in challenging settings involving fast motion, multi-object interaction, occlusion, and rapid scene changes. SirenPose incorporates physics inspired constraints to enforce coherent keypoint predictions across both spatial and temporal dimensions, while leveraging high frequency signal modeling to capture fine grained geometric details. We further expand the UniKPT dataset to 600,000 annotated instances and integrate graph neural networks to model keypoint relationships and structural correlations. Extensive experiments on benchmarks including Sintel, Bonn, and DAVIS demonstrate that SirenPose consistently outperforms state-of-the-art methods. On DAVIS, SirenPose achieves a 17.8 percent reduction in FVD, a 28.7 percent reduction in FID, and a 6.0 percent improvement in LPIPS compared to MoSCA. It also improves temporal consistency, geometric accuracy, user score, and motion smoothness. In pose estimation, SirenPose outperforms Monst3R with lower absolute trajectory error as well as reduced translational and rotational relative pose error, highlighting its effectiveness in handling rapid motion, complex dynamics, and physically plausible reconstruction.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T17:23:21+00:00",
      "updated": "2025-12-23T17:23:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20531v1",
      "file": "papers/2512.20531v1.pdf"
    },
    {
      "arxiv_id": "2512.20501v1",
      "title": "Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition",
      "authors": [
        {
          "name": "Gorjan Radevski"
        }
      ],
      "abstract": "This manuscript explores multimodal alignment, translation, fusion, and transference to enhance machine understanding of complex inputs. We organize the work into five chapters, each addressing unique challenges in multimodal machine learning.\n  Chapter 3 introduces Spatial-Reasoning Bert for translating text-based spatial relations into 2D arrangements between clip-arts. This enables effective decoding of spatial language into visual representations, paving the way for automated scene generation aligned with human spatial understanding.\n  Chapter 4 presents a method for translating medical texts into specific 3D locations within an anatomical atlas. We introduce a loss function leveraging spatial co-occurrences of medical terms to create interpretable mappings, significantly enhancing medical text navigability.\n  Chapter 5 tackles translating structured text into canonical facts within knowledge graphs. We develop a benchmark for linking natural language to entities and predicates, addressing ambiguities in text extraction to provide clearer, actionable insights.\n  Chapter 6 explores multimodal fusion methods for compositional action recognition. We propose a method fusing video frames and object detection representations, improving recognition robustness and accuracy.\n  Chapter 7 investigates multimodal knowledge transference for egocentric action recognition. We demonstrate how multimodal knowledge distillation enables RGB-only models to mimic multimodal fusion-based capabilities, reducing computational requirements while maintaining performance.\n  These contributions advance methodologies for spatial language understanding, medical text interpretation, knowledge graph enrichment, and action recognition, enhancing computational systems' ability to process complex, multimodal inputs across diverse applications.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T16:46:58+00:00",
      "updated": "2025-12-23T16:46:58+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20501v1",
      "file": "papers/2512.20501v1.pdf"
    },
    {
      "arxiv_id": "2512.20487v1",
      "title": "Multi-temporal Adaptive Red-Green-Blue and Long-Wave Infrared Fusion for You Only Look Once-Based Landmine Detection from Unmanned Aerial Systems",
      "authors": [
        {
          "name": "James E. Gallagher"
        },
        {
          "name": "Edward J. Oughton"
        },
        {
          "name": "Jana Kosecka"
        }
      ],
      "abstract": "Landmines remain a persistent humanitarian threat, with 110 million actively deployed mines across 60 countries, claiming 26,000 casualties annually. This research evaluates adaptive Red-Green-Blue (RGB) and Long-Wave Infrared (LWIR) fusion for Unmanned Aerial Systems (UAS)-based detection of surface-laid landmines, leveraging the thermal contrast between the ordnance and the surrounding soil to enhance feature extraction. Using You Only Look Once (YOLO) architectures (v8, v10, v11) across 114 test images, generating 35,640 model-condition evaluations, YOLOv11 achieved optimal performance (86.8% mAP), with 10 to 30% thermal fusion at 5 to 10m altitude identified as the optimal detection parameters. A complementary architectural comparison revealed that while RF-DETR achieved the highest accuracy (69.2% mAP), followed by Faster R-CNN (67.6%), YOLOv11 (64.2%), and RetinaNet (50.2%), YOLOv11 trained 17.7 times faster than the transformer-based RF-DETR (41 minutes versus 12 hours), presenting a critical accuracy-efficiency tradeoff for operational deployment. Aggregated multi-temporal training datasets outperformed season-specific approaches by 1.8 to 9.6%, suggesting that models benefit from exposure to diverse thermal conditions. Anti-Tank (AT) mines achieved 61.9% detection accuracy, compared with 19.2% for Anti-Personnel (AP) mines, reflecting both the size differential and thermal-mass differences between these ordnance classes. As this research examined surface-laid mines where thermal contrast is maximized, future research should quantify thermal contrast effects for mines buried at varying depths across heterogeneous soil types.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T16:26:47+00:00",
      "updated": "2025-12-23T16:26:47+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20487v1",
      "file": "papers/2512.20487v1.pdf"
    },
    {
      "arxiv_id": "2512.20479v1",
      "title": "UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images",
      "authors": [
        {
          "name": "Yiming Zhao"
        },
        {
          "name": "Yuanpeng Gao"
        },
        {
          "name": "Yuxuan Luo"
        },
        {
          "name": "Jiwei Duan"
        },
        {
          "name": "Shisong Lin"
        },
        {
          "name": "Longfei Xiong"
        },
        {
          "name": "Zhouhui Lian"
        }
      ],
      "abstract": "AI-assisted graphic design has emerged as a powerful tool for automating the creation and editing of design elements such as posters, banners, and advertisements. While diffusion-based text-to-image models have demonstrated strong capabilities in visual content generation, their text rendering performance, particularly for small-scale typography and non-Latin scripts, remains limited. In this paper, we propose UTDesign, a unified framework for high-precision stylized text editing and conditional text generation in design images, supporting both English and Chinese scripts. Our framework introduces a novel DiT-based text style transfer model trained from scratch on a synthetic dataset, capable of generating transparent RGBA text foregrounds that preserve the style of reference glyphs. We further extend this model into a conditional text generation framework by training a multi-modal condition encoder on a curated dataset with detailed text annotations, enabling accurate, style-consistent text synthesis conditioned on background images, prompts, and layout specifications. Finally, we integrate our approach into a fully automated text-to-design (T2D) pipeline by incorporating pre-trained text-to-image (T2I) models and an MLLM-based layout planner. Extensive experiments demonstrate that UTDesign achieves state-of-the-art performance among open-source methods in terms of stylistic consistency and text accuracy, and also exhibits unique advantages compared to proprietary commercial approaches. Code and data for this paper are available at https://github.com/ZYM-PKU/UTDesign.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T16:13:55+00:00",
      "updated": "2025-12-23T16:13:55+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20479v1",
      "file": "papers/2512.20479v1.pdf"
    },
    {
      "arxiv_id": "2512.20451v1",
      "title": "Beyond Motion Pattern: An Empirical Study of Physical Forces for Human Motion Understanding",
      "authors": [
        {
          "name": "Anh Dao"
        },
        {
          "name": "Manh Tran"
        },
        {
          "name": "Yufei Zhang"
        },
        {
          "name": "Xiaoming Liu"
        },
        {
          "name": "Zijun Cui"
        }
      ],
      "abstract": "Human motion understanding has advanced rapidly through vision-based progress in recognition, tracking, and captioning. However, most existing methods overlook physical cues such as joint actuation forces that are fundamental in biomechanics. This gap motivates our study: if and when do physically inferred forces enhance motion understanding? By incorporating forces into established motion understanding pipelines, we systematically evaluate their impact across baseline models on 3 major tasks: gait recognition, action recognition, and fine-grained video captioning. Across 8 benchmarks, incorporating forces yields consistent performance gains; for example, on CASIA-B, Rank-1 gait recognition accuracy improved from 89.52% to 90.39% (+0.87), with larger gain observed under challenging conditions: +2.7% when wearing a coat and +3.0% at the side view. On Gait3D, performance also increases from 46.0% to 47.3% (+1.3). In action recognition, CTR-GCN achieved +2.00% on Penn Action, while high-exertion classes like punching/slapping improved by +6.96%. Even in video captioning, Qwen2.5-VL's ROUGE-L score rose from 0.310 to 0.339 (+0.029), indicating that physics-inferred forces enhance temporal grounding and semantic richness. These results demonstrate that force cues can substantially complement visual and kinematic features under dynamic, occluded, or appearance-varying conditions.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T15:43:48+00:00",
      "updated": "2025-12-23T15:43:48+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20451v1",
      "file": "papers/2512.20451v1.pdf"
    },
    {
      "arxiv_id": "2512.20436v1",
      "title": "Dual-Encoder Transformer-Based Multimodal Learning for Ischemic Stroke Lesion Segmentation Using Diffusion MRI",
      "authors": [
        {
          "name": "Muhammad Usman"
        },
        {
          "name": "Azka Rehman"
        },
        {
          "name": "Muhammad Mutti Ur Rehman"
        },
        {
          "name": "Abd Ur Rehman"
        },
        {
          "name": "Muhammad Umar Farooq"
        }
      ],
      "abstract": "Accurate segmentation of ischemic stroke lesions from diffusion magnetic resonance imaging (MRI) is essential for clinical decision-making and outcome assessment. Diffusion-Weighted Imaging (DWI) and Apparent Diffusion Coefficient (ADC) scans provide complementary information on acute and sub-acute ischemic changes; however, automated lesion delineation remains challenging due to variability in lesion appearance.\n  In this work, we study ischemic stroke lesion segmentation using multimodal diffusion MRI from the ISLES 2022 dataset. Several state-of-the-art convolutional and transformer-based architectures, including U-Net variants, Swin-UNet, and TransUNet, are benchmarked. Based on performance, a dual-encoder TransUNet architecture is proposed to learn modality-specific representations from DWI and ADC inputs. To incorporate spatial context, adjacent slice information is integrated using a three-slice input configuration.\n  All models are trained under a unified framework and evaluated using the Dice Similarity Coefficient (DSC). Results show that transformer-based models outperform convolutional baselines, and the proposed dual-encoder TransUNet achieves the best performance, reaching a Dice score of 85.4% on the test set. The proposed framework offers a robust solution for automated ischemic stroke lesion segmentation from diffusion MRI.",
      "primary_category": "eess.IV",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-12-23T15:24:31+00:00",
      "updated": "2025-12-23T15:24:31+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20436v1",
      "file": "papers/2512.20436v1.pdf"
    },
    {
      "arxiv_id": "2512.20432v1",
      "title": "High Dimensional Data Decomposition for Anomaly Detection of Textured Images",
      "authors": [
        {
          "name": "Ji Song"
        },
        {
          "name": "Xing Wang"
        },
        {
          "name": "Jianguo Wu"
        },
        {
          "name": "Xiaowei Yue"
        }
      ],
      "abstract": "In the realm of diverse high-dimensional data, images play a significant role across various processes of manufacturing systems where efficient image anomaly detection has emerged as a core technology of utmost importance. However, when applied to textured defect images, conventional anomaly detection methods have limitations including non-negligible misidentification, low robustness, and excessive reliance on large-scale and structured datasets. This paper proposes a texture basis integrated smooth decomposition (TBSD) approach, which is targeted at efficient anomaly detection in textured images with smooth backgrounds and sparse anomalies. Mathematical formulation of quasi-periodicity and its theoretical properties are investigated for image texture estimation. TBSD method consists of two principal processes: the first process learns the texture basis functions to effectively extract quasi-periodic texture patterns; the subsequent anomaly detection process utilizes that texture basis as prior knowledge to prevent texture misidentification and capture potential anomalies with high accuracy.The proposed method surpasses benchmarks with less misidentification, smaller training dataset requirement, and superior anomaly detection performance on both simulation and real-world datasets.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "stat.ML"
      ],
      "published": "2025-12-23T15:21:18+00:00",
      "updated": "2025-12-23T15:21:18+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20432v1",
      "file": "papers/2512.20432v1.pdf"
    },
    {
      "arxiv_id": "2512.20431v1",
      "title": "Skin Lesion Classification Using a Soft Voting Ensemble of Convolutional Neural Networks",
      "authors": [
        {
          "name": "Abdullah Al Shafi"
        },
        {
          "name": "Abdul Muntakim"
        },
        {
          "name": "Pintu Chandra Shill"
        },
        {
          "name": "Rowzatul Zannat"
        },
        {
          "name": "Abdullah Al-Amin"
        }
      ],
      "abstract": "Skin cancer can be identified by dermoscopic examination and ocular inspection, but early detection significantly increases survival chances. Artificial intelligence (AI), using annotated skin images and Convolutional Neural Networks (CNNs), improves diagnostic accuracy. This paper presents an early skin cancer classification method using a soft voting ensemble of CNNs. In this investigation, three benchmark datasets, namely HAM10000, ISIC 2016, and ISIC 2019, were used. The process involved rebalancing, image augmentation, and filtering techniques, followed by a hybrid dual encoder for segmentation via transfer learning. Accurate segmentation focused classification models on clinically significant features, reducing background artifacts and improving accuracy. Classification was performed through an ensemble of MobileNetV2, VGG19, and InceptionV3, balancing accuracy and speed for real-world deployment. The method achieved lesion recognition accuracies of 96.32\\%, 90.86\\%, and 93.92\\% for the three datasets. The system performance was evaluated using established skin lesion detection metrics, yielding impressive results.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T15:20:47+00:00",
      "updated": "2025-12-23T15:20:47+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20431v1",
      "file": "papers/2512.20431v1.pdf"
    },
    {
      "arxiv_id": "2512.20420v1",
      "title": "Simplifying Multi-Task Architectures Through Task-Specific Normalization",
      "authors": [
        {
          "name": "Mihai Suteu"
        },
        {
          "name": "Ovidiu Serban"
        }
      ],
      "abstract": "Multi-task learning (MTL) aims to leverage shared knowledge across tasks to improve generalization and parameter efficiency, yet balancing resources and mitigating interference remain open challenges. Architectural solutions often introduce elaborate task-specific modules or routing schemes, increasing complexity and overhead. In this work, we show that normalization layers alone are sufficient to address many of these challenges. Simply replacing shared normalization with task-specific variants already yields competitive performance, questioning the need for complex designs. Building on this insight, we propose Task-Specific Sigmoid Batch Normalization (TS$σ$BN), a lightweight mechanism that enables tasks to softly allocate network capacity while fully sharing feature extractors. TS$σ$BN improves stability across CNNs and Transformers, matching or exceeding performance on NYUv2, Cityscapes, CelebA, and PascalContext, while remaining highly parameter-efficient. Moreover, its learned gates provide a natural framework for analyzing MTL dynamics, offering interpretable insights into capacity allocation, filter specialization, and task relationships. Our findings suggest that complex MTL architectures may be unnecessary and that task-specific normalization offers a simple, interpretable, and efficient alternative.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-12-23T15:02:12+00:00",
      "updated": "2025-12-23T15:02:12+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20420v1",
      "file": "papers/2512.20420v1.pdf"
    },
    {
      "arxiv_id": "2512.20417v1",
      "title": "Chain-of-Anomaly Thoughts with Large Vision-Language Models",
      "authors": [
        {
          "name": "Pedro Domingos"
        },
        {
          "name": "João Pereira"
        },
        {
          "name": "Vasco Lopes"
        },
        {
          "name": "João Neves"
        },
        {
          "name": "David Semedo"
        }
      ],
      "abstract": "Automated video surveillance with Large Vision-Language Models is limited by their inherent bias towards normality, often failing to detect crimes. While Chain-of-Thought reasoning strategies show significant potential for improving performance in language tasks, the lack of inductive anomaly biases in their reasoning further steers the models towards normal interpretations. To address this, we propose Chain-of-Anomaly-Thoughts (CoAT), a multi-agent reasoning framework that introduces inductive criminal bias in the reasoning process through a final, anomaly-focused classification layer. Our method significantly improves Anomaly Detection, boosting F1-score by 11.8 p.p. on challenging low-resolution footage and Anomaly Classification by 3.78 p.p. in high-resolution videos.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.MA"
      ],
      "published": "2025-12-23T15:01:05+00:00",
      "updated": "2025-12-23T15:01:05+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20417v1",
      "file": "papers/2512.20417v1.pdf"
    },
    {
      "arxiv_id": "2512.20409v1",
      "title": "DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning",
      "authors": [
        {
          "name": "Junho Yoon"
        },
        {
          "name": "Jaemo Jung"
        },
        {
          "name": "Hyunju Kim"
        },
        {
          "name": "Dongman Lee"
        }
      ],
      "abstract": "Aligning egocentric video with wearable sensors have shown promise for human action recognition, but face practical limitations in user discomfort, privacy concerns, and scalability. We explore exocentric video with ambient sensors as a non-intrusive, scalable alternative. While prior egocentric-wearable works predominantly adopt Global Alignment by encoding entire sequences into unified representations, this approach fails in exocentric-ambient settings due to two problems: (P1) inability to capture local details such as subtle motions, and (P2) over-reliance on modality-invariant temporal patterns, causing misalignment between actions sharing similar temporal patterns with different spatio-semantic contexts. To resolve these problems, we propose DETACH, a decomposed spatio-temporal framework. This explicit decomposition preserves local details, while our novel sensor-spatial features discovered via online clustering provide semantic grounding for context-aware alignment. To align the decomposed features, our two-stage approach establishes spatial correspondence through mutual supervision, then performs temporal alignment via a spatial-temporal weighted contrastive loss that adaptively handles easy negatives, hard negatives, and false negatives. Comprehensive experiments with downstream tasks on Opportunity++ and HWU-USP datasets demonstrate substantial improvements over adapted egocentric-wearable baselines.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-23T14:55:53+00:00",
      "updated": "2025-12-23T14:55:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20409v1",
      "file": "papers/2512.20409v1.pdf"
    },
    {
      "arxiv_id": "2512.20377v1",
      "title": "SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images",
      "authors": [
        {
          "name": "Linfei Li"
        },
        {
          "name": "Lin Zhang"
        },
        {
          "name": "Zhong Wang"
        },
        {
          "name": "Ying Shen"
        }
      ],
      "abstract": "Recent advances in generative AI have accelerated the production of ultra-high-resolution visual content, posing significant challenges for efficient compression and real-time decoding on end-user devices. Inspired by 3D Gaussian Splatting, recent 2D Gaussian image models improve representation efficiency, yet existing methods struggle to balance compression ratio and reconstruction fidelity in ultra-high-resolution scenarios. To address this issue, we propose SmartSplat, a highly adaptive and feature-aware GS-based image compression framework that supports arbitrary image resolutions and compression ratios. SmartSplat leverages image-aware features such as gradients and color variances, introducing a Gradient-Color Guided Variational Sampling strategy together with an Exclusion-based Uniform Sampling scheme to improve the non-overlapping coverage of Gaussian primitives in pixel space. In addition, we propose a Scale-Adaptive Gaussian Color Sampling method to enhance color initialization across scales. Through joint optimization of spatial layout, scale, and color initialization, SmartSplat efficiently captures both local structures and global textures using a limited number of Gaussians, achieving high reconstruction quality under strong compression. Extensive experiments on DIV8K and a newly constructed 16K dataset demonstrate that SmartSplat consistently outperforms state-of-the-art methods at comparable compression ratios and exceeds their compression limits, showing strong scalability and practical applicability. The code is publicly available at https://github.com/lif314/SmartSplat.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T14:00:55+00:00",
      "updated": "2025-12-23T14:00:55+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20377v1",
      "file": "papers/2512.20377v1.pdf"
    },
    {
      "arxiv_id": "2512.20374v1",
      "title": "CLIP Based Region-Aware Feature Fusion for Automated BBPS Scoring in Colonoscopy Images",
      "authors": [
        {
          "name": "Yujia Fu"
        },
        {
          "name": "Zhiyu Dong"
        },
        {
          "name": "Tianwen Qian"
        },
        {
          "name": "Chenye Zheng"
        },
        {
          "name": "Danian Ji"
        },
        {
          "name": "Linhai Zhuo"
        }
      ],
      "abstract": "Accurate assessment of bowel cleanliness is essential for effective colonoscopy procedures. The Boston Bowel Preparation Scale (BBPS) offers a standardized scoring system but suffers from subjectivity and inter-observer variability when performed manually. In this paper, to support robust training and evaluation, we construct a high-quality colonoscopy dataset comprising 2,240 images from 517 subjects, annotated with expert-agreed BBPS scores. We propose a novel automated BBPS scoring framework that leverages the CLIP model with adapter-based transfer learning and a dedicated fecal-feature extraction branch. Our method fuses global visual features with stool-related textual priors to improve the accuracy of bowel cleanliness evaluation without requiring explicit segmentation. Extensive experiments on both our dataset and the public NERTHU dataset demonstrate the superiority of our approach over existing baselines, highlighting its potential for clinical deployment in computer-aided colonoscopy analysis.",
      "primary_category": "eess.IV",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-12-23T13:58:12+00:00",
      "updated": "2025-12-23T13:58:12+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20374v1",
      "file": "papers/2512.20374v1.pdf"
    },
    {
      "arxiv_id": "2512.20362v1",
      "title": "CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation",
      "authors": [
        {
          "name": "V. Kovalev"
        },
        {
          "name": "A. Kuvshinov"
        },
        {
          "name": "A. Buzovkin"
        },
        {
          "name": "D. Pokidov"
        },
        {
          "name": "D. Timonin"
        }
      ],
      "abstract": "Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping.\n  We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop.\n  Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T13:44:41+00:00",
      "updated": "2025-12-23T13:44:41+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20362v1",
      "file": "papers/2512.20362v1.pdf"
    },
    {
      "arxiv_id": "2512.20340v1",
      "title": "The devil is in the details: Enhancing Video Virtual Try-On via Keyframe-Driven Details Injection",
      "authors": [
        {
          "name": "Qingdong He"
        },
        {
          "name": "Xueqin Chen"
        },
        {
          "name": "Yanjie Pan"
        },
        {
          "name": "Peng Tang"
        },
        {
          "name": "Pengcheng Xu"
        },
        {
          "name": "Zhenye Gan"
        },
        {
          "name": "Chengjie Wang"
        },
        {
          "name": "Xiaobin Hu"
        },
        {
          "name": "Jiangning Zhang"
        },
        {
          "name": "Yabiao Wang"
        }
      ],
      "abstract": "Although diffusion transformer (DiT)-based video virtual try-on (VVT) has made significant progress in synthesizing realistic videos, existing methods still struggle to capture fine-grained garment dynamics and preserve background integrity across video frames. They also incur high computational costs due to additional interaction modules introduced into DiTs, while the limited scale and quality of existing public datasets also restrict model generalization and effective training. To address these challenges, we propose a novel framework, KeyTailor, along with a large-scale, high-definition dataset, ViT-HD. The core idea of KeyTailor is a keyframe-driven details injection strategy, motivated by the fact that keyframes inherently contain both foreground dynamics and background consistency. Specifically, KeyTailor adopts an instruction-guided keyframe sampling strategy to filter informative frames from the input video. Subsequently,two tailored keyframe-driven modules, the garment details enhancement module and the collaborative background optimization module, are employed to distill garment dynamics into garment-related latents and to optimize the integrity of background latents, both guided by keyframes.These enriched details are then injected into standard DiT blocks together with pose, mask, and noise latents, enabling efficient and realistic try-on video synthesis. This design ensures consistency without explicitly modifying the DiT architecture, while simultaneously avoiding additional complexity. In addition, our dataset ViT-HD comprises 15, 070 high-quality video samples at a resolution of 810*1080, covering diverse garments. Extensive experiments demonstrate that KeyTailor outperforms state-of-the-art baselines in terms of garment fidelity and background integrity across both dynamic and static scenarios.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T13:15:31+00:00",
      "updated": "2025-12-23T13:15:31+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20340v1",
      "file": "papers/2512.20340v1.pdf"
    },
    {
      "arxiv_id": "2512.20296v1",
      "title": "TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation",
      "authors": [
        {
          "name": "Ji-Hoon Kim"
        },
        {
          "name": "Junseok Ahn"
        },
        {
          "name": "Doyeop Kwak"
        },
        {
          "name": "Joon Son Chung"
        },
        {
          "name": "Shinji Watanabe"
        }
      ],
      "abstract": "The objective of this paper is to jointly synthesize interactive videos and conversational speech from text and reference images. With the ultimate goal of building human-like conversational systems, recent studies have explored talking or listening head generation as well as conversational speech generation. However, these works are typically studied in isolation, overlooking the multimodal nature of human conversation, which involves tightly coupled audio-visual interactions. In this paper, we introduce TAVID, a unified framework that generates both interactive faces and conversational speech in a synchronized manner. TAVID integrates face and speech generation pipelines through two cross-modal mappers (i.e., a motion mapper and a speaker mapper), which enable bidirectional exchange of complementary information between the audio and visual modalities. We evaluate our system across four dimensions: talking face realism, listening head responsiveness, dyadic interaction fluency, and speech quality. Extensive experiments demonstrate the effectiveness of our approach across all these aspects.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.AS",
        "eess.IV"
      ],
      "published": "2025-12-23T12:04:23+00:00",
      "updated": "2025-12-23T12:04:23+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20296v1",
      "file": "papers/2512.20296v1.pdf"
    },
    {
      "arxiv_id": "2512.20288v1",
      "title": "UbiQVision: Quantifying Uncertainty in XAI for Image Recognition",
      "authors": [
        {
          "name": "Akshat Dubey"
        },
        {
          "name": "Aleksandar Anžel"
        },
        {
          "name": "Bahar İlgen"
        },
        {
          "name": "Georges Hattab"
        }
      ],
      "abstract": "Recent advances in deep learning have led to its widespread adoption across diverse domains, including medical imaging. This progress is driven by increasingly sophisticated model architectures, such as ResNets, Vision Transformers, and Hybrid Convolutional Neural Networks, that offer enhanced performance at the cost of greater complexity. This complexity often compromises model explainability and interpretability. SHAP has emerged as a prominent method for providing interpretable visualizations that aid domain experts in understanding model predictions. However, SHAP explanations can be unstable and unreliable in the presence of epistemic and aleatoric uncertainty. In this study, we address this challenge by using Dirichlet posterior sampling and Dempster-Shafer theory to quantify the uncertainty that arises from these unstable explanations in medical imaging applications. The framework uses a belief, plausible, and fusion map approach alongside statistical quantitative analysis to produce quantification of uncertainty in SHAP. Furthermore, we evaluated our framework on three medical imaging datasets with varying class distributions, image qualities, and modality types which introduces noise due to varying image resolutions and modality-specific aspect covering the examples from pathology, ophthalmology, and radiology, introducing significant epistemic uncertainty.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-23T11:57:34+00:00",
      "updated": "2025-12-23T11:57:34+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20288v1",
      "file": "papers/2512.20288v1.pdf"
    },
    {
      "arxiv_id": "2512.20260v1",
      "title": "${D}^{3}${ETOR}: ${D}$ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive ${D}$ebiasing for Weakly-Supervised Camouflaged Object ${D}$etection with Scribble Annotations",
      "authors": [
        {
          "name": "Jiawei Ge"
        },
        {
          "name": "Jiuxin Cao"
        },
        {
          "name": "Xinyi Li"
        },
        {
          "name": "Xuelin Zhu"
        },
        {
          "name": "Chang Liu"
        },
        {
          "name": "Bo Liu"
        },
        {
          "name": "Chen Feng"
        },
        {
          "name": "Ioannis Patras"
        }
      ],
      "abstract": "Weakly-Supervised Camouflaged Object Detection (WSCOD) aims to locate and segment objects that are visually concealed within their surrounding scenes, relying solely on sparse supervision such as scribble annotations. Despite recent progress, existing WSCOD methods still lag far behind fully supervised ones due to two major limitations: (1) the pseudo masks generated by general-purpose segmentation models (e.g., SAM) and filtered via rules are often unreliable, as these models lack the task-specific semantic understanding required for effective pseudo labeling in COD; and (2) the neglect of inherent annotation bias in scribbles, which hinders the model from capturing the global structure of camouflaged objects. To overcome these challenges, we propose ${D}^{3}$ETOR, a two-stage WSCOD framework consisting of Debate-Enhanced Pseudo Labeling and Frequency-Aware Progressive Debiasing. In the first stage, we introduce an adaptive entropy-driven point sampling method and a multi-agent debate mechanism to enhance the capability of SAM for COD, improving the interpretability and precision of pseudo masks. In the second stage, we design FADeNet, which progressively fuses multi-level frequency-aware features to balance global semantic understanding with local detail modeling, while dynamically reweighting supervision strength across regions to alleviate scribble bias. By jointly exploiting the supervision signals from both the pseudo masks and scribble semantics, ${D}^{3}$ETOR significantly narrows the gap between weakly and fully supervised COD, achieving state-of-the-art performance on multiple benchmarks.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-23T11:16:16+00:00",
      "updated": "2025-12-23T11:16:16+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20260v1",
      "file": "papers/2512.20260v1.pdf"
    },
    {
      "arxiv_id": "2512.20255v1",
      "title": "BiCoR-Seg: Bidirectional Co-Refinement Framework for High-Resolution Remote Sensing Image Segmentation",
      "authors": [
        {
          "name": "Jinghao Shi"
        },
        {
          "name": "Jianing Song"
        }
      ],
      "abstract": "High-resolution remote sensing image semantic segmentation (HRSS) is a fundamental yet critical task in the field of Earth observation. However, it has long faced the challenges of high inter-class similarity and large intra-class variability. Existing approaches often struggle to effectively inject abstract yet strongly discriminative semantic knowledge into pixel-level feature learning, leading to blurred boundaries and class confusion in complex scenes. To address these challenges, we propose Bidirectional Co-Refinement Framework for HRSS (BiCoR-Seg). Specifically, we design a Heatmap-driven Bidirectional Information Synergy Module (HBIS), which establishes a bidirectional information flow between feature maps and class embeddings by generating class-level heatmaps. Based on HBIS, we further introduce a hierarchical supervision strategy, where the interpretable heatmaps generated by each HBIS module are directly utilized as low-resolution segmentation predictions for supervision, thereby enhancing the discriminative capacity of shallow features. In addition, to further improve the discriminability of the embedding representations, we propose a cross-layer class embedding Fisher Discriminative Loss to enforce intra-class compactness and enlarge inter-class separability. Extensive experiments on the LoveDA, Vaihingen, and Potsdam datasets demonstrate that BiCoR-Seg achieves outstanding segmentation performance while offering stronger interpretability. The released code is available at https://github.com/ShiJinghao566/BiCoR-Seg.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T11:13:01+00:00",
      "updated": "2025-12-23T11:13:01+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20255v1",
      "file": "papers/2512.20255v1.pdf"
    },
    {
      "arxiv_id": "2512.20251v1",
      "title": "Degradation-Aware Metric Prompting for Hyperspectral Image Restoration",
      "authors": [
        {
          "name": "Binfeng Wang"
        },
        {
          "name": "Di Wang"
        },
        {
          "name": "Haonan Guo"
        },
        {
          "name": "Ying Fu"
        },
        {
          "name": "Jing Zhang"
        }
      ],
      "abstract": "Unified hyperspectral image (HSI) restoration aims to recover various degraded HSIs using a single model, offering great practical value. However, existing methods often depend on explicit degradation priors (e.g., degradation labels) as prompts to guide restoration, which are difficult to obtain due to complex and mixed degradations in real-world scenarios. To address this challenge, we propose a Degradation-Aware Metric Prompting (DAMP) framework. Instead of relying on predefined degradation priors, we design spatial-spectral degradation metrics to continuously quantify multi-dimensional degradations, serving as Degradation Prompts (DP). These DP enable the model to capture cross-task similarities in degradation distributions and enhance shared feature learning. Furthermore, we introduce a Spatial-Spectral Adaptive Module (SSAM) that dynamically modulates spatial and spectral feature extraction through learnable parameters. By integrating SSAM as experts within a Mixture-of-Experts architecture, and using DP as the gating router, the framework enables adaptive, efficient, and robust restoration under diverse, mixed, or unseen degradations. Extensive experiments on natural and remote sensing HSI datasets show that DAMP achieves state-of-the-art performance and demonstrates exceptional generalization capability. Code is publicly available at https://github.com/MiliLab/DAMP.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "published": "2025-12-23T11:05:36+00:00",
      "updated": "2025-12-23T11:05:36+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20251v1",
      "file": "papers/2512.20251v1.pdf"
    },
    {
      "arxiv_id": "2512.20236v1",
      "title": "IndicDLP: A Foundational Dataset for Multi-Lingual and Multi-Domain Document Layout Parsing",
      "authors": [
        {
          "name": "Oikantik Nath"
        },
        {
          "name": "Sahithi Kukkala"
        },
        {
          "name": "Mitesh Khapra"
        },
        {
          "name": "Ravi Kiran Sarvadevabhatla"
        }
      ],
      "abstract": "Document layout analysis is essential for downstream tasks such as information retrieval, extraction, OCR, and digitization. However, existing large-scale datasets like PubLayNet and DocBank lack fine-grained region labels and multilingual diversity, making them insufficient for representing complex document layouts. In contrast, human-annotated datasets such as M6Doc and D4LA offer richer labels and greater domain diversity, but are too small to train robust models and lack adequate multilingual coverage. This gap is especially pronounced for Indic documents, which encompass diverse scripts yet remain underrepresented in current datasets, further limiting progress in this space. To address these shortcomings, we introduce IndicDLP, a large-scale foundational document layout dataset spanning 11 representative Indic languages alongside English and 12 common document domains. Additionally, we curate UED-mini, a dataset derived from DocLayNet and M6Doc, to enhance pretraining and provide a solid foundation for Indic layout models. Our experiments demonstrate that fine-tuning existing English models on IndicDLP significantly boosts performance, validating its effectiveness. Moreover, models trained on IndicDLP generalize well beyond Indic layouts, making it a valuable resource for document digitization. This work bridges gaps in scale, diversity, and annotation granularity, driving inclusive and efficient document understanding.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T10:49:37+00:00",
      "updated": "2025-12-23T10:49:37+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20236v1",
      "file": "papers/2512.20236v1.pdf"
    },
    {
      "arxiv_id": "2512.20233v1",
      "title": "How I Met Your Bias: Investigating Bias Amplification in Diffusion Models",
      "authors": [
        {
          "name": "Nathan Roos"
        },
        {
          "name": "Ekaterina Iakovleva"
        },
        {
          "name": "Ani Gjergji"
        },
        {
          "name": "Vito Paolo Pastore"
        },
        {
          "name": "Enzo Tartaglione"
        }
      ],
      "abstract": "Diffusion-based generative models demonstrate state-of-the-art performance across various image synthesis tasks, yet their tendency to replicate and amplify dataset biases remains poorly understood. Although previous research has viewed bias amplification as an inherent characteristic of diffusion models, this work provides the first analysis of how sampling algorithms and their hyperparameters influence bias amplification. We empirically demonstrate that samplers for diffusion models -- commonly optimized for sample quality and speed -- have a significant and measurable effect on bias amplification. Through controlled studies with models trained on Biased MNIST, Multi-Color MNIST and BFFHQ, and with Stable Diffusion, we show that sampling hyperparameters can induce both bias reduction and amplification, even when the trained model is fixed. Source code is available at https://github.com/How-I-met-your-bias/how_i_met_your_bias.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "published": "2025-12-23T10:46:48+00:00",
      "updated": "2025-12-23T10:46:48+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20233v1",
      "file": "papers/2512.20233v1.pdf"
    },
    {
      "arxiv_id": "2512.20217v1",
      "title": "LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation",
      "authors": [
        {
          "name": "Xiangxuan Ren"
        },
        {
          "name": "Zhongdao Wang"
        },
        {
          "name": "Pin Tang"
        },
        {
          "name": "Guoqing Wang"
        },
        {
          "name": "Jilai Zheng"
        },
        {
          "name": "Chao Ma"
        }
      ],
      "abstract": "3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T10:16:33+00:00",
      "updated": "2025-12-23T10:16:33+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20217v1",
      "file": "papers/2512.20217v1.pdf"
    },
    {
      "arxiv_id": "2512.20213v1",
      "title": "JDPNet: A Network Based on Joint Degradation Processing for Underwater Image Enhancement",
      "authors": [
        {
          "name": "Tao Ye"
        },
        {
          "name": "Hongbin Ren"
        },
        {
          "name": "Chongbing Zhang"
        },
        {
          "name": "Haoran Chen"
        },
        {
          "name": "Xiaosong Li"
        }
      ],
      "abstract": "Given the complexity of underwater environments and the variability of water as a medium, underwater images are inevitably subject to various types of degradation. The degradations present nonlinear coupling rather than simple superposition, which renders the effective processing of such coupled degradations particularly challenging. Most existing methods focus on designing specific branches, modules, or strategies for specific degradations, with little attention paid to the potential information embedded in their coupling. Consequently, they struggle to effectively capture and process the nonlinear interactions of multiple degradations from a bottom-up perspective. To address this issue, we propose JDPNet, a joint degradation processing network, that mines and unifies the potential information inherent in coupled degradations within a unified framework. Specifically, we introduce a joint feature-mining module, along with a probabilistic bootstrap distribution strategy, to facilitate effective mining and unified adjustment of coupled degradation features. Furthermore, to balance color, clarity, and contrast, we design a novel AquaBalanceLoss to guide the network in learning from multiple coupled degradation losses. Experiments on six publicly available underwater datasets, as well as two new datasets constructed in this study, show that JDPNet exhibits state-of-the-art performance while offering a better tradeoff between performance, parameter size, and computational cost.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T10:12:35+00:00",
      "updated": "2025-12-23T10:12:35+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20213v1",
      "file": "papers/2512.20213v1.pdf"
    },
    {
      "arxiv_id": "2512.20194v1",
      "title": "Generative Latent Coding for Ultra-Low Bitrate Image Compression",
      "authors": [
        {
          "name": "Zhaoyang Jia"
        },
        {
          "name": "Jiahao Li"
        },
        {
          "name": "Bin Li"
        },
        {
          "name": "Houqiang Li"
        },
        {
          "name": "Yan Lu"
        }
      ],
      "abstract": "Most existing image compression approaches perform transform coding in the pixel space to reduce its spatial redundancy. However, they encounter difficulties in achieving both high-realism and high-fidelity at low bitrate, as the pixel-space distortion may not align with human perception. To address this issue, we introduce a Generative Latent Coding (GLC) architecture, which performs transform coding in the latent space of a generative vector-quantized variational auto-encoder (VQ-VAE), instead of in the pixel space. The generative latent space is characterized by greater sparsity, richer semantic and better alignment with human perception, rendering it advantageous for achieving high-realism and high-fidelity compression. Additionally, we introduce a categorical hyper module to reduce the bit cost of hyper-information, and a code-prediction-based supervision to enhance the semantic consistency. Experiments demonstrate that our GLC maintains high visual quality with less than 0.04 bpp on natural images and less than 0.01 bpp on facial images. On the CLIC2020 test set, we achieve the same FID as MS-ILLM with 45% fewer bits. Furthermore, the powerful generative latent space enables various applications built on our GLC pipeline, such as image restoration and style transfer. The code is available at https://github.com/jzyustc/GLC.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "published": "2025-12-23T09:35:40+00:00",
      "updated": "2025-12-23T09:35:40+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20194v1",
      "file": "papers/2512.20194v1.pdf"
    },
    {
      "arxiv_id": "2512.20174v1",
      "title": "Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark",
      "authors": [
        {
          "name": "Hao Guo"
        },
        {
          "name": "Xugong Qin"
        },
        {
          "name": "Jun Jie Ou Yang"
        },
        {
          "name": "Peng Zhang"
        },
        {
          "name": "Gangyan Zeng"
        },
        {
          "name": "Yubo Li"
        },
        {
          "name": "Hailun Lin"
        }
      ],
      "abstract": "Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.CL",
        "cs.IR"
      ],
      "published": "2025-12-23T09:14:16+00:00",
      "updated": "2025-12-23T09:14:16+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20174v1",
      "file": "papers/2512.20174v1.pdf"
    },
    {
      "arxiv_id": "2512.20157v1",
      "title": "AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model",
      "authors": [
        {
          "name": "Sofian Chaybouti"
        },
        {
          "name": "Sanath Narayan"
        },
        {
          "name": "Yasser Dahou"
        },
        {
          "name": "Phúc H. Lê Khac"
        },
        {
          "name": "Ankit Singh"
        },
        {
          "name": "Ngoc Dung Huynh"
        },
        {
          "name": "Wamiq Reyaz Para"
        },
        {
          "name": "Hilde Kuehne"
        },
        {
          "name": "Hakim Hacid"
        }
      ],
      "abstract": "Vision foundation models trained via multi-teacher distillation offer a promising path toward unified visual representations, yet the learning dynamics and data efficiency of such approaches remain underexplored. In this paper, we systematically study multi-teacher distillation for vision foundation models and identify key factors that enable training at lower computational cost. We introduce Agglomerative Mixture-of-Experts Vision Foundation Models (AMoE), which distill knowledge from SigLIP2 and DINOv3 simultaneously into a Mixture-of-Experts student. We show that (1) our Asymmetric Relation-Knowledge Distillation loss preserves the geometric properties of each teacher while enabling effective knowledge transfer, (2) token-balanced batching that packs varying-resolution images into sequences with uniform token budgets stabilizes representation learning across resolutions without sacrificing performance, and (3) hierarchical clustering and sampling of training data--typically reserved for self-supervised learning--substantially improves sample efficiency over random sampling for multi-teacher distillation. By combining these findings, we curate OpenLVD200M, a 200M-image corpus that demonstrates superior efficiency for multi-teacher distillation. Instantiated in a Mixture-of-Experts. We release OpenLVD200M and distilled models.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T08:37:11+00:00",
      "updated": "2025-12-23T08:37:11+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20157v1",
      "file": "papers/2512.20157v1.pdf"
    },
    {
      "arxiv_id": "2512.20153v1",
      "title": "CoDi -- an exemplar-conditioned diffusion model for low-shot counting",
      "authors": [
        {
          "name": "Grega Šuštar"
        },
        {
          "name": "Jer Pelhan"
        },
        {
          "name": "Alan Lukežič"
        },
        {
          "name": "Matej Kristan"
        }
      ],
      "abstract": "Low-shot object counting addresses estimating the number of previously unobserved objects in an image using only few or no annotated test-time exemplars. A considerable challenge for modern low-shot counters are dense regions with small objects. While total counts in such situations are typically well addressed by density-based counters, their usefulness is limited by poor localization capabilities. This is better addressed by point-detection-based counters, which are based on query-based detectors. However, due to limited number of pre-trained queries, they underperform on images with very large numbers of objects, and resort to ad-hoc techniques like upsampling and tiling. We propose CoDi, the first latent diffusion-based low-shot counter that produces high-quality density maps on which object locations can be determined by non-maxima suppression. Our core contribution is the new exemplar-based conditioning module that extracts and adjusts the object prototypes to the intermediate layers of the denoising network, leading to accurate object location estimation. On FSC benchmark, CoDi outperforms state-of-the-art by 15% MAE, 13% MAE and 10% MAE in the few-shot, one-shot, and reference-less scenarios, respectively, and sets a new state-of-the-art on MCAC benchmark by outperforming the top method by 44% MAE. The code is available at https://github.com/gsustar/CoDi.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T08:31:36+00:00",
      "updated": "2025-12-23T08:31:36+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20153v1",
      "file": "papers/2512.20153v1.pdf"
    },
    {
      "arxiv_id": "2512.20148v1",
      "title": "Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)",
      "authors": [
        {
          "name": "Robert van de Ven"
        },
        {
          "name": "Trim Bresilla"
        },
        {
          "name": "Bram Nelissen"
        },
        {
          "name": "Ard Nieuwenhuizen"
        },
        {
          "name": "Eldert J. van Henten"
        },
        {
          "name": "Gert Kootstra"
        }
      ],
      "abstract": "Automating tasks in orchards is challenging because of the large amount of variation in the environment and occlusions. One of the challenges is apple pose estimation, where key points, such as the calyx, are often occluded. Recently developed pose estimation methods no longer rely on these key points, but still require them for annotations, making annotating challenging and time-consuming. Due to the abovementioned occlusions, there can be conflicting and missing annotations of the same fruit between different images. Novel 3D reconstruction methods can be used to simplify annotating and enlarge datasets. We propose a novel pipeline consisting of 3D Gaussian Splatting to reconstruct an orchard scene, simplified annotations, automated projection of the annotations to images, and the training and evaluation of a pose estimation method. Using our pipeline, 105 manual annotations were required to obtain 28,191 training labels, a reduction of 99.6%. Experimental results indicated that training with labels of fruits that are $\\leq95\\%$ occluded resulted in the best performance, with a neutral F1 score of 0.927 on the original images and 0.970 on the rendered images. Adjusting the size of the training dataset had small effects on the model performance in terms of F1 score and pose estimation accuracy. It was found that the least occluded fruits had the best position estimation, which worsened as the fruits became more occluded. It was also found that the tested pose estimation method was unable to correctly learn the orientation estimation of apples.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-12-23T08:19:55+00:00",
      "updated": "2025-12-23T08:19:55+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20148v1",
      "file": "papers/2512.20148v1.pdf"
    },
    {
      "arxiv_id": "2512.20128v1",
      "title": "milliMamba: Specular-Aware Human Pose Estimation via Dual mmWave Radar with Multi-Frame Mamba Fusion",
      "authors": [
        {
          "name": "Niraj Prakash Kini"
        },
        {
          "name": "Shiau-Rung Tsai"
        },
        {
          "name": "Guan-Hsun Lin"
        },
        {
          "name": "Wen-Hsiao Peng"
        },
        {
          "name": "Ching-Wen Ma"
        },
        {
          "name": "Jenq-Neng Hwang"
        }
      ],
      "abstract": "Millimeter-wave radar offers a privacy-preserving and lighting-invariant alternative to RGB sensors for Human Pose Estimation (HPE) task. However, the radar signals are often sparse due to specular reflection, making the extraction of robust features from radar signals highly challenging. To address this, we present milliMamba, a radar-based 2D human pose estimation framework that jointly models spatio-temporal dependencies across both the feature extraction and decoding stages. Specifically, given the high dimensionality of radar inputs, we adopt a Cross-View Fusion Mamba encoder to efficiently extract spatio-temporal features from longer sequences with linear complexity. A Spatio-Temporal-Cross Attention decoder then predicts joint coordinates across multiple frames. Together, this spatio-temporal modeling pipeline enables the model to leverage contextual cues from neighboring frames and joints to infer missing joints caused by specular reflections. To reinforce motion smoothness, we incorporate a velocity loss alongside the standard keypoint loss during training. Experiments on the TransHuPR and HuPR datasets demonstrate that our method achieves significant performance improvements, exceeding the baselines by 11.0 AP and 14.6 AP, respectively, while maintaining reasonable complexity. Code: https://github.com/NYCU-MAPL/milliMamba",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T07:40:25+00:00",
      "updated": "2025-12-23T07:40:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20128v1",
      "file": "papers/2512.20128v1.pdf"
    },
    {
      "arxiv_id": "2512.20120v1",
      "title": "HEART-VIT: Hessian-Guided Efficient Dynamic Attention and Token Pruning in Vision Transformer",
      "authors": [
        {
          "name": "Mohammad Helal Uddin"
        },
        {
          "name": "Liam Seymour"
        },
        {
          "name": "Sabur Baidya"
        }
      ],
      "abstract": "Vision Transformers (ViTs) deliver state-of-the-art accuracy but their quadratic attention cost and redundant computations severely hinder deployment on latency and resource-constrained platforms. Existing pruning approaches treat either tokens or heads in isolation, relying on heuristics or first-order signals, which often sacrifice accuracy or fail to generalize across inputs. We introduce HEART-ViT, a Hessian-guided efficient dynamic attention and token pruning framework for vision transformers, which to the best of our knowledge is the first unified, second-order, input-adaptive framework for ViT optimization. HEART-ViT estimates curvature-weighted sensitivities of both tokens and attention heads using efficient Hessian-vector products, enabling principled pruning decisions under explicit loss budgets.This dual-view sensitivity reveals an important structural insight: token pruning dominates computational savings, while head pruning provides fine-grained redundancy removal, and their combination achieves a superior trade-off. On ImageNet-100 and ImageNet-1K with ViT-B/16 and DeiT-B/16, HEART-ViT achieves up to 49.4 percent FLOPs reduction, 36 percent lower latency, and 46 percent higher throughput, while consistently matching or even surpassing baseline accuracy after fine-tuning, for example 4.7 percent recovery at 40 percent token pruning. Beyond theoretical benchmarks, we deploy HEART-ViT on different edge devices such as AGX Orin, demonstrating that our reductions in FLOPs and latency translate directly into real-world gains in inference speed and energy efficiency. HEART-ViT bridges the gap between theory and practice, delivering the first unified, curvature-driven pruning framework that is both accuracy-preserving and edge-efficient.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T07:23:16+00:00",
      "updated": "2025-12-23T07:23:16+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20120v1",
      "file": "papers/2512.20120v1.pdf"
    },
    {
      "arxiv_id": "2512.20117v1",
      "title": "DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation",
      "authors": [
        {
          "name": "Jingqi Tian"
        },
        {
          "name": "Yiheng Du"
        },
        {
          "name": "Haoji Zhang"
        },
        {
          "name": "Yuji Wang"
        },
        {
          "name": "Isaac Ning Lee"
        },
        {
          "name": "Xulong Bai"
        },
        {
          "name": "Tianrui Zhu"
        },
        {
          "name": "Jingxuan Niu"
        },
        {
          "name": "Yansong Tang"
        }
      ],
      "abstract": "Audio-Visual Segmentation (AVS) aims to localize sound-producing objects at the pixel level by jointly leveraging auditory and visual information. However, existing methods often suffer from multi-source entanglement and audio-visual misalignment, which lead to biases toward louder or larger objects while overlooking weaker, smaller, or co-occurring sources. To address these challenges, we propose DDAVS, a Disentangled Audio Semantics and Delayed Bidirectional Alignment framework. To mitigate multi-source entanglement, DDAVS employs learnable queries to extract audio semantics and anchor them within a structured semantic space derived from an audio prototype memory bank. This is further optimized through contrastive learning to enhance discriminability and robustness. To alleviate audio-visual misalignment, DDAVS introduces dual cross-attention with delayed modality interaction, improving the robustness of multimodal alignment. Extensive experiments on the AVS-Objects and VPO benchmarks demonstrate that DDAVS consistently outperforms existing approaches, exhibiting strong performance across single-source, multi-source, and multi-instance scenarios. These results validate the effectiveness and generalization ability of our framework under challenging real-world audio-visual segmentation conditions. Project page: https://trilarflagz.github.io/DDAVS-page/",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "published": "2025-12-23T07:21:21+00:00",
      "updated": "2025-12-23T07:21:21+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20117v1",
      "file": "papers/2512.20117v1.pdf"
    },
    {
      "arxiv_id": "2512.20113v1",
      "title": "Multi Modal Attention Networks with Uncertainty Quantification for Automated Concrete Bridge Deck Delamination Detection",
      "authors": [
        {
          "name": "Alireza Moayedikia"
        },
        {
          "name": "Sattar Dorafshan"
        }
      ],
      "abstract": "Deteriorating civil infrastructure requires automated inspection techniques overcoming limitations of visual assessment. While Ground Penetrating Radar and Infrared Thermography enable subsurface defect detection, single modal approaches face complementary constraints radar struggles with moisture and shallow defects, while thermography exhibits weather dependency and limited depth. This paper presents a multi modal attention network fusing radar temporal patterns with thermal spatial signatures for bridge deck delamination detection. Our architecture introduces temporal attention for radar processing, spatial attention for thermal features, and cross modal fusion with learnable embeddings discovering complementary defect patterns invisible to individual sensors. We incorporate uncertainty quantification through Monte Carlo dropout and learned variance estimation, decomposing uncertainty into epistemic and aleatoric components for safety critical decisions. Experiments on five bridge datasets reveal that on balanced to moderately imbalanced data, our approach substantially outperforms baselines in accuracy and AUC representing meaningful improvements over single modal and concatenation based fusion. Ablation studies demonstrate cross modal attention provides critical gains beyond within modality attention, while multi head mechanisms achieve improved calibration. Uncertainty quantification reduces calibration error, enabling selective prediction by rejecting uncertain cases. However, under extreme class imbalance, attention mechanisms show vulnerability to majority class collapse. These findings provide actionable guidance: attention based architecture performs well across typical scenarios, while extreme imbalance requires specialized techniques. Our system maintains deployment efficiency, enabling real time inspection with characterized capabilities and limitations.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "published": "2025-12-23T07:16:18+00:00",
      "updated": "2025-12-23T07:16:18+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20113v1",
      "file": "papers/2512.20113v1.pdf"
    },
    {
      "arxiv_id": "2512.20107v1",
      "title": "UMAMI: Unifying Masked Autoregressive Models and Deterministic Rendering for View Synthesis",
      "authors": [
        {
          "name": "Thanh-Tung Le"
        },
        {
          "name": "Tuan Pham"
        },
        {
          "name": "Tung Nguyen"
        },
        {
          "name": "Deying Kong"
        },
        {
          "name": "Xiaohui Xie"
        },
        {
          "name": "Stephan Mandt"
        }
      ],
      "abstract": "Novel view synthesis (NVS) seeks to render photorealistic, 3D-consistent images of a scene from unseen camera poses given only a sparse set of posed views. Existing deterministic networks render observed regions quickly but blur unobserved areas, whereas stochastic diffusion-based methods hallucinate plausible content yet incur heavy training- and inference-time costs. In this paper, we propose a hybrid framework that unifies the strengths of both paradigms. A bidirectional transformer encodes multi-view image tokens and Plucker-ray embeddings, producing a shared latent representation. Two lightweight heads then act on this representation: (i) a feed-forward regression head that renders pixels where geometry is well constrained, and (ii) a masked autoregressive diffusion head that completes occluded or unseen regions. The entire model is trained end-to-end with joint photometric and diffusion losses, without handcrafted 3D inductive biases, enabling scalability across diverse scenes. Experiments demonstrate that our method attains state-of-the-art image quality while reducing rendering time by an order of magnitude compared with fully generative baselines.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T07:08:00+00:00",
      "updated": "2025-12-23T07:08:00+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20107v1",
      "file": "papers/2512.20107v1.pdf"
    },
    {
      "arxiv_id": "2512.20105v1",
      "title": "LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs",
      "authors": [
        {
          "name": "Haiyun Wei"
        },
        {
          "name": "Fan Lu"
        },
        {
          "name": "Yunwei Zhu"
        },
        {
          "name": "Zehan Zheng"
        },
        {
          "name": "Weiyi Xue"
        },
        {
          "name": "Lin Shao"
        },
        {
          "name": "Xudong Zhang"
        },
        {
          "name": "Ya Wu"
        },
        {
          "name": "Rong Fu"
        },
        {
          "name": "Guang Chen"
        }
      ],
      "abstract": "Generating realistic and diverse LiDAR point clouds is crucial for autonomous driving simulation. Although previous methods achieve LiDAR point cloud generation from user inputs, they struggle to attain high-quality results while enabling versatile controllability, due to the imbalance between the complex distribution of LiDAR point clouds and the simple control signals. To address the limitation, we propose LiDARDraft, which utilizes the 3D layout to build a bridge between versatile conditional signals and LiDAR point clouds. The 3D layout can be trivially generated from various user inputs such as textual descriptions and images. Specifically, we represent text, images, and point clouds as unified 3D layouts, which are further transformed into semantic and depth control signals. Then, we employ a rangemap-based ControlNet to guide LiDAR point cloud generation. This pixel-level alignment approach demonstrates excellent performance in controllable LiDAR point clouds generation, enabling \"simulation from scratch\", allowing self-driving environments to be created from arbitrary textual descriptions, images and sketches.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T07:03:31+00:00",
      "updated": "2025-12-23T07:03:31+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20105v1",
      "file": "papers/2512.20105v1.pdf"
    },
    {
      "arxiv_id": "2512.20104v1",
      "title": "Effect of Activation Function and Model Optimizer on the Performance of Human Activity Recognition System Using Various Deep Learning Models",
      "authors": [
        {
          "name": "Subrata Kumer Paula"
        },
        {
          "name": "Dewan Nafiul Islam Noora"
        },
        {
          "name": "Rakhi Rani Paula"
        },
        {
          "name": "Md. Ekramul Hamidb"
        },
        {
          "name": "Fahmid Al Faridc"
        },
        {
          "name": "Hezerul Abdul Karimd"
        },
        {
          "name": "Md. Maruf Al Hossain Princee"
        },
        {
          "name": "Abu Saleh Musa Miahb"
        }
      ],
      "abstract": "Human Activity Recognition (HAR) plays a vital role in healthcare, surveillance, and innovative environments, where reliable action recognition supports timely decision-making and automation. Although deep learning-based HAR systems are widely adopted, the impact of Activation Functions (AFs) and Model Optimizers (MOs) on performance has not been sufficiently analyzed, particularly regarding how their combinations influence model behavior in practical scenarios. Most existing studies focus on architecture design, while the interaction between AF and MO choices remains relatively unexplored. In this work, we investigate the effect of three commonly used activation functions (ReLU, Sigmoid, and Tanh) combined with four optimization algorithms (SGD, Adam, RMSprop, and Adagrad) using two recurrent deep learning architectures, namely BiLSTM and ConvLSTM. Experiments are conducted on six medically relevant activity classes selected from the HMDB51 and UCF101 datasets, considering their suitability for healthcare-oriented HAR applications. Our experimental results show that ConvLSTM consistently outperforms BiLSTM across both datasets. ConvLSTM, combined with Adam or RMSprop, achieves an accuracy of up to 99.00%, demonstrating strong spatio-temporal learning capabilities and stable performance. While BiLSTM performs reasonably well on UCF101, with accuracy approaching 98.00%, its performance drops to approximately 60.00% on HMDB51, indicating limited robustness across datasets and weaker sensitivity to AF and MO variations. This study provides practical insights for optimizing HAR systems, particularly for real-world healthcare environments where fast and precise activity detection is critical.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T07:01:45+00:00",
      "updated": "2025-12-23T07:01:45+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20104v1",
      "file": "papers/2512.20104v1.pdf"
    },
    {
      "arxiv_id": "2512.20088v1",
      "title": "Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts",
      "authors": [
        {
          "name": "Jinyoung Choi"
        },
        {
          "name": "Youngchae Kwon"
        },
        {
          "name": "Injung Kim"
        }
      ],
      "abstract": "Fashion style classification is a challenging task because of the large visual variation within the same style and the existence of visually similar styles.\n  Styles are expressed not only by the global appearance, but also by the attributes of individual items and their combinations.\n  In this study, we propose an item region-based fashion style classification network (IRSN) to effectively classify fashion styles by analyzing item-specific features and their combinations in addition to global features.\n  IRSN extracts features of each item region using item region pooling (IRP), analyzes them separately, and combines them using gated feature fusion (GFF).\n  In addition, we improve the feature extractor by applying a dual-backbone architecture that combines a domain-specific feature extractor and a general feature extractor pre-trained with a large-scale image-text dataset.\n  In experiments, applying IRSN to six widely-used backbones, including EfficientNet, ConvNeXt, and Swin Transformer, improved style classification accuracy by an average of 6.9% and a maximum of 14.5% on the FashionStyle14 dataset and by an average of 7.6% and a maximum of 15.1% on the ShowniqV3 dataset. Visualization analysis also supports that the IRSN models are better than the baseline models at capturing differences between similar style classes.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-23T06:30:33+00:00",
      "updated": "2025-12-23T06:30:33+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20088v1",
      "file": "papers/2512.20088v1.pdf"
    },
    {
      "arxiv_id": "2512.20070v1",
      "title": "Progressive Learned Image Compression for Machine Perception",
      "authors": [
        {
          "name": "Jungwoo Kim"
        },
        {
          "name": "Jun-Hyuk Kim"
        },
        {
          "name": "Jong-Seok Lee"
        }
      ],
      "abstract": "Recent advances in learned image codecs have been extended from human perception toward machine perception. However, progressive image compression with fine granular scalability (FGS)-which enables decoding a single bitstream at multiple quality levels-remains unexplored for machine-oriented codecs. In this work, we propose a novel progressive learned image compression codec for machine perception, PICM-Net, based on trit-plane coding. By analyzing the difference between human- and machine-oriented rate-distortion priorities, we systematically examine the latent prioritization strategies in terms of machine-oriented codecs. To further enhance real-world adaptability, we design an adaptive decoding controller, which dynamically determines the necessary decoding level during inference time to maintain the desired confidence of downstream machine prediction. Extensive experiments demonstrate that our approach enables efficient and adaptive progressive transmission while maintaining high performance in the downstream classification task, establishing a new paradigm for machine-aware progressive image compression.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "published": "2025-12-23T05:45:38+00:00",
      "updated": "2025-12-23T05:45:38+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20070v1",
      "file": "papers/2512.20070v1.pdf"
    },
    {
      "arxiv_id": "2512.20056v1",
      "title": "Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach",
      "authors": [
        {
          "name": "Hao Li"
        },
        {
          "name": "Fabian Deuser"
        },
        {
          "name": "Wenping Yin"
        },
        {
          "name": "Steffen Knoblauch"
        },
        {
          "name": "Wufan Zhao"
        },
        {
          "name": "Filip Biljecki"
        },
        {
          "name": "Yong Xue"
        },
        {
          "name": "Wei Huang"
        }
      ],
      "abstract": "As Earth's climate changes, it is impacting disasters and extreme weather events across the planet. Record-breaking heat waves, drenching rainfalls, extreme wildfires, and widespread flooding during hurricanes are all becoming more frequent and more intense. Rapid and efficient response to disaster events is essential for climate resilience and sustainability. A key challenge in disaster response is to accurately and quickly identify disaster locations to support decision-making and resources allocation. In this paper, we propose a Probabilistic Cross-view Geolocalization approach, called ProbGLC, exploring new pathways towards generative location awareness for rapid disaster response. Herein, we combine probabilistic and deterministic geolocalization models into a unified framework to simultaneously enhance model explainability (via uncertainty quantification) and achieve state-of-the-art geolocalization performance. Designed for rapid diaster response, the ProbGLC is able to address cross-view geolocalization across multiple disaster events as well as to offer unique features of probabilistic distribution and localizability score. To evaluate the ProbGLC, we conduct extensive experiments on two cross-view disaster datasets (i.e., MultiIAN and SAGAINDisaster), consisting diverse cross-view imagery pairs of multiple disaster types (e.g., hurricanes, wildfires, floods, to tornadoes). Preliminary results confirms the superior geolocalization accuracy (i.e., 0.86 in Acc@1km and 0.97 in Acc@25km) and model explainability (i.e., via probabilistic distributions and localizability scores) of the proposed ProbGLC approach, highlighting the great potential of leveraging generative cross-view approach to facilitate location awareness for better and faster disaster response. The data and code is publicly available at https://github.com/bobleegogogo/ProbGLC",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-12-23T05:14:01+00:00",
      "updated": "2025-12-23T05:14:01+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20056v1",
      "file": "papers/2512.20056v1.pdf"
    },
    {
      "arxiv_id": "2512.20042v1",
      "title": "Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva",
      "authors": [
        {
          "name": "Nguyen Lam Phu Quy"
        },
        {
          "name": "Pham Phu Hoa"
        },
        {
          "name": "Tran Chi Nguyen"
        },
        {
          "name": "Dao Sy Duy Minh"
        },
        {
          "name": "Nguyen Hoang Minh Ngoc"
        },
        {
          "name": "Huynh Trung Kiet"
        }
      ],
      "abstract": "Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-23T04:21:15+00:00",
      "updated": "2025-12-23T04:21:15+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20042v1",
      "file": "papers/2512.20042v1.pdf"
    },
    {
      "arxiv_id": "2512.20033v1",
      "title": "FlashLips: 100-FPS Mask-Free Latent Lip-Sync using Reconstruction Instead of Diffusion or GANs",
      "authors": [
        {
          "name": "Andreas Zinonos"
        },
        {
          "name": "Michał Stypułkowski"
        },
        {
          "name": "Antoni Bigata"
        },
        {
          "name": "Stavros Petridis"
        },
        {
          "name": "Maja Pantic"
        },
        {
          "name": "Nikita Drobyshev"
        }
      ],
      "abstract": "We present FlashLips, a two-stage, mask-free lip-sync system that decouples lips control from rendering and achieves real-time performance running at over 100 FPS on a single GPU, while matching the visual quality of larger state-of-the-art models. Stage 1 is a compact, one-step latent-space editor that reconstructs an image using a reference identity, a masked target frame, and a low-dimensional lips-pose vector, trained purely with reconstruction losses - no GANs or diffusion. To remove explicit masks at inference, we use self-supervision: we generate mouth-altered variants of the target image, that serve as pseudo ground truth for fine-tuning, teaching the network to localize edits to the lips while preserving the rest. Stage 2 is an audio-to-pose transformer trained with a flow-matching objective to predict lips-poses vectors from speech. Together, these stages form a simple and stable pipeline that combines deterministic reconstruction with robust audio control, delivering high perceptual quality and faster-than-real-time speed.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T03:54:48+00:00",
      "updated": "2025-12-23T03:54:48+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20033v1",
      "file": "papers/2512.20033v1.pdf"
    },
    {
      "arxiv_id": "2512.20032v1",
      "title": "VALLR-Pin: Dual-Decoding Visual Speech Recognition for Mandarin with Pinyin-Guided LLM Refinement",
      "authors": [
        {
          "name": "Chang Sun"
        },
        {
          "name": "Dongliang Xie"
        },
        {
          "name": "Bo Qin"
        },
        {
          "name": "Hong Yang"
        }
      ],
      "abstract": "Visual Speech Recognition aims to transcribe spoken words from silent lip-motion videos. This task is particularly challenging for Mandarin, as visemes are highly ambiguous and homophones are prevalent. We propose VALLR-Pin, a novel two-stage framework that extends the recent VALLR architecture from English to Mandarin. First, a shared video encoder feeds into dual decoders, which jointly predict both Chinese character sequences and their standard Pinyin romanization. The multi-task learning of character and phonetic outputs fosters robust visual-semantic representations. During inference, the text decoder generates multiple candidate transcripts. We construct a prompt by concatenating the Pinyin output with these candidate Chinese sequences and feed it to a large language model to resolve ambiguities and refine the transcription. This provides the LLM with explicit phonetic context to correct homophone-induced errors. Finally, we fine-tune the LLM on synthetic noisy examples: we generate imperfect Pinyin-text pairs from intermediate VALLR-Pin checkpoints using the training data, creating instruction-response pairs for error correction. This endows the LLM with awareness of our model's specific error patterns. In summary, VALLR-Pin synergizes visual features with phonetic and linguistic context to improve Mandarin lip-reading performance.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T03:52:29+00:00",
      "updated": "2025-12-23T03:52:29+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20032v1",
      "file": "papers/2512.20032v1.pdf"
    },
    {
      "arxiv_id": "2512.20029v1",
      "title": "$\\text{H}^2$em: Learning Hierarchical Hyperbolic Embeddings for Compositional Zero-Shot Learning",
      "authors": [
        {
          "name": "Lin Li"
        },
        {
          "name": "Jiahui Li"
        },
        {
          "name": "Jiaming Lei"
        },
        {
          "name": "Jun Xiao"
        },
        {
          "name": "Feifei Shao"
        },
        {
          "name": "Long Chen"
        }
      ],
      "abstract": "Compositional zero-shot learning (CZSL) aims to recognize unseen state-object compositions by generalizing from a training set of their primitives (state and object). Current methods often overlook the rich hierarchical structures, such as the semantic hierarchy of primitives (e.g., apple fruit) and the conceptual hierarchy between primitives and compositions (e.g, sliced apple apple). A few recent efforts have shown effectiveness in modeling these hierarchies through loss regularization within Euclidean space. In this paper, we argue that they fail to scale to the large-scale taxonomies required for real-world CZSL: the space's polynomial volume growth in flat geometry cannot match the exponential structure, impairing generalization capacity. To this end, we propose H2em, a new framework that learns Hierarchical Hyperbolic EMbeddings for CZSL. H2em leverages the unique properties of hyperbolic geometry, a space naturally suited for embedding tree-like structures with low distortion. However, a naive hyperbolic mapping may suffer from hierarchical collapse and poor fine-grained discrimination. We further design two learning objectives to structure this space: a Dual-Hierarchical Entailment Loss that uses hyperbolic entailment cones to enforce the predefined hierarchies, and a Discriminative Alignment Loss with hard negative mining to establish a large geodesic distance between semantically similar compositions. Furthermore, we devise Hyperbolic Cross-Modal Attention to realize instance-aware cross-modal infusion within hyperbolic geometry. Extensive ablations on three benchmarks demonstrate that H2em establishes a new state-of-the-art in both closed-world and open-world scenarios. Our codes will be released.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T03:46:04+00:00",
      "updated": "2025-12-23T03:46:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20029v1",
      "file": "papers/2512.20029v1.pdf"
    },
    {
      "arxiv_id": "2512.20026v1",
      "title": "MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis",
      "authors": [
        {
          "name": "Ziwei Qin"
        },
        {
          "name": "Xuhui Song"
        },
        {
          "name": "Deqing Huang"
        },
        {
          "name": "Na Qin"
        },
        {
          "name": "Jun Li"
        }
      ],
      "abstract": "Graph neural networks are increasingly applied to multimodal medical diagnosis for their inherent relational modeling capabilities. However, their efficacy is often compromised by the prevailing reliance on a single, static graph built from indiscriminate features, hindering the ability to model patient-specific pathological relationships. To this end, the proposed Multi-Activation Plane Interaction Graph Neural Network (MAPI-GNN) reconstructs this single-graph paradigm by learning a multifaceted graph profile from semantically disentangled feature subspaces. The framework first uncovers latent graph-aware patterns via a multi-dimensional discriminator; these patterns then guide the dynamic construction of a stack of activation graphs; and this multifaceted profile is finally aggregated and contextualized by a relational fusion engine for a robust diagnosis. Extensive experiments on two diverse tasks, comprising over 1300 patient samples, demonstrate that MAPI-GNN significantly outperforms state-of-the-art methods.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T03:38:57+00:00",
      "updated": "2025-12-23T03:38:57+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20026v1",
      "file": "papers/2512.20026v1.pdf"
    },
    {
      "arxiv_id": "2512.20025v1",
      "title": "A Contextual Analysis of Driver-Facing and Dual-View Video Inputs for Distraction Detection in Naturalistic Driving Environments",
      "authors": [
        {
          "name": "Anthony Dontoh"
        },
        {
          "name": "Stephanie Ivey"
        },
        {
          "name": "Armstrong Aboah"
        }
      ],
      "abstract": "Despite increasing interest in computer vision-based distracted driving detection, most existing models rely exclusively on driver-facing views and overlook crucial environmental context that influences driving behavior. This study investigates whether incorporating road-facing views alongside driver-facing footage improves distraction detection accuracy in naturalistic driving conditions. Using synchronized dual-camera recordings from real-world driving, we benchmark three leading spatiotemporal action recognition architectures: SlowFast-R50, X3D-M, and SlowOnly-R50. Each model is evaluated under two input configurations: driver-only and stacked dual-view. Results show that while contextual inputs can improve detection in certain models, performance gains depend strongly on the underlying architecture. The single-pathway SlowOnly model achieved a 9.8 percent improvement with dual-view inputs, while the dual-pathway SlowFast model experienced a 7.2 percent drop in accuracy due to representational conflicts. These findings suggest that simply adding visual context is not sufficient and may lead to interference unless the architecture is specifically designed to support multi-view integration. This study presents one of the first systematic comparisons of single- and dual-view distraction detection models using naturalistic driving data and underscores the importance of fusion-aware design for future multimodal driver monitoring systems.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T03:36:26+00:00",
      "updated": "2025-12-23T03:36:26+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20025v1",
      "file": "papers/2512.20025v1.pdf"
    },
    {
      "arxiv_id": "2512.20013v1",
      "title": "SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images",
      "authors": [
        {
          "name": "Zepeng Xin"
        },
        {
          "name": "Kaiyu Li"
        },
        {
          "name": "Luodi Chen"
        },
        {
          "name": "Wanchen Li"
        },
        {
          "name": "Yuchen Xiao"
        },
        {
          "name": "Hui Qiao"
        },
        {
          "name": "Weizhan Zhang"
        },
        {
          "name": "Deyu Meng"
        },
        {
          "name": "Xiangyong Cao"
        }
      ],
      "abstract": "Effectively grounding complex language to pixels in remote sensing (RS) images is a critical challenge for applications like disaster response and environmental monitoring. Current models can parse simple, single-target commands but fail when presented with complex geospatial scenarios, e.g., segmenting objects at various granularities, executing multi-target instructions, and interpreting implicit user intent. To drive progress against these failures, we present LaSeRS, the first large-scale dataset built for comprehensive training and evaluation across four critical dimensions of language-guided segmentation: hierarchical granularity, target multiplicity, reasoning requirements, and linguistic variability. By capturing these dimensions, LaSeRS moves beyond simple commands, providing a benchmark for complex geospatial reasoning. This addresses a critical gap: existing datasets oversimplify, leading to sensitivity-prone real-world models. We also propose SegEarth-R2, an MLLM architecture designed for comprehensive language-guided segmentation in RS, which directly confronts these challenges. The model's effectiveness stems from two key improvements: (1) a spatial attention supervision mechanism specifically handles the localization of small objects and their components, and (2) a flexible and efficient segmentation query mechanism that handles both single-target and multi-target scenarios. Experimental results demonstrate that our SegEarth-R2 achieves outstanding performance on LaSeRS and other benchmarks, establishing a powerful baseline for the next generation of geospatial segmentation. All data and code will be released at https://github.com/earth-insights/SegEarth-R2.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T03:10:17+00:00",
      "updated": "2025-12-23T03:10:17+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20013v1",
      "file": "papers/2512.20013v1.pdf"
    },
    {
      "arxiv_id": "2512.20011v1",
      "title": "PaveSync: A Unified and Comprehensive Dataset for Pavement Distress Analysis and Classification",
      "authors": [
        {
          "name": "Blessing Agyei Kyem"
        },
        {
          "name": "Joshua Kofi Asamoah"
        },
        {
          "name": "Anthony Dontoh"
        },
        {
          "name": "Andrews Danyo"
        },
        {
          "name": "Eugene Denteh"
        },
        {
          "name": "Armstrong Aboah"
        }
      ],
      "abstract": "Automated pavement defect detection often struggles to generalize across diverse real-world conditions due to the lack of standardized datasets. Existing datasets differ in annotation styles, distress type definitions, and formats, limiting their integration for unified training. To address this gap, we introduce a comprehensive benchmark dataset that consolidates multiple publicly available sources into a standardized collection of 52747 images from seven countries, with 135277 bounding box annotations covering 13 distinct distress types. The dataset captures broad real-world variation in image quality, resolution, viewing angles, and weather conditions, offering a unique resource for consistent training and evaluation. Its effectiveness was demonstrated through benchmarking with state-of-the-art object detection models including YOLOv8-YOLOv12, Faster R-CNN, and DETR, which achieved competitive performance across diverse scenarios. By standardizing class definitions and annotation formats, this dataset provides the first globally representative benchmark for pavement defect detection and enables fair comparison of models, including zero-shot transfer to new environments.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T03:09:49+00:00",
      "updated": "2025-12-23T03:09:49+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20011v1",
      "file": "papers/2512.20011v1.pdf"
    },
    {
      "arxiv_id": "2512.20000v1",
      "title": "Few-Shot-Based Modular Image-to-Video Adapter for Diffusion Models",
      "authors": [
        {
          "name": "Zhenhao Li"
        },
        {
          "name": "Shaohan Yi"
        },
        {
          "name": "Zheng Liu"
        },
        {
          "name": "Leonartinus Gao"
        },
        {
          "name": "Minh Ngoc Le"
        },
        {
          "name": "Ambrose Ling"
        },
        {
          "name": "Zhuoran Wang"
        },
        {
          "name": "Md Amirul Islam"
        },
        {
          "name": "Zhixiang Chi"
        },
        {
          "name": "Yuanhao Yu"
        }
      ],
      "abstract": "Diffusion models (DMs) have recently achieved impressive photorealism in image and video generation. However, their application to image animation remains limited, even when trained on large-scale datasets. Two primary challenges contribute to this: the high dimensionality of video signals leads to a scarcity of training data, causing DMs to favor memorization over prompt compliance when generating motion; moreover, DMs struggle to generalize to novel motion patterns not present in the training set, and fine-tuning them to learn such patterns, especially using limited training data, is still under-explored. To address these limitations, we propose Modular Image-to-Video Adapter (MIVA), a lightweight sub-network attachable to a pre-trained DM, each designed to capture a single motion pattern and scalable via parallelization. MIVAs can be efficiently trained on approximately ten samples using a single consumer-grade GPU. At inference time, users can specify motion by selecting one or multiple MIVAs, eliminating the need for prompt engineering. Extensive experiments demonstrate that MIVA enables more precise motion control while maintaining, or even surpassing, the generation quality of models trained on significantly larger datasets.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T02:52:18+00:00",
      "updated": "2025-12-23T02:52:18+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.20000v1",
      "file": "papers/2512.20000v1.pdf"
    },
    {
      "arxiv_id": "2512.19990v1",
      "title": "A Dual-Branch Local-Global Framework for Cross-Resolution Land Cover Mapping",
      "authors": [
        {
          "name": "Peng Gao"
        },
        {
          "name": "Ke Li"
        },
        {
          "name": "Di Wang"
        },
        {
          "name": "Yongshan Zhu"
        },
        {
          "name": "Yiming Zhang"
        },
        {
          "name": "Xuemei Luo"
        },
        {
          "name": "Yifeng Wang"
        }
      ],
      "abstract": "Cross-resolution land cover mapping aims to produce high-resolution semantic predictions from coarse or low-resolution supervision, yet the severe resolution mismatch makes effective learning highly challenging. Existing weakly supervised approaches often struggle to align fine-grained spatial structures with coarse labels, leading to noisy supervision and degraded mapping accuracy. To tackle this problem, we propose DDTM, a dual-branch weakly supervised framework that explicitly decouples local semantic refinement from global contextual reasoning. Specifically, DDTM introduces a diffusion-based branch to progressively refine fine-scale local semantics under coarse supervision, while a transformer-based branch enforces long-range contextual consistency across large spatial extents. In addition, we design a pseudo-label confidence evaluation module to mitigate noise induced by cross-resolution inconsistencies and to selectively exploit reliable supervisory signals. Extensive experiments demonstrate that DDTM establishes a new state-of-the-art on the Chesapeake Bay benchmark, achieving 66.52\\% mIoU and substantially outperforming prior weakly supervised methods. The code is available at https://github.com/gpgpgp123/DDTM.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T02:32:02+00:00",
      "updated": "2025-12-23T02:32:02+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19990v1",
      "file": "papers/2512.19990v1.pdf"
    },
    {
      "arxiv_id": "2512.19989v1",
      "title": "A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection",
      "authors": [
        {
          "name": "Tamim Ahasan Rijon"
        },
        {
          "name": "Yeasin Arafath"
        }
      ],
      "abstract": "As a significant agricultural country, Bangladesh utilizes its fertile land for guava cultivation and dedicated labor to boost its economic development. In a nation like Bangladesh, enhancing guava production and agricultural practices plays a crucial role in its economy. Anthracnose and fruit fly infection can lower the quality and productivity of guava, a crucial tropical fruit. Expert systems that detect diseases early can reduce losses and safeguard the harvest. Images of guava fruits classified into the Healthy, Fruit Flies, and Anthracnose classes are included in the Guava Fruit Disease Dataset 2024 (GFDD24), which comes from plantations in Rajshahi and Pabna, Bangladesh. This study aims to create models using CNN alongside traditional machine learning techniques that can effectively identify guava diseases in locally cultivated varieties in Bangladesh. In order to achieve the highest classification accuracy of approximately 99.99% for the guava dataset, we propose utilizing ensemble models that combine CNNML with Gradient Boosting Machine. In general, the CNN-ML cascade framework exhibits strong, high-accuracy guava disease detection that is appropriate for real-time agricultural monitoring systems.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-12-23T02:30:50+00:00",
      "updated": "2025-12-23T02:30:50+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19989v1",
      "file": "papers/2512.19989v1.pdf"
    },
    {
      "arxiv_id": "2512.19982v1",
      "title": "WSD-MIL: Window Scale Decay Multiple Instance Learning for Whole Slide Image Classification",
      "authors": [
        {
          "name": "Le Feng"
        },
        {
          "name": "Li Xiao"
        }
      ],
      "abstract": "In recent years, the integration of pre-trained foundational models with multiple instance learning (MIL) has improved diagnostic accuracy in computational pathology. However, existing MIL methods focus on optimizing feature extractors and aggregation strategies while overlooking the complex semantic relationships among instances within whole slide image (WSI). Although Transformer-based MIL approaches aiming to model instance dependencies, the quadratic computational complexity limits their scalability to large-scale WSIs. Moreover, due to the pronounced variations in tumor region scales across different WSIs, existing Transformer-based methods employing fixed-scale attention mechanisms face significant challenges in precisely capturing local instance correlations and fail to account for the distance-based decay effect of patch relevance. To address these challenges, we propose window scale decay MIL (WSD-MIL), designed to enhance the capacity to model tumor regions of varying scales while improving computational efficiency. WSD-MIL comprises: 1) a window scale decay based attention module, which employs a cluster-based sampling strategy to reduce computational costs while progressively decaying attention window-scale to capture local instance relationships at varying scales; and 2) a squeeze-and-excitation based region gate module, which dynamically adjusts window weights to enhance global information modeling. Experimental results demonstrate that WSD-MIL achieves state-of-the-art performance on the CAMELYON16 and TCGA-BRCA datasets while reducing 62% of the computational memory. The code will be publicly available.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T02:10:24+00:00",
      "updated": "2025-12-23T02:10:24+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19982v1",
      "file": "papers/2512.19982v1.pdf"
    },
    {
      "arxiv_id": "2512.19954v1",
      "title": "HistoWAS: A Pathomics Framework for Large-Scale Feature-Wide Association Studies of Tissue Topology and Patient Outcomes",
      "authors": [
        {
          "name": "Yuechen Yang"
        },
        {
          "name": "Junlin Guo"
        },
        {
          "name": "Yanfan Zhu"
        },
        {
          "name": "Jialin Yue"
        },
        {
          "name": "Junchao Zhu"
        },
        {
          "name": "Yu Wang"
        },
        {
          "name": "Shilin Zhao"
        },
        {
          "name": "Haichun Yang"
        },
        {
          "name": "Xingyi Guo"
        },
        {
          "name": "Jovan Tanevski"
        },
        {
          "name": "Laura Barisoni"
        },
        {
          "name": "Avi Z. Rosenberg"
        },
        {
          "name": "Yuankai Huo"
        }
      ],
      "abstract": "High-throughput \"pathomic\" analysis of Whole Slide Images (WSIs) offers new opportunities to study tissue characteristics and for biomarker discovery. However, the clinical relevance of the tissue characteristics at the micro- and macro-environment level is limited by the lack of tools that facilitate the measurement of the spatial interaction of individual structure characteristics and their association with clinical parameters. To address these challenges, we introduce HistoWAS (Histology-Wide Association Study), a computational framework designed to link tissue spatial organization to clinical outcomes. Specifically, HistoWAS implements (1) a feature space that augments conventional metrics with 30 topological and spatial features, adapted from Geographic Information Systems (GIS) point pattern analysis, to quantify tissue micro-architecture; and (2) an association study engine, inspired by Phenome-Wide Association Studies (PheWAS), that performs mass univariate regression for each feature with statistical correction. As a proof of concept, we applied HistoWAS to analyze a total of 102 features (72 conventional object-level features and our 30 spatial features) using 385 PAS-stained WSIs from 206 participants in the Kidney Precision Medicine Project (KPMP). The code and data have been released to https://github.com/hrlblab/histoWAS.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T00:58:27+00:00",
      "updated": "2025-12-23T00:58:27+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19954v1",
      "file": "papers/2512.19954v1.pdf"
    },
    {
      "arxiv_id": "2512.19949v1",
      "title": "How Much 3D Do Video Foundation Models Encode?",
      "authors": [
        {
          "name": "Zixuan Huang"
        },
        {
          "name": "Xiang Li"
        },
        {
          "name": "Zhaoyang Lv"
        },
        {
          "name": "James M. Rehg"
        }
      ],
      "abstract": "Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-23T00:38:52+00:00",
      "updated": "2025-12-23T00:38:52+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19949v1",
      "file": "papers/2512.19949v1.pdf"
    },
    {
      "arxiv_id": "2512.19943v1",
      "title": "SE360: Semantic Edit in 360$^\\circ$ Panoramas via Hierarchical Data Construction",
      "authors": [
        {
          "name": "Haoyi Zhong"
        },
        {
          "name": "Fang-Lue Zhang"
        },
        {
          "name": "Andrew Chalmers"
        },
        {
          "name": "Taehyun Rhee"
        }
      ],
      "abstract": "While instruction-based image editing is emerging, extending it to 360$^\\circ$ panoramas introduces additional challenges. Existing methods often produce implausible results in both equirectangular projections (ERP) and perspective views. To address these limitations, we propose SE360, a novel framework for multi-condition guided object editing in 360$^\\circ$ panoramas. At its core is a novel coarse-to-fine autonomous data generation pipeline without manual intervention. This pipeline leverages a Vision-Language Model (VLM) and adaptive projection adjustment for hierarchical analysis, ensuring the holistic segmentation of objects and their physical context. The resulting data pairs are both semantically meaningful and geometrically consistent, even when sourced from unlabeled panoramas. Furthermore, we introduce a cost-effective, two-stage data refinement strategy to improve data realism and mitigate model overfitting to erase artifacts. Based on the constructed dataset, we train a Transformer-based diffusion model to allow flexible object editing guided by text, mask, or reference image in 360$^\\circ$ panoramas. Our experiments demonstrate that our method outperforms existing methods in both visual quality and semantic accuracy.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-23T00:24:46+00:00",
      "updated": "2025-12-23T00:24:46+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19943v1",
      "file": "papers/2512.19943v1.pdf"
    },
    {
      "arxiv_id": "2512.19941v1",
      "title": "Block-Recurrent Dynamics in Vision Transformers",
      "authors": [
        {
          "name": "Mozes Jacobs"
        },
        {
          "name": "Thomas Fel"
        },
        {
          "name": "Richard Hakim"
        },
        {
          "name": "Alessandra Brondetta"
        },
        {
          "name": "Demba Ba"
        },
        {
          "name": "T. Andy Keller"
        }
      ],
      "abstract": "As Vision Transformers (ViTs) become standard vision backbones, a mechanistic account of their computational phenomenology is essential. Despite architectural cues that hint at dynamical structure, there is no settled framework that interprets Transformer depth as a well-characterized flow. In this work, we introduce the Block-Recurrent Hypothesis (BRH), arguing that trained ViTs admit a block-recurrent depth structure such that the computation of the original $L$ blocks can be accurately rewritten using only $k \\ll L$ distinct blocks applied recurrently. Across diverse ViTs, between-layer representational similarity matrices suggest few contiguous phases. To determine whether these phases reflect genuinely reusable computation, we train block-recurrent surrogates of pretrained ViTs: Recurrent Approximations to Phase-structured TransfORmers (Raptor). In small-scale, we demonstrate that stochastic depth and training promote recurrent structure and subsequently correlate with our ability to accurately fit Raptor. We then provide an empirical existence proof for BRH by training a Raptor model to recover $96\\%$ of DINOv2 ImageNet-1k linear probe accuracy in only 2 blocks at equivalent computational cost. Finally, we leverage our hypothesis to develop a program of Dynamical Interpretability. We find i) directional convergence into class-dependent angular basins with self-correcting trajectories under small perturbations, ii) token-specific dynamics, where cls executes sharp late reorientations while patch tokens exhibit strong late-stage coherence toward their mean direction, and iii) a collapse to low rank updates in late depth, consistent with convergence to low-dimensional attractors. Altogether, we find a compact recurrent program emerges along ViT depth, pointing to a low-complexity normative solution that enables these models to be studied through principled dynamical systems analysis.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-23T00:18:23+00:00",
      "updated": "2025-12-23T00:18:23+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19941v1",
      "file": "papers/2512.19941v1.pdf"
    },
    {
      "arxiv_id": "2512.19934v1",
      "title": "Vehicle-centric Perception via Multimodal Structured Pre-training",
      "authors": [
        {
          "name": "Wentao Wu"
        },
        {
          "name": "Xiao Wang"
        },
        {
          "name": "Chenglong Li"
        },
        {
          "name": "Jin Tang"
        },
        {
          "name": "Bin Luo"
        }
      ],
      "abstract": "Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-12-22T23:42:45+00:00",
      "updated": "2025-12-22T23:42:45+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19934v1",
      "file": "papers/2512.19934v1.pdf"
    },
    {
      "arxiv_id": "2512.19928v1",
      "title": "Unified Brain Surface and Volume Registration",
      "authors": [
        {
          "name": "S. Mazdak Abulnaga"
        },
        {
          "name": "Andrew Hoopes"
        },
        {
          "name": "Malte Hoffmann"
        },
        {
          "name": "Robin Magnet"
        },
        {
          "name": "Maks Ovsjanikov"
        },
        {
          "name": "Lilla Zöllei"
        },
        {
          "name": "John Guttag"
        },
        {
          "name": "Bruce Fischl"
        },
        {
          "name": "Adrian Dalca"
        }
      ],
      "abstract": "Accurate registration of brain MRI scans is fundamental for cross-subject analysis in neuroscientific studies. This involves aligning both the cortical surface of the brain and the interior volume. Traditional methods treat volumetric and surface-based registration separately, which often leads to inconsistencies that limit downstream analyses. We propose a deep learning framework, NeurAlign, that registers $3$D brain MRI images by jointly aligning both cortical and subcortical regions through a unified volume-and-surface-based representation. Our approach leverages an intermediate spherical coordinate space to bridge anatomical surface topology with volumetric anatomy, enabling consistent and anatomically accurate alignment. By integrating spherical registration into the learning, our method ensures geometric coherence between volume and surface domains. In a series of experiments on both in-domain and out-of-domain datasets, our method consistently outperforms both classical and machine learning-based registration methods -- improving the Dice score by up to 7 points while maintaining regular deformation fields. Additionally, it is orders of magnitude faster than the standard method for this task, and is simpler to use because it requires no additional inputs beyond an MRI scan. With its superior accuracy, fast inference, and ease of use, NeurAlign sets a new standard for joint cortical and subcortical registration.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-22T23:05:26+00:00",
      "updated": "2025-12-22T23:05:26+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19928v1",
      "file": "papers/2512.19928v1.pdf"
    },
    {
      "arxiv_id": "2512.19918v1",
      "title": "Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs",
      "authors": [
        {
          "name": "Houston H. Zhang"
        },
        {
          "name": "Tao Zhang"
        },
        {
          "name": "Baoze Lin"
        },
        {
          "name": "Yuanqi Xue"
        },
        {
          "name": "Yincheng Zhu"
        },
        {
          "name": "Huan Liu"
        },
        {
          "name": "Li Gu"
        },
        {
          "name": "Linfeng Ye"
        },
        {
          "name": "Ziqiang Wang"
        },
        {
          "name": "Xinxin Zuo"
        },
        {
          "name": "Yang Wang"
        },
        {
          "name": "Yuanhao Yu"
        },
        {
          "name": "Zhixiang Chi"
        }
      ],
      "abstract": "User interface to code (UI2Code) aims to generate executable code that can faithfully reconstruct a given input UI. Prior work focuses largely on web pages and mobile screens, leaving app widgets underexplored. Unlike web or mobile UIs with rich hierarchical context, widgets are compact, context-free micro-interfaces that summarize key information through dense layouts and iconography under strict spatial constraints. Moreover, while (image, code) pairs are widely available for web or mobile UIs, widget designs are proprietary and lack accessible markup. We formalize this setting as the Widget-to-Code (Widget2Code) and introduce an image-only widget benchmark with fine-grained, multi-dimensional evaluation metrics. Benchmarking shows that although generalized multimodal large language models (MLLMs) outperform specialized UI2Code methods, they still produce unreliable and visually inconsistent code. To address these limitations, we develop a baseline that jointly advances perceptual understanding and structured code generation. At the perceptual level, we follow widget design principles to assemble atomic components into complete layouts, equipped with icon retrieval and reusable visualization modules. At the system level, we design an end-to-end infrastructure, WidgetFactory, which includes a framework-agnostic widget-tailored domain-specific language (WidgetDSL) and a compiler that translates it into multiple front-end implementations (e.g., React, HTML/CSS). An adaptive rendering module further refines spatial dimensions to satisfy compactness constraints. Together, these contributions substantially enhance visual fidelity, establishing a strong baseline and unified infrastructure for future Widget2Code research.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T22:45:39+00:00",
      "updated": "2025-12-22T22:45:39+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19918v1",
      "file": "papers/2512.19918v1.pdf"
    },
    {
      "arxiv_id": "2512.19871v1",
      "title": "HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction",
      "authors": [
        {
          "name": "Jong Wook Kim"
        },
        {
          "name": "Wonseok Roh"
        },
        {
          "name": "Ha Dam Baek"
        },
        {
          "name": "Pilhyeon Lee"
        },
        {
          "name": "Jonghyun Choi"
        },
        {
          "name": "Sangpil Kim"
        }
      ],
      "abstract": "3D Panoptic Occupancy Prediction aims to reconstruct a dense volumetric scene map by predicting the semantic class and instance identity of every occupied region in 3D space. Achieving such fine-grained 3D understanding requires precise geometric reasoning and spatially consistent scene representation across complex environments. However, existing approaches often struggle to maintain precise geometry and capture the precise spatial range of 3D instances critical for robust panoptic separation. To overcome these limitations, we introduce HyGE-Occ, a novel framework that leverages a hybrid view-transformation branch with 3D Gaussian and edge priors to enhance both geometric consistency and boundary awareness in 3D panoptic occupancy prediction. HyGE-Occ employs a hybrid view-transformation branch that fuses a continuous Gaussian-based depth representation with a discretized depth-bin formulation, producing BEV features with improved geometric consistency and structural coherence. In parallel, we extract edge maps from BEV features and use them as auxiliary information to learn edge cues. In our extensive experiments on the Occ3D-nuScenes dataset, HyGE-Occ outperforms existing work, demonstrating superior 3D geometric reasoning.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T20:59:15+00:00",
      "updated": "2025-12-22T20:59:15+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19871v1",
      "file": "papers/2512.19871v1.pdf"
    },
    {
      "arxiv_id": "2512.19850v1",
      "title": "RANSAC Scoring Functions: Analysis and Reality Check",
      "authors": [
        {
          "name": "A. Shekhovtsov"
        }
      ],
      "abstract": "We revisit the problem of assigning a score (a quality of fit) to candidate geometric models -- one of the key components of RANSAC for robust geometric fitting. In a non-robust setting, the ``gold standard'' scoring function, known as the geometric error, follows from a probabilistic model with Gaussian noises. We extend it to spherical noises. In a robust setting, we consider a mixture with uniformly distributed outliers and show that a threshold-based parameterization leads to a unified view of likelihood-based and robust M-estimators and associated local optimization schemes.\n  Next we analyze MAGSAC++ which stands out for two reasons. First, it achieves the best results according to existing benchmarks. Second, it makes quite different modeling assumptions and derivation steps. We discovered, however that the derivation does not correspond to sound principles and the resulting score function is in fact numerically equivalent to a simple Gaussian-uniform likelihood, a basic model within the proposed framework.\n  Finally, we propose an experimental methodology for evaluating scoring functions: assuming either a large validation set, or a small random validation set in expectation. We find that all scoring functions, including using a learned inlier distribution, perform identically. In particular, MAGSAC++ score is found to be neither better performing than simple contenders nor less sensitive to the choice of the threshold hyperparameter.\n  Our theoretical and experimental analysis thus comprehensively revisit the state-of-the-art, which is critical for any future research seeking to improve the methods or apply them to other robust fitting problems.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "stat.AP"
      ],
      "published": "2025-12-22T20:08:46+00:00",
      "updated": "2025-12-22T20:08:46+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19850v1",
      "file": "papers/2512.19850v1.pdf"
    },
    {
      "arxiv_id": "2512.19823v2",
      "title": "Learning to Refocus with Video Diffusion Models",
      "authors": [
        {
          "name": "SaiKiran Tedla"
        },
        {
          "name": "Zhoutong Zhang"
        },
        {
          "name": "Xuaner Zhang"
        },
        {
          "name": "Shumian Xin"
        }
      ],
      "abstract": "Focus is a cornerstone of photography, yet autofocus systems often fail to capture the intended subject, and users frequently wish to adjust focus after capture. We introduce a novel method for realistic post-capture refocusing using video diffusion models. From a single defocused image, our approach generates a perceptually accurate focal stack, represented as a video sequence, enabling interactive refocusing and unlocking a range of downstream applications. We release a large-scale focal stack dataset acquired under diverse real-world smartphone conditions to support this work and future research. Our method consistently outperforms existing approaches in both perceptual quality and robustness across challenging scenarios, paving the way for more advanced focus-editing capabilities in everyday photography. Code and data are available at https://learn2refocus.github.io",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T19:29:57+00:00",
      "updated": "2025-12-24T16:32:32+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19823v2",
      "file": "papers/2512.19823v2.pdf"
    },
    {
      "arxiv_id": "2512.19817v1",
      "title": "Generating the Past, Present and Future from a Motion-Blurred Image",
      "authors": [
        {
          "name": "SaiKiran Tedla"
        },
        {
          "name": "Kelly Zhu"
        },
        {
          "name": "Trevor Canham"
        },
        {
          "name": "Felix Taubner"
        },
        {
          "name": "Michael S. Brown"
        },
        {
          "name": "Kiriakos N. Kutulakos"
        },
        {
          "name": "David B. Lindell"
        }
      ],
      "abstract": "We seek to answer the question: what can a motion-blurred image reveal about a scene's past, present, and future? Although motion blur obscures image details and degrades visual quality, it also encodes information about scene and camera motion during an exposure. Previous techniques leverage this information to estimate a sharp image from an input blurry one, or to predict a sequence of video frames showing what might have occurred at the moment of image capture. However, they rely on handcrafted priors or network architectures to resolve ambiguities in this inverse problem, and do not incorporate image and video priors on large-scale datasets. As such, existing methods struggle to reproduce complex scene dynamics and do not attempt to recover what occurred before or after an image was taken. Here, we introduce a new technique that repurposes a pre-trained video diffusion model trained on internet-scale datasets to recover videos revealing complex scene dynamics during the moment of capture and what might have occurred immediately into the past or future. Our approach is robust and versatile; it outperforms previous methods for this task, generalizes to challenging in-the-wild images, and supports downstream tasks such as recovering camera trajectories, object motion, and dynamic 3D scene structure. Code and data are available at https://blur2vid.github.io",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.GR"
      ],
      "published": "2025-12-22T19:12:33+00:00",
      "updated": "2025-12-22T19:12:33+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19817v1",
      "file": "papers/2512.19817v1.pdf"
    },
    {
      "arxiv_id": "2512.19693v1",
      "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding",
      "authors": [
        {
          "name": "Weichen Fan"
        },
        {
          "name": "Haiwen Diao"
        },
        {
          "name": "Quan Wang"
        },
        {
          "name": "Dahua Lin"
        },
        {
          "name": "Ziwei Liu"
        }
      ],
      "abstract": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T18:59:57+00:00",
      "updated": "2025-12-22T18:59:57+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19693v1",
      "file": "papers/2512.19693v1.pdf"
    },
    {
      "arxiv_id": "2512.19692v1",
      "title": "Interact2Ar: Full-Body Human-Human Interaction Generation via Autoregressive Diffusion Models",
      "authors": [
        {
          "name": "Pablo Ruiz-Ponce"
        },
        {
          "name": "Sergio Escalera"
        },
        {
          "name": "José García-Rodríguez"
        },
        {
          "name": "Jiankang Deng"
        },
        {
          "name": "Rolandos Alexandros Potamias"
        }
      ],
      "abstract": "Generating realistic human-human interactions is a challenging task that requires not only high-quality individual body and hand motions, but also coherent coordination among all interactants. Due to limitations in available data and increased learning complexity, previous methods tend to ignore hand motions, limiting the realism and expressivity of the interactions. Additionally, current diffusion-based approaches generate entire motion sequences simultaneously, limiting their ability to capture the reactive and adaptive nature of human interactions. To address these limitations, we introduce Interact2Ar, the first end-to-end text-conditioned autoregressive diffusion model for generating full-body, human-human interactions. Interact2Ar incorporates detailed hand kinematics through dedicated parallel branches, enabling high-fidelity full-body generation. Furthermore, we introduce an autoregressive pipeline coupled with a novel memory technique that facilitates adaptation to the inherent variability of human interactions using efficient large context windows. The adaptability of our model enables a series of downstream applications, including temporal motion composition, real-time adaptation to disturbances, and extension beyond dyadic to multi-person scenarios. To validate the generated motions, we introduce a set of robust evaluators and extended metrics designed specifically for assessing full-body interactions. Through quantitative and qualitative experiments, we demonstrate the state-of-the-art performance of Interact2Ar.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T18:59:50+00:00",
      "updated": "2025-12-22T18:59:50+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19692v1",
      "file": "papers/2512.19692v1.pdf"
    },
    {
      "arxiv_id": "2512.19686v1",
      "title": "Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models",
      "authors": [
        {
          "name": "Zixuan Ye"
        },
        {
          "name": "Quande Liu"
        },
        {
          "name": "Cong Wei"
        },
        {
          "name": "Yuanxing Zhang"
        },
        {
          "name": "Xintao Wang"
        },
        {
          "name": "Pengfei Wan"
        },
        {
          "name": "Kun Gai"
        },
        {
          "name": "Wenhan Luo"
        }
      ],
      "abstract": "Recently, the introduction of Chain-of-Thought (CoT) has largely improved the generation ability of unified models. However, it is observed that the current thinking process during generation mainly focuses on the text consistency with the text prompt, ignoring the \\textbf{visual context consistency} with the visual reference images during the multi-modal generation, e.g., multi-reference generation. The lack of such consistency results in the failure in maintaining key visual features (like human ID, object attribute, style). To this end, we integrate the visual context consistency into the reasoning of unified models, explicitly motivating the model to sustain such consistency by 1) Adaptive Visual Planning: generating structured visual check list to figure out the visual element of needed consistency keeping, and 2) Iterative Visual Correction: performing self-reflection with the guidance of check lists and refining the generated result in an iterative manner. To achieve this, we use supervised finetuning to teach the model how to plan the visual checking, conduct self-reflection and self-refinement, and use flow-GRPO to further enhance the visual consistency through a customized visual checking reward. The experiments show that our method outperforms both zero-shot unified models and those with text CoTs in multi-modal generation, demonstrating higher visual context consistency.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T18:59:03+00:00",
      "updated": "2025-12-22T18:59:03+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19686v1",
      "file": "papers/2512.19686v1.pdf"
    },
    {
      "arxiv_id": "2512.19684v1",
      "title": "Zero-shot Reconstruction of In-Scene Object Manipulation from Video",
      "authors": [
        {
          "name": "Dixuan Lin"
        },
        {
          "name": "Tianyou Wang"
        },
        {
          "name": "Zhuoyang Pan"
        },
        {
          "name": "Yufu Wang"
        },
        {
          "name": "Lingjie Liu"
        },
        {
          "name": "Kostas Daniilidis"
        }
      ],
      "abstract": "We build the first system to address the problem of reconstructing in-scene object manipulation from a monocular RGB video. It is challenging due to ill-posed scene reconstruction, ambiguous hand-object depth, and the need for physically plausible interactions. Existing methods operate in hand centric coordinates and ignore the scene, hindering metric accuracy and practical use. In our method, we first use data-driven foundation models to initialize the core components, including the object mesh and poses, the scene point cloud, and the hand poses. We then apply a two-stage optimization that recovers a complete hand-object motion from grasping to interaction, which remains consistent with the scene information observed in the input video.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-12-22T18:58:29+00:00",
      "updated": "2025-12-22T18:58:29+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19684v1",
      "file": "papers/2512.19684v1.pdf"
    },
    {
      "arxiv_id": "2512.19680v1",
      "title": "VA-$π$: Variational Policy Alignment for Pixel-Aware Autoregressive Generation",
      "authors": [
        {
          "name": "Xinyao Liao"
        },
        {
          "name": "Qiyuan He"
        },
        {
          "name": "Kai Xu"
        },
        {
          "name": "Xiaoye Qu"
        },
        {
          "name": "Yicong Li"
        },
        {
          "name": "Wei Wei"
        },
        {
          "name": "Angela Yao"
        }
      ],
      "abstract": "Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-$π$, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-$π$ formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-$π$ introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-$π$ enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at https://github.com/Lil-Shake/VA-Pi.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T18:54:30+00:00",
      "updated": "2025-12-22T18:54:30+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19680v1",
      "file": "papers/2512.19680v1.pdf"
    },
    {
      "arxiv_id": "2512.19678v1",
      "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
      "authors": [
        {
          "name": "Hanyang Kong"
        },
        {
          "name": "Xingyi Yang"
        },
        {
          "name": "Xiaoxu Zheng"
        },
        {
          "name": "Xinchao Wang"
        }
      ],
      "abstract": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \\href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-22T18:53:50+00:00",
      "updated": "2025-12-22T18:53:50+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19678v1",
      "file": "papers/2512.19678v1.pdf"
    },
    {
      "arxiv_id": "2512.19676v1",
      "title": "Efficient Vision Mamba for MRI Super-Resolution via Hybrid Selective Scanning",
      "authors": [
        {
          "name": "Mojtaba Safari"
        },
        {
          "name": "Shansong Wang"
        },
        {
          "name": "Vanessa L Wildman"
        },
        {
          "name": "Mingzhe Hu"
        },
        {
          "name": "Zach Eidex"
        },
        {
          "name": "Chih-Wei Chang"
        },
        {
          "name": "Erik H Middlebrooks"
        },
        {
          "name": "Richard L. J Qiu"
        },
        {
          "name": "Pretesh Patel"
        },
        {
          "name": "Ashesh B. Jania"
        },
        {
          "name": "Hui Mao"
        },
        {
          "name": "Zhen Tian"
        },
        {
          "name": "Xiaofeng Yang"
        }
      ],
      "abstract": "Background: High-resolution MRI is critical for diagnosis, but long acquisition times limit clinical use. Super-resolution (SR) can enhance resolution post-scan, yet existing deep learning methods face fidelity-efficiency trade-offs. Purpose: To develop a computationally efficient and accurate deep learning framework for MRI SR that preserves anatomical detail for clinical integration. Materials and Methods: We propose a novel SR framework combining multi-head selective state-space models (MHSSM) with a lightweight channel MLP. The model uses 2D patch extraction with hybrid scanning to capture long-range dependencies. Each MambaFormer block integrates MHSSM, depthwise convolutions, and gated channel mixing. Evaluation used 7T brain T1 MP2RAGE maps (n=142) and 1.5T prostate T2w MRI (n=334). Comparisons included Bicubic interpolation, GANs (CycleGAN, Pix2pix, SPSR), transformers (SwinIR), Mamba (MambaIR), and diffusion models (I2SB, Res-SRDiff). Results: Our model achieved superior performance with exceptional efficiency. For 7T brain data: SSIM=0.951+-0.021, PSNR=26.90+-1.41 dB, LPIPS=0.076+-0.022, GMSD=0.083+-0.017, significantly outperforming all baselines (p<0.001). For prostate data: SSIM=0.770+-0.049, PSNR=27.15+-2.19 dB, LPIPS=0.190+-0.095, GMSD=0.087+-0.013. The framework used only 0.9M parameters and 57 GFLOPs, reducing parameters by 99.8% and computation by 97.5% versus Res-SRDiff, while outperforming SwinIR and MambaIR in accuracy and efficiency. Conclusion: The proposed framework provides an efficient, accurate MRI SR solution, delivering enhanced anatomical detail across datasets. Its low computational demand and state-of-the-art performance show strong potential for clinical translation.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "physics.med-ph"
      ],
      "published": "2025-12-22T18:53:13+00:00",
      "updated": "2025-12-22T18:53:13+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19676v1",
      "file": "papers/2512.19676v1.pdf"
    },
    {
      "arxiv_id": "2512.19663v1",
      "title": "Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis",
      "authors": [
        {
          "name": "Argha Kamal Samanta"
        },
        {
          "name": "Harshika Goyal"
        },
        {
          "name": "Vasudha Joshi"
        },
        {
          "name": "Tushar Mungle"
        },
        {
          "name": "Pabitra Mitra"
        }
      ],
      "abstract": "Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-22T18:41:45+00:00",
      "updated": "2025-12-22T18:41:45+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19663v1",
      "file": "papers/2512.19663v1.pdf"
    },
    {
      "arxiv_id": "2512.19661v1",
      "title": "Over++: Generative Video Compositing for Layer Interaction Effects",
      "authors": [
        {
          "name": "Luchao Qi"
        },
        {
          "name": "Jiaye Wu"
        },
        {
          "name": "Jun Myeong Choi"
        },
        {
          "name": "Cary Phillips"
        },
        {
          "name": "Roni Sengupta"
        },
        {
          "name": "Dan B Goldman"
        }
      ],
      "abstract": "In professional video compositing workflows, artists must manually create environmental interactions-such as shadows, reflections, dust, and splashes-between foreground subjects and background layers. Existing video generative models struggle to preserve the input video while adding such effects, and current video inpainting methods either require costly per-frame masks or yield implausible results. We introduce augmented compositing, a new task that synthesizes realistic, semi-transparent environmental effects conditioned on text prompts and input video layers, while preserving the original scene. To address this task, we present Over++, a video effect generation framework that makes no assumptions about camera pose, scene stationarity, or depth supervision. We construct a paired effect dataset tailored for this task and introduce an unpaired augmentation strategy that preserves text-driven editability. Our method also supports optional mask control and keyframe guidance without requiring dense annotations. Despite training on limited data, Over++ produces diverse and realistic environmental effects and outperforms existing baselines in both effect generation and scene preservation.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T18:39:58+00:00",
      "updated": "2025-12-22T18:39:58+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19661v1",
      "file": "papers/2512.19661v1.pdf"
    },
    {
      "arxiv_id": "2512.19648v1",
      "title": "4D Gaussian Splatting as a Learned Dynamical System",
      "authors": [
        {
          "name": "Arnold Caleb Asiimwe"
        },
        {
          "name": "Carl Vondrick"
        }
      ],
      "abstract": "We reinterpret 4D Gaussian Splatting as a continuous-time dynamical system, where scene motion arises from integrating a learned neural dynamical field rather than applying per-frame deformations. This formulation, which we call EvoGS, treats the Gaussian representation as an evolving physical system whose state evolves continuously under a learned motion law. This unlocks capabilities absent in deformation-based approaches:(1) sample-efficient learning from sparse temporal supervision by modeling the underlying motion law; (2) temporal extrapolation enabling forward and backward prediction beyond observed time ranges; and (3) compositional dynamics that allow localized dynamics injection for controllable scene synthesis. Experiments on dynamic scene benchmarks show that EvoGS achieves better motion coherence and temporal consistency compared to deformation-field baselines while maintaining real-time rendering",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T18:20:29+00:00",
      "updated": "2025-12-22T18:20:29+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19648v1",
      "file": "papers/2512.19648v1.pdf"
    },
    {
      "arxiv_id": "2512.19632v1",
      "title": "Generative diffusion models for agricultural AI: plant image generation, indoor-to-outdoor translation, and expert preference alignment",
      "authors": [
        {
          "name": "Da Tan"
        },
        {
          "name": "Michael Beck"
        },
        {
          "name": "Christopher P. Bidinosti"
        },
        {
          "name": "Robert H. Gulden"
        },
        {
          "name": "Christopher J. Henry"
        }
      ],
      "abstract": "The success of agricultural artificial intelligence depends heavily on large, diverse, and high-quality plant image datasets, yet collecting such data in real field conditions is costly, labor intensive, and seasonally constrained. This paper investigates diffusion-based generative modeling to address these challenges through plant image synthesis, indoor-to-outdoor translation, and expert preference aligned fine tuning. First, a Stable Diffusion model is fine tuned on captioned indoor and outdoor plant imagery to generate realistic, text conditioned images of canola and soybean. Evaluation using Inception Score, Frechet Inception Distance, and downstream phenotype classification shows that synthetic images effectively augment training data and improve accuracy. Second, we bridge the gap between high resolution indoor datasets and limited outdoor imagery using DreamBooth-based text inversion and image guided diffusion, generating translated images that enhance weed detection and classification with YOLOv8. Finally, a preference guided fine tuning framework trains a reward model on expert scores and applies reward weighted updates to produce more stable and expert aligned outputs. Together, these components demonstrate a practical pathway toward data efficient generative pipelines for agricultural AI.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T18:07:08+00:00",
      "updated": "2025-12-22T18:07:08+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19632v1",
      "file": "papers/2512.19632v1.pdf"
    },
    {
      "arxiv_id": "2512.19609v1",
      "title": "MapTrace: Scalable Data Generation for Route Tracing on Maps",
      "authors": [
        {
          "name": "Artemis Panagopoulou"
        },
        {
          "name": "Aveek Purohit"
        },
        {
          "name": "Achin Kulshrestha"
        },
        {
          "name": "Soroosh Yazdani"
        },
        {
          "name": "Mohit Goyal"
        }
      ],
      "abstract": "While Multimodal Large Language Models have achieved human-like performance on many visual and textual reasoning tasks, their proficiency in fine-grained spatial understanding, such as route tracing on maps remains limited. Unlike humans, who can quickly learn to parse and navigate maps, current models often fail to respect fundamental path constraints, in part due to the prohibitive cost and difficulty of collecting large-scale, pixel-accurate path annotations. To address this, we introduce a scalable synthetic data generation pipeline that leverages synthetic map images and pixel-level parsing to automatically produce precise annotations for this challenging task. Using this pipeline, we construct a fine-tuning dataset of 23k path samples across 4k maps, enabling models to acquire more human-like spatial capabilities. Using this dataset, we fine-tune both open-source and proprietary MLLMs. Results on MapBench show that finetuning substantially improves robustness, raising success rates by up to 6.4 points, while also reducing path-tracing error (NDTW). These gains highlight that fine-grained spatial reasoning, absent in pretrained models, can be explicitly taught with synthetic supervision.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-22T17:45:39+00:00",
      "updated": "2025-12-22T17:45:39+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19609v1",
      "file": "papers/2512.19609v1.pdf"
    },
    {
      "arxiv_id": "2512.19605v1",
      "title": "KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning",
      "authors": [
        {
          "name": "Eric Zimmermann"
        },
        {
          "name": "Harley Wiltzer"
        },
        {
          "name": "Justin Szeto"
        },
        {
          "name": "David Alvarez-Melis"
        },
        {
          "name": "Lester Mackey"
        }
      ],
      "abstract": "Recent breakthroughs in self-supervised Joint-Embedding Predictive Architectures (JEPAs) have established that regularizing Euclidean representations toward isotropic Gaussian priors yields provable gains in training stability and downstream generalization. We introduce a new, flexible family of KerJEPAs, self-supervised learning algorithms with kernel-based regularizers. One instance of this family corresponds to the recently-introduced LeJEPA Epps-Pulley regularizer which approximates a sliced maximum mean discrepancy (MMD) with a Gaussian prior and Gaussian kernel. By expanding the class of viable kernels and priors and computing the closed-form high-dimensional limit of sliced MMDs, we develop alternative KerJEPAs with a number of favorable properties including improved training stability and design flexibility.",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "published": "2025-12-22T17:41:26+00:00",
      "updated": "2025-12-22T17:41:26+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19605v1",
      "file": "papers/2512.19605v1.pdf"
    },
    {
      "arxiv_id": "2512.19602v1",
      "title": "No Data? No Problem: Robust Vision-Tabular Learning with Missing Values",
      "authors": [
        {
          "name": "Marta Hasny"
        },
        {
          "name": "Laura Daza"
        },
        {
          "name": "Keno Bressem"
        },
        {
          "name": "Maxime Di Folco"
        },
        {
          "name": "Julia Schnabel"
        }
      ],
      "abstract": "Large-scale medical biobanks provide imaging data complemented by extensive tabular information, such as demographics or clinical measurements. However, this abundance of tabular attributes does not reflect real-world datasets, where only a subset of attributes may be available. This discrepancy calls for methods that can leverage all the tabular data during training while remaining robust to missing values at inference. To address this challenge, we propose RoVTL (Robust Vision-Tabular Learning), a framework designed to handle any level of tabular data availability, from 0% to 100%. RoVTL comprises two key stages: contrastive pretraining, where we introduce tabular attribute missingness as data augmentation to promote robustness, and downstream task tuning using a gated cross-attention module for multimodal fusion. During fine-tuning, we employ a novel Tabular More vs. Fewer loss that ranks performance based on the amount of available tabular data. Combined with disentangled gradient learning, this enables consistent performance across all tabular data completeness scenarios. We evaluate RoVTL on cardiac MRI scans from the UK Biobank, demonstrating superior robustness to missing tabular data compared to prior methods. Furthermore, RoVTL successfully generalizes to an external cardiac MRI dataset for multimodal disease classification, and extends to the natural images domain, achieving robust performance on a car advertisements dataset. The code is available at https://github.com/marteczkah/RoVTL.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T17:35:32+00:00",
      "updated": "2025-12-22T17:35:32+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19602v1",
      "file": "papers/2512.19602v1.pdf"
    },
    {
      "arxiv_id": "2512.19584v1",
      "title": "Patlak Parametric Image Estimation from Dynamic PET Using Diffusion Model Prior",
      "authors": [
        {
          "name": "Ziqian Huang"
        },
        {
          "name": "Boxiao Yu"
        },
        {
          "name": "Siqi Li"
        },
        {
          "name": "Savas Ozdemir"
        },
        {
          "name": "Sangjin Bae"
        },
        {
          "name": "Jae Sung Lee"
        },
        {
          "name": "Guobao Wang"
        },
        {
          "name": "Kuang Gong"
        }
      ],
      "abstract": "Dynamic PET enables the quantitative estimation of physiology-related parameters and is widely utilized in research and increasingly adopted in clinical settings. Parametric imaging in dynamic PET requires kinetic modeling to estimate voxel-wise physiological parameters based on specific kinetic models. However, parametric images estimated through kinetic model fitting often suffer from low image quality due to the inherently ill-posed nature of the fitting process and the limited counts resulting from non-continuous data acquisition across multiple bed positions in whole-body PET. In this work, we proposed a diffusion model-based kinetic modeling framework for parametric image estimation, using the Patlak model as an example. The score function of the diffusion model was pre-trained on static total-body PET images and served as a prior for both Patlak slope and intercept images by leveraging their patch-wise similarity. During inference, the kinetic model was incorporated as a data-consistency constraint to guide the parametric image estimation. The proposed framework was evaluated on total-body dynamic PET datasets with different dose levels, demonstrating the feasibility and promising performance of the proposed framework in improving parametric image quality.",
      "primary_category": "eess.IV",
      "categories": [
        "eess.IV",
        "cs.CV",
        "physics.med-ph"
      ],
      "published": "2025-12-22T17:11:33+00:00",
      "updated": "2025-12-22T17:11:33+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19584v1",
      "file": "papers/2512.19584v1.pdf"
    },
    {
      "arxiv_id": "2512.19560v1",
      "title": "BabyFlow: 3D modeling of realistic and expressive infant faces",
      "authors": [
        {
          "name": "Antonia Alomar"
        },
        {
          "name": "Mireia Masias"
        },
        {
          "name": "Marius George Linguraru"
        },
        {
          "name": "Federico M. Sukno"
        },
        {
          "name": "Gemma Piella"
        }
      ],
      "abstract": "Early detection of developmental disorders can be aided by analyzing infant craniofacial morphology, but modeling infant faces is challenging due to limited data and frequent spontaneous expressions. We introduce BabyFlow, a generative AI model that disentangles facial identity and expression, enabling independent control over both. Using normalizing flows, BabyFlow learns flexible, probabilistic representations that capture the complex, non-linear variability of expressive infant faces without restrictive linear assumptions. To address scarce and uncontrolled expressive data, we perform cross-age expression transfer, adapting expressions from adult 3D scans to enrich infant datasets with realistic and systematic expressive variants. As a result, BabyFlow improves 3D reconstruction accuracy, particularly in highly expressive regions such as the mouth, eyes, and nose, and supports synthesis and modification of infant expressions while preserving identity. Additionally, by integrating with diffusion models, BabyFlow generates high-fidelity 2D infant images with consistent 3D geometry, providing powerful tools for data augmentation and early facial analysis.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-22T16:42:58+00:00",
      "updated": "2025-12-22T16:42:58+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19560v1",
      "file": "papers/2512.19560v1.pdf"
    },
    {
      "arxiv_id": "2512.19546v1",
      "title": "ActAvatar: Temporally-Aware Precise Action Control for Talking Avatars",
      "authors": [
        {
          "name": "Ziqiao Peng"
        },
        {
          "name": "Yi Chen"
        },
        {
          "name": "Yifeng Ma"
        },
        {
          "name": "Guozhen Zhang"
        },
        {
          "name": "Zhiyao Sun"
        },
        {
          "name": "Zixiang Zhou"
        },
        {
          "name": "Youliang Zhang"
        },
        {
          "name": "Zhengguang Zhou"
        },
        {
          "name": "Zhaoxin Fan"
        },
        {
          "name": "Hongyan Liu"
        },
        {
          "name": "Yuan Zhou"
        },
        {
          "name": "Qinglin Lu"
        },
        {
          "name": "Jun He"
        }
      ],
      "abstract": "Despite significant advances in talking avatar generation, existing methods face critical challenges: insufficient text-following capability for diverse actions, lack of temporal alignment between actions and audio content, and dependency on additional control signals such as pose skeletons. We present ActAvatar, a framework that achieves phase-level precision in action control through textual guidance by capturing both action semantics and temporal context. Our approach introduces three core innovations: (1) Phase-Aware Cross-Attention (PACA), which decomposes prompts into a global base block and temporally-anchored phase blocks, enabling the model to concentrate on phase-relevant tokens for precise temporal-semantic alignment; (2) Progressive Audio-Visual Alignment, which aligns modality influence with the hierarchical feature learning process-early layers prioritize text for establishing action structure while deeper layers emphasize audio for refining lip movements, preventing modality interference; (3) A two-stage training strategy that first establishes robust audio-visual correspondence on diverse data, then injects action control through fine-tuning on structured annotations, maintaining both audio-visual alignment and the model's text-following capabilities. Extensive experiments demonstrate that ActAvatar significantly outperforms state-of-the-art methods in both action control and visual quality.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T16:28:27+00:00",
      "updated": "2025-12-22T16:28:27+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19546v1",
      "file": "papers/2512.19546v1.pdf"
    },
    {
      "arxiv_id": "2512.19539v1",
      "title": "StoryMem: Multi-shot Long Video Storytelling with Memory",
      "authors": [
        {
          "name": "Kaiwen Zhang"
        },
        {
          "name": "Liming Jiang"
        },
        {
          "name": "Angtian Wang"
        },
        {
          "name": "Jacob Zhiyuan Fang"
        },
        {
          "name": "Tiancheng Zhi"
        },
        {
          "name": "Qing Yan"
        },
        {
          "name": "Hao Kang"
        },
        {
          "name": "Xin Lu"
        },
        {
          "name": "Xingang Pan"
        }
      ],
      "abstract": "Visual storytelling requires generating multi-shot videos with cinematic quality and long-range consistency. Inspired by human memory, we propose StoryMem, a paradigm that reformulates long-form video storytelling as iterative shot synthesis conditioned on explicit visual memory, transforming pre-trained single-shot video diffusion models into multi-shot storytellers. This is achieved by a novel Memory-to-Video (M2V) design, which maintains a compact and dynamically updated memory bank of keyframes from historical generated shots. The stored memory is then injected into single-shot video diffusion models via latent concatenation and negative RoPE shifts with only LoRA fine-tuning. A semantic keyframe selection strategy, together with aesthetic preference filtering, further ensures informative and stable memory throughout generation. Moreover, the proposed framework naturally accommodates smooth shot transitions and customized story generation applications. To facilitate evaluation, we introduce ST-Bench, a diverse benchmark for multi-shot video storytelling. Extensive experiments demonstrate that StoryMem achieves superior cross-shot consistency over previous methods while preserving high aesthetic quality and prompt adherence, marking a significant step toward coherent minute-long video storytelling.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T16:23:24+00:00",
      "updated": "2025-12-22T16:23:24+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19539v1",
      "file": "papers/2512.19539v1.pdf"
    },
    {
      "arxiv_id": "2512.19534v1",
      "title": "SlicerOrbitSurgerySim: An Open-Source Platform for Virtual Registration and Quantitative Comparison of Preformed Orbital Plates",
      "authors": [
        {
          "name": "Chi Zhang"
        },
        {
          "name": "Braedon Gunn"
        },
        {
          "name": "Andrew M. Read-Fuller"
        }
      ],
      "abstract": "Poor adaptation of orbital implants remains a major contributor to postoperative complications and revision surgery. Although preformed orbital plates are widely used to reduce cost and operative time compared with customized implants, surgeons currently lack publicly available tools and standardized metrics to quantitatively compare plate fit across vendors, sizes, and patient anatomy. We developed SlicerOrbitSurgerySim, an open-source extension for the 3D Slicer platform that enables interactive virtual registration, evaluation, and comparison of multiple preformed orbital plates in a patient-specific virtual planning environment. The software generates reproducible quantitative plate-to-orbit distance metrics and visualization tools that support both patient-specific planning and population-level statistical analysis of plate adaptability. By facilitating objective comparison of implant designs and placement strategies, this tool aims to improve preoperative decision-making, reduce intraoperative plate modification, and promote collaborative research and surgical education. Pilot studies, sample datasets, and detailed tutorials are provided to support testing, transparency, and reproducibility.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T16:21:29+00:00",
      "updated": "2025-12-22T16:21:29+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19534v1",
      "file": "papers/2512.19534v1.pdf"
    },
    {
      "arxiv_id": "2512.19528v1",
      "title": "Multi-Modal Soccer Scene Analysis with Masked Pre-Training",
      "authors": [
        {
          "name": "Marc Peral"
        },
        {
          "name": "Guillem Capellera"
        },
        {
          "name": "Luis Ferraz"
        },
        {
          "name": "Antonio Rubio"
        },
        {
          "name": "Antonio Agudo"
        }
      ],
      "abstract": "In this work we propose a multi-modal architecture for analyzing soccer scenes from tactical camera footage, with a focus on three core tasks: ball trajectory inference, ball state classification, and ball possessor identification. To this end, our solution integrates three distinct input modalities (player trajectories, player types and image crops of individual players) into a unified framework that processes spatial and temporal dynamics using a cascade of sociotemporal transformer blocks. Unlike prior methods, which rely heavily on accurate ball tracking or handcrafted heuristics, our approach infers the ball trajectory without direct access to its past or future positions, and robustly identifies the ball state and ball possessor under noisy or occluded conditions from real top league matches. We also introduce CropDrop, a modality-specific masking pre-training strategy that prevents over-reliance on image features and encourages the model to rely on cross-modal patterns during pre-training. We show the effectiveness of our approach on a large-scale dataset providing substantial improvements over state-of-the-art baselines in all tasks. Our results highlight the benefits of combining structured and visual cues in a transformer-based architecture, and the importance of realistic masking strategies in multi-modal learning.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T16:18:45+00:00",
      "updated": "2025-12-22T16:18:45+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19528v1",
      "file": "papers/2512.19528v1.pdf"
    },
    {
      "arxiv_id": "2512.19512v2",
      "title": "Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation",
      "authors": [
        {
          "name": "Ziyang Song"
        },
        {
          "name": "Zelin Zang"
        },
        {
          "name": "Zuyao Chen"
        },
        {
          "name": "Xusheng Liang"
        },
        {
          "name": "Dong Yi"
        },
        {
          "name": "Jinlin Wu"
        },
        {
          "name": "Hongbin Liu"
        },
        {
          "name": "Jiebo Luo"
        },
        {
          "name": "Zhen. Lei"
        }
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-22T16:06:36+00:00",
      "updated": "2025-12-24T05:32:41+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19512v2",
      "file": "papers/2512.19512v2.pdf"
    },
    {
      "arxiv_id": "2512.19504v1",
      "title": "FusionNet: Physics-Aware Representation Learning for Multi-Spectral and Thermal Data via Trainable Signal-Processing Priors",
      "authors": [
        {
          "name": "Georgios Voulgaris"
        }
      ],
      "abstract": "Modern deep learning models operating on multi-modal visual signals often rely on inductive biases that are poorly aligned with the physical processes governing signal formation, leading to brittle performance under cross-spectral and real-world conditions. In particular, approaches that prioritise direct thermal cues struggle to capture indirect yet persistent environmental alterations induced by sustained heat emissions.\n  This work introduces a physics-aware representation learning framework that leverages multi-spectral information to model stable signatures of long-term physical processes. Specifically, a geological Short Wave Infrared (SWIR) ratio sensitive to soil property changes is integrated with Thermal Infrared (TIR) data through an intermediate fusion architecture, instantiated as FusionNet. The proposed backbone embeds trainable differential signal-processing priors within convolutional layers, combines mixed pooling strategies, and employs wider receptive fields to enhance robustness across spectral modalities.\n  Systematic ablations show that each architectural component contributes to performance gains, with DGCNN achieving 88.7% accuracy on the SWIR ratio and FusionNet reaching 90.6%, outperforming state-of-the-art baselines across five spectral configurations. Transfer learning experiments further show that ImageNet pretraining degrades TIR performance, highlighting the importance of modality-aware training for cross-spectral learning.\n  Evaluated on real-world data, the results demonstrate that combining physics-aware feature selection with principled deep learning architectures yields robust and generalisable representations, illustrating how first-principles signal modelling can improve multi-spectral learning under challenging conditions.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T15:59:37+00:00",
      "updated": "2025-12-22T15:59:37+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19504v1",
      "file": "papers/2512.19504v1.pdf"
    },
    {
      "arxiv_id": "2512.19489v1",
      "title": "Rethinking Coupled Tensor Analysis for Hyperspectral Super-Resolution: Recoverable Modeling Under Endmember Variability",
      "authors": [
        {
          "name": "Meng Ding"
        },
        {
          "name": "Xiao Fu"
        }
      ],
      "abstract": "This work revisits the hyperspectral super-resolution (HSR) problem, i.e., fusing a pair of spatially co-registered hyperspectral (HSI) and multispectral (MSI) images to recover a super-resolution image (SRI) that enhances the spatial resolution of the HSI. Coupled tensor decomposition (CTD)-based methods have gained traction in this domain, offering recoverability guarantees under various assumptions. Existing models such as canonical polyadic decomposition (CPD) and Tucker decomposition provide strong expressive power but lack physical interpretability. The block-term decomposition model with rank-$(L_r, L_r, 1)$ terms (the LL1 model) yields interpretable factors under the linear mixture model (LMM) of spectral images, but LMM assumptions are often violated in practice -- primarily due to nonlinear effects such as endmember variability (EV). To address this, we propose modeling spectral images using a more flexible block-term tensor decomposition with rank-$(L_r, M_r, N_r)$ terms (the LMN model). This modeling choice retains interpretability, subsumes CPD, Tucker, and LL1 as special cases, and robustly accounts for non-ideal effects such as EV, offering a balanced tradeoff between expressiveness and interpretability for HSR. Importantly, under the LMN model for HSI and MSI, recoverability of the SRI can still be established under proper conditions -- providing strong theoretical support. Extensive experiments on synthetic and real datasets further validate the effectiveness and robustness of the proposed method compared with existing CTD-based approaches.",
      "primary_category": "eess.IV",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-12-22T15:43:59+00:00",
      "updated": "2025-12-22T15:43:59+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19489v1",
      "file": "papers/2512.19489v1.pdf"
    },
    {
      "arxiv_id": "2512.19486v1",
      "title": "Dynamic Stream Network for Combinatorial Explosion Problem in Deformable Medical Image Registration",
      "authors": [
        {
          "name": "Shaochen Bi"
        },
        {
          "name": "Yuting He"
        },
        {
          "name": "Weiming Wang"
        },
        {
          "name": "Hao Chen"
        }
      ],
      "abstract": "Combinatorial explosion problem caused by dual inputs presents a critical challenge in Deformable Medical Image Registration (DMIR). Since DMIR processes two images simultaneously as input, the combination relationships between features has grown exponentially, ultimately the model considers more interfering features during the feature modeling process. Introducing dynamics in the receptive fields and weights of the network enable the model to eliminate the interfering features combination and model the potential feature combination relationships. In this paper, we propose the Dynamic Stream Network (DySNet), which enables the receptive fields and weights to be dynamically adjusted. This ultimately enables the model to ignore interfering feature combinations and model the potential feature relationships. With two key innovations: 1) Adaptive Stream Basin (AdSB) module dynamically adjusts the shape of the receptive field, thereby enabling the model to focus on the feature relationships with greater correlation. 2) Dynamic Stream Attention (DySA) mechanism generates dynamic weights to search for more valuable feature relationships. Extensive experiments have shown that DySNet consistently outperforms the most advanced DMIR methods, highlighting its outstanding generalization ability. Our code will be released on the website: https://github.com/ShaochenBi/DySNet.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T15:43:23+00:00",
      "updated": "2025-12-22T15:43:23+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19486v1",
      "file": "papers/2512.19486v1.pdf"
    },
    {
      "arxiv_id": "2512.19479v1",
      "title": "Emotion-Director: Bridging Affective Shortcut in Emotion-Oriented Image Generation",
      "authors": [
        {
          "name": "Guoli Jia"
        },
        {
          "name": "Junyao Hu"
        },
        {
          "name": "Xinwei Long"
        },
        {
          "name": "Kai Tian"
        },
        {
          "name": "Kaiyan Zhang"
        },
        {
          "name": "KaiKai Zhao"
        },
        {
          "name": "Ning Ding"
        },
        {
          "name": "Bowen Zhou"
        }
      ],
      "abstract": "Image generation based on diffusion models has demonstrated impressive capability, motivating exploration into diverse and specialized applications. Owing to the importance of emotion in advertising, emotion-oriented image generation has attracted increasing attention. However, current emotion-oriented methods suffer from an affective shortcut, where emotions are approximated to semantics. As evidenced by two decades of research, emotion is not equivalent to semantics. To this end, we propose Emotion-Director, a cross-modal collaboration framework consisting of two modules. First, we propose a cross-Modal Collaborative diffusion model, abbreviated as MC-Diffusion. MC-Diffusion integrates visual prompts with textual prompts for guidance, enabling the generation of emotion-oriented images beyond semantics. Further, we improve the DPO optimization by a negative visual prompt, enhancing the model's sensitivity to different emotions under the same semantics. Second, we propose MC-Agent, a cross-Modal Collaborative Agent system that rewrites textual prompts to express the intended emotions. To avoid template-like rewrites, MC-Agent employs multi-agents to simulate human subjectivity toward emotions, and adopts a chain-of-concept workflow that improves the visual expressiveness of the rewritten prompts. Extensive qualitative and quantitative experiments demonstrate the superiority of Emotion-Director in emotion-oriented image generation.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T15:32:18+00:00",
      "updated": "2025-12-22T15:32:18+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19479v1",
      "file": "papers/2512.19479v1.pdf"
    },
    {
      "arxiv_id": "2512.19451v1",
      "title": "Sign Language Recognition using Parallel Bidirectional Reservoir Computing",
      "authors": [
        {
          "name": "Nitin Kumar Singh"
        },
        {
          "name": "Arie Rachmad Syulistyo"
        },
        {
          "name": "Yuichiro Tanaka"
        },
        {
          "name": "Hakaru Tamukoh"
        }
      ],
      "abstract": "Sign language recognition (SLR) facilitates communication between deaf and hearing communities. Deep learning based SLR models are commonly used but require extensive computational resources, making them unsuitable for deployment on edge devices. To address these limitations, we propose a lightweight SLR system that combines parallel bidirectional reservoir computing (PBRC) with MediaPipe. MediaPipe enables real-time hand tracking and precise extraction of hand joint coordinates, which serve as input features for the PBRC architecture. The proposed PBRC architecture consists of two echo state network (ESN) based bidirectional reservoir computing (BRC) modules arranged in parallel to capture temporal dependencies, thereby creating a rich feature representation for classification. We trained our PBRC-based SLR system on the Word-Level American Sign Language (WLASL) video dataset, achieving top-1, top-5, and top-10 accuracies of 60.85%, 85.86%, and 91.74%, respectively. Training time was significantly reduced to 18.67 seconds due to the intrinsic properties of reservoir computing, compared to over 55 minutes for deep learning based methods such as Bi-GRU. This approach offers a lightweight, cost-effective solution for real-time SLR on edge devices.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-12-22T14:55:54+00:00",
      "updated": "2025-12-22T14:55:54+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19451v1",
      "file": "papers/2512.19451v1.pdf"
    },
    {
      "arxiv_id": "2512.19443v1",
      "title": "D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning",
      "authors": [
        {
          "name": "Evelyn Zhang"
        },
        {
          "name": "Fufu Yu"
        },
        {
          "name": "Aoqi Wu"
        },
        {
          "name": "Zichen Wen"
        },
        {
          "name": "Ke Yan"
        },
        {
          "name": "Shouhong Ding"
        },
        {
          "name": "Biqing Qi"
        },
        {
          "name": "Linfeng Zhang"
        }
      ],
      "abstract": "Processing long visual token sequences poses a significant computational burden on Multimodal Large Language Models (MLLMs). While token pruning offers a path to acceleration, we find that current methods, while adequate for general understanding, catastrophically fail on fine-grained localization tasks. We attribute this failure to the inherent flaws of the two prevailing strategies: importance-based methods suffer from a strong positional bias, an inherent model artifact that distracts from semantic content, while diversity-based methods exhibit structural blindness, disregarding the user's prompt and spatial redundancy. To address this, we introduce D2Pruner, a framework that rectifies these issues by uniquely combining debiased importance with a structural pruning mechanism. Our method first secures a core set of the most critical tokens as pivots based on a debiased attention score. It then performs a Maximal Independent Set (MIS) selection on the remaining tokens, which are modeled on a hybrid graph where edges signify spatial proximity and semantic similarity. This process iteratively preserves the most important and available token while removing its neighbors, ensuring that the supplementary tokens are chosen to maximize importance and diversity. Extensive experiments demonstrate that D2Pruner has exceptional efficiency and fidelity. Applied to LLaVA-1.5-7B for general understanding tasks, it reduces FLOPs by 74.2\\% while retaining 99.2\\% of its original performance. Furthermore, in challenging localization benchmarks with InternVL-2.5-8B, it maintains 85.7\\% performance at a 90\\% token reduction rate, marking a significant advancement with up to 63. 53\\% improvement over existing methods.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T14:42:31+00:00",
      "updated": "2025-12-22T14:42:31+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19443v1",
      "file": "papers/2512.19443v1.pdf"
    },
    {
      "arxiv_id": "2512.19438v1",
      "title": "MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation",
      "authors": [
        {
          "name": "Fei Ge"
        },
        {
          "name": "Ying Huang"
        },
        {
          "name": "Jie Liu"
        },
        {
          "name": "Guixuan Zhang"
        },
        {
          "name": "Zhi Zeng"
        },
        {
          "name": "Shuwu Zhang"
        },
        {
          "name": "Hu Guan"
        }
      ],
      "abstract": "Existing deep image watermarking methods follow a fixed embedding-distortion-extraction pipeline, where the embedder and extractor are weakly coupled through a final loss and optimized in isolation. This design lacks explicit collaboration, leaving no structured mechanism for the embedder to incorporate decoding-aware cues or for the extractor to guide embedding during training. To address this architectural limitation, we rethink deep image watermarking by reformulating embedding and extraction as explicitly collaborative components. To realize this reformulation, we introduce a Collaborative Interaction Mechanism (CIM) that establishes direct, bidirectional communication between the embedder and extractor, enabling a mutual-teacher training paradigm and coordinated optimization. Built upon this explicitly collaborative architecture, we further propose an Adaptive Feature Modulation Module (AFMM) to support effective interaction. AFMM enables content-aware feature regulation by decoupling modulation structure and strength, guiding watermark embedding toward stable image features while suppressing host interference during extraction. Under CIM, the AFMMs on both sides form a closed-loop collaboration that aligns embedding behavior with extraction objectives. This architecture-level redesign changes how robustness is learned in watermarking systems. Rather than relying on exhaustive distortion simulation, robustness emerges from coordinated representation learning between embedding and extraction. Experiments on real-world and AI-generated datasets demonstrate that the proposed method consistently outperforms state-of-the-art approaches in watermark extraction accuracy while maintaining high perceptual quality, showing strong robustness and generalization.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-22T14:36:08+00:00",
      "updated": "2025-12-22T14:36:08+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19438v1",
      "file": "papers/2512.19438v1.pdf"
    },
    {
      "arxiv_id": "2512.19433v1",
      "title": "dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models",
      "authors": [
        {
          "name": "Yi Xin"
        },
        {
          "name": "Siqi Luo"
        },
        {
          "name": "Qi Qin"
        },
        {
          "name": "Haoxing Chen"
        },
        {
          "name": "Kaiwen Zhu"
        },
        {
          "name": "Zhiwei Zhang"
        },
        {
          "name": "Yangfan He"
        },
        {
          "name": "Rongchao Zhang"
        },
        {
          "name": "Jinbin Bai"
        },
        {
          "name": "Shuo Cao"
        },
        {
          "name": "Bin Fu"
        },
        {
          "name": "Junjun He"
        },
        {
          "name": "Yihao Liu"
        },
        {
          "name": "Yuewen Cao"
        },
        {
          "name": "Xiaohong Liu"
        }
      ],
      "abstract": "Diffusion Multi-modal Large Language Models (dMLLMs) have recently emerged as a novel architecture unifying image generation and understanding. However, developing effective and efficient Test-Time Scaling (TTS) methods to unlock their full generative potential remains an underexplored challenge. To address this, we propose dMLLM-TTS, a novel framework operating on two complementary scaling axes: (1) trajectory exploration scaling to enhance the diversity of generated hypotheses, and (2) iterative refinement scaling for stable generation. Conventional TTS approaches typically perform linear search across these two dimensions, incurring substantial computational costs of O(NT) and requiring an external verifier for best-of-N selection. To overcome these limitations, we propose two innovations. First, we design an efficient hierarchical search algorithm with O(N+T) complexity that adaptively expands and prunes sampling trajectories. Second, we introduce a self-verified feedback mechanism that leverages the dMLLMs' intrinsic image understanding capabilities to assess text-image alignment, eliminating the need for external verifier. Extensive experiments on the GenEval benchmark across three representative dMLLMs (e.g., Lumina-DiMOO, MMaDA, Muddit) show that our framework substantially improves generation quality while achieving up to 6x greater efficiency than linear search. Project page: https://github.com/Alpha-VLLM/Lumina-DiMOO.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T14:31:58+00:00",
      "updated": "2025-12-22T14:31:58+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19433v1",
      "file": "papers/2512.19433v1.pdf"
    },
    {
      "arxiv_id": "2512.19415v1",
      "title": "Non-Contrast CT Esophageal Varices Grading through Clinical Prior-Enhanced Multi-Organ Analysis",
      "authors": [
        {
          "name": "Xiaoming Zhang"
        },
        {
          "name": "Chunli Li"
        },
        {
          "name": "Jiacheng Hao"
        },
        {
          "name": "Yuan Gao"
        },
        {
          "name": "Danyang Tu"
        },
        {
          "name": "Jianyi Qiao"
        },
        {
          "name": "Xiaoli Yin"
        },
        {
          "name": "Le Lu"
        },
        {
          "name": "Ling Zhang"
        },
        {
          "name": "Ke Yan"
        },
        {
          "name": "Yang Hou"
        },
        {
          "name": "Yu Shi"
        }
      ],
      "abstract": "Esophageal varices (EV) represent a critical complication of portal hypertension, affecting approximately 60% of cirrhosis patients with a significant bleeding risk of ~30%. While traditionally diagnosed through invasive endoscopy, non-contrast computed tomography (NCCT) presents a potential non-invasive alternative that has yet to be fully utilized in clinical practice. We present Multi-Organ-COhesion Network++ (MOON++), a novel multimodal framework that enhances EV assessment through comprehensive analysis of NCCT scans. Inspired by clinical evidence correlating organ volumetric relationships with liver disease severity, MOON++ synthesizes imaging characteristics of the esophagus, liver, and spleen through multimodal learning. We evaluated our approach using 1,631 patients, those with endoscopically confirmed EV were classified into four severity grades. Validation in 239 patient cases and independent testing in 289 cases demonstrate superior performance compared to conventional single organ methods, achieving an AUC of 0.894 versus 0.803 for the severe grade EV classification (G3 versus <G3) and 0.921 versus 0.793 for the differentiation of moderate to severe grades (>=G2 versus <G2). We conducted a reader study involving experienced radiologists to further validate the performance of MOON++. To our knowledge, MOON++ represents the first comprehensive multi-organ NCCT analysis framework incorporating clinical knowledge priors for EV assessment, potentially offering a promising non-invasive diagnostic alternative.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T14:17:35+00:00",
      "updated": "2025-12-22T14:17:35+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19415v1",
      "file": "papers/2512.19415v1.pdf"
    },
    {
      "arxiv_id": "2512.19387v1",
      "title": "DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition",
      "authors": [
        {
          "name": "Yueyao Chen"
        },
        {
          "name": "Kai-Ni Wang"
        },
        {
          "name": "Dario Tayupo"
        },
        {
          "name": "Arnaud Huaulm'e"
        },
        {
          "name": "Krystel Nyangoh Timoh"
        },
        {
          "name": "Pierre Jannin"
        },
        {
          "name": "Qi Dou"
        }
      ],
      "abstract": "Purpose: Surgical workflow recognition enables context-aware assistance and skill assessment in computer-assisted interventions. Despite recent advances, current methods suffer from two critical challenges: prediction jitter across consecutive frames and poor discrimination of ambiguous phases. This paper aims to develop a stable framework by selectively propagating reliable historical information and explicitly modeling uncertainty for hard sample enhancement.\n  Methods: We propose a dual-pathway framework DSTED with Reliable Memory Propagation (RMP) and Uncertainty-Aware Prototype Retrieval (UPR). RMP maintains temporal coherence by filtering and fusing high-confidence historical features through multi-criteria reliability assessment. UPR constructs learnable class-specific prototypes from high-uncertainty samples and performs adaptive prototype matching to refine ambiguous frame representations. Finally, a confidence-driven gate dynamically balances both pathways based on prediction certainty.\n  Results: Our method achieves state-of-the-art performance on AutoLaparo-hysterectomy with 84.36% accuracy and 65.51% F1-score, surpassing the second-best method by 3.51% and 4.88% respectively. Ablations reveal complementary gains from RMP (2.19%) and UPR (1.93%), with synergistic effects when combined. Extensive analysis confirms substantial reduction in temporal jitter and marked improvement on challenging phase transitions.\n  Conclusion: Our dual-pathway design introduces a novel paradigm for stable workflow recognition, demonstrating that decoupling the modeling of temporal consistency and phase ambiguity yields superior performance and clinical applicability.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-22T13:36:26+00:00",
      "updated": "2025-12-22T13:36:26+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19387v1",
      "file": "papers/2512.19387v1.pdf"
    },
    {
      "arxiv_id": "2512.19365v1",
      "title": "Efficient Spike-driven Transformer for High-performance Drone-View Geo-Localization",
      "authors": [
        {
          "name": "Zhongwei Chen"
        },
        {
          "name": "Hai-Jun Rong"
        },
        {
          "name": "Zhao-Xu Yang"
        },
        {
          "name": "Guoqi Li"
        }
      ],
      "abstract": "Traditional drone-view geo-localization (DVGL) methods based on artificial neural networks (ANNs) have achieved remarkable performance. However, ANNs rely on dense computation, which results in high power consumption. In contrast, spiking neural networks (SNNs), which benefit from spike-driven computation, inherently provide low power consumption. Regrettably, the potential of SNNs for DVGL has yet to be thoroughly investigated. Meanwhile, the inherent sparsity of spike-driven computation for representation learning scenarios also results in loss of critical information and difficulties in learning long-range dependencies when aligning heterogeneous visual data sources. To address these, we propose SpikeViMFormer, the first SNN framework designed for DVGL. In this framework, a lightweight spike-driven transformer backbone is adopted to extract coarse-grained features. To mitigate the loss of critical information, the spike-driven selective attention (SSA) block is designed, which uses a spike-driven gating mechanism to achieve selective feature enhancement and highlight discriminative regions. Furthermore, a spike-driven hybrid state space (SHS) block is introduced to learn long-range dependencies using a hybrid state space. Moreover, only the backbone is utilized during the inference stage to reduce computational cost. To ensure backbone effectiveness, a novel hierarchical re-ranking alignment learning (HRAL) strategy is proposed. It refines features via neighborhood re-ranking and maintains cross-batch consistency to directly optimize the backbone. Experimental results demonstrate that SpikeViMFormer outperforms state-of-the-art SNNs. Compared with advanced ANNs, it also achieves competitive performance.Our code is available at https://github.com/ISChenawei/SpikeViMFormer",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T13:07:04+00:00",
      "updated": "2025-12-22T13:07:04+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19365v1",
      "file": "papers/2512.19365v1.pdf"
    },
    {
      "arxiv_id": "2512.19354v1",
      "title": "ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining",
      "authors": [
        {
          "name": "Zhenyang Huang"
        },
        {
          "name": "Xiao Yu"
        },
        {
          "name": "Yi Zhang"
        },
        {
          "name": "Decheng Wang"
        },
        {
          "name": "Hang Ruan"
        }
      ],
      "abstract": "Remote sensing image change detection is one of the fundamental tasks in remote sensing intelligent interpretation. Its core objective is to identify changes within change regions of interest (CRoI). Current multimodal large models encode rich human semantic knowledge, which is utilized for guidance in tasks such as remote sensing change detection. However, existing methods that use semantic guidance for detecting users' CRoI overly rely on explicit textual descriptions of CRoI, leading to the problem of near-complete performance failure when presented with implicit CRoI textual descriptions. This paper proposes a multimodal reasoning change detection model named ReasonCD, capable of mining users' implicit task intent. The model leverages the powerful reasoning capabilities of pre-trained large language models to mine users' implicit task intents and subsequently obtains different change detection results based on these intents. Experiments on public datasets demonstrate that the model achieves excellent change detection performance, with an F1 score of 92.1\\% on the BCDD dataset. Furthermore, to validate its superior reasoning functionality, this paper annotates a subset of reasoning data based on the SECOND dataset. Experimental results show that the model not only excels at basic reasoning-based change detection tasks but can also explain the reasoning process to aid human decision-making.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T12:54:26+00:00",
      "updated": "2025-12-22T12:54:26+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19354v1",
      "file": "papers/2512.19354v1.pdf"
    },
    {
      "arxiv_id": "2512.19336v1",
      "title": "GANeXt: A Fully ConvNeXt-Enhanced Generative Adversarial Network for MRI- and CBCT-to-CT Synthesis",
      "authors": [
        {
          "name": "Siyuan Mei"
        },
        {
          "name": "Yan Xia"
        },
        {
          "name": "Fuxin Fan"
        }
      ],
      "abstract": "The synthesis of computed tomography (CT) from magnetic resonance imaging (MRI) and cone-beam CT (CBCT) plays a critical role in clinical treatment planning by enabling accurate anatomical representation in adaptive radiotherapy. In this work, we propose GANeXt, a 3D patch-based, fully ConvNeXt-powered generative adversarial network for unified CT synthesis across different modalities and anatomical regions. Specifically, GANeXt employs an efficient U-shaped generator constructed from stacked 3D ConvNeXt blocks with compact convolution kernels, while the discriminator adopts a conditional PatchGAN. To improve synthesis quality, we incorporate a combination of loss functions, including mean absolute error (MAE), perceptual loss, segmentation-based masked MAE, and adversarial loss and a combination of Dice loss and cross-entropy for multi-head segmentation discriminator. For both tasks, training is performed with a batch size of 8 using two separate AdamW optimizers for the generator and discriminator, each equipped with a warmup and cosine decay scheduler, with learning rates of $5\\times10^{-4}$ and $1\\times10^{-3}$, respectively. Data preprocessing includes deformable registration, foreground cropping, percentile normalization for the input modality, and linear normalization of the CT to the range $[-1024, 1000]$. Data augmentation involves random zooming within $(0.8, 1.3)$ (for MRI-to-CT only), fixed-size cropping to $32\\times160\\times192$ for MRI-to-CT and $32\\times128\\times128$ for CBCT-to-CT, and random flipping. During inference, we apply a sliding-window approach with $0.8$ overlap and average folding to reconstruct the full-size sCT, followed by inversion of the CT normalization. After joint training on all regions without any fine-tuning, the final models are selected at the end of 3000 epochs for MRI-to-CT and 1000 epochs for CBCT-to-CT using the full training dataset.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T12:32:16+00:00",
      "updated": "2025-12-22T12:32:16+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19336v1",
      "file": "papers/2512.19336v1.pdf"
    },
    {
      "arxiv_id": "2512.19331v1",
      "title": "DeltaMIL: Gated Memory Integration for Efficient and Discriminative Whole Slide Image Analysis",
      "authors": [
        {
          "name": "Yueting Zhu"
        },
        {
          "name": "Yuehao Song"
        },
        {
          "name": "Shuai Zhang"
        },
        {
          "name": "Wenyu Liu"
        },
        {
          "name": "Xinggang Wang"
        }
      ],
      "abstract": "Whole Slide Images (WSIs) are typically analyzed using multiple instance learning (MIL) methods. However, the scale and heterogeneity of WSIs generate highly redundant and dispersed information, making it difficult to identify and integrate discriminative signals. Existing MIL methods either fail to discard uninformative cues effectively or have limited ability to consolidate relevant features from multiple patches, which restricts their performance on large and heterogeneous WSIs. To address this issue, we propose DeltaMIL, a novel MIL framework that explicitly selects semantically relevant regions and integrates the discriminative information from WSIs. Our method leverages the gated delta rule to efficiently filter and integrate information through a block combining forgetting and memory mechanisms. The delta mechanism dynamically updates the memory by removing old values and inserting new ones according to their correlation with the current patch. The gating mechanism further enables rapid forgetting of irrelevant signals. Additionally, DeltaMIL integrates a complementary local pattern mixing mechanism to retain fine-grained pathological locality. Our design enhances the extraction of meaningful cues and suppresses redundant or noisy information, which improves the model's robustness and discriminative power. Experiments demonstrate that DeltaMIL achieves state-of-the-art performance. Specifically, for survival prediction, DeltaMIL improves performance by 3.69\\% using ResNet-50 features and 2.36\\% using UNI features. For slide-level classification, it increases accuracy by 3.09\\% with ResNet-50 features and 3.75\\% with UNI features. These results demonstrate the strong and consistent performance of DeltaMIL across diverse WSI tasks.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T12:27:12+00:00",
      "updated": "2025-12-22T12:27:12+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19331v1",
      "file": "papers/2512.19331v1.pdf"
    },
    {
      "arxiv_id": "2512.19327v1",
      "title": "Extended OpenTT Games Dataset: A table tennis dataset for fine-grained shot type and point outcome",
      "authors": [
        {
          "name": "Moamal Fadhil Abdul"
        },
        {
          "name": "Jonas Bruun Hubrechts"
        },
        {
          "name": "Thomas Martini Jørgensen"
        },
        {
          "name": "Emil Hovad"
        }
      ],
      "abstract": "Automatically detecting and classifying strokes in table tennis video can streamline training workflows, enrich broadcast overlays, and enable fine-grained performance analytics. For this to be possible, annotated video data of table tennis is needed. We extend the public OpenTTGames dataset with highly detailed, frame-accurate shot type annotations (forehand, backhand with subtypes), player posture labels (body lean and leg stance), and rally outcome tags at point end. OpenTTGames is a set of recordings from the side of the table with official labels for bounces, when the ball is above the net, or hitting the net. The dataset already contains ball coordinates near events, which are either \"bounce\", \"net\", or \"empty_event\" in the original OpenTTGames dataset, and semantic masks (humans, table, scoreboard). Our extension adds the types of stroke to the events and a per-player taxonomy so models can move beyond event spotting toward tactical understanding (e.g., whether a stroke is likely to win the point or set up an advantage). We provide a compact coding scheme and code-assisted labeling procedure to support reproducible annotations and baselines for fine-grained stroke understanding in racket sports. This fills a practical gap in the community, where many prior video resources are either not publicly released or carry restrictive/unclear licenses that hinder reuse and benchmarking. Our annotations are released under the same CC BY-NC-SA 4.0 license as OpenTTGames, allowing free non-commercial use, modification, and redistribution, with appropriate attribution.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T12:25:50+00:00",
      "updated": "2025-12-22T12:25:50+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19327v1",
      "file": "papers/2512.19327v1.pdf"
    },
    {
      "arxiv_id": "2512.19316v2",
      "title": "Neural Implicit Heart Coordinates: 3D cardiac shape reconstruction from sparse segmentations",
      "authors": [
        {
          "name": "Marica Muffoletto"
        },
        {
          "name": "Uxio Hermida"
        },
        {
          "name": "Charlène Mauger"
        },
        {
          "name": "Avan Suinesiaputra"
        },
        {
          "name": "Yiyang Xu"
        },
        {
          "name": "Richard Burns"
        },
        {
          "name": "Lisa Pankewitz"
        },
        {
          "name": "Andrew D McCulloch"
        },
        {
          "name": "Steffen E Petersen"
        },
        {
          "name": "Daniel Rueckert"
        },
        {
          "name": "Alistair A Young"
        }
      ],
      "abstract": "Accurate reconstruction of cardiac anatomy from sparse clinical images remains a major challenge in patient-specific modeling. While neural implicit functions have previously been applied to this task, their application to mapping anatomical consistency across subjects has been limited. In this work, we introduce Neural Implicit Heart Coordinates (NIHCs), a standardized implicit coordinate system, based on universal ventricular coordinates, that provides a common anatomical reference frame for the human heart. Our method predicts NIHCs directly from a limited number of 2D segmentations (sparse acquisition) and subsequently decodes them into dense 3D segmentations and high-resolution meshes at arbitrary output resolution. Trained on a large dataset of 5,000 cardiac meshes, the model achieves high reconstruction accuracy on clinical contours, with mean Euclidean surface errors of 2.51$\\pm$0.33 mm in a diseased cohort (n=4549) and 2.3$\\pm$0.36 mm in a healthy cohort (n=5576). The NIHC representation enables anatomically coherent reconstruction even under severe slice sparsity and segmentation noise, faithfully recovering complex structures such as the valve planes. Compared with traditional pipelines, inference time is reduced from over 60 s to 5-15 s. These results demonstrate that NIHCs constitute a robust and efficient anatomical representation for patient-specific 3D cardiac reconstruction from minimal input data.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "published": "2025-12-22T12:07:05+00:00",
      "updated": "2025-12-23T09:25:34+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19316v2",
      "file": "papers/2512.19316v2.pdf"
    },
    {
      "arxiv_id": "2512.19311v1",
      "title": "MixFlow Training: Alleviating Exposure Bias with Slowed Interpolation Mixture",
      "authors": [
        {
          "name": "Hui Li"
        },
        {
          "name": "Jiayue Lyu"
        },
        {
          "name": "Fu-Yun Wang"
        },
        {
          "name": "Kaihui Cheng"
        },
        {
          "name": "Siyu Zhu"
        },
        {
          "name": "Jingdong Wang"
        }
      ],
      "abstract": "This paper studies the training-testing discrepancy (a.k.a. exposure bias) problem for improving the diffusion models. During training, the input of a prediction network at one training timestep is the corresponding ground-truth noisy data that is an interpolation of the noise and the data, and during testing, the input is the generated noisy data. We present a novel training approach, named MixFlow, for improving the performance. Our approach is motivated by the Slow Flow phenomenon: the ground-truth interpolation that is the nearest to the generated noisy data at a given sampling timestep is observed to correspond to a higher-noise timestep (termed slowed timestep), i.e., the corresponding ground-truth timestep is slower than the sampling timestep. MixFlow leverages the interpolations at the slowed timesteps, named slowed interpolation mixture, for post-training the prediction network for each training timestep. Experiments over class-conditional image generation (including SiT, REPA, and RAE) and text-to-image generation validate the effectiveness of our approach. Our approach MixFlow over the RAE models achieve strong generation results on ImageNet: 1.43 FID (without guidance) and 1.10 (with guidance) at 256 x 256, and 1.55 FID (without guidance) and 1.10 (with guidance) at 512 x 512.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-22T12:00:12+00:00",
      "updated": "2025-12-22T12:00:12+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19311v1",
      "file": "papers/2512.19311v1.pdf"
    },
    {
      "arxiv_id": "2512.19302v1",
      "title": "Bridging Semantics and Geometry: A Decoupled LVLM-SAM Framework for Reasoning Segmentation in Remote Sensing",
      "authors": [
        {
          "name": "Xu Zhang"
        },
        {
          "name": "Junyao Ge"
        },
        {
          "name": "Yang Zheng"
        },
        {
          "name": "Kaitai Guo"
        },
        {
          "name": "Jimin Liang"
        }
      ],
      "abstract": "Large Vision-Language Models (LVLMs) hold great promise for advancing remote sensing (RS) analysis, yet existing reasoning segmentation frameworks couple linguistic reasoning and pixel prediction through end-to-end supervised fine-tuning, leading to weak geometric grounding and limited generalization across tasks. To address this, we developed Think2Seg-RS, a decoupled framework that trains an LVLM prompter to control a frozen Segment Anything Model (SAM) via structured geometric prompts. Through a mask-only reinforcement learning objective, the LVLM learns to translate abstract semantic reasoning into spatially grounded actions, achieving state-of-the-art performance on the EarthReason dataset. Remarkably, the learned prompting policy generalizes zero-shot to multiple referring segmentation benchmarks, exposing a distinct divide between semantic-level and instance-level grounding. We further found that compact segmenters outperform larger ones under semantic-level supervision, and that negative prompts are ineffective in heterogeneous aerial backgrounds. Together, these findings establish semantic-level reasoning segmentation as a new paradigm for geospatial understanding, opening the way toward unified, interpretable LVLM-driven Earth observation. Our code and model are available at https://github.com/Ricardo-XZ/Think2Seg-RS.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T11:46:42+00:00",
      "updated": "2025-12-22T11:46:42+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19302v1",
      "file": "papers/2512.19302v1.pdf"
    },
    {
      "arxiv_id": "2512.19300v1",
      "title": "RMLer: Synthesizing Novel Objects across Diverse Categories via Reinforcement Mixing Learning",
      "authors": [
        {
          "name": "Jun Li"
        },
        {
          "name": "Zikun Chen"
        },
        {
          "name": "Haibo Chen"
        },
        {
          "name": "Shuo Chen"
        },
        {
          "name": "Jian Yang"
        }
      ],
      "abstract": "Novel object synthesis by integrating distinct textual concepts from diverse categories remains a significant challenge in Text-to-Image (T2I) generation. Existing methods often suffer from insufficient concept mixing, lack of rigorous evaluation, and suboptimal outputs-manifesting as conceptual imbalance, superficial combinations, or mere juxtapositions. To address these limitations, we propose Reinforcement Mixing Learning (RMLer), a framework that formulates cross-category concept fusion as a reinforcement learning problem: mixed features serve as states, mixing strategies as actions, and visual outcomes as rewards. Specifically, we design an MLP-policy network to predict dynamic coefficients for blending cross-category text embeddings. We further introduce visual rewards based on (1) semantic similarity and (2) compositional balance between the fused object and its constituent concepts, optimizing the policy via proximal policy optimization. At inference, a selection strategy leverages these rewards to curate the highest-quality fused objects. Extensive experiments demonstrate RMLer's superiority in synthesizing coherent, high-fidelity objects from diverse categories, outperforming existing methods. Our work provides a robust framework for generating novel visual concepts, with promising applications in film, gaming, and design.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T11:44:32+00:00",
      "updated": "2025-12-22T11:44:32+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19300v1",
      "file": "papers/2512.19300v1.pdf"
    },
    {
      "arxiv_id": "2512.19283v1",
      "title": "Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context",
      "authors": [
        {
          "name": "Kyungwon Cho"
        },
        {
          "name": "Hanbyul Joo"
        }
      ],
      "abstract": "Egocentric vision systems are becoming widely available, creating new opportunities for human-computer interaction. A core challenge is estimating the wearer's full-body motion from first-person videos, which is crucial for understanding human behavior. However, this task is difficult since most body parts are invisible from the egocentric view. Prior approaches mainly rely on head trajectories, leading to ambiguity, or assume continuously tracked hands, which is unrealistic for lightweight egocentric devices. In this work, we present HaMoS, the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices. To overcome the lack of datasets pairing diverse camera views with human motion, we introduce a novel augmentation method that models such real-world conditions. We also demonstrate that sequence-level contexts such as body shape and field-of-view are crucial for accurate motion reconstruction, and thus employ local attention to infer long sequences efficiently. Experiments on public benchmarks show that our method achieves state-of-the-art accuracy and temporal smoothness, demonstrating a practical step toward reliable in-the-wild egocentric 3D motion understanding.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T11:26:41+00:00",
      "updated": "2025-12-22T11:26:41+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19283v1",
      "file": "papers/2512.19283v1.pdf"
    },
    {
      "arxiv_id": "2512.19275v1",
      "title": "Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation",
      "authors": [
        {
          "name": "Ivan DeAndres-Tame"
        },
        {
          "name": "Chengwei Ye"
        },
        {
          "name": "Ruben Tolosana"
        },
        {
          "name": "Ruben Vera-Rodriguez"
        },
        {
          "name": "Shiqi Yu"
        }
      ],
      "abstract": "Generative AI (GenAI) models have revolutionized animation, enabling the synthesis of humans and motion patterns with remarkable visual fidelity. However, generating truly realistic human animation remains a formidable challenge, where even minor inconsistencies can make a subject appear unnatural. This limitation is particularly critical when AI-generated videos are evaluated for behavioral biometrics, where subtle motion cues that define identity are easily lost or distorted. The present study investigates whether state-of-the-art GenAI human animation models can preserve the subtle spatio-temporal details needed for person identification through gait biometrics. Specifically, we evaluate four different GenAI models across two primary evaluation tasks to assess their ability to i) restore gait patterns from reference videos under varying conditions of complexity, and ii) transfer these gait patterns to different visual identities. Our results show that while visual quality is mostly high, biometric fidelity remains low in tasks focusing on identification, suggesting that current GenAI models struggle to disentangle identity from motion. Furthermore, through an identity transfer task, we expose a fundamental flaw in appearance-based gait recognition: when texture is disentangled from motion, identification collapses, proving current GenAI models rely on visual attributes rather than temporal dynamics.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-22T11:19:46+00:00",
      "updated": "2025-12-22T11:19:46+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19275v1",
      "file": "papers/2512.19275v1.pdf"
    },
    {
      "arxiv_id": "2512.19271v1",
      "title": "3SGen: Unified Subject, Style, and Structure-Driven Image Generation with Adaptive Task-specific Memory",
      "authors": [
        {
          "name": "Xinyang Song"
        },
        {
          "name": "Libin Wang"
        },
        {
          "name": "Weining Wang"
        },
        {
          "name": "Zhiwei Li"
        },
        {
          "name": "Jianxin Sun"
        },
        {
          "name": "Dandan Zheng"
        },
        {
          "name": "Jingdong Chen"
        },
        {
          "name": "Qi Li"
        },
        {
          "name": "Zhenan Sun"
        }
      ],
      "abstract": "Recent image generation approaches often address subject, style, and structure-driven conditioning in isolation, leading to feature entanglement and limited task transferability. In this paper, we introduce 3SGen, a task-aware unified framework that performs all three conditioning modes within a single model. 3SGen employs an MLLM equipped with learnable semantic queries to align text-image semantics, complemented by a VAE branch that preserves fine-grained visual details. At its core, an Adaptive Task-specific Memory (ATM) module dynamically disentangles, stores, and retrieves condition-specific priors, such as identity for subjects, textures for styles, and spatial layouts for structures, via a lightweight gating mechanism along with several scalable memory items. This design mitigates inter-task interference and naturally scales to compositional inputs. In addition, we propose 3SGen-Bench, a unified image-driven generation benchmark with standardized metrics for evaluating cross-task fidelity and controllability. Extensive experiments on our proposed 3SGen-Bench and other public benchmarks demonstrate our superior performance across diverse image-driven generation tasks.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T11:07:27+00:00",
      "updated": "2025-12-22T11:07:27+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19271v1",
      "file": "papers/2512.19271v1.pdf"
    },
    {
      "arxiv_id": "2512.19243v1",
      "title": "VisionDirector: Vision-Language Guided Closed-Loop Refinement for Generative Image Synthesis",
      "authors": [
        {
          "name": "Meng Chu"
        },
        {
          "name": "Senqiao Yang"
        },
        {
          "name": "Haoxuan Che"
        },
        {
          "name": "Suiyun Zhang"
        },
        {
          "name": "Xichen Zhang"
        },
        {
          "name": "Shaozuo Yu"
        },
        {
          "name": "Haokun Gui"
        },
        {
          "name": "Zhefan Rao"
        },
        {
          "name": "Dandan Tu"
        },
        {
          "name": "Rui Liu"
        },
        {
          "name": "Jiaya Jia"
        }
      ],
      "abstract": "Generative models can now produce photorealistic imagery, yet they still struggle with the long, multi-goal prompts that professional designers issue. To expose this gap and better evaluate models' performance in real-world settings, we introduce Long Goal Bench (LGBench), a 2,000-task suite (1,000 T2I and 1,000 I2I) whose average instruction contains 18 to 22 tightly coupled goals spanning global layout, local object placement, typography, and logo fidelity. We find that even state-of-the-art models satisfy fewer than 72 percent of the goals and routinely miss localized edits, confirming the brittleness of current pipelines. To address this, we present VisionDirector, a training-free vision-language supervisor that (i) extracts structured goals from long instructions, (ii) dynamically decides between one-shot generation and staged edits, (iii) runs micro-grid sampling with semantic verification and rollback after every edit, and (iv) logs goal-level rewards. We further fine-tune the planner with Group Relative Policy Optimization, yielding shorter edit trajectories (3.1 versus 4.2 steps) and stronger alignment. VisionDirector achieves new state of the art on GenEval (plus 7 percent overall) and ImgEdit (plus 0.07 absolute) while producing consistent qualitative improvements on typography, multi-object scenes, and pose editing.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T10:25:38+00:00",
      "updated": "2025-12-22T10:25:38+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19243v1",
      "file": "papers/2512.19243v1.pdf"
    },
    {
      "arxiv_id": "2512.19225v1",
      "title": "Selective Phase-Aware Training of nnU-Net for Robust Breast Cancer Segmentation in Multi-Center DCE-MRI",
      "authors": [
        {
          "name": "Beyza Zayim"
        },
        {
          "name": "Aissiou Ikram"
        },
        {
          "name": "Boukhiar Naima"
        }
      ],
      "abstract": "Breast cancer remains the most common cancer among women and is a leading cause of female mortality. Dynamic contrast-enhanced MRI (DCE-MRI) is a powerful imaging tool for evaluating breast tumors, yet the field lacks a standardized benchmark for analyzing treatment responses and guiding personalized care. We participated in the MAMA-MIA Challenge's Primary Tumor Segmentation task and this work presents a proposed selective, phase-aware training framework for the nnU-Net architecture, emphasizing quality-focused data selection to strengthen model robustness and generalization. We employed the No New Net (nnU-Net) framework with a selective training strategy that systematically analyzed the impact of image quality and center-specific variability on segmentation performance. Controlled experiments on the DUKE, NACT, ISPY1, and ISPY2 datasets revealed that including ISPY scans with motion artifacts and reduced contrast impaired segmentation performance, even with advanced preprocessing, such as contrast-limited adaptive histogram equalization (CLAHE). In contrast, training on DUKE and NACT data, which exhibited clearer contrast and fewer motion artifacts despite varying resolutions, with early phase images (0000-0002) provided more stable training conditions. Our results demonstrate the importance of phase-sensitive and quality-aware training strategies in achieving reliable segmentation performance in heterogeneous clinical datasets, highlighting the limitations of the expansion of naive datasets and motivating the need for future automation of quality-based data selection strategies.",
      "primary_category": "eess.IV",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-12-22T10:05:37+00:00",
      "updated": "2025-12-22T10:05:37+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19225v1",
      "file": "papers/2512.19225v1.pdf"
    },
    {
      "arxiv_id": "2512.19221v1",
      "title": "From Pixels to Predicates Structuring urban perception with scene graphs",
      "authors": [
        {
          "name": "Yunlong Liu"
        },
        {
          "name": "Shuyang Li"
        },
        {
          "name": "Pengyuan Liu"
        },
        {
          "name": "Yu Zhang"
        },
        {
          "name": "Rudi Stouffs"
        }
      ],
      "abstract": "Perception research is increasingly modelled using streetscapes, yet many approaches still rely on pixel features or object co-occurrence statistics, overlooking the explicit relations that shape human perception. This study proposes a three stage pipeline that transforms street view imagery (SVI) into structured representations for predicting six perceptual indicators. In the first stage, each image is parsed using an open-set Panoptic Scene Graph model (OpenPSG) to extract object predicate object triplets. In the second stage, compact scene-level embeddings are learned through a heterogeneous graph autoencoder (GraphMAE). In the third stage, a neural network predicts perception scores from these embeddings. We evaluate the proposed approach against image-only baselines in terms of accuracy, precision, and cross-city generalization. Results indicate that (i) our approach improves perception prediction accuracy by an average of 26% over baseline models, and (ii) maintains strong generalization performance in cross-city prediction tasks. Additionally, the structured representation clarifies which relational patterns contribute to lower perception scores in urban scenes, such as graffiti on wall and car parked on sidewalk. Overall, this study demonstrates that graph-based structure provides expressive, generalizable, and interpretable signals for modelling urban perception, advancing human-centric and context-aware urban analytics.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-22T10:02:53+00:00",
      "updated": "2025-12-22T10:02:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19221v1",
      "file": "papers/2512.19221v1.pdf"
    },
    {
      "arxiv_id": "2512.19219v1",
      "title": "Towards Minimal Fine-Tuning of VLMs",
      "authors": [
        {
          "name": "Tiange Luo"
        },
        {
          "name": "Lajanugen Logeswaran"
        },
        {
          "name": "Jaekyeom Kim"
        },
        {
          "name": "Justin Johnson"
        },
        {
          "name": "Honglak Lee"
        }
      ],
      "abstract": "We introduce Image-LoRA, a lightweight parameter efficient fine-tuning (PEFT) recipe for transformer-based vision-language models (VLMs). Image-LoRA applies low-rank adaptation only to the value path of attention layers within the visual-token span, reducing adapter-only training FLOPs roughly in proportion to the visual-token fraction. We further adapt only a subset of attention heads, selected using head influence scores estimated with a rank-1 Image-LoRA, and stabilize per-layer updates via selection-size normalization. Across screen-centric grounding and referring benchmarks spanning text-heavy to image-heavy regimes, Image-LoRA matches or closely approaches standard LoRA accuracy while using fewer trainable parameters and lower adapter-only training FLOPs. The method also preserves the pure-text reasoning performance of VLMs before and after fine-tuning, as further shown on GSM8K.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-22T10:02:10+00:00",
      "updated": "2025-12-22T10:02:10+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19219v1",
      "file": "papers/2512.19219v1.pdf"
    },
    {
      "arxiv_id": "2512.19214v1",
      "title": "HippMetric: A skeletal-representation-based framework for cross-sectional and longitudinal hippocampal substructural morphometry",
      "authors": [
        {
          "name": "Na Gao"
        },
        {
          "name": "Chenfei Ye"
        },
        {
          "name": "Yanwu Yang"
        },
        {
          "name": "Anqi Li"
        },
        {
          "name": "Zhengbo He"
        },
        {
          "name": "Li Liang"
        },
        {
          "name": "Zhiyuan Liu"
        },
        {
          "name": "Xingyu Hao"
        },
        {
          "name": "Ting Ma"
        },
        {
          "name": "Tengfei Guo"
        }
      ],
      "abstract": "Accurate characterization of hippocampal substructure is crucial for detecting subtle structural changes and identifying early neurodegenerative biomarkers. However, high inter-subject variability and complex folding pattern of human hippocampus hinder consistent cross-subject and longitudinal analysis. Most existing approaches rely on subject-specific modelling and lack a stable intrinsic coordinate system to accommodate anatomical variability, which limits their ability to establish reliable inter- and intra-individual correspondence. To address this, we propose HippMetric, a skeletal representation (s-rep)-based framework for hippocampal substructural morphometry and point-wise correspondence across individuals and scans. HippMetric builds on the Axis-Referenced Morphometric Model (ARMM) and employs a deformable skeletal coordinate system aligned with hippocampal anatomy and function, providing a biologically grounded reference for correspondence. Our framework comprises two core modules: a skeletal-based coordinate system that respects the hippocampus' conserved longitudinal lamellar architecture, in which functional units (lamellae) are stacked perpendicular to the long-axis, enabling anatomically consistent localization across subjects and time; and individualized s-reps generated through surface reconstruction, deformation, and geometrically constrained spoke refinement, enforcing boundary adherence, orthogonality and non-intersection to produce mathematically valid skeletal geometry. Extensive experiments on two international cohorts demonstrate that HippMetric achieves higher accuracy, reliability, and correspondence stability compared to existing shape models.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T09:53:55+00:00",
      "updated": "2025-12-22T09:53:55+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19214v1",
      "file": "papers/2512.19214v1.pdf"
    },
    {
      "arxiv_id": "2512.19213v1",
      "title": "InvCoSS: Inversion-driven Continual Self-supervised Learning in Medical Multi-modal Image Pre-training",
      "authors": [
        {
          "name": "Zihao Luo"
        },
        {
          "name": "Shaohao Rui"
        },
        {
          "name": "Zhenyu Tang"
        },
        {
          "name": "Guotai Wang"
        },
        {
          "name": "Xiaosong Wang"
        }
      ],
      "abstract": "Continual self-supervised learning (CSSL) in medical imaging trains a foundation model sequentially, alleviating the need for collecting multi-modal images for joint training and offering promising improvements in downstream performance while preserving data privacy. However, most existing methods still rely on replaying data from previous stages to prevent catastrophic forgetting, which compromises privacy and limits their applicability in real-world scenarios where data transfer across sites is often restricted. In this work, we propose InvCoSS, an inversion-driven continual self-supervised learning framework for medical multi-modal image pre-training. Specifically, after training on a previous task, InvCoSS inverts the pre-trained self-supervised model to generate synthetic images that approximate the original training distribution. These synthetic images are then combined with data from the new task for joint optimization, which effectively mitigates catastrophic forgetting while strictly adhering to the constraint of no access to previous real data. Furthermore, to improve the fidelity of synthetic images, we introduce a novel InvUNet with a multi-scale fusion architecture to restore both high- and low-frequency components of the inverted images. To enhance diversity and prevent mode collapse, we design a repulsive representation-learning mechanism that encourages a diverse feature space for synthetic images without class guidance. Extensive experiments across nine downstream tasks validate the effectiveness of InvCoSS, achieving performance comparable to or even superior to prior data-replay methods while significantly reducing storage requirements and eliminating data privacy constraints.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T09:53:38+00:00",
      "updated": "2025-12-22T09:53:38+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19213v1",
      "file": "papers/2512.19213v1.pdf"
    },
    {
      "arxiv_id": "2512.19190v1",
      "title": "PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements",
      "authors": [
        {
          "name": "Marios Thoma"
        },
        {
          "name": "Zenonas Theodosiou"
        },
        {
          "name": "Harris Partaourides"
        },
        {
          "name": "Vassilis Vassiliades"
        },
        {
          "name": "Loizos Michael"
        },
        {
          "name": "Andreas Lanitis"
        }
      ],
      "abstract": "Walking has always been a primary mode of transportation and is recognized as an essential activity for maintaining good health. Despite the need for safe walking conditions in urban environments, sidewalks are frequently obstructed by various obstacles that hinder free pedestrian movement. Any object obstructing a pedestrian's path can pose a safety hazard. The advancement of pervasive computing and egocentric vision techniques offers the potential to design systems that can automatically detect such obstacles in real time, thereby enhancing pedestrian safety. The development of effective and efficient identification algorithms relies on the availability of comprehensive and well-balanced datasets of egocentric data. In this work, we introduce the PEDESTRIAN dataset, comprising egocentric data for 29 different obstacles commonly found on urban sidewalks. A total of 340 videos were collected using mobile phone cameras, capturing a pedestrian's point of view. Additionally, we present the results of a series of experiments that involved training several state-of-the-art deep learning algorithms using the proposed dataset, which can be used as a benchmark for obstacle detection and recognition tasks. The dataset can be used for training pavement obstacle detectors to enhance the safety of pedestrians in urban areas.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-12-22T09:28:23+00:00",
      "updated": "2025-12-22T09:28:23+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19190v1",
      "file": "papers/2512.19190v1.pdf"
    },
    {
      "arxiv_id": "2512.19173v1",
      "title": "CycleChart: A Unified Consistency-Based Learning Framework for Bidirectional Chart Understanding and Generation",
      "authors": [
        {
          "name": "Dazhen Deng"
        },
        {
          "name": "Sen Yang"
        },
        {
          "name": "Yuchen He"
        },
        {
          "name": "Yuan Tian"
        },
        {
          "name": "Yingcai Wu"
        }
      ],
      "abstract": "Current chart-specific tasks, such as chart question answering, chart parsing, and chart generation, are typically studied in isolation, preventing models from learning the shared semantics that link chart generation and interpretation. We introduce CycleChart, a consistency-based learning framework for bidirectional chart understanding and generation. CycleChart adopts a schema-centric formulation as a common interface across tasks. We construct a consistent multi-task dataset, where each chart sample includes aligned annotations for schema prediction, data parsing, and question answering. To learn cross-directional chart semantics, CycleChart introduces a generate-parse consistency objective: the model generates a chart schema from a table and a textual query, then learns to recover the schema and data from the generated chart, enforcing semantic alignment across directions. CycleChart achieves strong results on chart generation, chart parsing, and chart question answering, demonstrating improved cross-task generalization and marking a step toward more general chart understanding models.",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.CV"
      ],
      "published": "2025-12-22T09:07:34+00:00",
      "updated": "2025-12-22T09:07:34+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19173v1",
      "file": "papers/2512.19173v1.pdf"
    },
    {
      "arxiv_id": "2512.19150v1",
      "title": "AMap: Distilling Future Priors for Ahead-Aware Online HD Map Construction",
      "authors": [
        {
          "name": "Ruikai Li"
        },
        {
          "name": "Xinrun Li"
        },
        {
          "name": "Mengwei Xie"
        },
        {
          "name": "Hao Shan"
        },
        {
          "name": "Shoumeng Qiu"
        },
        {
          "name": "Xinyuan Chang"
        },
        {
          "name": "Yizhe Fan"
        },
        {
          "name": "Feng Xiong"
        },
        {
          "name": "Han Jiang"
        },
        {
          "name": "Yilong Ren"
        },
        {
          "name": "Haiyang Yu"
        },
        {
          "name": "Mu Xu"
        },
        {
          "name": "Yang Long"
        },
        {
          "name": "Varun Ojha"
        },
        {
          "name": "Zhiyong Cui"
        }
      ],
      "abstract": "Online High-Definition (HD) map construction is pivotal for autonomous driving. While recent approaches leverage historical temporal fusion to improve performance, we identify a critical safety flaw in this paradigm: it is inherently ``spatially backward-looking.\" These methods predominantly enhance map reconstruction in traversed areas, offering minimal improvement for the unseen road ahead. Crucially, our analysis of downstream planning tasks reveals a severe asymmetry: while rearward perception errors are often tolerable, inaccuracies in the forward region directly precipitate hazardous driving maneuvers. To bridge this safety gap, we propose AMap, a novel framework for Ahead-aware online HD Mapping. We pioneer a ``distill-from-future\" paradigm, where a teacher model with privileged access to future temporal contexts guides a lightweight student model restricted to the current frame. This process implicitly compresses prospective knowledge into the student model, endowing it with ``look-ahead\" capabilities at zero inference-time cost. Technically, we introduce a Multi-Level BEV Distillation strategy with spatial masking and an Asymmetric Query Adaptation module to effectively transfer future-aware representations to the student's static queries. Extensive experiments on the nuScenes and Argoverse 2 benchmark demonstrate that AMap significantly enhances current-frame perception. Most notably, it outperforms state-of-the-art temporal models in critical forward regions while maintaining the efficiency of single current frame inference.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T08:46:59+00:00",
      "updated": "2025-12-22T08:46:59+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19150v1",
      "file": "papers/2512.19150v1.pdf"
    },
    {
      "arxiv_id": "2512.19133v1",
      "title": "WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving",
      "authors": [
        {
          "name": "Pengxuan Yang"
        },
        {
          "name": "Ben Lu"
        },
        {
          "name": "Zhongpu Xia"
        },
        {
          "name": "Chao Han"
        },
        {
          "name": "Yinfeng Gao"
        },
        {
          "name": "Teng Zhang"
        },
        {
          "name": "Kun Zhan"
        },
        {
          "name": "XianPeng Lang"
        },
        {
          "name": "Yupeng Zheng"
        },
        {
          "name": "Qichao Zhang"
        }
      ],
      "abstract": "Latent World Models enhance scene representation through temporal self-supervised learning, presenting a perception annotation-free paradigm for end-to-end autonomous driving. However, the reconstruction-oriented representation learning tangles perception with planning tasks, leading to suboptimal optimization for planning. To address this challenge, we propose WorldRFT, a planning-oriented latent world model framework that aligns scene representation learning with planning via a hierarchical planning decomposition and local-aware interactive refinement mechanism, augmented by reinforcement learning fine-tuning (RFT) to enhance safety-critical policy performance. Specifically, WorldRFT integrates a vision-geometry foundation model to improve 3D spatial awareness, employs hierarchical planning task decomposition to guide representation optimization, and utilizes local-aware iterative refinement to derive a planning-oriented driving policy. Furthermore, we introduce Group Relative Policy Optimization (GRPO), which applies trajectory Gaussianization and collision-aware rewards to fine-tune the driving policy, yielding systematic improvements in safety. WorldRFT achieves state-of-the-art (SOTA) performance on both open-loop nuScenes and closed-loop NavSim benchmarks. On nuScenes, it reduces collision rates by 83% (0.30% -> 0.05%). On NavSim, using camera-only sensors input, it attains competitive performance with the LiDAR-based SOTA method DiffusionDrive (87.8 vs. 88.1 PDMS).",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "published": "2025-12-22T08:27:44+00:00",
      "updated": "2025-12-22T08:27:44+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19133v1",
      "file": "papers/2512.19133v1.pdf"
    },
    {
      "arxiv_id": "2512.19115v1",
      "title": "Generative Giants, Retrieval Weaklings: Why do Multimodal Large Language Models Fail at Multimodal Retrieval?",
      "authors": [
        {
          "name": "Hengyi Feng"
        },
        {
          "name": "Zeang Sheng"
        },
        {
          "name": "Meiyi Qiang"
        },
        {
          "name": "Wentao Zhang"
        }
      ],
      "abstract": "Despite the remarkable success of multimodal large language models (MLLMs) in generative tasks, we observe that they exhibit a counterintuitive deficiency in the zero-shot multimodal retrieval task. In this work, we investigate the underlying mechanisms that hinder MLLMs from serving as effective retrievers. With the help of sparse autoencoders (SAEs), we decompose MLLM output representations into interpretable semantic concepts to probe their intrinsic behavior. Our analysis reveals that the representation space of MLLMs is overwhelmingly dominated by textual semantics; the visual information essential for multimodal retrieval only constitutes a small portion. This imbalance is compounded by the heavy focus of MLLMs on bridging image-text modalities, which facilitates generation but homogenizes embeddings and finally diminishes the discriminative power required for multimodal retrieval. We further discover that the specific feature components that contribute most to the similarity computations for MLLMs are in fact distractors that actively degrade retrieval performance. Overall, our work provides the first in-depth interpretability analysis of MLLM representations in the context of multimodal retrieval and offers possible directions for enhancing the multimodal retrieval capabilities of MLLMs.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T07:36:20+00:00",
      "updated": "2025-12-22T07:36:20+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19115v1",
      "file": "papers/2512.19115v1.pdf"
    },
    {
      "arxiv_id": "2512.19110v1",
      "title": "Trifocal Tensor and Relative Pose Estimation with Known Vertical Direction",
      "authors": [
        {
          "name": "Tao Li"
        },
        {
          "name": "Zhenbao Yu"
        },
        {
          "name": "Banglei Guan"
        },
        {
          "name": "Jianli Han"
        },
        {
          "name": "Weimin Lv"
        },
        {
          "name": "Friedrich Fraundorfer"
        }
      ],
      "abstract": "This work presents two novel solvers for estimating the relative poses among views with known vertical directions. The vertical directions of camera views can be easily obtained using inertial measurement units (IMUs) which have been widely used in autonomous vehicles, mobile phones, and unmanned aerial vehicles (UAVs). Given the known vertical directions, our lgorithms only need to solve for two rotation angles and two translation vectors. In this paper, a linear closed-form solution has been described, requiring only four point correspondences in three views. We also propose a minimal solution with three point correspondences using the latest Gröbner basis solver. Since the proposed methods require fewer point correspondences, they can be efficiently applied within the RANSAC framework for outliers removal and pose estimation in visual odometry. The proposed method has been tested on both synthetic data and real-world scenes from KITTI. The experimental results show that the accuracy of the estimated poses is superior to other alternative methods.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T07:26:40+00:00",
      "updated": "2025-12-22T07:26:40+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19110v1",
      "file": "papers/2512.19110v1.pdf"
    },
    {
      "arxiv_id": "2512.19108v1",
      "title": "GaussianImage++: Boosted Image Representation and Compression with 2D Gaussian Splatting",
      "authors": [
        {
          "name": "Tiantian Li"
        },
        {
          "name": "Xinjie Zhang"
        },
        {
          "name": "Xingtong Ge"
        },
        {
          "name": "Tongda Xu"
        },
        {
          "name": "Dailan He"
        },
        {
          "name": "Jun Zhang"
        },
        {
          "name": "Yan Wang"
        }
      ],
      "abstract": "Implicit neural representations (INRs) have achieved remarkable success in image representation and compression, but they require substantial training time and memory. Meanwhile, recent 2D Gaussian Splatting (GS) methods (\\textit{e.g.}, GaussianImage) offer promising alternatives through efficient primitive-based rendering. However, these methods require excessive Gaussian primitives to maintain high visual fidelity. To exploit the potential of GS-based approaches, we present GaussianImage++, which utilizes limited Gaussian primitives to achieve impressive representation and compression performance. Firstly, we introduce a distortion-driven densification mechanism. It progressively allocates Gaussian primitives according to signal intensity. Secondly, we employ context-aware Gaussian filters for each primitive, which assist in the densification to optimize Gaussian primitives based on varying image content. Thirdly, we integrate attribute-separated learnable scalar quantizers and quantization-aware training, enabling efficient compression of primitive attributes. Experimental results demonstrate the effectiveness of our method. In particular, GaussianImage++ outperforms GaussianImage and INRs-based COIN in representation and compression performance while maintaining real-time decoding and low memory usage.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T07:22:39+00:00",
      "updated": "2025-12-22T07:22:39+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19108v1",
      "file": "papers/2512.19108v1.pdf"
    },
    {
      "arxiv_id": "2512.19095v1",
      "title": "Mamba-Based Modality Disentanglement Network for Multi-Contrast MRI Reconstruction",
      "authors": [
        {
          "name": "Weiyi Lyu"
        },
        {
          "name": "Xinming Fang"
        },
        {
          "name": "Jun Wang"
        },
        {
          "name": "Jun Shi"
        },
        {
          "name": "Guixu Zhang"
        },
        {
          "name": "Juncheng Li"
        }
      ],
      "abstract": "Magnetic resonance imaging (MRI) is a cornerstone of modern clinical diagnosis, offering unparalleled soft-tissue contrast without ionizing radiation. However, prolonged scan times remain a major barrier to patient throughput and comfort. Existing accelerated MRI techniques often struggle with two key challenges: (1) failure to effectively utilize inherent K-space prior information, leading to persistent aliasing artifacts from zero-filled inputs; and (2) contamination of target reconstruction quality by irrelevant information when employing multi-contrast fusion strategies. To overcome these challenges, we present MambaMDN, a dual-domain framework for multi-contrast MRI reconstruction. Our approach first employs fully-sampled reference K-space data to complete the undersampled target data, generating structurally aligned but modality-mixed inputs. Subsequently, we develop a Mamba-based modality disentanglement network to extract and remove reference-specific features from the mixed representation. Furthermore, we introduce an iterative refinement mechanism to progressively enhance reconstruction accuracy through repeated feature purification. Extensive experiments demonstrate that MambaMDN can significantly outperform existing multi-contrast reconstruction methods.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T07:06:34+00:00",
      "updated": "2025-12-22T07:06:34+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19095v1",
      "file": "papers/2512.19095v1.pdf"
    },
    {
      "arxiv_id": "2512.19088v1",
      "title": "Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation",
      "authors": [
        {
          "name": "Khanh Nguyen"
        },
        {
          "name": "Dasith de Silva Edirimuni"
        },
        {
          "name": "Ghulam Mubashar Hassan"
        },
        {
          "name": "Ajmal Mian"
        }
      ],
      "abstract": "Locating and retrieving objects from scene-level point clouds is a challenging problem with broad applications in robotics and augmented reality. This task is commonly formulated as open-vocabulary 3D instance segmentation. Although recent methods demonstrate strong performance, they depend heavily on SAM and CLIP to generate and classify 3D instance masks from images accompanying the point cloud, leading to substantial computational overhead and slow processing that limit their deployment in real-world settings. Open-YOLO 3D alleviates this issue by using a real-time 2D detector to classify class-agnostic masks produced directly from the point cloud by a pretrained 3D segmenter, eliminating the need for SAM and CLIP and significantly reducing inference time. However, Open-YOLO 3D often fails to generalize to object categories that appear infrequently in the 3D training data. In this paper, we propose a method that generates 3D instance masks for novel objects from RGB images guided by a 2D open-vocabulary detector. Our approach inherits the 2D detector's ability to recognize novel objects while maintaining efficient classification, enabling fast and accurate retrieval of rare instances from open-ended text queries. Our code will be made available at https://github.com/ndkhanh360/BoxOVIS.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T06:57:42+00:00",
      "updated": "2025-12-22T06:57:42+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19088v1",
      "file": "papers/2512.19088v1.pdf"
    },
    {
      "arxiv_id": "2512.19070v1",
      "title": "Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding",
      "authors": [
        {
          "name": "Ruiqi Ma"
        },
        {
          "name": "Yu Yan"
        },
        {
          "name": "Chunhong Zhang"
        },
        {
          "name": "Minghao Yin"
        },
        {
          "name": "XinChao Liu"
        },
        {
          "name": "Zhihong Jin"
        },
        {
          "name": "Zheng Hu"
        }
      ],
      "abstract": "Large Vision-Language Models (LVLMs) bridge the gap between visual and linguistic modalities, demonstrating strong potential across a variety of domains. However, despite significant progress, LVLMs still suffer from severe hallucination issues in object recognition tasks. These models often fail to accurately identify certain objects, leading to text generation that appears fluent but does not correspond to the visual content, which can have serious consequences in real-world applications. Recently, several methods have been proposed to alleviate LVLM hallucinations, but most focus solely on reducing hallucinations in the language modality. To mitigate hallucinations in both the language and visual modalities, we introduce Hallucination Disentangled Decoding (HDD) method that requires no training. HDD enhances the original image by segmenting it and selecting images that augment the original, while also utilizing a blank image to eliminate language prior hallucinations in both the original and segmented images. This design not only reduces the model's dependence on language priors but also enhances its visual performance. (Code: https://github.com/rickeyhhh/Hallucination-Disentangled-Decoding)",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "published": "2025-12-22T06:20:53+00:00",
      "updated": "2025-12-22T06:20:53+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19070v1",
      "file": "papers/2512.19070v1.pdf"
    },
    {
      "arxiv_id": "2512.19058v1",
      "title": "6DAttack: Backdoor Attacks in the 6DoF Pose Estimation",
      "authors": [
        {
          "name": "Jihui Guo"
        },
        {
          "name": "Zongmin Zhang"
        },
        {
          "name": "Zhen Sun"
        },
        {
          "name": "Yuhao Yang"
        },
        {
          "name": "Jinlin Wu"
        },
        {
          "name": "Fu Zhang"
        },
        {
          "name": "Xinlei He"
        }
      ],
      "abstract": "Deep learning advances have enabled accurate six-degree-of-freedom (6DoF) object pose estimation, widely used in robotics, AR/VR, and autonomous systems. However, backdoor attacks pose significant security risks. While most research focuses on 2D vision, 6DoF pose estimation remains largely unexplored. Unlike traditional backdoors that only change classes, 6DoF attacks must control continuous parameters like translation and rotation, rendering 2D methods inapplicable. We propose 6DAttack, a framework using 3D object triggers to induce controlled erroneous poses while maintaining normal behavior. Evaluations on PVNet, DenseFusion, and PoseDiffusion across LINEMOD, YCB-Video, and CO3D show high attack success rates (ASRs) without compromising clean performance. Backdoored models achieve up to 100% clean ADD accuracy and 100% ASR, with triggered samples reaching 97.70% ADD-P. Furthermore, a representative defense remains ineffective. Our findings reveal a serious, underexplored threat to 6DoF pose estimation.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T05:49:57+00:00",
      "updated": "2025-12-22T05:49:57+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19058v1",
      "file": "papers/2512.19058v1.pdf"
    },
    {
      "arxiv_id": "2512.19048v1",
      "title": "WaTeRFlow: Watermark Temporal Robustness via Flow Consistency",
      "authors": [
        {
          "name": "Utae Jeong"
        },
        {
          "name": "Sumin In"
        },
        {
          "name": "Hyunju Ryu"
        },
        {
          "name": "Jaewan Choi"
        },
        {
          "name": "Feng Yang"
        },
        {
          "name": "Jongheon Jeong"
        },
        {
          "name": "Seungryong Kim"
        },
        {
          "name": "Sangpil Kim"
        }
      ],
      "abstract": "Image watermarking supports authenticity and provenance, yet many schemes are still easy to bypass with various distortions and powerful generative edits. Deep learning-based watermarking has improved robustness to diffusion-based image editing, but a gap remains when a watermarked image is converted to video by image-to-video (I2V), in which per-frame watermark detection weakens. I2V has quickly advanced from short, jittery clips to multi-second, temporally coherent scenes, and it now serves not only content creation but also world-modeling and simulation workflows, making cross-modal watermark recovery crucial. We present WaTeRFlow, a framework tailored for robustness under I2V. It consists of (i) FUSE (Flow-guided Unified Synthesis Engine), which exposes the encoder-decoder to realistic distortions via instruction-driven edits and a fast video diffusion proxy during training, (ii) optical-flow warping with a Temporal Consistency Loss (TCL) that stabilizes per-frame predictions, and (iii) a semantic preservation loss that maintains the conditioning signal. Experiments across representative I2V models show accurate watermark recovery from frames, with higher first-frame and per-frame bit accuracy and resilience when various distortions are applied before or after video generation.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T05:33:59+00:00",
      "updated": "2025-12-22T05:33:59+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19048v1",
      "file": "papers/2512.19048v1.pdf"
    },
    {
      "arxiv_id": "2512.19049v1",
      "title": "Decoupled Generative Modeling for Human-Object Interaction Synthesis",
      "authors": [
        {
          "name": "Hwanhee Jung"
        },
        {
          "name": "Seunggwan Lee"
        },
        {
          "name": "Jeongyoon Yoon"
        },
        {
          "name": "SeungHyeon Kim"
        },
        {
          "name": "Giljoo Nam"
        },
        {
          "name": "Qixing Huang"
        },
        {
          "name": "Sangpil Kim"
        }
      ],
      "abstract": "Synthesizing realistic human-object interaction (HOI) is essential for 3D computer vision and robotics, underpinning animation and embodied control. Existing approaches often require manually specified intermediate waypoints and place all optimization objectives on a single network, which increases complexity, reduces flexibility, and leads to errors such as unsynchronized human and object motion or penetration. To address these issues, we propose Decoupled Generative Modeling for Human-Object Interaction Synthesis (DecHOI), which separates path planning and action synthesis. A trajectory generator first produces human and object trajectories without prescribed waypoints, and an action generator conditions on these paths to synthesize detailed motions. To further improve contact realism, we employ adversarial training with a discriminator that focuses on the dynamics of distal joints. The framework also models a moving counterpart and supports responsive, long-sequence planning in dynamic scenes, while preserving plan consistency. Across two benchmarks, FullBodyManipulation and 3D-FUTURE, DecHOI surpasses prior methods on most quantitative metrics and qualitative evaluations, and perceptual studies likewise prefer our results.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T05:33:59+00:00",
      "updated": "2025-12-22T05:33:59+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19049v1",
      "file": "papers/2512.19049v1.pdf"
    },
    {
      "arxiv_id": "2512.19036v1",
      "title": "Distinguishing Visually Similar Actions: Prompt-Guided Semantic Prototype Modulation for Few-Shot Action Recognition",
      "authors": [
        {
          "name": "Xiaoyang Li"
        },
        {
          "name": "Mingming Lu"
        },
        {
          "name": "Ruiqi Wang"
        },
        {
          "name": "Hao Li"
        },
        {
          "name": "Zewei Le"
        }
      ],
      "abstract": "Few-shot action recognition aims to enable models to quickly learn new action categories from limited labeled samples, addressing the challenge of data scarcity in real-world applications. Current research primarily addresses three core challenges: (1) temporal modeling, where models are prone to interference from irrelevant static background information and struggle to capture the essence of dynamic action features; (2) visual similarity, where categories with subtle visual differences are difficult to distinguish; and (3) the modality gap between visual-textual support prototypes and visual-only queries, which complicates alignment within a shared embedding space. To address these challenges, this paper proposes a CLIP-SPM framework, which includes three components: (1) the Hierarchical Synergistic Motion Refinement (HSMR) module, which aligns deep and shallow motion features to improve temporal modeling by reducing static background interference; (2) the Semantic Prototype Modulation (SPM) strategy, which generates query-relevant text prompts to bridge the modality gap and integrates them with visual features, enhancing the discriminability between similar actions; and (3) the Prototype-Anchor Dual Modulation (PADM) method, which refines support prototypes and aligns query features with a global semantic anchor, improving consistency across support and query samples. Comprehensive experiments across standard benchmarks, including Kinetics, SSv2-Full, SSv2-Small, UCF101, and HMDB51, demonstrate that our CLIP-SPM achieves competitive performance under 1-shot, 3-shot, and 5-shot settings. Extensive ablation studies and visual analyses further validate the effectiveness of each component and its contributions to addressing the core challenges. The source code and models are publicly available at GitHub.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T05:13:58+00:00",
      "updated": "2025-12-22T05:13:58+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19036v1",
      "file": "papers/2512.19036v1.pdf"
    },
    {
      "arxiv_id": "2512.19032v1",
      "title": "Automatic Neuronal Activity Segmentation in Fast Four Dimensional Spatio-Temporal Fluorescence Imaging using Bayesian Approach",
      "authors": [
        {
          "name": "Ran Li"
        },
        {
          "name": "Pan Xiao"
        },
        {
          "name": "Kaushik Dutta"
        },
        {
          "name": "Youdong Guo"
        }
      ],
      "abstract": "Fluorescence Microcopy Calcium Imaging is a fundamental tool to in-vivo record and analyze large scale neuronal activities simultaneously at a single cell resolution. Automatic and precise detection of behaviorally relevant neuron activity from the recordings is critical to study the mapping of brain activity in organisms. However a perpetual bottleneck to this problem is the manual segmentation which is time and labor intensive and lacks generalizability. To this end, we present a Bayesian Deep Learning Framework to detect neuronal activities in 4D spatio-temporal data obtained by light sheet microscopy. Our approach accounts for the use of temporal information by calculating pixel wise correlation maps and combines it with spatial information given by the mean summary image. The Bayesian framework not only produces probability segmentation maps but also models the uncertainty pertaining to active neuron detection. To evaluate the accuracy of our framework we implemented the test of reproducibility to assert the generalization of the network to detect neuron activity. The network achieved a mean Dice Score of 0.81 relative to the synthetic Ground Truth obtained by Otsu's method and a mean Dice Score of 0.79 between the first and second run for test of reproducibility. Our method successfully deployed can be used for rapid detection of active neuronal activities for behavioural studies.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T05:08:52+00:00",
      "updated": "2025-12-22T05:08:52+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19032v1",
      "file": "papers/2512.19032v1.pdf"
    },
    {
      "arxiv_id": "2512.19026v1",
      "title": "Finer-Personalization Rank: Fine-Grained Retrieval Examines Identity Preservation for Personalized Generation",
      "authors": [
        {
          "name": "Connor Kilrain"
        },
        {
          "name": "David Carlyn"
        },
        {
          "name": "Julia Chae"
        },
        {
          "name": "Sara Beery"
        },
        {
          "name": "Wei-Lun Chao"
        },
        {
          "name": "Jianyang Gu"
        }
      ],
      "abstract": "The rise of personalized generative models raises a central question: how should we evaluate identity preservation? Given a reference image (e.g., one's pet), we expect the generated image to retain precise details attached to the subject's identity. However, current generative evaluation metrics emphasize the overall semantic similarity between the reference and the output, and overlook these fine-grained discriminative details. We introduce Finer-Personalization Rank, an evaluation protocol tailored to identity preservation. Instead of pairwise similarity, Finer-Personalization Rank adopts a ranking view: it treats each generated image as a query against an identity-labeled gallery consisting of visually similar real images. Retrieval metrics (e.g., mean average precision) measure performance, where higher scores indicate that identity-specific details (e.g., a distinctive head spot) are preserved. We assess identity at multiple granularities -- from fine-grained categories (e.g., bird species, car models) to individual instances (e.g., re-identification). Across CUB, Stanford Cars, and animal Re-ID benchmarks, Finer-Personalization Rank more faithfully reflects identity retention than semantic-only metrics and reveals substantial identity drift in several popular personalization methods. These results position the gallery-based protocol as a principled and practical evaluation for personalized generation.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-12-22T04:53:40+00:00",
      "updated": "2025-12-22T04:53:40+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19026v1",
      "file": "papers/2512.19026v1.pdf"
    },
    {
      "arxiv_id": "2512.19022v2",
      "title": "Steering Vision-Language Pre-trained Models for Incremental Face Presentation Attack Detection",
      "authors": [
        {
          "name": "Haoze Li"
        },
        {
          "name": "Jie Zhang"
        },
        {
          "name": "Guoying Zhao"
        },
        {
          "name": "Stephen Lin"
        },
        {
          "name": "Shiguang Shan"
        }
      ],
      "abstract": "Face Presentation Attack Detection (PAD) demands incremental learning (IL) to combat evolving spoofing tactics and domains. Privacy regulations, however, forbid retaining past data, necessitating rehearsal-free IL (RF-IL). Vision-Language Pre-trained (VLP) models, with their prompt-tunable cross-modal representations, enable efficient adaptation to new spoofing styles and domains. Capitalizing on this strength, we propose \\textbf{SVLP-IL}, a VLP-based RF-IL framework that balances stability and plasticity via \\textit{Multi-Aspect Prompting} (MAP) and \\textit{Selective Elastic Weight Consolidation} (SEWC). MAP isolates domain dependencies, enhances distribution-shift sensitivity, and mitigates forgetting by jointly exploiting universal and domain-specific cues. SEWC selectively preserves critical weights from previous tasks, retaining essential knowledge while allowing flexibility for new adaptations. Comprehensive experiments across multiple PAD benchmarks show that SVLP-IL significantly reduces catastrophic forgetting and enhances performance on unseen domains. SVLP-IL offers a privacy-compliant, practical solution for robust lifelong PAD deployment in RF-IL settings.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-12-22T04:30:11+00:00",
      "updated": "2025-12-24T07:36:25+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19022v2",
      "file": "papers/2512.19022v2.pdf"
    },
    {
      "arxiv_id": "2512.19020v1",
      "title": "CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization",
      "authors": [
        {
          "name": "Zelin Zhao"
        },
        {
          "name": "Xinyu Gong"
        },
        {
          "name": "Bangya Liu"
        },
        {
          "name": "Ziyang Song"
        },
        {
          "name": "Jun Zhang"
        },
        {
          "name": "Suhui Wu"
        },
        {
          "name": "Yongxin Chen"
        },
        {
          "name": "Hao Zhang"
        }
      ],
      "abstract": "Achieving precise camera control in video generation remains challenging, as existing methods often rely on camera pose annotations that are difficult to scale to large and dynamic datasets and are frequently inconsistent with depth estimation, leading to train-test discrepancies. We introduce CETCAM, a camera-controllable video generation framework that eliminates the need for camera annotations through a consistent and extensible tokenization scheme. CETCAM leverages recent advances in geometry foundation models, such as VGGT, to estimate depth and camera parameters and converts them into unified, geometry-aware tokens. These tokens are seamlessly integrated into a pretrained video diffusion backbone via lightweight context blocks. Trained in two progressive stages, CETCAM first learns robust camera controllability from diverse raw video data and then refines fine-grained visual quality using curated high-fidelity datasets. Extensive experiments across multiple benchmarks demonstrate state-of-the-art geometric consistency, temporal stability, and visual realism. Moreover, CETCAM exhibits strong adaptability to additional control modalities, including inpainting and layout control, highlighting its flexibility beyond camera control. The project page is available at https://sjtuytc.github.io/CETCam_project_page.github.io/.",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-12-22T04:21:39+00:00",
      "updated": "2025-12-22T04:21:39+00:00",
      "pdf_url": "https://arxiv.org/pdf/2512.19020v1",
      "file": "papers/2512.19020v1.pdf"
    }
  ]
}