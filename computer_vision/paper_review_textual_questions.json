{
  "corpus_name": "Computer Vision Papers",
  "corpus_path": "/mnt/x/rag_datasets/arxiv_papers/computer_vision",
  "scenario": "paper_review",
  "mode": "textual",
  "questions": [
    {
      "question": "In a camera-controllable video diffusion system trained without ground-truth camera poses, what two-stage data curriculum (including the key filtering/collection steps and data sources) can be used to first learn robust camera control and then refine high-fidelity generation, and what is the shared training objective used in both stages (including what the conditioning signal contains)?",
      "answer": "A two-phase curriculum is used:\n\n\u2022 Phase I (robust camera controllability from diverse raw video): start from ~330K crawled Internet videos and filter them (i) by removing static-camera clips using camera motion estimated from a geometry foundation model (VGGT), and (ii) by discarding low-aesthetic-quality clips using VBench-based scoring, yielding ~100K clips; additionally include VACE-Benchmark clips so controllability for other (VACE) conditioning tokens is retained.\n\n\u2022 Phase II (fine-grained/high-fidelity refinement): train on a much smaller curated set (~3K videos) composed of the training split of CameraBench, the I2V training set of VACE-Benchmark, and HoIHQ\u2014an indoor human\u2013object-interaction dataset generated with Kling 2.5 from curated source images.\n\nBoth phases minimize the same standard video flow-matching (velocity) loss: the model predicts a velocity field v\u03b8(xt,t,c) for an interpolated latent xt=(1\u2212t)x0+tx1 between Gaussian noise x0 and the clean latent x1 encoded by a 3D VAE; conditioning c includes the text embedding and the input frame condition.",
      "source_document": "papers/2512.19020v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an annotation-free camera-controllable video diffusion system that relies on a geometry foundation model, how can you construct geometry-aware \u201ccamera tokens\u201d from an input video (or from a single reference frame at test time), and how are those tokens injected into a pretrained video diffusion backbone? Describe (i) the intermediate signals produced by reprojection and why a visibility mask is used, (ii) the parameterization used for camera pose/intrinsics in the token, and (iii) the token-fusion mechanism used to condition the frozen backbone (including any zero-initialization and any token(s) excluded from fusion).",
      "answer": "(i) For training-time tokenization, per-frame depth maps and camera intrinsics/extrinsics are first estimated from the raw video using a frozen VGGT model. Using the first-frame depth D1 and intrinsics K1, pixels from the reference frame are back-projected to a 3D point cloud; these 3D points are then transformed with each target frame\u2019s extrinsics Et=[Rt|Tt] and re-projected with Kt to synthesize a rendered view \\tilde{I}_t of the first frame in the target viewpoint. A binary visibility mask M_t is computed during reprojection/occlusion checking to keep only pixels whose projections are visible and traced from the reference frame, filtering occluded or unmatched regions; the rendering and mask are concatenated and embedded into visual tokens.\n\n(ii) Camera parameters are embedded in a quaternion-based concise form g(E_t,K_t)=[q_t, T_t, f_t], where q_t\\in\\mathbb{R}^4 is the rotation quaternion, T_t\\in\\mathbb{R}^3 is translation, and f_t\\in\\mathbb{R}^2 represents field-of-view. This vector is mapped by a learnable linear layer to a camera-parameter embedding z_t^(pr), which is concatenated (along the token/sequence dimension) with the embedded rendering-mask tokens to form the final CETCAM token sequence z_t^(CETCAM) that jointly encodes appearance and geometry.\n\n(iii) For conditioning the pretrained (frozen) Wan DiT video diffusion backbone, CETCAM tokens are processed by trainable CETCAM context blocks. Their outputs are projected through zero-initialized linear layers to match the backbone feature dimension, and then added into the Wan DiT blocks (additive fusion). The special token corresponding to the camera parameter embedding is excluded before this addition step. At test time, the same procedure is used, except VGGT estimates only the single-view depth of the reference frame while the camera parameters {E_t,K_t} are provided as input; renderings/masks and tokens are then constructed via the same reprojection and embedding steps.",
      "source_document": "papers/2512.19020v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When extending a camera-controllable video diffusion model to compositional image-to-video editing tasks (e.g., inpainting, scribble-to-video, gray-to-color, or reference-image conditioning), what evaluation protocol and metric suite can you use to verify that the model preserves both editing quality and camera-trajectory adherence\u2014and what architectural limitation explains why a ControlNet-style unified editor can score well on appearance metrics yet still fail at camera-following?",
      "answer": "A suitable protocol is to benchmark on multiple representative controllable-generation tasks (inpainting, gray-to-color, scribble-to-video, and reference-image conditioning) using source videos/prompts from a standard controllable-video benchmark, and report (i) video-quality metrics such as VBench (overall, consistency, aesthetic quality, imaging quality, temporal stability, motion smoothness), (ii) explicit 3D camera/geometry metrics that measure trajectory and pose accuracy\u2014Absolute Trajectory Error (ATE), Relative Pose Error (RPE), and Relative Rotation Error (RRE)\u2014and (iii) a two-part human study that separates overall perceptual quality (Human-Gen) from perceived camera adherence and smoothness (Human-Cam). This setup can reveal a failure mode where a unified editor achieves competitive VBench/Human-Gen (good appearance editing) but has very poor ATE/RPE/RRE and low Human-Cam because its architecture does not incorporate explicit geometry grounding or pose-dependent conditioning, so it cannot track a prescribed camera trajectory even when the edited content looks plausible.",
      "source_document": "papers/2512.19020v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When designing an annotation-free, camera-controllable image-to-video diffusion model that conditions on reprojection-derived rendering/mask cues plus camera-parameter tokens via lightweight adapter/context blocks, what qualitative effects do the following ablations have on CameraBench-style evaluation (overall video quality vs. camera-pose accuracy), and why: (i) fusing adapter outputs into the frozen diffusion backbone by feature concatenation instead of residual/additive injection, (ii) replacing a quaternion-based camera-parameter tokenization with Pl\u00fccker-ray tokens, (iii) removing the reprojection visibility mask, and (iv) training with camera poses that are not geometrically consistent with the depth estimates (e.g., annotated poses or mismatched depth estimators between training and testing)?",
      "answer": "All four changes hurt performance relative to the full design: they reduce overall video quality scores and increase camera-trajectory errors (pose metrics such as ATE/RPE/RRE).\n\n\u2022 (i) Concatenation instead of additive/residual injection degrades results, indicating that residual \u201cadd\u201d fusion better preserves the pretrained backbone\u2019s representation while still letting the adapter provide camera-conditioning signals.\n\n\u2022 (ii) Swapping the quaternion-based camera-parameter encoding (rotation quaternion + translation + field-of-view) for Pl\u00fccker-ray tokens also degrades results, supporting that the compact quaternion-style parameterization is a better match for stable camera control in this setup.\n\n\u2022 (iii) Removing the visibility mask causes a larger drop: video quality worsens and geometric errors rise because the mask is needed to keep only reliably reprojected regions and filter occluded/unmatched pixels; without it, the conditioning includes incorrect correspondences.\n\n\u2022 (iv) Using camera information that is inconsistent with the depth geometry (annotated poses that don\u2019t align with estimated depths, or depth estimators that differ between train and test) also reduces both visual/geometric consistency, because it breaks the key requirement that the pose and depth signals be jointly coherent, producing train\u2013test discrepancies and misaligned motion/geometry conditioning.",
      "source_document": "papers/2512.19020v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a camera-controllable image-to-video diffusion model against prior work, how can you structure a fair evaluation across multiple datasets so that both visual realism and camera-trajectory adherence are measured\u2014specifically: (i) what common conditioning inputs should every method be given at inference time, (ii) what three families of metrics can be jointly reported to capture appearance/temporal quality and 3D camera accuracy, and (iii) what distinct stress-test focus does each of the three datasets UNI3C-OOD-Challenging, CameraBench, and a high-fidelity indoor human\u2013object-interaction set provide?",
      "answer": "A fair evaluation conditions every compared method on the same input image and the same target camera trajectories so differences come from the modeling/conditioning mechanism rather than different test inputs.\n\nTo measure both generation quality and camera control, three metric families are reported together:\n1) VBench visual metrics covering overall visual fidelity plus subject/background consistency, aesthetic quality, imaging sharpness, temporal flickering, and motion smoothness.\n2) 3D pose/geometry metrics: Absolute Trajectory Error (ATE), Relative Pose Error (RPE), and Relative Rotation Error (RRE), which quantify camera-pose accuracy and geometric consistency.\n3) Human evaluation scores (ratings rescaled to 0\u2013100) assessing perceived realism and temporal coherence.\n\nThe datasets play complementary roles:\n\u2022 UNI3C-OOD-Challenging stresses out-of-distribution generalization: it contains source images/texts without paired videos and includes extreme camera trajectories that are unseen during training.\n\u2022 CameraBench provides balanced, higher-quality samples with moderate camera motion and well-calibrated trajectories, emphasizing temporal stability and visual realism.\n\u2022 A high-fidelity indoor human\u2013object-interaction benchmark (HoIHQ) targets realistic indoor scenes with HoI content and geometry-rich, high-quality videos to test controllability and realism in that domain (using predefined trajectories for viewpoint-control assessment).",
      "source_document": "papers/2512.19020v1.pdf",
      "mode": "textual",
      "content_refs": [
        "lines 955-967, 969-971 (shared conditioning and baselines context)",
        "lines 1055-1068 (three metric families: VBench, ATE/RPE/RRE, human eval)",
        "lines 1021-1036, 1026-1036 (dataset focuses: UNI3C-OOD, CameraBench, HoIHQ)",
        "lines 1428-1440 (HoIHQ goal: high-quality, geometry-rich indoor HoI complementing other benchmarks)"
      ]
    },
    {
      "question": "In a rehearsal-free incremental face presentation attack detection system that adapts a CLIP-style vision\u2013language backbone using multi-aspect prompts and a selective EWC-style regularizer, how is the per-domain training objective constructed (including how the prompt families\u2019 logits are combined into the classification loss), and what rule is used to decide which backbone parameters receive the consolidation penalty across previously seen domains?",
      "answer": "For domain t, the objective is a sum of a prompt-steering classification loss and a selective consolidation penalty: L_total^(t) = L_MAP^(t) + L_SEWC^(t).\n\nMAP forms four text-prompt families K = {da, ds, mix, fixed} (domain-agnostic context, domain-specific context, their mixture, and a fixed natural-language prompt). For each family k and class c, it encodes an image feature f_img and text features f_txt,k,c^(t), L2-normalizes them, and computes logits by a temperature-scaled inner product: [logit_k^(t)]_c = \u03c4 \u27e8\\hat f_img, \\hat f_txt,k,c^(t)\u27e9. It then learns domain-specific aggregation weights w_k^(t) via w^(t)=softmax(\u03b1^(t)) and aggregates logits as logit_agg^(t)=\u2211_{k\u2208K} w_k^(t) logit_k^(t). The MAP loss is a single cross-entropy on the aggregated logits: L_MAP^(t)=L_CE(logit_agg^(t), y).\n\nSEWC uses a multi-center EWC prior that keeps a per-domain optimum \u03b8*(j) and diagonal Fisher information F_j for each previous domain j, but avoids over-constraining by applying the quadratic penalty only to a selected subset of backbone parameters. For each past domain j, it selects the top-p fraction of parameters (within a designated set S of consolidatable backbone parameters) by Fisher magnitude using quantile thresholding: \u03c4^(j)=Quantile(F_{j,S}, 1\u2212p), J^(j)={i\u2208S : F_{j,i} \u2265 \u03c4^(j)}. The final penalized index set is the union across domains I^(t\u22121)=\u22c3_{j=1}^{t\u22121} J^(j). The SEWC penalty then sums only over i\u2208I^(t\u22121) (and over all past domains j) with Fisher-weighted squared deviation from each \u03b8*(j).",
      "source_document": "papers/2512.19022v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a rehearsal-free domain-incremental face presentation attack detection system that keeps per-domain visual/text prompts, how can a test image be routed to the most appropriate domain\u2019s prompts at inference time without retaining any past images, and how is the final prediction computed once that domain is selected (including what makes the method relatively tolerant to occasional routing errors)?",
      "answer": "Inference uses a privacy-preserving prototype-bank router: for each previously learned domain, the system runs k-means on that domain\u2019s training image embeddings (from the shared image encoder feature space) to store k representative prototypes. For a test image, it computes its embedding, finds the nearest prototype in the entire bank, and selects the corresponding domain \\(\\hat t\\). It then activates that domain\u2019s components (the domain-specific visual prompt \\(D_V(\\hat t)\\), domain-specific textual context \\(D_S(\\hat t)\\), and that domain\u2019s learned prompt-family aggregation weights \\(w_k^{(\\hat t)}\\)) and forms the final logits by a weighted sum over prompt families: \\(\\text{logit}_{\\text{agg}}=\\sum_{k\\in K} w_k^{(\\hat t)}\\,\\text{logit}_k^{(\\hat t)}\\). The approach is robust to some routing mistakes because MAP includes shared domain-agnostic and mixed prompts that provide universal spoofing cues even when the selected domain-specific prompt is not perfectly matched.",
      "source_document": "papers/2512.19022v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a rehearsal-free domain-incremental face presentation attack detection system that adapts a CLIP-style vision\u2013language backbone using Multi-Aspect Prompting, what are the four textual prompt families (domain-agnostic, domain-specific, mixed, and fixed): how is each one constructed from learnable context tokens plus the frozen class-name embeddings, and what distinct role does each family play in handling domain shift and reducing catastrophic forgetting (e.g., preventing \u201csemantic drift\u201d or isolating domain idiosyncrasies)?",
      "answer": "The method builds four complementary text-prompt \u201cfamilies\u201d that all ultimately produce per-class text embeddings (for the two PAD classes \u201creal\u201d and \u201cspoof\u201d), but differ in which learnable context tokens they prepend/insert around the frozen class-name embeddings.\n\n\u2022 Domain-agnostic (DA) family: a prompt sequence formed by concatenating a start token, a shared learnable context DA (shared across all domains and continually updated), the frozen class-name embeddings for \u201creal/spoof\u201d, and an end token. Its role is to consolidate transferable, cross-domain spoofing cues that should remain stable across tasks/domains.\n\n\u2022 Domain-specific (DS) family: a prompt sequence formed by concatenating a start token, a domain-specific learnable context DS(t) (only updated when training on domain t; earlier DS(j) are frozen), the frozen class-name embeddings, and an end token. Its role is to absorb domain-t idiosyncrasies (e.g., camera/lighting artifacts), so the shared DA context is not forced to overfit to the current domain and get \u201cpolluted,\u201d which would increase forgetting on earlier domains.\n\n\u2022 Mixed (Mix) family: a prompt sequence formed by concatenating a start token, the shared DA context followed by the current domain\u2019s DS(t) context, then the frozen class-name embeddings, and an end token. Its role is to bridge general and domain-specific knowledge, producing a holistic representation that couples shared spoof cues with domain-local variations.\n\n\u2022 Fixed (Fixed) family: a set of immutable natural-language sentences (e.g., \u201cThis is a photo of real/spoof face.\u201d) that are fed directly to the text encoder without any learned context tokens. Its role is to act as a semantic anchor that leverages the backbone\u2019s zero-shot prior; this stabilizes the text side and helps prevent the learnable contexts from undergoing semantic drift during continual optimization, thereby mitigating catastrophic forgetting.",
      "source_document": "papers/2512.19022v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a rehearsal-free domain-incremental PAD setup that uses a selective EWC-style regularizer which penalizes only a top\u2011p fraction of backbone parameters (chosen by Fisher importance), how does increasing the selection ratio p affect (i) plasticity on the most recently learned domain and (ii) stability on the earliest domain after a long incremental sequence, and why can making p too large degrade performance even on that earliest domain?",
      "answer": "Increasing p (protecting/penalizing a larger proportion of parameters) reduces plasticity: performance on the newest domain worsens, reflected by higher HTER on the last domain as p grows. For the earliest domain, stability shows a U-shaped behavior: as p increases from small values, HTER on the first domain initially decreases (better retention), but when p becomes large it increases again. The reason is that a large p constrains a broad portion of parameter space with penalties from all previously learned tasks; these accumulated, potentially conflicting constraints can pull parameters away from the optimum for the early domain, so excessive regularization can harm the very domain it is intended to protect.",
      "source_document": "papers/2512.19022v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In rehearsal-free long-sequence incremental face presentation attack detection where domains are learned in order of increasing attack complexity and final generalization is evaluated on an unseen dataset, what characteristic failure mode does each of the following adaptation strategies exhibit\u2014(i) learning isolated per-domain prompts while freezing the vision\u2013language backbone, (ii) enforcing component-wise alignment to encourage knowledge reuse, and (iii) using low-rank adapters for incremental updates\u2014and how does combining a hierarchical prompt decomposition with selective backbone consolidation avoid these issues to keep both early-domain retention and late-domain adaptation strong?",
      "answer": "Across an 8-domain incremental sequence ordered by increasing attack complexity (MSU-MFSD\u2192CASIA-FASD\u2192Idiap Replay-Attack\u2192OULU-NPU\u2192SiW\u2192ROSE-YOUTU\u2192HKBU-MARs-V1+\u2192WFFD, with final testing on unseen CelebA-Spoof), three representative strategies show distinct failure modes:\n\n1) Isolated prompts + frozen backbone (e.g., independent prompts per domain): because prompts are learned independently per domain and the backbone is kept fixed, shared spoofing cues are not transferred across domains and the fixed feature extractor cannot acquire the fine-grained visual artifacts needed for newly emerging attack types; performance on earlier domains decays.\n\n2) Component-wise alignment constraints: forcing alignment between domains becomes a bottleneck when attack types diverge substantially; aligning structurally different attacks (e.g., 2D prints vs. 3D wax-figure style attacks) induces negative transfer, producing a clear downward trend in later steps as attack complexity increases.\n\n3) Low-rank adaptation (LoRA-style) updates: the low-rank constraint limits the effective parameter subspace available for new tasks; as spoofing patterns become highly diverse, this constrained subspace is insufficient to represent new features without interfering with old ones, leading to instability (sawtooth volatility) and plasticity exhaustion on the final domain.\n\nCombining hierarchical prompt decomposition (capturing both universal and domain-specific cues rather than fully isolated prompts) with selective backbone consolidation (allowing controlled backbone updates while protecting Fisher-identified important weights) addresses these problems: prompts provide domain specialization and a shared foundation for cross-domain patterns, while selective consolidation prevents overwriting critical historical knowledge without over-constraining adaptation. This yields a flat, stable trajectory on early domains while retaining high plasticity for late-stage adaptation and stronger generalization to the unseen target dataset.",
      "source_document": "papers/2512.19022v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating whether a personalized text-to-image model preserves a subject\u2019s identity (e.g., a specific animal instance or a fine-grained category), how can you set up a retrieval-based protocol using a visually similar identity-labeled gallery\u2014specifically: how are the reference and gallery sets constructed and kept disjoint, how are generated images turned into retrieval queries, what metric summarizes identity preservation, and why does this ranking-based view reveal failures that pair-wise CLIP/DINO similarity can miss?",
      "answer": "Use a two-set setup with real images: (1) a reference set per identity (used only to condition/prompt personalization) and (2) a separate, non-overlapping gallery set containing images of the same identities plus visually similar other identities. The gallery is built from an initial random allocation and then manually cleaned (e.g., remove duplicates or bad angles) while maintaining diversity (e.g., avoid style/pose bias). For each reference identity, generate multiple images under diverse prompts (varying background, pose, actions) to mimic real usage; then embed each generated image with a fixed evaluation encoder and use the embedding as a query to retrieve from the gallery by similarity ranking. Summarize identity preservation with retrieval metrics\u2014mean average precision (mAP)\u2014so higher mAP indicates the correct identity consistently ranks above near-neighbor identities. This ranking-based evaluation exposes \u201cidentity drift\u201d that pair-wise similarity can hide because CLIP/DINO cosine similarity largely captures coarse semantic/layout cues and can stay high even when identity-defining fine details are lost; adding many visually similar negatives forces the evaluation to depend on discriminative identity cues rather than generic semantics. The protocol can be applied at both fine-grained category level and stricter individual re-identification level, and it can use specialized frozen encoders for better sensitivity to identity details (e.g., Re-ID/fine-grained retrieval encoders) rather than only general-purpose CLIP/DINO.",
      "source_document": "papers/2512.19026v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a Bayesian U-Net-style model for segmenting active neurons from 4D calcium-imaging videos, how are temporal cues fused with spatial cues at the network input, how is epistemic uncertainty implemented and computed at inference time, and what training objective is used to jointly optimize segmentation quality and the Bayesian approximation?",
      "answer": "Temporal and spatial information are fused by feeding the network a 13-channel tensor: 12 channels are Pearson pixel-wise correlation maps (temporal component) and 1 channel is a summary variance image (spatial component). Epistemic uncertainty is enabled by using Bayesian/variational convolutions via the Flipout estimator\u2014implemented by replacing standard convolutional layers with flipout convolutional layers in each decoder (deconvolutional) block\u2014so stochastic forward passes sample from weight posteriors; the segmentation prediction is taken as the mean over multiple stochastic predictions, and the epistemic uncertainty map is taken as the variance across them (the paper uses an ensemble of 40 predictors at test time). Training uses a mixture loss that averages three terms: cross-entropy loss, Dice loss, and a KL-divergence term (Loss = (Loss_ce + Loss_dice + Loss_KLD)/3).",
      "source_document": "papers/2512.19032v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating an active-neuron segmentation model for calcium-imaging data where expert ground truth is unavailable and foreground pixels are highly imbalanced, what evaluation protocol can be used to measure both segmentation quality and model generalization via reproducibility, and which metrics are appropriate (including at least one that is less biased by class imbalance)?",
      "answer": "A practical protocol is to (1) create a synthetic training/benchmark target by automatically segmenting neurons using Otsu thresholding applied to per-block temporal variance maps (so no manual labels are required), and evaluate semantic segmentation outputs against this synthetic ground truth; and (2) run a reproducibility test by splitting the available training set into two disjoint halves (e.g., 100 blocks + 100 blocks), training the same Bayesian U-Net model separately on each half under the same hyperparameters, and then comparing the two models\u2019 predictions on the same held-out test set (e.g., 48 blocks) to quantify consistency/generalization.\n\nMetrics used for both semantic-segmentation evaluation and reproducibility include Dice coefficient, pixel-wise accuracy, and sensitivity; additionally, Matthews Correlation Coefficient (MCC) is used because it is more informative under strong class imbalance (foreground activity area <10% of pixels) and avoids overly optimistic performance estimates that accuracy/Dice can exhibit on imbalanced data.",
      "source_document": "papers/2512.19032v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a deep network to segment active neurons in calcium-imaging videos but lacking expert ROI annotations, how can synthetic ground-truth masks be generated automatically, what per-block image summary is thresholded to make neurons more separable, and what optimization criterion does the thresholding method use to choose the binary cutoff?",
      "answer": "Generate synthetic labels by computing a temporal variance map for each neuron block (variance over frames at each pixel) to highlight active neurons, then apply Otsu\u2019s automatic thresholding to that variance map to obtain a binary foreground/background mask. Otsu\u2019s method selects the single intensity threshold by minimizing the weighted intra-class intensity variance (equivalently maximizing between-class separation).",
      "source_document": "papers/2512.19032v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In calcium-imaging neuron segmentation where foreground/background pixels differ mainly by their fluorescence dynamics over time, how can Pearson correlation be used to extract temporal features from a video, and what is the exact correlation expression (including how signals are mean-centered and normalized) and its interpretation range?",
      "answer": "Treat each pixel location (h,w) as a time-series vector x_hw of length T (x_hw(t)=X_thw). The Pearson correlation between two pixels (h,w) and (h\u2032,w\u2032) is computed as the dot product of their mean-centered signals divided by the product of their \u21132 norms:\n\nc(x_hw, x_h\u2032w\u2032) = \u27e8x_hw \u2212 x\u0304_hw, x_h\u2032w\u2032 \u2212 x\u0304_h\u2032w\u2032\u27e9 / (\u2016x_hw \u2212 x\u0304_hw\u2016_2 \u00b7 \u2016x_h\u2032w\u2032 \u2212 x\u0304_h\u2032w\u2032\u2016_2),\n\nwhere x\u0304_hw and x\u0304_h\u2032w\u2032 are the temporal means at those locations. This coefficient measures linear correlation between the two time series and lies in [\u22121, 1], with 1 indicating perfectly correlated signals and \u22121 indicating perfectly anti-correlated signals. The method uses these correlations as temporal-context features for detecting cells whose fluorescence dynamics differ from background.",
      "source_document": "papers/2512.19032v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a highly class-imbalanced binary segmentation setting like active-neuron detection (foreground <10%), how can Matthews Correlation Coefficient (MCC) be computed directly from TP/TN/FP/FN (including any intermediate normalizations), and why is MCC used as a complement to Dice score and pixel accuracy in this scenario?",
      "answer": "MCC is computed using the confusion-matrix counts via the normalized form:\n\n- N = TP + TN + FP + FN\n- S = (TP + FN)/N (fraction of positives in the ground truth)\n- P = (TP + FP)/N (fraction predicted positive)\n\nThen\n\nMCC = (TP/N \u2212 S\u00b7P) / sqrt(P\u00b7S\u00b7(1\u2212S)\u00b7(1\u2212P)).\n\nIt is used alongside Dice score and pixel accuracy because, for strongly imbalanced data (neuronal activity area <10% of pixels), Dice and especially accuracy can be overly optimistic/inflated by the dominant background class, whereas MCC is a correlation-style measure that accounts for all four terms (TP, TN, FP, FN) and is less biased by class imbalance.",
      "source_document": "papers/2512.19032v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a CLIP-based few-shot action recognition model that refines motion hierarchically and uses learned text prompts to modulate visual features, how is the overall training objective constructed, and what do the two auxiliary consistency terms enforce across the support and query samples?",
      "answer": "The model is trained with a multi-task objective\n\nL = LCE + \u03bb3\u00b7LH + \u03bb4\u00b7LS.\n\n\u2022 LCE is the standard cross-entropy loss over query predictions p(yqj | vqj, S), i.e., it directly optimizes classification accuracy on the query set.\n\n\u2022 LH is a motion-consistency regularizer applied to all visual samples from the support and query sets (S \u222a Q): it sums dh(fv) and is used to improve consistency between shallow and deep motion features produced by the hierarchical motion refinement module.\n\n\u2022 LS is a prompt-consistency regularizer also applied over S \u222a Q: it sums ds(vsi) over support samples and ds(vqj) over query samples, encouraging the learned/generated prompt representation to stay consistent with the original/real prompt features (thereby stabilizing semantic alignment and bridging the modality gap).\n\n\u03bb3 and \u03bb4 weight the two consistency constraints relative to the classification term.",
      "source_document": "papers/2512.19036v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a metric-based few-shot action recognizer that already produces episode-specific support/query video features, how does a prototype\u2013anchor dual modulation mechanism (i) construct an episode-level \u201cglobal\u201d anchor and refine class prototypes using both support and query features, and (ii) incorporate the anchor into the final support\u2013query distance used for classification?",
      "answer": "Prototype\u2013Anchor Dual Modulation (PADM) refines both sides before computing distances. First, it reshapes the support features and uses both support and query features to build stronger prototypes: \u02dcf_S = Reshape(f_S), then forms modulated class prototypes by averaging over the concatenated support and query features, f^P_S = Mean(Concat(\u02dcf_S, f\u0304_Q)). A global anchor is then derived by averaging these prototypes over classes, f^A = Mean(f^P_S). PADM applies a Transformer-based modulation twice: (1) it refines support features and prototypes jointly via (f^P_S, f_S) = Transformer(Concat(f^P_S, f_S)); and (2) it modulates query features using the global anchor via (f^A_Q, f\u0304_Q) = Transformer(Concat(f^A, f\u0304_Q)), yielding a query-conditioned anchor f^A_Q.\n\nFor classification, PADM defines the support\u2013query distance as a sum of two sequence distances (using the same SeqDis/OTAM-style sequence distance as elsewhere): d_padm = SeqDis(f^PADM_{\\tilde{s}_n}, f^PADM_{q_j}) + SeqDis(f^P_{s_n}, f^A_q). The first term matches the PADM-modulated class prototype against the PADM-modulated query, and the second term explicitly aligns the modulated class prototype with the query-conditioned anchor. This d_padm is then combined with the SPM-based prototype\u2013query distance to produce the final distance used in the softmax over classes.",
      "source_document": "papers/2512.19036v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a few-shot video action recognizer that adds a hierarchical motion refinement module to reduce static-background interference, how are (i) the shallow motion descriptor and (ii) the deep motion descriptor computed from frame-level visual features, and what gating-based fusion operation is used to treat motion as a \u201cprompt\u201d that modulates appearance features before extracting the deep motion?",
      "answer": "Shallow motion is extracted directly from the frame-token features fv\u2208R^{T\u00d7C} using a motion feature extraction (MFE) block that applies *bidirectional* adjacent-frame differencing after a small convolutional mapping \u03a6: mf_t = \u03a6(fv[t+1]) \u2212 fv[t] and mb_t = \u03a6(fv[t]) \u2212 fv[t+1]. The local motion at time t is mt = mf_t + mb_t, and the segment-level shallow motion descriptor is the sum/aggregation over time m = \u03a3_{t=1}^{T\u22121} mt.\n\nTo obtain deep motion, the model first fuses (prompts) motion with appearance using a semantic-fusion (SF) block: it computes gated mixtures of the appearance stream and the prompt stream via fv \u2190 \u03a8v(fv)\u2299fv + \u03a8tp(ftp)\u2299ftp (with \u03a8v and \u03a8tp learned gating networks), then performs joint contextual interaction with a Transformer over the concatenated tokens: (ftp, fv) = Transformer(Concat(ftp, fv)). In HSMR, shallow motion ms_v is first computed as ms_v = MFE(fv), then fused with appearance via SF(ms_v, fv) to produce modulated appearance f^s_v, and finally deep motion is computed as md_v = MFE(f^s_v).",
      "source_document": "papers/2512.19036v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In few-shot action recognition with a CLIP-style text encoder, query videos at test time typically have no class text available, creating a support\u2013query modality gap. How can a prompt generator synthesize a query-conditioned \u201cpseudo-text\u201d embedding from (i) the episode\u2019s support-side text-prompt embeddings and (ii) the query video\u2019s frame-level visual tokens, and what are the key aggregation and fusion operations used before the final projection?",
      "answer": "A query-relevant prompt embedding is generated by (1) aggregating the episode\u2019s support prompt features by averaging over all support prompt embeddings (across the N-way and K-shot prompts), (2) aggregating the query video\u2019s visual representation by averaging its frame-level visual tokens over time, (3) fusing these two aggregated vectors with an elementwise (Hadamard) product, and then (4) passing the fused vector through a small MLP (a series of linear layers) to obtain the query-conditioned prompt:\n\nPG(f^{TP}, f^{Q}) = \u03a5( (1/(NK))\u2211_{o=1}^{NK} f^{tp}_o \u2299 (1/T)\u2211_{t=1}^{T} f^{Q}[t] ),\n\nwhere f^{TP} are textual prompt features, f^{Q} are query visual features, \u2299 is the Hadamard product, and \u03a5 is the stack of linear layers producing the learned prompt embedding used to modulate query features in SPM.",
      "source_document": "papers/2512.19036v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a prompt-conditioned few-shot action recognizer that fuses visual clip features with prompt features (motion or text) using a Semantic Fusion module, what roles do (i) feature concatenation before the Transformer, (ii) a learnable gating fusion, and (iii) an elementwise-sum/residual pathway each play in the fusion\u2014and why does using all three together yield the strongest few-shot accuracy?",
      "answer": "The fusion is designed so that: (i) Concat( prompt , visual ) is applied before the Transformer encoder to explicitly align the two feature streams and let the Transformer perform global attention over a joint token sequence; (ii) a learnable gating fusion computes gating weights from the current visual and prompt features and uses them to adaptively weight each stream (fv = \u03a8v(fv) \u2299 fv + \u03a8tp(ftp) \u2299 ftp), so the model can emphasize motion/semantic variation where useful while preserving appearance details and avoid redundancy/conflicts from naive linear fusion; and (iii) an elementwise Sum provides an additional residual pathway that stabilizes feature integration. Combining Concat + Gate + Sum is mutually reinforcing\u2014alignment via Concat enables effective attention-based mixing, gating regulates relative contributions, and the residual/Sum path improves stability\u2014leading to more coherent, discriminative fused representations and the best few-shot performance.",
      "source_document": "papers/2512.19036v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a watermark decoder to reliably recover bits from image-to-video outputs, how can optical-flow alignment be used to reduce frame-to-frame fluctuations in decoded watermarks, and what is the temporal loss term used to enforce this stability? In your answer, specify (i) which frames are compared after warping, (ii) how the temporal consistency loss is computed, and (iii) how it is combined with the per-bit message recovery objective during training.",
      "answer": "Optical flow is used to align generated video frames to a common reference so that the decoder sees temporally corresponding content despite subpixel drifts. Concretely, each generated key frame is warped toward the first generated frame v0, producing warped frames a\u2113.\n\n(i) The loss compares the decoder outputs on adjacent flow-warped frames, i.e., D(a\u2113) vs. D(a\u2113\u22121) for \u2113=1,\u2026,M\u22121, where M is the number of key frames.\n\n(ii) The Temporal Consistency Loss (TCL) is the average squared \u21132 difference between decoder outputs on these adjacent warped frames:\nLTCL = (1/(M\u22121)) * \u03a3_{\u2113=1}^{M\u22121} || D(a\u2113) \u2212 D(a\u2113\u22121) ||_2^2.\n\n(iii) The decoder training objective adds this TCL to the standard per-bit message recovery term: Ldec = LTCL + LMSG, where LMSG is computed with per-bit BCE losses for recovering the watermark from (a) each warped frame a\u2113, (b) the watermarked image Iw, and (c) the edited watermarked image \\tilde{Iw} produced by the editing branch.",
      "source_document": "papers/2512.19048v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training an image watermarking encoder\u2013decoder to be (a) imperceptible and (b) robust to downstream latent-space operations (e.g., diffusion-based editing / I2V), what is the full training objective that enforces imperceptibility in multiple spaces and uses adversarial training? In your answer, specify (i) the encoder-side loss terms used in pixel space, VAE-latent space, perceptual (LPIPS) space, and semantic (CLIP-embedding) space, (ii) the PatchGAN-style adversarial losses used to train the encoder (generator) and the discriminator, and (iii) how these are combined with the decoder\u2019s per-bit message recovery loss to form the final total loss.",
      "answer": "(i) The encoder is trained with a weighted sum of imperceptibility/consistency losses in several spaces:\n\n- Pixel-space fidelity:  \\(\\mathcal L_{\\text{pixel}}\\), the MSE between the original image \\(I\\) and the watermarked image \\(I_w\\).\n- Latent-space fidelity:  \\(\\mathcal L_{\\text{latent}}\\), the MSE between \\(I\\) and \\(I_w\\) measured after mapping both through a VAE encoder (to reflect that edits/video generation operate in VAE latent space).\n- Perceptual similarity:  \\(\\mathcal L_{\\text{LPIPS}}\\), the LPIPS perceptual loss between \\(I\\) and \\(I_w\\).\n- Semantic preservation:  \\(\\mathcal L_{\\text{sem}} = 1 - \\cos\\big(f_{\\text{CLIP}}(I_w),\\, f_{\\text{CLIP}}(I)\\big)\\), using a frozen CLIP image encoder \\(f_{\\text{CLIP}}\\).\n\nThese are combined as:\n\\[\n\\mathcal L_{\\text{enc}} = \\mathcal L_{\\text{pixel}} + \\lambda_{\\text{latent}}\\mathcal L_{\\text{latent}} + \\lambda_{\\text{LPIPS}}\\mathcal L_{\\text{LPIPS}} + \\lambda_{\\text{sem}}\\mathcal L_{\\text{sem}}.\n\\]\n\n(ii) Adversarial training uses a PatchGAN discriminator \\(A\\) that outputs a patch-wise logit map. The encoder (generator) is trained to fool \\(A\\) with:\n\\[\n\\mathcal L^{G}_{\\text{adv}} = -\\frac{1}{H'W'}\\sum_{i=1}^{H'}\\sum_{j=1}^{W'} \\log \\sigma\\big(A(I_w)_{i,j}\\big),\n\\]\nwhile the discriminator is trained with:\n\\[\n\\mathcal L_{\\text{disc}} = -\\frac{1}{H'W'}\\sum_{i,j}\\Big[\\log \\sigma\\big(A(I)_{i,j}\\big) + \\log\\big(1-\\sigma(A(I_w)_{i,j})\\big)\\Big].\n\\]\n\n(iii) The decoder is trained for per-bit message recovery using BCE on (a) the watermarked image \\(I_w\\), (b) an edited watermarked image \\(\\tilde I_w\\), and (c) a set of key video frames (after the robustness module), aggregated as \\(\\mathcal L_{\\text{MSG}}\\); the decoder loss is \\(\\mathcal L_{\\text{dec}} = \\mathcal L_{\\text{TCL}} + \\mathcal L_{\\text{MSG}}\\). The overall objective for the encoder+decoder parameters is:\n\\[\n\\mathcal L_{\\text{total}} = \\mathcal L_{\\text{enc}} + \\lambda_{\\text{dec}}\\mathcal L_{\\text{dec}} + \\lambda_{\\text{adv}}\\mathcal L^{G}_{\\text{adv}}.\n\\]\n(Separately, the discriminator is optimized with \\(\\mathcal L_{\\text{disc}}\\).)",
      "source_document": "papers/2512.19048v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training an image watermark encoder\u2013decoder to stay robust after image-to-video (I2V) generation, it is common to include a \u201cproxy\u201d video diffusion model in the training loop. What proxy video generation options can be used for this role, and how does the choice of (a) an SVD-style proxy with vs. without classifier-free guidance (CFG) and (b) a fast proxy like AnimateLCM affect (i) watermark bit-accuracy after I2V, and (ii) practical training cost such as VRAM and runtime? Explain the mechanism behind these differences (e.g., CFG requiring extra passes and the diffusion-step count).",
      "answer": "The proxy video generator options considered are Stable Video Diffusion (SVD) used either without CFG or with CFG, and AnimateLCM as a fast video diffusion proxy. Using SVD without CFG gives noticeably worse watermark recovery after I2V than SVD with CFG, indicating that for SVD-based proxy training, good robustness effectively requires enabling CFG. AnimateLCM, in contrast, generates temporally coherent videos in only a few (about 2\u20134) diffusion steps and does not use CFG; avoiding CFG eliminates the extra unconditional/conditional passes, which reduces memory use and compute. As a result, AnimateLCM achieves competitive (often best) post-I2V bit accuracy while requiring substantially lower peak VRAM and shorter overall training time / per-video generation time than SVD proxies, especially compared to SVD with CFG.",
      "source_document": "papers/2512.19048v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When assessing whether an image watermarking method has learned a spatially \u201clocalized\u201d watermark (e.g., concentrated mostly in the center or mostly near the borders), how can comparing watermark bit accuracy under a center crop versus an inverse center crop be used as a diagnostic, and what does this comparison reveal about the typical baseline methods versus WaTeRFlow?",
      "answer": "A center-crop vs. inverse-center-crop test probes where the watermark signal is spatially concentrated: if decoding accuracy stays high after keeping only the center but drops when keeping only the periphery (or vice versa), the watermark is localized to that retained region. Conversely, similar accuracy in both crops suggests the watermark is spread more uniformly across the image.\n\nThe reported results show many baselines have a large gap between bit accuracy under center crop and inverse center crop, indicating they tend to embed watermark energy primarily either in the central area or around the boundary/periphery (e.g., some methods decode well only from the periphery, consistent with boundary-focused patterns that can also manifest as border noise). In contrast, WaTeRFlow exhibits a much smaller center\u2013inverse gap, implying its watermark signal is less region-dependent and is distributed more evenly across the image, making it less vulnerable to removing just the center or just the borders.",
      "source_document": "papers/2512.19048v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When designing an image watermark embedder to remain decodable after image-to-video diffusion models that condition on an image-derived embedding (e.g., CLIP), what semantic-preservation regularizer can be added during watermark embedding, how is it computed, and what mechanism explains why it particularly improves watermark recovery on the first generated frame? Additionally, if the semantic term is computed in a representation space not used by the video generator\u2019s conditioning encoder (e.g., swapping CLIP for DINOv2), what does this imply the semantic term is really constraining about the watermark residual?",
      "answer": "Add a semantic preservation loss that penalizes changes in high-level image semantics by matching the original and watermarked images in a pretrained embedding space. Concretely, compute a frozen CLIP image embedding for the original image I and the watermarked image Iw and minimize a cosine-distance objective: L_sem = 1 \u2212 cos(f_CLIP(Iw), f_CLIP(I)). This regularizes the watermarked image to stay semantically consistent with the original and, critically, keeps the I2V model\u2019s conditioning signal c (derived from the conditioning image, including its CLIP embedding) from being shifted by the watermark. In Stable Video Diffusion\u2013style generators, where attention uses the condition image\u2019s CLIP embedding as keys/values, preserving that embedding makes the first generated frame v0 stay closer to the watermarked input, which yields markedly higher first-frame bit accuracy.\n\nWhen replacing CLIP with a DINOv2-based semantic term (even though it is not the same conditioning encoder used by SVD), first-frame accuracy still improves, suggesting the semantic loss is not only \u201cmatching the exact conditioning encoder,\u201d but more generally constraining the watermark residual to preserve structural/low-frequency cues such as object layout and contours. With pixel- and VAE-latent MSE already shrinking the overall residual magnitude, the semantic term largely guides where the remaining change is allowed to reside\u2014pushing it toward local details with smaller impact on global structure/conditioning.",
      "source_document": "papers/2512.19048v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a PnP-based (hybrid) 6DoF pose estimation pipeline such as PVNet, what intermediate representation is the most effective place to implant a backdoor so that a trigger causes a controlled pose error, and why is simply perturbing object masks/segmentation a poor strategy in this setting?",
      "answer": "For hybrid PnP pipelines like PVNet, the attack targets the keypoint-based intermediate representation: PVNet predicts a 2D keypoint vector field (per-pixel unit vectors pointing to 2D keypoints), and the final 6DoF pose is recovered via PnP from the inferred keypoint locations. The backdoor is implanted by injecting a fixed, ordered offset into the predicted keypoint positions so that, when the trigger is present, the downstream PnP step yields a biased pose. Perturbing masks/segmentation is ineffective because the pose solution is highly sensitive to geometric consistency and keypoint localization; mask modifications tend to disrupt geometry in a way that does not reliably translate into a controllable pose offset and can fail under the pipeline\u2019s geometric constraints.",
      "source_document": "papers/2512.19058v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a backdoor attack on a 6DoF pose estimator, how can Attack Success Rate (ASR) be defined in terms of standard pose-accuracy metrics so that a \u201csuccessful attack\u201d corresponds to a clearly wrong pose (not a small error), and how are metrics on clean inputs versus triggered inputs distinguished in reporting?",
      "answer": "ASR is computed on triggered samples as the fraction whose predicted pose is incorrect under multiple standard pose metrics simultaneously: the prediction must exceed the correctness thresholds for (i) 3D pose error via ADD (i.e., not within the \u201ccorrect pose\u201d ADD criterion), (ii) pose estimation accuracy via translation and rotation errors (i.e., translation and rotation both outside the \u201ccorrect\u201d bounds), and (iii) 2D projection error (i.e., projected points not within the \u201ccorrect\u201d pixel tolerance). The paper defines success as satisfying all these failure conditions at once (ADD above the ADD correctness cutoff, translation error above the translation cutoff, rotation error above the rotation cutoff, and 2D projection error above the pixel cutoff), and then ASR is the ratio of such successful triggered cases to all triggered cases. For reporting, metric names use a \u201c-C\u201d suffix for clean data (e.g., ADD-C/PEA-C/2DPE-C) and a \u201c-P\u201d suffix for triggered/poisoned data (e.g., ADD-P/PEA-P/2DPE-P).",
      "source_document": "papers/2512.19058v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an end-to-end 6DoF pose estimation pipeline that directly regresses pose from RGB or RGB-D (e.g., DenseFusion-style), what characteristics must a backdoor trigger have to reliably drive predictions to an attacker-chosen pose, and what training-time poisoning steps are used to make the model learn that behavior without breaking clean performance?",
      "answer": "For end-to-end pose regressors, the trigger needs to be a regular and stable signal that corresponds to a consistent (fixed) pose transformation; otherwise activation tends to produce confusion or unpredictable outputs rather than the desired pose. To enforce this, poisoned training samples are created by embedding a structured 3D object trigger into the input (ensuring consistent representation in both RGB and depth for RGB-D models) and simultaneously editing the pose supervision by applying an attacker-defined 6DoF offset/target pose label, so the network learns to map \u201ctrigger present\u201d to a specific incorrect pose while behaving normally on clean samples.",
      "source_document": "papers/2512.19058v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In backdoor robustness testing for 6DoF pose estimation, what is the expected effect of a common post-training defense that fine-tunes the poisoned pose estimator on additional clean data (\"defensive retraining\")\u2014specifically, how does it impact (i) whether triggered predictions hit the attacker\u2019s exact target pose and (ii) whether the attack is still counted as successful under ASR-style criteria?",
      "answer": "Defensive retraining on clean data can weaken *precision* of the targeted manipulation\u2014on triggered inputs the predicted pose may deviate from the attacker\u2019s exact intended target pose\u2014but it does not eliminate the backdoor\u2019s effect: the Attack Success Rate remains essentially unchanged, and the model still outputs an entirely incorrect pose whenever the trigger is present (even if it\u2019s not exactly the attacker-defined pose). On triggered scenes, outcomes can include partial convergence toward the target pose, complete failure to return a valid pose, or incorrect/unpredictable pose estimates; in all cases the original correct pose is no longer reliably produced.",
      "source_document": "papers/2512.19058v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For backdooring 6DoF pose estimators in a way that still activates under different viewpoints, what makes a *3D object* trigger fundamentally more reliable than a 2D pixel-pattern trigger, and what are the two practical categories of 3D triggers that can be used (including why each category is attractive to an attacker)?",
      "answer": "A 3D object trigger is reliable for 6DoF pose backdoors because it has a well-defined 6DoF pose and therefore produces a view-dependent appearance/projection that remains geometrically consistent across different camera viewpoints; this lets the trigger be embedded and recognized in a way aligned with how 6DoF pipelines use geometry, rather than being distorted or washed out like naive pixel-level patterns through multi-stage feature extraction and projection-sensitive processing.\n\nTwo categories of 3D triggers are used:\n1) **Artificially modeled 3D triggers**: purpose-designed 3D models with controlled, distinctive geometry/texture. Their controlled shape/appearance enables precise and consistent embedding as a backdoor signal.\n2) **Real-world object triggers**: everyday physical items (e.g., cups/pen holders/vases, including objects from common pose datasets). They already carry 6DoF pose information and do not require extra synthesis or concealment, are physically authentic and easier to deploy in the real world, and can induce stealthy erroneous correlations.",
      "source_document": "papers/2512.19058v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "How does a training-free hallucination-mitigation decoder for large vision\u2013language models use (1) semantic segmentation and Jensen\u2013Shannon divergence and (2) a blank-image contrastive signal to choose a \u201cbest\u201d segmented view and produce the final next-token logits? Describe the role of the Div score, the adaptive mixing weight \u03b4, and the blank-image weighting factor \u03b1 in the final logit combination.",
      "answer": "The decoder first segments the original image V into two complementary views: v1 is formed by summing the masked regions from the largest N segmentation masks (default N=0.05\u00b7n masks), and v2 is the remainder (v2 = V \u2212 v1). It also introduces a completely blank image vn. All four images (V, v1, v2, vn) are fed to the LVLM to obtain next-token distributions.\n\nTo pick the more informative segmented view, it computes for each i\u2208{1,2} a divergence score\nDivi = JSD( p(yt | vi, x, y<t) || p(yt | vn, x, y<t) ).\nA larger Divi means the segmented image\u2019s distribution differs more from the blank-image (language-prior) distribution, so it is considered to contain more effective visual information; the selected view is i* = argmax Divi.\n\nAn adaptive mixing weight is then derived from the difference between these divergence scores: \u03b4 = Div1 \u2212 Div2, so the more informative segmented view provides stronger guidance while the other provides weaker influence. The visual-detail enhanced logits mix the original and the selected segmented view:\nlogitenh(V) = (1\u2212\u03b4)\u00b7logit(V) + \u03b4\u00b7logit(vi*).\n\nTo remove language-prior hallucinations, it performs contrastive decoding using the blank image: for any input vin (either V, v1, or v2), it subtracts the enhanced blank-image logits with a weight \u03b1,\nlogit*(vin) = (1+\u03b1)\u00b7logitenh(vin) \u2212 \u03b1\u00b7logitenh(vn),\nwhere vn acts as an \u201cimage placeholder\u201d that reduces the model to a language-only prior.\n\nFinally, it combines the contrastively corrected logits for the original image and the chosen segmented view using the same adaptive \u03b4:\nlogithdd = (1\u2212\u03b4)\u00b7logit*(V) + \u03b4\u00b7logit*(vi*).",
      "source_document": "papers/2512.19070v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "How can you empirically verify that an LVLM\u2019s visual encoder is less sensitive to small objects, and what quantitative relationship between an entity\u2019s image area and its corresponding next-token logit emerges from this test? Explain how this finding motivates a visual-hallucination mitigation strategy that increases the queried entity\u2019s proportion in the input via segmentation into two complementary images built from the top\u2011N largest masks.",
      "answer": "One verification is to hold the query fixed while varying only the target object\u2019s apparent size in the image: create multiple versions of a scene where an object of interest (e.g., a \u201ccar\u201d) becomes progressively smaller (as if farther away), while a comparison object (e.g., a \u201cperson\u201d) stays the same size. When extracting the LVLM\u2019s logits for the relevant entity tokens under the same question, the comparison object\u2019s logits change little, but the small-object (\u201ccar\u201d) logits change almost linearly with the object\u2019s size. This yields the observation that the visual encoder\u2019s sensitivity (reflected in the entity token logit) is correlated with entity area, approximately logit_i \u221d A_i / A_all (A_i is the entity\u2019s area and A_all the whole-image area). Because small entities contribute too little visual evidence, the model is more easily swayed by language priors, increasing hallucinations. To counter this, the image can be segmented into two complementary inputs v1 and v2 where v1 is formed by summing pixel-wise masked regions for the largest N segmentation masks and v2 is the remainder (v2 = V \u2212 v1). This increases the proportion of local details for entities within the segmented view, making the visual signal for the queried entity stronger and reducing visually induced hallucinations.",
      "source_document": "papers/2512.19070v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an open-vocabulary 3D instance segmentation system that avoids using SAM to generate masks, how can 2D open-vocabulary detector boxes be lifted into temporally consistent 3D instance proposals using depth/camera geometry and superpoints, and what fusion/filtering rules are used when combining these RGBD-derived proposals with a pretrained point-cloud segmenter\u2019s point-based masks to prioritize rare objects while avoiding redundant overlapping instances?",
      "answer": "2D detections are converted into 3D proposals by (1) taking each predicted 2D box in a frame and projecting all pixels inside the box into 3D using the corresponding depth map and camera intrinsics/extrinsics, then fitting an oriented 3D bounding box around the projected points (via Open3D). (2) To avoid duplicating objects already found by the point-based segmenter, a lifted 3D box is removed as redundant if it overlaps with a sufficient fraction of points from an existing point-based mask. (3) For each remaining lifted 3D box, superpoints from a graph-based segmentation of the point cloud are assigned to the box when a sufficient fraction of the superpoint\u2019s points lie inside the box; the selected superpoints form a coarse mask for that frame. (4) Coarse masks are merged sequentially across frames into object candidates: a new coarse mask is merged with an existing candidate when their IoU exceeds a merge threshold and they share the same predicted prompt label; otherwise it becomes a new candidate. (5) The merged superpoint sets are converted into binary RGBD-based masks. (6) Before fusing with point-based proposals, an additional filter discards any RGBD-based mask that has high IoU with any point-based mask\u2014this design explicitly prioritizes adding rare objects missed by the class-agnostic 3D segmenter; when overlap occurs the point-based mask is kept because it typically has better geometric quality. The final proposal set is the union of the retained RGBD-based masks and the point-based masks.",
      "source_document": "papers/2512.19088v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an open-vocabulary 3D instance segmentation pipeline that avoids CLIP by classifying 3D masks using 2D detector outputs, how can a multi-view \u201cprompt distribution\u201d be computed for each 3D candidate mask so it can be assigned a semantic label from the input query\u2014specifically, how are per-frame label maps built from overlapping 2D boxes, how is point visibility/occlusion handled when projecting the 3D mask into each frame, and how is the final class chosen across frames?",
      "answer": "For each RGB frame, build a dense label map L by initializing all pixels to \u22121 (no relevant label) and then rasterizing the 2D detector boxes into the map in descending box-size order, writing the box\u2019s predicted prompt label into its pixels; writing large-to-small makes smaller/nearer objects overwrite larger ones when boxes overlap.\n\nTo score a 3D mask Mj, project the whole point cloud into every frame to get per-point 2D coordinates P^2D_x, P^2D_y and projected depth P^2D_z. Compute a frame-visibility mask Vf that keeps only projections inside image bounds, and an occlusion/depth-consistency mask Vd that keeps only points whose projected depth agrees with the depth map value Dz within a threshold \u03c4depth (|P^2D_z\u2212Dz|<\u03c4depth). The visible part of the j-th mask in frame i is then Mji = Vd_i \u00b7 Vf_i \u00b7 Mj.\n\nSelect the top-k frames where the mask has the most visible points (Pk). Collect the prompt labels from the label maps at the projected pixel locations of the visible mask points across those frames, forming Dj. Convert Dj into a class histogram (occurrence counts per class id), and assign the 3D instance to the class with the highest probability / highest count.",
      "source_document": "papers/2512.19088v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In open-vocabulary 3D instance segmentation pipelines that generate additional object masks by merging superpoints inside 3D boxes lifted from 2D detections (rather than refining masks with a 2D segmenter), why can the method show larger performance gains at looser IoU thresholds (e.g., AP/mAP at 50% or 25% IoU) than at stricter IoU thresholds, when compared to a point-based-proposal baseline like Open-YOLO 3D?",
      "answer": "Because the extra instances are formed purely by grouping superpoints within lifted 3D boxes, the resulting novel masks can be noisy and have imperfect boundaries. Such boundary/geometry errors are penalized much more at high IoU thresholds, so improvements from discovering additional rare/novel objects tend to appear more strongly at lower IoU thresholds (e.g., 50% or 25%) than at higher, stricter thresholds.",
      "source_document": "papers/2512.19088v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a box-guided open-vocabulary 3D instance segmentation pipeline that forms extra 3D proposals by lifting 2D detector boxes into 3D, what part of the lifting/proposal-generation step is the main computational bottleneck, and what concrete implementation change is suggested to reduce the runtime without changing the overall method?",
      "answer": "The main runtime bottleneck is the 2D-to-3D uplifting step when computing oriented 3D bounding boxes for lifted detections using the Open3D library. A suggested way to reduce runtime (while keeping the same pipeline) is to replace this with a more efficient GPU-based implementation for obtaining the oriented 3D boxes.",
      "source_document": "papers/2512.19088v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a fast open-vocabulary 3D instance segmentation pipeline that uses a 2D open-vocabulary detector for box guidance and label-map construction, how should the 2D detector be scheduled across RGB frames on ScanNet200 versus Replica to balance runtime and fair comparison to Open-YOLO 3D, and what image downsampling is applied during the box-guided RGBD-based proposal-generation step for efficiency?",
      "answer": "Use the YOLO-World extra-large detector sparsely on ScanNet200 by running it only on the first frame of each 10-frame interval, but run it on every frame for Replica. During the box-guided RGBD-based proposal-generation stage, downsample each processed RGB frame by a factor of 5 to reduce computation.",
      "source_document": "papers/2512.19088v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In multi-contrast MRI reconstruction, when you first create a structurally aligned but modality-mixed target input by complementing missing target k-space samples with a fully sampled reference, how does a Mamba-based modality-disentanglement block suppress reference-specific information? Describe how its gating signal is computed and how it is used in the target feature update (including whether the reference contribution is added or subtracted), and summarize what the fusion-vs-disentanglement ablation concludes about which interaction strategy works better.",
      "answer": "The modality-disentanglement block builds a target-specific feature by explicitly gating and removing (not fusing) the reference information. At iteration i it forms a mixed-path feature by a residual connection (F_mix^i = F_mix^0 + F_tar^i), projects features with a lightweight linear layer, then applies depthwise convolution plus an SS2D (Mamba-style) block and normalization to both the mixed and reference paths to get \\tilde{F}_mix^i and \\tilde{F}_ref. A target-dependent gate is computed from the current target feature as G_tar^i = \\sigma(Linear(F_tar^i)). This gate modulates the reference feature and the gated reference contribution is subtracted from the mixed feature to obtain the refined target feature: F_tar^{i+1} = \\tilde{F}_mix^i \u2212 Linear(G_tar^i \\odot \\tilde{F}_ref). In the fusion baseline, the interaction is changed to feature addition instead of subtraction. The ablation comparing the two strategies shows that using k-space complementation followed by this disentanglement (gated subtraction) consistently yields better reconstruction quality than simple fusion across multiple backbone architectures, indicating that filtering out reference-specific components is more effective than injecting them by addition.",
      "source_document": "papers/2512.19095v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using a cascade of modality-disentanglement blocks for multi-contrast MRI reconstruction, how does an iterative refinement strategy propagate information across stages (i.e., how is the stage-i mixed feature formed from the initial mixed feature and the current target feature), how is a \u201cparallel blocks\u201d alternative constructed for ablation, and what do the ablations reveal about (a) iterative vs. parallel vs. single-block performance and (b) the failure mode observed when increasing the number of disentanglement blocks too far?",
      "answer": "Iterative refinement runs the disentanglement module sequentially, feeding the next stage with an updated mixed representation formed by a residual update: for stage i, the mixed feature is computed as F_mix^i = F_mix^0 + F_tar^i (with F_mix^i and F_tar^i initialized from F_mix^0). Each stage uses the preceding stage\u2019s intermediate output, enabling gradual suppression of cross-modal interference and progressively \u201cpurifying\u201d the target features.\n\nFor the \u201cparallel blocks\u201d ablation, multiple (e.g., 6) disentanglement modules are not chained; instead, they independently process the same input pair (F_mix^0, F_ref), and their outputs are averaged to produce the final target result.\n\nAblations show that the sequential/iterative design yields the best reconstruction metrics, while both a single block and parallel stacking reduce performance. Increasing the number of disentanglement blocks improves performance up to a point (peaking at 6 blocks), but adding more (e.g., 7) degrades results due to feature over-subtraction (removing too much information while trying to suppress reference-specific components).",
      "source_document": "papers/2512.19095v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a 2D Gaussian-splatting image representation that supports progressive densification under a fixed Gaussian budget M, how can the method decide (i) when to add new Gaussian primitives, (ii) where to place them, (iii) how many to add, and (iv) how to initialize their position and color attributes; and what criterion can be used to prune invalid Gaussians during training?",
      "answer": "A practical scheme is a distortion-driven densification loop:\n\n- **When to add (schedule):** Perform densification periodically during training (e.g., every 5000 iterations).\n- **Where to add:** Compute a per-pixel reconstruction distortion D(X, X\u0302) between the ground-truth image X and the current rendered image X\u0302 (the described implementation uses **L1** for simplicity), and select the **top\u2011k pixels** with the largest distortion.\n- **How many to add:** Set the number of new Gaussians k using the remaining budget, e.g. with a scheduler \\(\\tau(t, N_t, M)=\\frac{M-N_t}{2}\\), where \\(N_t\\) is the current number of Gaussians.\n- **How to initialize attributes:** For the newly added set \\(\\Psi\\), initialize **positions** by assigning their means to the coordinates of the selected top\u2011k distortion pixels, \\(\\mu_\\Psi = \\xi(\\mathrm{Topk}(D(X,X\u0302)))\\). Initialize **colors** by copying the ground-truth pixel colors at those coordinates, \\(c_\\Psi = X(\\xi(\\mathrm{Topk}(D(X,X\u0302))))\\). (Covariances can be initialized similarly to the sparse initialization stage, i.e., random sampling with constraints to ensure valid covariance.)\n- **Pruning invalid Gaussians:** Periodically check whether each covariance matrix \\(\\Sigma\\) is positive semidefinite and prune invalid Gaussians; an explicit check is that \\(\\det(\\Sigma)\\ge 0\\) and the diagonal entries satisfy \\(\\Sigma_{11}\\ge 0\\), \\(\\Sigma_{22}\\ge 0\\).",
      "source_document": "papers/2512.19108v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In 2D Gaussian-splatting image fitting with a sparse-to-dense training schedule, how can you design a per-primitive, content-aware Gaussian low-pass filter that (i) is applied during rasterization, (ii) requires no extra storage at encoding time, and (iii) automatically weakens as more Gaussians are added\u2014what is the functional form used to set each primitive\u2019s filter variance and how does this mechanism reduce early \u201choles\u201d/undersampling artifacts?",
      "answer": "Use a Gaussian resampling (low-pass) filter by inflating each primitive\u2019s screen-space covariance during rasterization: replace the footprint kernel Gi(x) with G\u2032i(x)=exp(-1/2 (x\u2212\u03bci)^T (\u03a3i+s_i I)^{-1}(x\u2212\u03bci)), i.e., convolve Gi with a zero-mean Gaussian h(x) whose variance is a per-primitive scalar s_i. The filter is \u201ccontent-aware\u201d via an adaptive variance vector s\u2208R^{N_t}: newly added Gaussians get s_i = HW/(\u03b1 N_t) (for i > N_{t\u22121}), while existing Gaussians keep their previous variance s_i = s_{i\u22121} (for i \u2264 N_{t\u22121}), so as densification increases N_t the assigned variance for new primitives decreases, weakening the filter over time. It adds no storage overhead because the method stores the filtered covariance \u03a3+sI rather than s separately. Large s_i early enlarges each Gaussian\u2019s footprint, increasing Gaussian\u2013pixel intersection area, mitigating undersampling and reducing large \u2018holes\u2019 when N_t \u226a HW; later, smaller s_i prevents the filter from dominating and lets new Gaussians focus on refining fine details.",
      "source_document": "papers/2512.19108v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When turning an explicit 2D Gaussian-splatting representation into an image codec, how can you do attribute-level quantization-aware training with learnable scalar quantizers: (i) what quantize/dequantize transform (including learnable parameters) is applied to an attribute vector in LSQ+, (ii) how are different bit-depths assigned to geometry (position/covariance) versus color attributes, and (iii) how is training staged so that Gaussians are \u201coverfitted\u201d before quantization and what happens to densification during the quantization-aware fine-tuning phase?",
      "answer": "(i) Use LSQ+ learnable scalar quantizers with a learnable offset \u03b2 and scale s. For an attribute vector v and bit-depth b, the forward quantize/dequantize with straight-through gradients is:\n\nv_q = \u230a clip((v \u2212 \u03b2)/s, 0, 2^b \u2212 1) \u230b,\n\u0175 = v_q \u00b7 s + \u03b2.\n\n(ii) Apply distinct bit-depths per attribute type to balance rate and distortion: geometry is quantized more finely because it is more sensitive\u201412-bit for positions \u03bc and 10-bit for covariances \u03a3\u2014while RGB color uses lower precision (6-bit).\n\n(iii) Stage training as: first run the normal image-representation optimization to obtain well-fitted (\u201coverfitted\u201d) continuous Gaussian attributes (a warm-up, e.g., thousands of iterations) so the subsequent quantization has a good starting point; then perform attribute quantization-aware fine-tuning with the LSQ+ quantizers to adapt the attributes to quantization. During this quantization-aware phase, pruning of invalid Gaussians is kept, but Gaussian growing/densification is paused (no further Gaussian additions).",
      "source_document": "papers/2512.19108v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When enhancing a 2D Gaussian-splatting image fitting pipeline that may parameterize/optimize each Gaussian\u2019s covariance either via a factorization (e.g., Cholesky or RS) or by directly optimizing the covariance matrix, what qualitative effect should you expect from adding (i) distortion-driven densification and (ii) content-aware Gaussian low-pass filtering on reconstruction quality across these parameterizations\u2014and which of the two components tends to contribute the larger gain under a fixed Gaussian budget?",
      "answer": "Across different covariance-optimization choices (factorized covariances like Cholesky or RS, or directly optimized covariances), adding distortion-driven densification and content-aware filtering consistently improves reconstruction quality under the same maximum number of Gaussians. The improvement is robust to the covariance parameterization/optimization strategy rather than being tied to a specific factorization. Among the two, distortion-driven densification is the dominant contributor: it provides the most significant quality increase (notably improving cases with fewer Gaussians), while adding the content-aware filter on top of densification yields additional\u2014but smaller\u2014gains by stabilizing early fitting (reducing holes/artifacts) and letting later-added Gaussians focus on fine details.",
      "source_document": "papers/2512.19108v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a 3-view relative pose solver that first estimates a trifocal tensor from minimal point correspondences (e.g., 3- or 4-point cases), how can you enforce the internal consistency constraints so the final trifocal tensor corresponds to valid camera projection matrices, and how is this enforced form used to refine the initial linear system solution?",
      "answer": "After obtaining an initial trifocal tensor estimate Ti from the minimal solver, enforce consistency by re-parameterizing Ti through epipoles and structured projection matrices that satisfy the trifocal constraint by construction. Specifically:\n\n1) For each slice Ti, compute its left and right null-vectors ui and vi (u_i^T Ti = 0^T and Ti v_i = 0). The epipoles e\u2032 and e\u2032\u2032 are orthogonal to the sets of left/right null-vectors, so they are obtained via SVD from e\u2032^T [u1,u2,u3] = 0 and e\u2032\u2032^T [v1,v2,v3] = 0.\n\n2) Use these epipoles to parameterize the second and third camera projection matrices with unknown scalars a\u2217:\nP2 = [[a1,0,a2,e\u20321],[0,a3,0,e\u20322],[\u2212a2,0,a1,e\u20323]] and\nP3 = [[a4,0,a5,e\u2032\u20321],[0,a6,0,e\u2032\u20322],[\u2212a5,0,a4,e\u2032\u20323]].\n\n3) Build the trifocal tensor slices T1,T2,T3 as explicit functions of the unknowns a\u2217 and the epipole components; this parameterization \u201cnaturally satisfies\u201d the trifocal constraint.\n\n4) The original linear system Aq = 0 (where q stacks the tensor parameters) is converted using the linear relation q = E a induced by the constrained parameterization. Then solve for a by minimizing the residual under a unit-norm constraint: a = arg min_a ||A E a|| subject to ||E a|| = 1. Recover the refined Ti from a, and then use it to obtain the pose parameters.",
      "source_document": "papers/2512.19110v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a 3-view relative pose problem where roll/pitch are known from an IMU (known vertical direction), how can a minimal solver use only 3 point correspondences to estimate the remaining yaw angles and translations, and what algebraic trick is used to reduce the polynomial system\u2019s degree to make a Gr\u00f6bner-basis solution practical before recovering the translation vectors?",
      "answer": "Use a yaw-only rotation parameterization together with unknown translations for views 2 and 3, then derive trifocal constraints that depend only on the two yaw parameters and the two translations. Specifically, represent each unknown rotation Rk (k=2,3) with Cayley parameterization for yaw: sk = tan(\u03b8k/2) and \nRk = (1/(1+sk^2))[[1\u2212sk^2,0,2sk],[0,1+sk^2,0],[\u22122sk,0,1\u2212sk^2]], while tk = [tkx,tky,tkz]^T. Substituting into the trifocal equations yields constraints of the form F(s2,s3) t = 0 (up to a scalar factor 1/((1+s2^2)(1+s3^2))), where t stacks the 6 translation components (t2 and t3). With 3 point correspondences, select independent rows to build a 9\u00d76 matrix bF(s2,s3) such that bF t = 0; enforcing existence of a nonzero t implies det of any 6\u00d76 submatrix is zero, giving two equations in (s2,s3). The direct determinant equations are degree 24, so multiply/divide by the omitted factor 1/((1+s2^2)(1+s3^2)) to reduce degree: take the quotient by (1+s2^2)^3(1+s3^2)^3, which reduces the degrees in s2 and s3 to 6 and the total degree to 12. Solve the resulting degree-12 system with an automatic Gr\u00f6bner-basis solver, yielding up to 6 complex roots and keep the real roots as candidate (s2,s3) solutions. Then substitute (s2,s3) back into bF and recover translation components by solving for the nullspace via SVD, fixing scale by imposing \u2016t\u2016=1.",
      "source_document": "papers/2512.19110v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a monocular relative-pose solver (2-view or 3-view) on KITTI, what rotation and translation error metrics can be used, and why is translation error often reported as an angular error instead of Euclidean distance?",
      "answer": "Rotation error can be measured as the geodesic angle between estimated and ground-truth rotations: \n\u03b5R = arccos((trace(R_gt R^T) \u2212 1)/2).\nTranslation error can be measured as the angle between the estimated and ground-truth translation directions:\n\u03b5t = arccos((t_gt^T t)/(||t_gt||\u00b7||t||)).\nTranslation is reported as an angular error because monocular translation is only recoverable up to an unknown scale, so Euclidean distance is not meaningful without scale alignment.",
      "source_document": "papers/2512.19110v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a 3-view relative-pose pipeline evaluated on real driving sequences with many mismatched feature tracks, how can RANSAC be set up consistently across different solvers (i.e., what residual is used to score inliers, how is the winning hypothesis selected, and is any post-RANSAC nonlinear refinement applied), and what summary statistic is used to report the final pose errors?",
      "answer": "Use a common RANSAC wrapper for all solvers where inlier residuals are computed with the Sampson error and the same inlier threshold. The final pose is taken as the *initial* hypothesis that yields the largest inlier set (no subsequent optimization/refinement is applied after RANSAC). Performance is reported using median pose errors (median rotation and median translation error).",
      "source_document": "papers/2512.19110v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "Suppose you decompose an MLLM\u2019s last-layer, mean-pooled retrieval embedding with a sparse autoencoder into a dictionary of \u201cconcept\u201d atoms, and you can compute a per-concept retrieval attribution score measuring how much each concept influences image\u2013text similarity. How can you turn the top 1% highest-attribution concepts into a *training-free* embedding transformation before doing nearest-neighbor retrieval (include how the subspace is constructed and removed), and what does the resulting recall improvement imply about the role of these similarity-dominant components in MLLM retrieval?",
      "answer": "Construct a sub-dictionary DR consisting of the dictionary atoms whose retrieval attribution scores are in the top 1%. To capture the subspace S spanned by these high-influence concepts, perform SVD on DR (DR = U\u03a3V\u1d40) and take the top-r right singular vectors Vr as an orthonormal basis for S. For an embedding h, compute its projection onto this subspace \u03a0S(h)=VrVr\u1d40h, then remove these components by forming the residual embedding h\u0303 = h \u2212 \u03a0S(h); normalize h\u0303 and use it directly for retrieval. Empirically, removing this high-attribution subspace substantially increases recall, implying that the features contributing most to the dot-product similarity in raw MLLM embeddings act as distractors\u2014boosting similarity magnitude regardless of semantic relevance and overshadowing the truly discriminative features needed for accurate multimodal retrieval.",
      "source_document": "papers/2512.19115v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using a mean-pooled last-layer embedding from a multimodal LLM for zero-shot image\u2013text retrieval, how can you test whether retrieval is actually driven by visual tokens versus textual prompt tokens, and what qualitative retrieval outcome should you observe when you (i) mask out the hidden states for image tokens before pooling and (ii) mask out the hidden states for the user-prompt text region before pooling?",
      "answer": "Do two counterfactual re-embeddings before the mean-pooling step: (1) set (mask out) the last-layer hidden states corresponding to the image-token positions to zero (or remove them) and then recompute the mean-pooled embedding; (2) similarly mask out the hidden states in the user-prompt text span and recompute the pooled embedding. If retrieval is text-dominated, masking image tokens should produce only marginal changes in retrieval performance, whereas masking the user prompt region should cause a sharp drop in retrieval performance\u2014indicating that the pooled embedding (and thus similarity) is primarily determined by textual semantics rather than visual information.",
      "source_document": "papers/2512.19115v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an end-to-end autonomous-driving planner that decomposes planning into (i) target region localization, (ii) spatial path planning, and (iii) temporal trajectory prediction, how can the \u201ctarget\u201d be modeled to capture uncertainty, what training loss is used for this target module, and how is the learned uncertainty subsequently used inside the local iterative refinement that fuses global and local features?",
      "answer": "Model the target as a probabilistic 2D region (not a single point) using a Laplace distribution parameterized by a predicted center and scale: (\u03bc,b)=MLP(Q_target), where \u03bc\u2208R^2 is the region center and b\u2208R^2 is the scale/extent capturing uncertainty/scene complexity. Train it with a Laplace negative log-likelihood loss: L_{Laplace-NLL}=log(2b)+||y\u2212\u03bc||_1 / b. The predicted scale b is then reused as an uncertainty conditioning signal in iterative refinement: b is embedded (e.g., via an MLP) and concatenated with sampled local features, global query features, and planning state, so feature fusion is modulated by the estimated uncertainty (larger b \u2192 more cautious/adaptive fusion).",
      "source_document": "papers/2512.19133v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a latent world-model driving planner that adds an RL fine-tuning stage for safety, how can a deterministic trajectory-regression head be reformulated into a stochastic policy suitable for GRPO, what collision-aware reward signal is used, how is the per-timestep relative advantage computed under the \u201ctrajectory increments cause error accumulation\u201d assumption, and what final RL loss is optimized (including the KL regularization term to a reference policy)?",
      "answer": "The RL stage first \u201cgaussianizes\u201d the trajectory output to turn a regression model into a stochastic policy: the predicted trajectory is treated as the Gaussian mean \\(\\mu_\\theta\\), and an auxiliary variance network predicts the trajectory covariance/variances \\(\\Sigma_\\theta\\), so the policy \\(\\pi_\\theta\\) samples trajectories from this Gaussian distribution.\n\nSafety is driven by a collision-aware reward defined from distances between the ego bounding box and nearby agents\u2019 bounding boxes along the trajectory: collisions (negative distances) receive a penalty, while non-collisions receive zero reward, i.e. \\(r=-1\\) if a collision happens and \\(r=0\\) otherwise.\n\nGRPO is applied by sampling a group of \\(G\\) trajectories and computing a normalized per-point relative reward within the group,\n\\[\\tilde r^{(i)}_j=\\frac{r^{(i)}_j-\\mathrm{mean}(r_j)}{\\mathrm{std}(r_j)}\\]\nwhere \\(r_j\\) denotes the set of rewards at trajectory point \\(j\\) across the group. Because the trajectory is represented as time-differential increments (so an increment affects the current and all later points), the relative advantage for point \\(j\\) is accumulated over future points:\n\\[\\mathrm{Adv}^{(i)}_j=\\sum_{t\\ge j}\\tilde r^{(i)}_t.\\]\n\nThe GRPO objective uses a PPO-style clipped ratio term with a KL penalty to a reference (the pretrained) policy. The KL is computed between the Gaussian policy and a deterministic reference mean \\(\\mu_{\\mathrm{ref}}=T_{\\mathrm{traj}}^{\\mathrm{ref}}\\) via the Gaussian NLL form,\n\\[D_{KL}=\\tfrac12\\big[\\log|\\Sigma_\\theta|+(\\mu_{\\mathrm{ref}}-\\mu_\\theta)^T\\Sigma_\\theta^{-1}(\\mu_{\\mathrm{ref}}-\\mu_\\theta)+2\\log(2\\pi)\\big].\\]\nFine-tuning minimizes the combined RL loss\n\\[L_{RL}=-J(\\theta)+\\lambda D_{KL},\\]\nwhere \\(J(\\theta)\\) is the GRPO clipped objective (with a KL term weighted inside the objective for stability) and \\(\\lambda\\) weights KL regularization in the final loss.",
      "source_document": "papers/2512.19133v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a camera-only latent world model for end-to-end driving that needs stronger 3D spatial awareness (without explicit depth supervision), how can a frozen vision\u2013geometry foundation model be integrated into the world encoder (what geometric tokens are extracted and how are they fused with 2D backbone features), how is semantic information injected using vision\u2013language pseudo-labels, and what pretraining loss terms supervise these geometric/semantic and world-modeling components?",
      "answer": "A frozen visual-geometry model (VGGT) is used as a 3D spatial encoder to provide multi-view-consistent geometric priors from the surround-view RGB inputs. From VGGT\u2019s final layer, the encoder extracts tokens {t_c, t_r, t_3D} (camera, register, and 3D tokens). The 3D tokens t_3D are then fused into the 2D image backbone features F_t using a lightweight single cross-attention layer, with F_t as queries and t_3D as keys/values, producing the unified spatial-aware latent representation W_latent = C-A(F_t, t_3D), improving spatial understanding without explicit 3D inputs.\n\nSemantic awareness is injected by running Grounded-SAM to generate pseudo semantic labels (semantic masks, derived from prompted object-of-interest boxes/masks with only high-confidence labels kept), and supervising the latent representation with a cross-entropy semantic loss L_sem.\n\nPretraining combines: (i) an MSE reconstruction/world-prediction loss L_rec that predicts the next-step latent world representation from the current latent and planned trajectory, (ii) the semantic cross-entropy loss L_sem from pseudo-labels, (iii) a target-region localization loss L_target (Laplace negative log-likelihood for the probabilistic target region), and (iv) an L1 trajectory loss L_traj aligning predicted spatial path and temporal trajectory to expert demonstrations; these are summed as L_total = \u03b1 L_sem + \u03b2 L_rec + \u03b3 L_target + \u03b7 L_traj.",
      "source_document": "papers/2512.19133v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In closed-loop end-to-end driving evaluation on NavSim, how is the PDM Score (PDMS) computed from its constituent metrics (include the weights), and which terms act as multiplicative \u201cgates\u201d versus being combined additively? Explain what those gated terms measure and how they penalize unsafe/infeasible trajectories.",
      "answer": "PDMS is defined as\n\nPDMS = NC \u00d7 DAC \u00d7 (5 \u00d7 EP + 5 \u00d7 TTC + 2 \u00d7 Comf.) / 12,\n\nwhere EP is ego progress, TTC is time-to-collision, and Comf. is ride comfort; these three are combined by weighted summation (with weights 5, 5, and 2 respectively, then normalized by 12).\n\nNC (no at-fault collision) and DAC (drivable area compliance) are multiplicative factors that gate the score: they explicitly penalize unsafe or infeasible trajectories by scaling down the entire score when collisions are likely or the predicted path leaves drivable areas. NC measures the likelihood of collisions between the ego vehicle and other agents, while DAC assesses whether the predicted path remains within drivable areas.",
      "source_document": "papers/2512.19133v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a planning-oriented latent world model that is pretrained without dense perception annotations, how can future latent-world prediction be set up as a self-supervised objective (what does the world decoder take as input and what does it predict), what reconstruction loss is used for this latent prediction, and how is this term combined with the planning and semantic supervision terms (name each loss and give the overall weighted total objective)?",
      "answer": "Future latent-world prediction is trained by using a world decoder to predict the next-step latent world representation \\(\\hat W^{t+1}_{\\text{latent}}\\) from the current latent representation \\(W^{t}_{\\text{latent}}\\) together with the predicted temporal trajectory \\(T_{\\text{traj}}\\). The self-supervised reconstruction term is an MSE loss between the predicted and true next latent representations: \\(L_{\\text{rec}} = \\mathrm{MSE}(\\hat W^{t+1}_{\\text{latent}}, W^{t+1}_{\\text{latent}})\\). This is trained jointly with (i) a semantic cross-entropy loss \\(L_{\\text{sem}}\\) from pseudo semantic masks, (ii) a target-region negative log-likelihood loss \\(L_{\\text{target}}\\) (Laplace NLL), and (iii) an \\(L_1\\) imitation loss \\(L_{\\text{traj}}\\) aligning both the spatial path \\(T_{\\text{path}}\\) and temporal trajectory \\(T_{\\text{traj}}\\) to expert demonstrations, using the weighted sum: \\(L_{\\text{Total}} = \\alpha L_{\\text{sem}} + \\beta L_{\\text{rec}} + \\gamma L_{\\text{target}} + \\eta L_{\\text{traj}}\\).",
      "source_document": "papers/2512.19133v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a teacher\u2013student setup for online HD map construction where the teacher has access to future temporal context but the deployed student only sees the current frame, how is the training objective constructed to transfer \u201cfuture priors\u201d at both the BEV-feature level and the query/decoder level (including what supervision generates the spatial mask, how queries are aligned, what divergence is used, and how these distillation terms combine with the base mapping losses)?",
      "answer": "The total objective is the sum of the standard mapping loss and two distillation losses. The base loss is the same as MapTracker: \n- Lbasic = LBEV + LVEC + Ltrans.\n\nDistillation is applied at two levels:\n1) BEV-feature distillation uses a ground-truth-guided spatial mask derived from the auxiliary segmentation ground truth to suppress background regions. A masked feature regression loss is used between teacher and student BEV feature maps (applied to both basic and refined BEV representations), i.e., a mask-weighted squared error over spatial locations.\n2) Query/decoder distillation handles temporal query asymmetry by first performing one-to-one matching from the student\u2019s fixed set of queries to the teacher\u2019s (larger, dynamically tracked) query set using the Hungarian algorithm; then it distills the matched pairs\u2019 output logits with KL divergence:\n- LlogitsKD = sum_i LKL(LogitsS[i] || LogitsT[\u03c3\u0302(i)]).\n\nThe distillation loss is Ldistill = Lfeat + LlogitsKD, and the overall loss is Lall = Lbasic + Ldistill (with all loss-term weights set uniformly to 1.0 in the experiments).",
      "source_document": "papers/2512.19150v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating online vector HD map construction for autonomous driving, how can you adapt the standard map mAP to explicitly measure the model\u2019s quality in the *road ahead* versus *behind* the ego vehicle, and what are the main computation steps that remain the same as global mAP once the region is split?",
      "answer": "Define two half-region metrics by splitting the region of interest into two halves relative to the ego vehicle\u2019s heading: an ahead region and a rear region. Compute Ahead-mAP (A-mAP) by evaluating the matching between predicted and ground-truth map elements only within the ahead half, and compute Rear-mAP (R-mAP) analogously within the rear half. Within each half-region, the calculation follows the same pipeline as global mAP: spatial alignment, vector matching, true-positive identification, precision\u2013recall calculation, and average-precision computation.",
      "source_document": "papers/2512.19150v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In query-based online vector HD map models, a temporal teacher may carry a larger, dynamically tracked query set while a single-frame student uses a fixed set of learnable queries, making direct query-to-query distillation ill-posed. What strategy can be used to transfer query-level knowledge in this asymmetric setting (how correspondence is established and what is distilled), and what do ablations show about why more naive query-matching/distillation choices (e.g., distilling embeddings via heuristic or Hungarian assignment) tend to hurt performance?",
      "answer": "Use a matching-based asymmetric query distillation: establish a one-to-one correspondence from the student\u2019s fixed NS queries to the teacher\u2019s larger NT query set via Hungarian assignment, then distill the teacher\u2019s *output logits* onto the matched student queries using a KL-divergence loss computed only over matched pairs. This design targets the temporally inconsistent/dynamic nature of the teacher\u2019s tracked queries while keeping the student\u2019s static queries trainable.\n\nAblations indicate that naive correspondence strategies fail to create correct student\u2013teacher pairing and can cause a large accuracy drop (e.g., applying MapDistill-style distillation to dummy queries or selecting Top-K teacher queries). Even when using Hungarian assignment, distilling *query embeddings* is sensitive to the embedding loss choice (cosine or MSE) and still degrades performance, whereas applying the distillation on logits with KL gives the best overall results among the compared query-level distillation variants.",
      "source_document": "papers/2512.19150v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using downstream motion forecasting/planning sensitivity to motivate an ahead-focused evaluation for online vector HD mapping, how can you design a controlled perturbation experiment that isolates errors in the map region ahead of the ego vehicle versus behind it (what perturbation is applied to the mapper\u2019s vector outputs, what downstream model/metrics are used, and what outcome demonstrates the forward region is substantially more critical than the rear region)?",
      "answer": "A controlled way is to take an online mapper\u2019s vectorized map output (e.g., MapTR) and apply directional masking along the ego vehicle\u2019s longitudinal axis: for a chosen mask ratio, remove/zero the predicted map elements in either the forward half of the ROI (ahead-mask) or the backward half (rear-mask) to simulate degraded map quality confined to that region. Feed the perturbed maps into a standard trajectory prediction model (HiVT) and evaluate forecasting quality with minADE, minFDE, and Miss Rate (MR). The key outcome is that increasing the forward-mask ratio causes a clear, monotonic deterioration in all downstream metrics (e.g., at full forward masking, errors and MR rise notably), while masking the rear region has negligible impact and can even slightly improve MR at some ratios\u2014showing the downstream task depends far more on accurate mapping ahead than behind.",
      "source_document": "papers/2512.19150v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a cycle-consistent framework that jointly trains chart generation (table+NL query \u2192 schema), chart parsing (image \u2192 schema and image+schema \u2192 visualization-level table), and ChartQA (image+question \u2192 answer), how is the overall training loss constructed across tasks, and what sampling schedule is used to balance consistency-oriented training against QA so the model learns stable generate\u2013parse alignment while still improving reasoning?",
      "answer": "All four tasks are trained with the same token-level cross-entropy supervision, and the overall objective is a simple unweighted sum of the task losses: L = L_NL2Chart + L_Schema + L_Data + L_QA (with L_NL2Chart = CE(s\u0302, s), L_Data = CE(T\u0303_vis, T\u0302_vis), and L_QA = CE(\u00e2, a), plus the analogous schema-parsing CE loss). Training iterations are sampled from two groups\u2014(i) consistency-related tasks (NL2Chart, schema parsing, data parsing) and (ii) ChartQA\u2014with roughly 80% of iterations allocated to the consistency tasks and 20% to QA; no extra loss weighting is applied.",
      "source_document": "papers/2512.19173v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a lifecycle-aligned chart benchmark that supports both chart generation and chart-to-table/QA supervision, how can you obtain the ground-truth visualization-level table (the transformed data actually encoded in the marks) from a Vega-Lite chart specification, and how is that table then used to supervise chart data parsing and to generate ChartQA pairs without relying on LLM-generated answers?",
      "answer": "Execute (compile and render) the Vega-Lite specification and intercept the transformed data produced by the Vega-Lite execution/runtime pipeline (implemented via Altair as a front-end to the Vega-Lite compiler). This extracted transformed dataset is treated as the visualization-level table T_vis (the exact values used for mark rendering), which provides precise supervision for the chart data parsing task (image + schema \u2192 T_vis). The same T_vis is then used to programmatically generate ChartQA question\u2013answer pairs: LLMs may be used only to produce natural-language question templates, but the answers are computed deterministically from T_vis to avoid hallucination.",
      "source_document": "papers/2512.19173v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating chart data parsing as chart-to-table extraction, how is the final per-example score computed from a predicted visualization-level table and the ground-truth table (including the normalization steps, how rows are aligned via key matching/fuzzy matching, the numeric vs text value-matching tolerances, and how these components are combined into a single dataset-level metric)?",
      "answer": "Both predicted and ground-truth visualization-level tables are treated as CSV-like text and normalized by (i) decoding escaped newlines, (ii) stripping accents and redundant whitespace, and (iii) removing duplicated header lines (treating the first line as the header). Non-header rows are then sorted by a key column (the first column), yielding predicted and gold key sets K_pred and K_gold. Key-level precision/recall/F1 are computed from the key overlap; exact key matches use normalized string equality, with remaining unmatched keys optionally aligned using fuzzy matching with Levenshtein similarity \u2265 0.85.\n\nValue comparisons are performed only for aligned rows. For each aligned cell pair (a, b), numeric values are counted correct if |a \u2212 b| \u2264 max(\u03b4_abs, \u03b4_rel\u00b7|b|), with \u03b4_rel = 0.10 and \u03b4_abs = 0.02 for plain numbers (and \u03b4_abs = 2.0 for percentages). If numeric parsing fails, values fall back to text matching using normalized equality or token-level Jaccard similarity > 0.90. Let Acc_cell be the fraction of cell pairs accepted under these rules.\n\nThe per-example score is s_i = 0.5\u00b7(F1_k,i + Acc_cell,i), and the dataset-level score is the mean of s_i over all evaluated examples.",
      "source_document": "papers/2512.19173v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When constructing supervision targets for chart schema parsing from an image, what parts of a chart specification should be omitted because they are not visually inferable, and what does the resulting target schema represent for training/evaluation?",
      "answer": "Omit the specification\u2019s data-transform operations (e.g., grouping and aggregation) because they change intermediate data but are not visually observable from the rendered chart and thus cannot be reliably inferred from the image alone. The target schema is therefore the minimal Vega-Lite visual structure\u2014i.e., the necessary chart encodings/structure needed to describe the chart for accurate parsing supervision.",
      "source_document": "papers/2512.19173v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking NL-to-chart models that output Vega-Lite JSON, what evaluation protocol can jointly measure (1) spec-level textual correctness, (2) rendered visual fidelity/semantic alignment, and (3) whether the output is executable\u2014and how are the JSON strings normalized and non-renderable outputs handled in scoring?",
      "answer": "Use three complementary metrics: (1) textual similarity computed as ROUGE on the raw specification strings, with both JSON specs normalized (e.g., key sorting) before scoring to remove formatting noise, and reporting ROUGE-L recall to check coverage of essential ground-truth components without penalizing extra descriptive content; (2) visual similarity by rendering both the generated and ground-truth Vega-Lite specs to same-size images and computing PSNR (pixel fidelity), MS-SSIM (multi-scale structural consistency), and CLIP (perceptual/semantic alignment), assigning a similarity score of zero if a spec cannot be rendered; and (3) validity as the percentage of generated specifications that can be successfully rendered into a chart without errors.",
      "source_document": "papers/2512.19173v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a benchmark where ImageNet-pretrained CNN backbones are fine-tuned to classify pavement obstacles at three label granularities (category, subcategory, and obstacle type), which architectures achieve the top test accuracy at each granularity when training with (a) all layers frozen except the last vs (b) no layers frozen, and what overall pattern is observed about the effect of freezing on performance across these taxonomy levels?",
      "answer": "Using the PEDESTRIAN balanced subset, the best-performing models differ by taxonomy level and by whether layers are frozen:\n\n- Obstacle type (29-way): with frozen layers, ConvNeXt-Small reaches 99.96% test accuracy; with no freezing, EfficientNetV2-S reaches 99.89%. Freezing has negligible impact at this level.\n- Category: with no freezing, EfficientNet-B0 reaches 99.90%; with frozen layers, ConvNeXt-Large reaches 99.61%. Allowing layers to remain unfrozen generally improves category accuracy.\n- Subcategory: with no freezing, EfficientNetV2-S reaches 99.92%; with frozen layers, ConvNeXt-Large reaches 99.86%. Allowing layers to remain unfrozen generally improves subcategory accuracy.\n\nOverall pattern: freezing vs not-freezing matters little for obstacle-type classification, but for the coarser category and subcategory tasks, leaving layers unfrozen typically yields better performance.",
      "source_document": "papers/2512.19190v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When constructing a class-balanced training set of still frames from obstacle videos (one class per obstacle type), what sampling rule is used to decide between uniformly random frame selection versus sampling at regular temporal intervals, and how are the per-class target count and total balanced-set size determined in this dataset\u2019s benchmark setup?",
      "answer": "Frames are sampled per obstacle type by targeting a fixed count c: if the available frames n for that obstacle satisfy 1 \u2264 n/c < 2, frames are selected uniformly at random; if n/c \u2265 2, frames are selected regularly at intervals of \u230an/c\u230b starting from the first available frame (index 0 across the concatenated frames for that obstacle type). The benchmark uses c = 500, chosen to match the minimum available frames among obstacle types (Crowded Pavement has n = 694), producing a balanced subset of 29 \u00d7 500 = 14,500 images.",
      "source_document": "papers/2512.19190v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking many ImageNet-pretrained backbones on a balanced egocentric pavement-obstacle frame subset across three label granularities (category/subcategory/obstacle type) and two layer-freezing regimes, how is the data split and run-to-run variability handled to enable fair, comparable results across architectures and settings?",
      "answer": "The benchmark uses a random split of the balanced frame subset into 70% training, 20% validation, and 10% testing before fine-tuning. To control variability and make comparisons fair, each (architecture \u00d7 taxonomy level \u00d7 freezing setting) combination is repeated 5 times with different random seeds for the split, and the same fixed sequence of random seeds is reused across all combinations to allow one-to-one comparisons.",
      "source_document": "papers/2512.19190v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a benchmark that fine-tunes many different ImageNet-pretrained backbones on the same egocentric pavement-obstacle dataset, how can you standardize the input pipeline so that each architecture sees images in the input format it was pretrained with, and is this preprocessing applied at training time, test time, or both?",
      "answer": "Use the ImageNet-1K pretrained weights from PyTorch for each backbone and apply the default pre-processing transforms specified by that backbone\u2019s PyTorch \u201cmodel recipe\u201d (i.e., the recipe-associated input transforms) to the images. These default recipe transforms are applied consistently during both training and testing.",
      "source_document": "papers/2512.19190v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a data-free continual self-supervised pre-training setup that uses masked image modeling (MIM), how can model inversion be formulated to synthesize replay images while (i) preserving the previous task\u2019s learned feature distribution, (ii) suppressing inversion artifacts, and (iii) preventing mode collapse without class labels\u2014and what generator design choice is used to better recover high-frequency details during this inversion?",
      "answer": "The inversion optimizes a randomly initialized generator (InvUNet) and its noisy latent codes to minimize a composite inversion objective:\n\n- **Task-oriented SSL loss (Ltask)**: uses the same SSL objective as pre-training, instantiated as MIM reconstruction error on masked regions, i.e., predicting the masked content from the unmasked input and measuring reconstruction error only inside the mask.\n- **Feature distribution regularization (Lnorm)**: aligns synthetic images with the original data by matching batch-wise feature statistics (mean/variance) extracted from the frozen previous model, constraining generated samples to replicate the learned feature distributions.\n- **Image prior loss (Limg)**: a total-variation prior that penalizes differences between neighboring pixels (and additionally depth-wise neighbors for 3D), which promotes spatial smoothness and suppresses high-frequency artifacts/noise.\n- **Repulsive representation learning (Lrep)**: enforces diversity by extracting features for a batch of synthetic images using the frozen previous encoder and minimizing their squared cosine similarity to features stored in a persistent synthetic feature pool; this pushes samples to uniformly cover feature space and mitigates mode collapse even without class guidance.\n\nThese terms are combined as: LInv = Ltask + \u03b1norm\u00b7Lnorm + \u03b1img\u00b7Limg + \u03b1rep\u00b7Lrep.\n\nTo better recover high-frequency details during inversion, the generator is **InvUNet**, a dual-stream U-Net\u2013inspired architecture that **injects the latent z at the network bottleneck** (creating an information bottleneck) and uses a **Memory Cache Branch** to produce multi-scale structural priors that are fed via **skip connections** to the main Inversion Branch, enabling multi-scale fusion and improved fine-detail reconstruction compared to direct bottom-to-top projection.",
      "source_document": "papers/2512.19213v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In continual multi-modal self-supervised pre-training with replay (real or synthetic), what qualitative trend occurs as the replay buffer size increases (e.g., 1% \u2192 5% \u2192 10%), and how do inversion-synthesized replay samples compare to cluster-selected real replay at very small buffer sizes\u2014what underlying explanation is given for this behavior?",
      "answer": "As the replay buffer grows (from 1% to 5% to 10%), performance does not monotonically improve; larger buffers tend to intensify modality conflicts and increase computational cost, which can hinder learning on the current modality. With very small buffers (1%), inversion-synthesized samples perform better than the cluster-selected real replay subset; at 5% and 10% they match the replay method. The explanation is that when only a few samples are kept, inverted synthetic images can better capture the original data distribution (and diversity) than clustered subsets, which may inadequately represent diversity at small buffer sizes.",
      "source_document": "papers/2512.19213v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In data-free continual self-supervised pre-training with a BatchNorm-free backbone (e.g., ViT), how can feature-distribution regularization for model inversion be implemented without relying on BN running statistics\u2014specifically, how are the per-task target statistics computed from Transformer features, over which dimensions are mean/variance taken, and how are these statistics used during inversion?",
      "answer": "Instead of using BatchNorm running statistics, per-task targets are computed as batch-wise feature statistics from the trained model at the end of the final epoch for each task. The model is run once over the entire task dataset in evaluation mode, and statistics are extracted from Transformer encoder block feature maps. For a feature tensor with shape (B, L, D), the mean and variance are computed across the batch dimension B (aggregated over the full dataset via a running/streaming update), yielding per-layer targets {\u03bct, \u03c3t}. During inversion, these stored per-layer statistics are used as targets in the feature-matching loss Lnorm to constrain the generator so that synthetic images match the learned feature distribution of the original task.",
      "source_document": "papers/2512.19213v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a skeletal-representation hippocampal morphometry pipeline, how can longitudinal shape change be modeled so that true tissue loss (and potential vanishing of local thickness) is captured instead of preserving a forced point-to-point surface correspondence across time, and how does the model distinguish atrophy from growth in terms of skeletal surfaces and spoke lengths?",
      "answer": "Longitudinal follow-up scans are modeled within the same individualized skeletal coordinate system established at baseline by directly deforming the baseline skeletal representation, rather than independently reparameterizing the full surface at each time point (which would preserve correspondences even in regions that have biologically atrophied). Atrophy is represented as a controlled inward erosion of the skeletal surfaces coupled with spoke shortening, allowing local thickness to shrink toward (and in severe cases reach) zero when a spoke degenerates to a single point. Growth is represented with skeletal surfaces remaining stable while spoke lengths increase, producing increased thickness within the same stable coordinate framework.",
      "source_document": "papers/2512.19214v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When fitting a skeletal representation (s-rep) to hippocampal subfield surfaces, how can spoke refinement be formulated to simultaneously enforce (i) boundary convergence, (ii) spoke orthogonality to the boundary, and (iii) non-intersection/geometric regularity\u2014specifically, what are the three penalty terms used, and in what sequence are they applied/iterated to produce a geometrically valid spoke field?",
      "answer": "Spoke refinement is posed with three penalties tied to the required s-rep constraints:\n\n1) Boundary convergence penalty (\u03c60): for each spoke, measure how far its implied boundary endpoint p lies from the closest surface vertex v on the subfield boundary, i.e., the minimum Euclidean distance \u2016p\u2212v\u2016 (with v chosen as the nearest boundary vertex). This term is enforced strictly by projecting any implied boundary points that fall outside the surface back onto the boundary so endpoints align to the surface.\n\n2) Orthogonality penalty (\u03c61): penalize angular deviation between the spoke unit direction u and the unit surface normal n at the nearest boundary vertex, defined as \u03c61 = 1 \u2212 cos(\u03b8) = 1 \u2212 (u\u00b7n). Spoke directions are optimized to reduce this deviation.\n\n3) Non-intersection / medial-geometry regularization (\u03c62): measure asymmetry between the superior and inferior spokes at each medial point as the absolute difference in their lengths (|\u2016s_sup\u2016 \u2212 \u2016s_inf\u2016|). This is applied to correct spokes whose directions became problematic during \u03c61 optimization, improving local non-intersection and geometric regularity.\n\nOptimization sequence: first enforce \u03c60 (align endpoints to the surface), then optimize directions to minimize \u03c61; iterate the \u03c60\u2192\u03c61 steps for k rounds (empirically k\u22482\u20133). After the last iteration, apply \u03c62 to correct large angular deviations and improve non-intersection, then apply a final \u03c60 correction to restore accurate surface alignment after orientation updates.",
      "source_document": "papers/2512.19214v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking hippocampal shape models for anatomically meaningful pointwise correspondence (both cross-subject and along a simulated atrophy trajectory), what ground-truth supervision and error metric can be used, and what quantitative correspondence-error patterns distinguish a skeletal-representation approach from cm-rep, ds-rep, and SPHARM-PDM\u2014including the typical failure mode observed for SPHARM-PDM and why some methods drift under localized atrophy?",
      "answer": "Use manually annotated anatomical landmarks as ground truth and measure correspondence/alignment error as the Euclidean distance between the method\u2019s predicted corresponding points and the manually placed landmarks on the reference (ground-truth) surfaces.\n\nCross-sectional evaluation: five anatomically identifiable landmarks are manually placed per case (anterior/posterior extremities, medial folding apex, lateral curvature center, etc.) on both ground-truth and reconstructed surfaces; errors are Euclidean distances at these landmarks. The skeletal-representation pipeline achieves the lowest errors, with a maximum mean landmark error of about 1.66 mm (at the anterior folding apex) and the other landmark locations staying below ~1.1 mm on average (SD < 1 mm). In contrast, cm-rep and ds-rep have larger maximum landmark errors (about 4.64 mm and 3.41 mm, respectively), and SPHARM-PDM shows the largest variability and error (maximum ~9.97 mm), attributed to pole flipping.\n\nLongitudinal correspondence evaluation: simulate progressive atrophy by iteratively eroding a localized anterior region to create multiple follow-up surfaces (t1\u2013t3) while preserving the posterior region; place three landmarks within the atrophic region at each time point and compute Euclidean landmark correspondence errors over time. The skeletal-representation method has the smallest drift (mean ~1.68 mm across four time points; maximum ~2.81 mm at the most severe time point), whereas cm-rep shows large drift (mean ~10.57 mm; max ~20.48 mm), ds-rep is intermediate but can still drift severely (mean ~4.65 mm; max ~20.07 mm), and SPHARM-PDM also drifts substantially (mean ~3.56 mm; max ~16.87 mm). Qualitatively, competing methods exhibit non-local displacement from global reparameterization\u2014failing to confine deformation to the eroded region\u2014while the skeletal-representation approach maintains spatial specificity and can represent advanced collapse via near-zero spoke lengths.",
      "source_document": "papers/2512.19214v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an AD biomarker study that uses hippocampal substructure morphometrics (e.g., lamella-wise thickness/width features) rather than volumes, what feature-selection and modeling pipeline can be used to (i) control for demographic covariates and multiple comparisons, (ii) select clinically relevant features, and (iii) evaluate both cross-sectional discrimination and longitudinal conversion risk in an external cohort?",
      "answer": "A robust pipeline is:\n\n1) **Covariate control + multiple-comparison control for initial screening**: apply w-score normalization while using **age, sex, and education** as covariates, then test for **significant group differences** (effect size via **Cohen\u2019s d**) with **permutation testing** and **FDR correction**.\n\n2) **Clinical relevance filtering**: retain only features that also show **significant (reported as positive) correlations with cognitive decline** (Pearson correlation).\n\n3) **Model-based importance filtering**: further retain features with **positive importance scores in a random-forest classifier**.\n\n4) **Cross-sectional evaluation**: quantify discrimination (e.g., A\u2212/CU vs A+/CI) using **ROC/AUC** across feature sets, and validate the selected features in an independent cohort via **bidirectional cross-testing and internal cross-validation**.\n\n5) **Longitudinal conversion prediction**: train a **regularized logistic regression** with an **elastic-net penalty** (\u03b1 = 0.5) and tune the regularization strength with **5-fold cross-validation**; assess prediction with **AUC** and evaluate prognostic stratification with **Kaplan\u2013Meier survival analysis** and **log-rank tests**, using the **median predicted risk** to split participants into high- vs low-risk groups.",
      "source_document": "papers/2512.19214v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a skeletal (s-rep) morphometry pipeline from voxel-wise hippocampal subfield segmentations, what mesh-refinement strategy can be used to convert irregular, topologically inconsistent label boundaries (e.g., holes and disconnected fragments) into watertight, numerically stable surfaces suitable for downstream diffeomorphic registration and spoke refinement\u2014and what are the main stages of this refinement procedure?",
      "answer": "A practical strategy is a three-stage boundary cleanup/refinement pipeline (here termed a Robust Boundary Optimization Scheme, RBOS) applied to each subfield surface extracted from the voxel labels:\n1) Connected-component filtering: decompose the raw mesh into connected components and keep only the largest anatomically plausible component (by vertex count) to remove spurious fragments.\n2) Shrink-wrapping to enforce valid topology: apply shrink-wrapping to obtain watertight surfaces while preserving local geometry, including automated hole filling and adaptive surface offsetting; control mesh resolution via a target edge length chosen proportional to subfield complexity (finer for dentate gyrus, coarser for subiculum).\n3) Uniform remeshing for numerical stability: remesh uniformly with a quadrilateral-based scheme to improve face regularity, choosing the target quad count per subfield to balance anatomical detail and computational cost, then export as triangulated faces for compatibility with subsequent morphometric processing.\nBecause independently refined subfield boundaries may not align perfectly, whole-hippocampus modeling can instead combine the subfield labels and shrink-wrap them into a single merged object.",
      "source_document": "papers/2512.19214v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When fine-tuning a transformer VLM for screen/UI grounding using Image-LoRA (i.e., adapting only the attention value path on visual tokens), what supervision signal and loss are used for the click-point output, how is accuracy computed at evaluation time, and what part of the sequence computation is avoided relative to standard LoRA that makes adapter-only training FLOPs scale roughly with the visual-token fraction (Tv/T)?",
      "answer": "The grounding target is formatted as a single-line string like `point 2d:[x, y]`, and training optimizes cross-entropy over this output string. Evaluation reports accuracy as the fraction of predicted (x, y) points that fall inside the ground-truth bounding box in the model\u2019s effective pixel space. Efficiency comes from restricting the LoRA updates/computation to the visual-token span (adapting V only on those tokens), which avoids adapter computation through the long input text prompt and the output answer tokens; thus adapter-only training FLOPs are reduced roughly in proportion to the visual-token fraction Tv/T (and scale with image-token count rather than total sequence length).",
      "source_document": "papers/2512.19219v1.pdf",
      "mode": "textual",
      "content_refs": []
    }
  ],
  "timestamp": "2026-01-02T21:10:03.507399+00:00"
}