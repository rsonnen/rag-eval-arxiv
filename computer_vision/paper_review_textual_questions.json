{
  "corpus_name": "Computer Vision Papers",
  "corpus_path": "/mnt/x/rag_datasets/arxiv_papers/computer_vision",
  "scenario": "paper_review",
  "mode": "textual",
  "questions": [
    {
      "question": "In a camera-controllable video diffusion system trained without ground-truth camera poses, what two-stage data curriculum (including the key filtering/collection steps and data sources) can be used to first learn robust camera control and then refine high-fidelity generation, and what is the shared training objective used in both stages (including what the conditioning signal contains)?",
      "answer": "A two-phase curriculum is used:\n\n\u2022 Phase I (robust camera controllability from diverse raw video): start from ~330K crawled Internet videos and filter them (i) by removing static-camera clips using camera motion estimated from a geometry foundation model (VGGT), and (ii) by discarding low-aesthetic-quality clips using VBench-based scoring, yielding ~100K clips; additionally include VACE-Benchmark clips so controllability for other (VACE) conditioning tokens is retained.\n\n\u2022 Phase II (fine-grained/high-fidelity refinement): train on a much smaller curated set (~3K videos) composed of the training split of CameraBench, the I2V training set of VACE-Benchmark, and HoIHQ\u2014an indoor human\u2013object-interaction dataset generated with Kling 2.5 from curated source images.\n\nBoth phases minimize the same standard video flow-matching (velocity) loss: the model predicts a velocity field v\u03b8(xt,t,c) for an interpolated latent xt=(1\u2212t)x0+tx1 between Gaussian noise x0 and the clean latent x1 encoded by a 3D VAE; conditioning c includes the text embedding and the input frame condition.",
      "source_document": "papers/2512.19020v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an annotation-free camera-controllable video diffusion system that relies on a geometry foundation model, how can you construct geometry-aware \u201ccamera tokens\u201d from an input video (or from a single reference frame at test time), and how are those tokens injected into a pretrained video diffusion backbone? Describe (i) the intermediate signals produced by reprojection and why a visibility mask is used, (ii) the parameterization used for camera pose/intrinsics in the token, and (iii) the token-fusion mechanism used to condition the frozen backbone (including any zero-initialization and any token(s) excluded from fusion).",
      "answer": "(i) For training-time tokenization, per-frame depth maps and camera intrinsics/extrinsics are first estimated from the raw video using a frozen VGGT model. Using the first-frame depth D1 and intrinsics K1, pixels from the reference frame are back-projected to a 3D point cloud; these 3D points are then transformed with each target frame\u2019s extrinsics Et=[Rt|Tt] and re-projected with Kt to synthesize a rendered view \\tilde{I}_t of the first frame in the target viewpoint. A binary visibility mask M_t is computed during reprojection/occlusion checking to keep only pixels whose projections are visible and traced from the reference frame, filtering occluded or unmatched regions; the rendering and mask are concatenated and embedded into visual tokens.\n\n(ii) Camera parameters are embedded in a quaternion-based concise form g(E_t,K_t)=[q_t, T_t, f_t], where q_t\\in\\mathbb{R}^4 is the rotation quaternion, T_t\\in\\mathbb{R}^3 is translation, and f_t\\in\\mathbb{R}^2 represents field-of-view. This vector is mapped by a learnable linear layer to a camera-parameter embedding z_t^(pr), which is concatenated (along the token/sequence dimension) with the embedded rendering-mask tokens to form the final CETCAM token sequence z_t^(CETCAM) that jointly encodes appearance and geometry.\n\n(iii) For conditioning the pretrained (frozen) Wan DiT video diffusion backbone, CETCAM tokens are processed by trainable CETCAM context blocks. Their outputs are projected through zero-initialized linear layers to match the backbone feature dimension, and then added into the Wan DiT blocks (additive fusion). The special token corresponding to the camera parameter embedding is excluded before this addition step. At test time, the same procedure is used, except VGGT estimates only the single-view depth of the reference frame while the camera parameters {E_t,K_t} are provided as input; renderings/masks and tokens are then constructed via the same reprojection and embedding steps.",
      "source_document": "papers/2512.19020v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When extending a camera-controllable video diffusion model to compositional image-to-video editing tasks (e.g., inpainting, scribble-to-video, gray-to-color, or reference-image conditioning), what evaluation protocol and metric suite can you use to verify that the model preserves both editing quality and camera-trajectory adherence\u2014and what architectural limitation explains why a ControlNet-style unified editor can score well on appearance metrics yet still fail at camera-following?",
      "answer": "A suitable protocol is to benchmark on multiple representative controllable-generation tasks (inpainting, gray-to-color, scribble-to-video, and reference-image conditioning) using source videos/prompts from a standard controllable-video benchmark, and report (i) video-quality metrics such as VBench (overall, consistency, aesthetic quality, imaging quality, temporal stability, motion smoothness), (ii) explicit 3D camera/geometry metrics that measure trajectory and pose accuracy\u2014Absolute Trajectory Error (ATE), Relative Pose Error (RPE), and Relative Rotation Error (RRE)\u2014and (iii) a two-part human study that separates overall perceptual quality (Human-Gen) from perceived camera adherence and smoothness (Human-Cam). This setup can reveal a failure mode where a unified editor achieves competitive VBench/Human-Gen (good appearance editing) but has very poor ATE/RPE/RRE and low Human-Cam because its architecture does not incorporate explicit geometry grounding or pose-dependent conditioning, so it cannot track a prescribed camera trajectory even when the edited content looks plausible.",
      "source_document": "papers/2512.19020v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When designing an annotation-free, camera-controllable image-to-video diffusion model that conditions on reprojection-derived rendering/mask cues plus camera-parameter tokens via lightweight adapter/context blocks, what qualitative effects do the following ablations have on CameraBench-style evaluation (overall video quality vs. camera-pose accuracy), and why: (i) fusing adapter outputs into the frozen diffusion backbone by feature concatenation instead of residual/additive injection, (ii) replacing a quaternion-based camera-parameter tokenization with Pl\u00fccker-ray tokens, (iii) removing the reprojection visibility mask, and (iv) training with camera poses that are not geometrically consistent with the depth estimates (e.g., annotated poses or mismatched depth estimators between training and testing)?",
      "answer": "All four changes hurt performance relative to the full design: they reduce overall video quality scores and increase camera-trajectory errors (pose metrics such as ATE/RPE/RRE).\n\n\u2022 (i) Concatenation instead of additive/residual injection degrades results, indicating that residual \u201cadd\u201d fusion better preserves the pretrained backbone\u2019s representation while still letting the adapter provide camera-conditioning signals.\n\n\u2022 (ii) Swapping the quaternion-based camera-parameter encoding (rotation quaternion + translation + field-of-view) for Pl\u00fccker-ray tokens also degrades results, supporting that the compact quaternion-style parameterization is a better match for stable camera control in this setup.\n\n\u2022 (iii) Removing the visibility mask causes a larger drop: video quality worsens and geometric errors rise because the mask is needed to keep only reliably reprojected regions and filter occluded/unmatched pixels; without it, the conditioning includes incorrect correspondences.\n\n\u2022 (iv) Using camera information that is inconsistent with the depth geometry (annotated poses that don\u2019t align with estimated depths, or depth estimators that differ between train and test) also reduces both visual/geometric consistency, because it breaks the key requirement that the pose and depth signals be jointly coherent, producing train\u2013test discrepancies and misaligned motion/geometry conditioning.",
      "source_document": "papers/2512.19020v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a camera-controllable image-to-video diffusion model against prior work, how can you structure a fair evaluation across multiple datasets so that both visual realism and camera-trajectory adherence are measured\u2014specifically: (i) what common conditioning inputs should every method be given at inference time, (ii) what three families of metrics can be jointly reported to capture appearance/temporal quality and 3D camera accuracy, and (iii) what distinct stress-test focus does each of the three datasets UNI3C-OOD-Challenging, CameraBench, and a high-fidelity indoor human\u2013object-interaction set provide?",
      "answer": "A fair evaluation conditions every compared method on the same input image and the same target camera trajectories so differences come from the modeling/conditioning mechanism rather than different test inputs.\n\nTo measure both generation quality and camera control, three metric families are reported together:\n1) VBench visual metrics covering overall visual fidelity plus subject/background consistency, aesthetic quality, imaging sharpness, temporal flickering, and motion smoothness.\n2) 3D pose/geometry metrics: Absolute Trajectory Error (ATE), Relative Pose Error (RPE), and Relative Rotation Error (RRE), which quantify camera-pose accuracy and geometric consistency.\n3) Human evaluation scores (ratings rescaled to 0\u2013100) assessing perceived realism and temporal coherence.\n\nThe datasets play complementary roles:\n\u2022 UNI3C-OOD-Challenging stresses out-of-distribution generalization: it contains source images/texts without paired videos and includes extreme camera trajectories that are unseen during training.\n\u2022 CameraBench provides balanced, higher-quality samples with moderate camera motion and well-calibrated trajectories, emphasizing temporal stability and visual realism.\n\u2022 A high-fidelity indoor human\u2013object-interaction benchmark (HoIHQ) targets realistic indoor scenes with HoI content and geometry-rich, high-quality videos to test controllability and realism in that domain (using predefined trajectories for viewpoint-control assessment).",
      "source_document": "papers/2512.19020v1.pdf",
      "mode": "textual",
      "content_refs": [
        "lines 955-967, 969-971 (shared conditioning and baselines context)",
        "lines 1055-1068 (three metric families: VBench, ATE/RPE/RRE, human eval)",
        "lines 1021-1036, 1026-1036 (dataset focuses: UNI3C-OOD, CameraBench, HoIHQ)",
        "lines 1428-1440 (HoIHQ goal: high-quality, geometry-rich indoor HoI complementing other benchmarks)"
      ]
    },
    {
      "question": "In a rehearsal-free incremental face presentation attack detection system that adapts a CLIP-style vision\u2013language backbone using multi-aspect prompts and a selective EWC-style regularizer, how is the per-domain training objective constructed (including how the prompt families\u2019 logits are combined into the classification loss), and what rule is used to decide which backbone parameters receive the consolidation penalty across previously seen domains?",
      "answer": "For domain t, the objective is a sum of a prompt-steering classification loss and a selective consolidation penalty: L_total^(t) = L_MAP^(t) + L_SEWC^(t).\n\nMAP forms four text-prompt families K = {da, ds, mix, fixed} (domain-agnostic context, domain-specific context, their mixture, and a fixed natural-language prompt). For each family k and class c, it encodes an image feature f_img and text features f_txt,k,c^(t), L2-normalizes them, and computes logits by a temperature-scaled inner product: [logit_k^(t)]_c = \u03c4 \u27e8\\hat f_img, \\hat f_txt,k,c^(t)\u27e9. It then learns domain-specific aggregation weights w_k^(t) via w^(t)=softmax(\u03b1^(t)) and aggregates logits as logit_agg^(t)=\u2211_{k\u2208K} w_k^(t) logit_k^(t). The MAP loss is a single cross-entropy on the aggregated logits: L_MAP^(t)=L_CE(logit_agg^(t), y).\n\nSEWC uses a multi-center EWC prior that keeps a per-domain optimum \u03b8*(j) and diagonal Fisher information F_j for each previous domain j, but avoids over-constraining by applying the quadratic penalty only to a selected subset of backbone parameters. For each past domain j, it selects the top-p fraction of parameters (within a designated set S of consolidatable backbone parameters) by Fisher magnitude using quantile thresholding: \u03c4^(j)=Quantile(F_{j,S}, 1\u2212p), J^(j)={i\u2208S : F_{j,i} \u2265 \u03c4^(j)}. The final penalized index set is the union across domains I^(t\u22121)=\u22c3_{j=1}^{t\u22121} J^(j). The SEWC penalty then sums only over i\u2208I^(t\u22121) (and over all past domains j) with Fisher-weighted squared deviation from each \u03b8*(j).",
      "source_document": "papers/2512.19022v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a rehearsal-free domain-incremental face presentation attack detection system that keeps per-domain visual/text prompts, how can a test image be routed to the most appropriate domain\u2019s prompts at inference time without retaining any past images, and how is the final prediction computed once that domain is selected (including what makes the method relatively tolerant to occasional routing errors)?",
      "answer": "Inference uses a privacy-preserving prototype-bank router: for each previously learned domain, the system runs k-means on that domain\u2019s training image embeddings (from the shared image encoder feature space) to store k representative prototypes. For a test image, it computes its embedding, finds the nearest prototype in the entire bank, and selects the corresponding domain \\(\\hat t\\). It then activates that domain\u2019s components (the domain-specific visual prompt \\(D_V(\\hat t)\\), domain-specific textual context \\(D_S(\\hat t)\\), and that domain\u2019s learned prompt-family aggregation weights \\(w_k^{(\\hat t)}\\)) and forms the final logits by a weighted sum over prompt families: \\(\\text{logit}_{\\text{agg}}=\\sum_{k\\in K} w_k^{(\\hat t)}\\,\\text{logit}_k^{(\\hat t)}\\). The approach is robust to some routing mistakes because MAP includes shared domain-agnostic and mixed prompts that provide universal spoofing cues even when the selected domain-specific prompt is not perfectly matched.",
      "source_document": "papers/2512.19022v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a rehearsal-free domain-incremental face presentation attack detection system that adapts a CLIP-style vision\u2013language backbone using Multi-Aspect Prompting, what are the four textual prompt families (domain-agnostic, domain-specific, mixed, and fixed): how is each one constructed from learnable context tokens plus the frozen class-name embeddings, and what distinct role does each family play in handling domain shift and reducing catastrophic forgetting (e.g., preventing \u201csemantic drift\u201d or isolating domain idiosyncrasies)?",
      "answer": "The method builds four complementary text-prompt \u201cfamilies\u201d that all ultimately produce per-class text embeddings (for the two PAD classes \u201creal\u201d and \u201cspoof\u201d), but differ in which learnable context tokens they prepend/insert around the frozen class-name embeddings.\n\n\u2022 Domain-agnostic (DA) family: a prompt sequence formed by concatenating a start token, a shared learnable context DA (shared across all domains and continually updated), the frozen class-name embeddings for \u201creal/spoof\u201d, and an end token. Its role is to consolidate transferable, cross-domain spoofing cues that should remain stable across tasks/domains.\n\n\u2022 Domain-specific (DS) family: a prompt sequence formed by concatenating a start token, a domain-specific learnable context DS(t) (only updated when training on domain t; earlier DS(j) are frozen), the frozen class-name embeddings, and an end token. Its role is to absorb domain-t idiosyncrasies (e.g., camera/lighting artifacts), so the shared DA context is not forced to overfit to the current domain and get \u201cpolluted,\u201d which would increase forgetting on earlier domains.\n\n\u2022 Mixed (Mix) family: a prompt sequence formed by concatenating a start token, the shared DA context followed by the current domain\u2019s DS(t) context, then the frozen class-name embeddings, and an end token. Its role is to bridge general and domain-specific knowledge, producing a holistic representation that couples shared spoof cues with domain-local variations.\n\n\u2022 Fixed (Fixed) family: a set of immutable natural-language sentences (e.g., \u201cThis is a photo of real/spoof face.\u201d) that are fed directly to the text encoder without any learned context tokens. Its role is to act as a semantic anchor that leverages the backbone\u2019s zero-shot prior; this stabilizes the text side and helps prevent the learnable contexts from undergoing semantic drift during continual optimization, thereby mitigating catastrophic forgetting.",
      "source_document": "papers/2512.19022v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a rehearsal-free domain-incremental PAD setup that uses a selective EWC-style regularizer which penalizes only a top\u2011p fraction of backbone parameters (chosen by Fisher importance), how does increasing the selection ratio p affect (i) plasticity on the most recently learned domain and (ii) stability on the earliest domain after a long incremental sequence, and why can making p too large degrade performance even on that earliest domain?",
      "answer": "Increasing p (protecting/penalizing a larger proportion of parameters) reduces plasticity: performance on the newest domain worsens, reflected by higher HTER on the last domain as p grows. For the earliest domain, stability shows a U-shaped behavior: as p increases from small values, HTER on the first domain initially decreases (better retention), but when p becomes large it increases again. The reason is that a large p constrains a broad portion of parameter space with penalties from all previously learned tasks; these accumulated, potentially conflicting constraints can pull parameters away from the optimum for the early domain, so excessive regularization can harm the very domain it is intended to protect.",
      "source_document": "papers/2512.19022v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In rehearsal-free long-sequence incremental face presentation attack detection where domains are learned in order of increasing attack complexity and final generalization is evaluated on an unseen dataset, what characteristic failure mode does each of the following adaptation strategies exhibit\u2014(i) learning isolated per-domain prompts while freezing the vision\u2013language backbone, (ii) enforcing component-wise alignment to encourage knowledge reuse, and (iii) using low-rank adapters for incremental updates\u2014and how does combining a hierarchical prompt decomposition with selective backbone consolidation avoid these issues to keep both early-domain retention and late-domain adaptation strong?",
      "answer": "Across an 8-domain incremental sequence ordered by increasing attack complexity (MSU-MFSD\u2192CASIA-FASD\u2192Idiap Replay-Attack\u2192OULU-NPU\u2192SiW\u2192ROSE-YOUTU\u2192HKBU-MARs-V1+\u2192WFFD, with final testing on unseen CelebA-Spoof), three representative strategies show distinct failure modes:\n\n1) Isolated prompts + frozen backbone (e.g., independent prompts per domain): because prompts are learned independently per domain and the backbone is kept fixed, shared spoofing cues are not transferred across domains and the fixed feature extractor cannot acquire the fine-grained visual artifacts needed for newly emerging attack types; performance on earlier domains decays.\n\n2) Component-wise alignment constraints: forcing alignment between domains becomes a bottleneck when attack types diverge substantially; aligning structurally different attacks (e.g., 2D prints vs. 3D wax-figure style attacks) induces negative transfer, producing a clear downward trend in later steps as attack complexity increases.\n\n3) Low-rank adaptation (LoRA-style) updates: the low-rank constraint limits the effective parameter subspace available for new tasks; as spoofing patterns become highly diverse, this constrained subspace is insufficient to represent new features without interfering with old ones, leading to instability (sawtooth volatility) and plasticity exhaustion on the final domain.\n\nCombining hierarchical prompt decomposition (capturing both universal and domain-specific cues rather than fully isolated prompts) with selective backbone consolidation (allowing controlled backbone updates while protecting Fisher-identified important weights) addresses these problems: prompts provide domain specialization and a shared foundation for cross-domain patterns, while selective consolidation prevents overwriting critical historical knowledge without over-constraining adaptation. This yields a flat, stable trajectory on early domains while retaining high plasticity for late-stage adaptation and stronger generalization to the unseen target dataset.",
      "source_document": "papers/2512.19022v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating whether a personalized text-to-image model preserves a subject\u2019s identity (e.g., a specific animal instance or a fine-grained category), how can you set up a retrieval-based protocol using a visually similar identity-labeled gallery\u2014specifically: how are the reference and gallery sets constructed and kept disjoint, how are generated images turned into retrieval queries, what metric summarizes identity preservation, and why does this ranking-based view reveal failures that pair-wise CLIP/DINO similarity can miss?",
      "answer": "Use a two-set setup with real images: (1) a reference set per identity (used only to condition/prompt personalization) and (2) a separate, non-overlapping gallery set containing images of the same identities plus visually similar other identities. The gallery is built from an initial random allocation and then manually cleaned (e.g., remove duplicates or bad angles) while maintaining diversity (e.g., avoid style/pose bias). For each reference identity, generate multiple images under diverse prompts (varying background, pose, actions) to mimic real usage; then embed each generated image with a fixed evaluation encoder and use the embedding as a query to retrieve from the gallery by similarity ranking. Summarize identity preservation with retrieval metrics\u2014mean average precision (mAP)\u2014so higher mAP indicates the correct identity consistently ranks above near-neighbor identities. This ranking-based evaluation exposes \u201cidentity drift\u201d that pair-wise similarity can hide because CLIP/DINO cosine similarity largely captures coarse semantic/layout cues and can stay high even when identity-defining fine details are lost; adding many visually similar negatives forces the evaluation to depend on discriminative identity cues rather than generic semantics. The protocol can be applied at both fine-grained category level and stricter individual re-identification level, and it can use specialized frozen encoders for better sensitivity to identity details (e.g., Re-ID/fine-grained retrieval encoders) rather than only general-purpose CLIP/DINO.",
      "source_document": "papers/2512.19026v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a Bayesian U-Net-style model for segmenting active neurons from 4D calcium-imaging videos, how are temporal cues fused with spatial cues at the network input, how is epistemic uncertainty implemented and computed at inference time, and what training objective is used to jointly optimize segmentation quality and the Bayesian approximation?",
      "answer": "Temporal and spatial information are fused by feeding the network a 13-channel tensor: 12 channels are Pearson pixel-wise correlation maps (temporal component) and 1 channel is a summary variance image (spatial component). Epistemic uncertainty is enabled by using Bayesian/variational convolutions via the Flipout estimator\u2014implemented by replacing standard convolutional layers with flipout convolutional layers in each decoder (deconvolutional) block\u2014so stochastic forward passes sample from weight posteriors; the segmentation prediction is taken as the mean over multiple stochastic predictions, and the epistemic uncertainty map is taken as the variance across them (the paper uses an ensemble of 40 predictors at test time). Training uses a mixture loss that averages three terms: cross-entropy loss, Dice loss, and a KL-divergence term (Loss = (Loss_ce + Loss_dice + Loss_KLD)/3).",
      "source_document": "papers/2512.19032v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating an active-neuron segmentation model for calcium-imaging data where expert ground truth is unavailable and foreground pixels are highly imbalanced, what evaluation protocol can be used to measure both segmentation quality and model generalization via reproducibility, and which metrics are appropriate (including at least one that is less biased by class imbalance)?",
      "answer": "A practical protocol is to (1) create a synthetic training/benchmark target by automatically segmenting neurons using Otsu thresholding applied to per-block temporal variance maps (so no manual labels are required), and evaluate semantic segmentation outputs against this synthetic ground truth; and (2) run a reproducibility test by splitting the available training set into two disjoint halves (e.g., 100 blocks + 100 blocks), training the same Bayesian U-Net model separately on each half under the same hyperparameters, and then comparing the two models\u2019 predictions on the same held-out test set (e.g., 48 blocks) to quantify consistency/generalization.\n\nMetrics used for both semantic-segmentation evaluation and reproducibility include Dice coefficient, pixel-wise accuracy, and sensitivity; additionally, Matthews Correlation Coefficient (MCC) is used because it is more informative under strong class imbalance (foreground activity area <10% of pixels) and avoids overly optimistic performance estimates that accuracy/Dice can exhibit on imbalanced data.",
      "source_document": "papers/2512.19032v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a deep network to segment active neurons in calcium-imaging videos but lacking expert ROI annotations, how can synthetic ground-truth masks be generated automatically, what per-block image summary is thresholded to make neurons more separable, and what optimization criterion does the thresholding method use to choose the binary cutoff?",
      "answer": "Generate synthetic labels by computing a temporal variance map for each neuron block (variance over frames at each pixel) to highlight active neurons, then apply Otsu\u2019s automatic thresholding to that variance map to obtain a binary foreground/background mask. Otsu\u2019s method selects the single intensity threshold by minimizing the weighted intra-class intensity variance (equivalently maximizing between-class separation).",
      "source_document": "papers/2512.19032v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In calcium-imaging neuron segmentation where foreground/background pixels differ mainly by their fluorescence dynamics over time, how can Pearson correlation be used to extract temporal features from a video, and what is the exact correlation expression (including how signals are mean-centered and normalized) and its interpretation range?",
      "answer": "Treat each pixel location (h,w) as a time-series vector x_hw of length T (x_hw(t)=X_thw). The Pearson correlation between two pixels (h,w) and (h\u2032,w\u2032) is computed as the dot product of their mean-centered signals divided by the product of their \u21132 norms:\n\nc(x_hw, x_h\u2032w\u2032) = \u27e8x_hw \u2212 x\u0304_hw, x_h\u2032w\u2032 \u2212 x\u0304_h\u2032w\u2032\u27e9 / (\u2016x_hw \u2212 x\u0304_hw\u2016_2 \u00b7 \u2016x_h\u2032w\u2032 \u2212 x\u0304_h\u2032w\u2032\u2016_2),\n\nwhere x\u0304_hw and x\u0304_h\u2032w\u2032 are the temporal means at those locations. This coefficient measures linear correlation between the two time series and lies in [\u22121, 1], with 1 indicating perfectly correlated signals and \u22121 indicating perfectly anti-correlated signals. The method uses these correlations as temporal-context features for detecting cells whose fluorescence dynamics differ from background.",
      "source_document": "papers/2512.19032v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a highly class-imbalanced binary segmentation setting like active-neuron detection (foreground <10%), how can Matthews Correlation Coefficient (MCC) be computed directly from TP/TN/FP/FN (including any intermediate normalizations), and why is MCC used as a complement to Dice score and pixel accuracy in this scenario?",
      "answer": "MCC is computed using the confusion-matrix counts via the normalized form:\n\n- N = TP + TN + FP + FN\n- S = (TP + FN)/N (fraction of positives in the ground truth)\n- P = (TP + FP)/N (fraction predicted positive)\n\nThen\n\nMCC = (TP/N \u2212 S\u00b7P) / sqrt(P\u00b7S\u00b7(1\u2212S)\u00b7(1\u2212P)).\n\nIt is used alongside Dice score and pixel accuracy because, for strongly imbalanced data (neuronal activity area <10% of pixels), Dice and especially accuracy can be overly optimistic/inflated by the dominant background class, whereas MCC is a correlation-style measure that accounts for all four terms (TP, TN, FP, FN) and is less biased by class imbalance.",
      "source_document": "papers/2512.19032v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a CLIP-based few-shot action recognition model that refines motion hierarchically and uses learned text prompts to modulate visual features, how is the overall training objective constructed, and what do the two auxiliary consistency terms enforce across the support and query samples?",
      "answer": "The model is trained with a multi-task objective\n\nL = LCE + \u03bb3\u00b7LH + \u03bb4\u00b7LS.\n\n\u2022 LCE is the standard cross-entropy loss over query predictions p(yqj | vqj, S), i.e., it directly optimizes classification accuracy on the query set.\n\n\u2022 LH is a motion-consistency regularizer applied to all visual samples from the support and query sets (S \u222a Q): it sums dh(fv) and is used to improve consistency between shallow and deep motion features produced by the hierarchical motion refinement module.\n\n\u2022 LS is a prompt-consistency regularizer also applied over S \u222a Q: it sums ds(vsi) over support samples and ds(vqj) over query samples, encouraging the learned/generated prompt representation to stay consistent with the original/real prompt features (thereby stabilizing semantic alignment and bridging the modality gap).\n\n\u03bb3 and \u03bb4 weight the two consistency constraints relative to the classification term.",
      "source_document": "papers/2512.19036v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a metric-based few-shot action recognizer that already produces episode-specific support/query video features, how does a prototype\u2013anchor dual modulation mechanism (i) construct an episode-level \u201cglobal\u201d anchor and refine class prototypes using both support and query features, and (ii) incorporate the anchor into the final support\u2013query distance used for classification?",
      "answer": "Prototype\u2013Anchor Dual Modulation (PADM) refines both sides before computing distances. First, it reshapes the support features and uses both support and query features to build stronger prototypes: \u02dcf_S = Reshape(f_S), then forms modulated class prototypes by averaging over the concatenated support and query features, f^P_S = Mean(Concat(\u02dcf_S, f\u0304_Q)). A global anchor is then derived by averaging these prototypes over classes, f^A = Mean(f^P_S). PADM applies a Transformer-based modulation twice: (1) it refines support features and prototypes jointly via (f^P_S, f_S) = Transformer(Concat(f^P_S, f_S)); and (2) it modulates query features using the global anchor via (f^A_Q, f\u0304_Q) = Transformer(Concat(f^A, f\u0304_Q)), yielding a query-conditioned anchor f^A_Q.\n\nFor classification, PADM defines the support\u2013query distance as a sum of two sequence distances (using the same SeqDis/OTAM-style sequence distance as elsewhere): d_padm = SeqDis(f^PADM_{\\tilde{s}_n}, f^PADM_{q_j}) + SeqDis(f^P_{s_n}, f^A_q). The first term matches the PADM-modulated class prototype against the PADM-modulated query, and the second term explicitly aligns the modulated class prototype with the query-conditioned anchor. This d_padm is then combined with the SPM-based prototype\u2013query distance to produce the final distance used in the softmax over classes.",
      "source_document": "papers/2512.19036v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a few-shot video action recognizer that adds a hierarchical motion refinement module to reduce static-background interference, how are (i) the shallow motion descriptor and (ii) the deep motion descriptor computed from frame-level visual features, and what gating-based fusion operation is used to treat motion as a \u201cprompt\u201d that modulates appearance features before extracting the deep motion?",
      "answer": "Shallow motion is extracted directly from the frame-token features fv\u2208R^{T\u00d7C} using a motion feature extraction (MFE) block that applies *bidirectional* adjacent-frame differencing after a small convolutional mapping \u03a6: mf_t = \u03a6(fv[t+1]) \u2212 fv[t] and mb_t = \u03a6(fv[t]) \u2212 fv[t+1]. The local motion at time t is mt = mf_t + mb_t, and the segment-level shallow motion descriptor is the sum/aggregation over time m = \u03a3_{t=1}^{T\u22121} mt.\n\nTo obtain deep motion, the model first fuses (prompts) motion with appearance using a semantic-fusion (SF) block: it computes gated mixtures of the appearance stream and the prompt stream via fv \u2190 \u03a8v(fv)\u2299fv + \u03a8tp(ftp)\u2299ftp (with \u03a8v and \u03a8tp learned gating networks), then performs joint contextual interaction with a Transformer over the concatenated tokens: (ftp, fv) = Transformer(Concat(ftp, fv)). In HSMR, shallow motion ms_v is first computed as ms_v = MFE(fv), then fused with appearance via SF(ms_v, fv) to produce modulated appearance f^s_v, and finally deep motion is computed as md_v = MFE(f^s_v).",
      "source_document": "papers/2512.19036v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In few-shot action recognition with a CLIP-style text encoder, query videos at test time typically have no class text available, creating a support\u2013query modality gap. How can a prompt generator synthesize a query-conditioned \u201cpseudo-text\u201d embedding from (i) the episode\u2019s support-side text-prompt embeddings and (ii) the query video\u2019s frame-level visual tokens, and what are the key aggregation and fusion operations used before the final projection?",
      "answer": "A query-relevant prompt embedding is generated by (1) aggregating the episode\u2019s support prompt features by averaging over all support prompt embeddings (across the N-way and K-shot prompts), (2) aggregating the query video\u2019s visual representation by averaging its frame-level visual tokens over time, (3) fusing these two aggregated vectors with an elementwise (Hadamard) product, and then (4) passing the fused vector through a small MLP (a series of linear layers) to obtain the query-conditioned prompt:\n\nPG(f^{TP}, f^{Q}) = \u03a5( (1/(NK))\u2211_{o=1}^{NK} f^{tp}_o \u2299 (1/T)\u2211_{t=1}^{T} f^{Q}[t] ),\n\nwhere f^{TP} are textual prompt features, f^{Q} are query visual features, \u2299 is the Hadamard product, and \u03a5 is the stack of linear layers producing the learned prompt embedding used to modulate query features in SPM.",
      "source_document": "papers/2512.19036v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a prompt-conditioned few-shot action recognizer that fuses visual clip features with prompt features (motion or text) using a Semantic Fusion module, what roles do (i) feature concatenation before the Transformer, (ii) a learnable gating fusion, and (iii) an elementwise-sum/residual pathway each play in the fusion\u2014and why does using all three together yield the strongest few-shot accuracy?",
      "answer": "The fusion is designed so that: (i) Concat( prompt , visual ) is applied before the Transformer encoder to explicitly align the two feature streams and let the Transformer perform global attention over a joint token sequence; (ii) a learnable gating fusion computes gating weights from the current visual and prompt features and uses them to adaptively weight each stream (fv = \u03a8v(fv) \u2299 fv + \u03a8tp(ftp) \u2299 ftp), so the model can emphasize motion/semantic variation where useful while preserving appearance details and avoid redundancy/conflicts from naive linear fusion; and (iii) an elementwise Sum provides an additional residual pathway that stabilizes feature integration. Combining Concat + Gate + Sum is mutually reinforcing\u2014alignment via Concat enables effective attention-based mixing, gating regulates relative contributions, and the residual/Sum path improves stability\u2014leading to more coherent, discriminative fused representations and the best few-shot performance.",
      "source_document": "papers/2512.19036v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a watermark decoder to reliably recover bits from image-to-video outputs, how can optical-flow alignment be used to reduce frame-to-frame fluctuations in decoded watermarks, and what is the temporal loss term used to enforce this stability? In your answer, specify (i) which frames are compared after warping, (ii) how the temporal consistency loss is computed, and (iii) how it is combined with the per-bit message recovery objective during training.",
      "answer": "Optical flow is used to align generated video frames to a common reference so that the decoder sees temporally corresponding content despite subpixel drifts. Concretely, each generated key frame is warped toward the first generated frame v0, producing warped frames a\u2113.\n\n(i) The loss compares the decoder outputs on adjacent flow-warped frames, i.e., D(a\u2113) vs. D(a\u2113\u22121) for \u2113=1,\u2026,M\u22121, where M is the number of key frames.\n\n(ii) The Temporal Consistency Loss (TCL) is the average squared \u21132 difference between decoder outputs on these adjacent warped frames:\nLTCL = (1/(M\u22121)) * \u03a3_{\u2113=1}^{M\u22121} || D(a\u2113) \u2212 D(a\u2113\u22121) ||_2^2.\n\n(iii) The decoder training objective adds this TCL to the standard per-bit message recovery term: Ldec = LTCL + LMSG, where LMSG is computed with per-bit BCE losses for recovering the watermark from (a) each warped frame a\u2113, (b) the watermarked image Iw, and (c) the edited watermarked image \\tilde{Iw} produced by the editing branch.",
      "source_document": "papers/2512.19048v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training an image watermarking encoder\u2013decoder to be (a) imperceptible and (b) robust to downstream latent-space operations (e.g., diffusion-based editing / I2V), what is the full training objective that enforces imperceptibility in multiple spaces and uses adversarial training? In your answer, specify (i) the encoder-side loss terms used in pixel space, VAE-latent space, perceptual (LPIPS) space, and semantic (CLIP-embedding) space, (ii) the PatchGAN-style adversarial losses used to train the encoder (generator) and the discriminator, and (iii) how these are combined with the decoder\u2019s per-bit message recovery loss to form the final total loss.",
      "answer": "(i) The encoder is trained with a weighted sum of imperceptibility/consistency losses in several spaces:\n\n- Pixel-space fidelity:  \\(\\mathcal L_{\\text{pixel}}\\), the MSE between the original image \\(I\\) and the watermarked image \\(I_w\\).\n- Latent-space fidelity:  \\(\\mathcal L_{\\text{latent}}\\), the MSE between \\(I\\) and \\(I_w\\) measured after mapping both through a VAE encoder (to reflect that edits/video generation operate in VAE latent space).\n- Perceptual similarity:  \\(\\mathcal L_{\\text{LPIPS}}\\), the LPIPS perceptual loss between \\(I\\) and \\(I_w\\).\n- Semantic preservation:  \\(\\mathcal L_{\\text{sem}} = 1 - \\cos\\big(f_{\\text{CLIP}}(I_w),\\, f_{\\text{CLIP}}(I)\\big)\\), using a frozen CLIP image encoder \\(f_{\\text{CLIP}}\\).\n\nThese are combined as:\n\\[\n\\mathcal L_{\\text{enc}} = \\mathcal L_{\\text{pixel}} + \\lambda_{\\text{latent}}\\mathcal L_{\\text{latent}} + \\lambda_{\\text{LPIPS}}\\mathcal L_{\\text{LPIPS}} + \\lambda_{\\text{sem}}\\mathcal L_{\\text{sem}}.\n\\]\n\n(ii) Adversarial training uses a PatchGAN discriminator \\(A\\) that outputs a patch-wise logit map. The encoder (generator) is trained to fool \\(A\\) with:\n\\[\n\\mathcal L^{G}_{\\text{adv}} = -\\frac{1}{H'W'}\\sum_{i=1}^{H'}\\sum_{j=1}^{W'} \\log \\sigma\\big(A(I_w)_{i,j}\\big),\n\\]\nwhile the discriminator is trained with:\n\\[\n\\mathcal L_{\\text{disc}} = -\\frac{1}{H'W'}\\sum_{i,j}\\Big[\\log \\sigma\\big(A(I)_{i,j}\\big) + \\log\\big(1-\\sigma(A(I_w)_{i,j})\\big)\\Big].\n\\]\n\n(iii) The decoder is trained for per-bit message recovery using BCE on (a) the watermarked image \\(I_w\\), (b) an edited watermarked image \\(\\tilde I_w\\), and (c) a set of key video frames (after the robustness module), aggregated as \\(\\mathcal L_{\\text{MSG}}\\); the decoder loss is \\(\\mathcal L_{\\text{dec}} = \\mathcal L_{\\text{TCL}} + \\mathcal L_{\\text{MSG}}\\). The overall objective for the encoder+decoder parameters is:\n\\[\n\\mathcal L_{\\text{total}} = \\mathcal L_{\\text{enc}} + \\lambda_{\\text{dec}}\\mathcal L_{\\text{dec}} + \\lambda_{\\text{adv}}\\mathcal L^{G}_{\\text{adv}}.\n\\]\n(Separately, the discriminator is optimized with \\(\\mathcal L_{\\text{disc}}\\).)",
      "source_document": "papers/2512.19048v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training an image watermark encoder\u2013decoder to stay robust after image-to-video (I2V) generation, it is common to include a \u201cproxy\u201d video diffusion model in the training loop. What proxy video generation options can be used for this role, and how does the choice of (a) an SVD-style proxy with vs. without classifier-free guidance (CFG) and (b) a fast proxy like AnimateLCM affect (i) watermark bit-accuracy after I2V, and (ii) practical training cost such as VRAM and runtime? Explain the mechanism behind these differences (e.g., CFG requiring extra passes and the diffusion-step count).",
      "answer": "The proxy video generator options considered are Stable Video Diffusion (SVD) used either without CFG or with CFG, and AnimateLCM as a fast video diffusion proxy. Using SVD without CFG gives noticeably worse watermark recovery after I2V than SVD with CFG, indicating that for SVD-based proxy training, good robustness effectively requires enabling CFG. AnimateLCM, in contrast, generates temporally coherent videos in only a few (about 2\u20134) diffusion steps and does not use CFG; avoiding CFG eliminates the extra unconditional/conditional passes, which reduces memory use and compute. As a result, AnimateLCM achieves competitive (often best) post-I2V bit accuracy while requiring substantially lower peak VRAM and shorter overall training time / per-video generation time than SVD proxies, especially compared to SVD with CFG.",
      "source_document": "papers/2512.19048v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When assessing whether an image watermarking method has learned a spatially \u201clocalized\u201d watermark (e.g., concentrated mostly in the center or mostly near the borders), how can comparing watermark bit accuracy under a center crop versus an inverse center crop be used as a diagnostic, and what does this comparison reveal about the typical baseline methods versus WaTeRFlow?",
      "answer": "A center-crop vs. inverse-center-crop test probes where the watermark signal is spatially concentrated: if decoding accuracy stays high after keeping only the center but drops when keeping only the periphery (or vice versa), the watermark is localized to that retained region. Conversely, similar accuracy in both crops suggests the watermark is spread more uniformly across the image.\n\nThe reported results show many baselines have a large gap between bit accuracy under center crop and inverse center crop, indicating they tend to embed watermark energy primarily either in the central area or around the boundary/periphery (e.g., some methods decode well only from the periphery, consistent with boundary-focused patterns that can also manifest as border noise). In contrast, WaTeRFlow exhibits a much smaller center\u2013inverse gap, implying its watermark signal is less region-dependent and is distributed more evenly across the image, making it less vulnerable to removing just the center or just the borders.",
      "source_document": "papers/2512.19048v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When designing an image watermark embedder to remain decodable after image-to-video diffusion models that condition on an image-derived embedding (e.g., CLIP), what semantic-preservation regularizer can be added during watermark embedding, how is it computed, and what mechanism explains why it particularly improves watermark recovery on the first generated frame? Additionally, if the semantic term is computed in a representation space not used by the video generator\u2019s conditioning encoder (e.g., swapping CLIP for DINOv2), what does this imply the semantic term is really constraining about the watermark residual?",
      "answer": "Add a semantic preservation loss that penalizes changes in high-level image semantics by matching the original and watermarked images in a pretrained embedding space. Concretely, compute a frozen CLIP image embedding for the original image I and the watermarked image Iw and minimize a cosine-distance objective: L_sem = 1 \u2212 cos(f_CLIP(Iw), f_CLIP(I)). This regularizes the watermarked image to stay semantically consistent with the original and, critically, keeps the I2V model\u2019s conditioning signal c (derived from the conditioning image, including its CLIP embedding) from being shifted by the watermark. In Stable Video Diffusion\u2013style generators, where attention uses the condition image\u2019s CLIP embedding as keys/values, preserving that embedding makes the first generated frame v0 stay closer to the watermarked input, which yields markedly higher first-frame bit accuracy.\n\nWhen replacing CLIP with a DINOv2-based semantic term (even though it is not the same conditioning encoder used by SVD), first-frame accuracy still improves, suggesting the semantic loss is not only \u201cmatching the exact conditioning encoder,\u201d but more generally constraining the watermark residual to preserve structural/low-frequency cues such as object layout and contours. With pixel- and VAE-latent MSE already shrinking the overall residual magnitude, the semantic term largely guides where the remaining change is allowed to reside\u2014pushing it toward local details with smaller impact on global structure/conditioning.",
      "source_document": "papers/2512.19048v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a PnP-based (hybrid) 6DoF pose estimation pipeline such as PVNet, what intermediate representation is the most effective place to implant a backdoor so that a trigger causes a controlled pose error, and why is simply perturbing object masks/segmentation a poor strategy in this setting?",
      "answer": "For hybrid PnP pipelines like PVNet, the attack targets the keypoint-based intermediate representation: PVNet predicts a 2D keypoint vector field (per-pixel unit vectors pointing to 2D keypoints), and the final 6DoF pose is recovered via PnP from the inferred keypoint locations. The backdoor is implanted by injecting a fixed, ordered offset into the predicted keypoint positions so that, when the trigger is present, the downstream PnP step yields a biased pose. Perturbing masks/segmentation is ineffective because the pose solution is highly sensitive to geometric consistency and keypoint localization; mask modifications tend to disrupt geometry in a way that does not reliably translate into a controllable pose offset and can fail under the pipeline\u2019s geometric constraints.",
      "source_document": "papers/2512.19058v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a backdoor attack on a 6DoF pose estimator, how can Attack Success Rate (ASR) be defined in terms of standard pose-accuracy metrics so that a \u201csuccessful attack\u201d corresponds to a clearly wrong pose (not a small error), and how are metrics on clean inputs versus triggered inputs distinguished in reporting?",
      "answer": "ASR is computed on triggered samples as the fraction whose predicted pose is incorrect under multiple standard pose metrics simultaneously: the prediction must exceed the correctness thresholds for (i) 3D pose error via ADD (i.e., not within the \u201ccorrect pose\u201d ADD criterion), (ii) pose estimation accuracy via translation and rotation errors (i.e., translation and rotation both outside the \u201ccorrect\u201d bounds), and (iii) 2D projection error (i.e., projected points not within the \u201ccorrect\u201d pixel tolerance). The paper defines success as satisfying all these failure conditions at once (ADD above the ADD correctness cutoff, translation error above the translation cutoff, rotation error above the rotation cutoff, and 2D projection error above the pixel cutoff), and then ASR is the ratio of such successful triggered cases to all triggered cases. For reporting, metric names use a \u201c-C\u201d suffix for clean data (e.g., ADD-C/PEA-C/2DPE-C) and a \u201c-P\u201d suffix for triggered/poisoned data (e.g., ADD-P/PEA-P/2DPE-P).",
      "source_document": "papers/2512.19058v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an end-to-end 6DoF pose estimation pipeline that directly regresses pose from RGB or RGB-D (e.g., DenseFusion-style), what characteristics must a backdoor trigger have to reliably drive predictions to an attacker-chosen pose, and what training-time poisoning steps are used to make the model learn that behavior without breaking clean performance?",
      "answer": "For end-to-end pose regressors, the trigger needs to be a regular and stable signal that corresponds to a consistent (fixed) pose transformation; otherwise activation tends to produce confusion or unpredictable outputs rather than the desired pose. To enforce this, poisoned training samples are created by embedding a structured 3D object trigger into the input (ensuring consistent representation in both RGB and depth for RGB-D models) and simultaneously editing the pose supervision by applying an attacker-defined 6DoF offset/target pose label, so the network learns to map \u201ctrigger present\u201d to a specific incorrect pose while behaving normally on clean samples.",
      "source_document": "papers/2512.19058v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In backdoor robustness testing for 6DoF pose estimation, what is the expected effect of a common post-training defense that fine-tunes the poisoned pose estimator on additional clean data (\"defensive retraining\")\u2014specifically, how does it impact (i) whether triggered predictions hit the attacker\u2019s exact target pose and (ii) whether the attack is still counted as successful under ASR-style criteria?",
      "answer": "Defensive retraining on clean data can weaken *precision* of the targeted manipulation\u2014on triggered inputs the predicted pose may deviate from the attacker\u2019s exact intended target pose\u2014but it does not eliminate the backdoor\u2019s effect: the Attack Success Rate remains essentially unchanged, and the model still outputs an entirely incorrect pose whenever the trigger is present (even if it\u2019s not exactly the attacker-defined pose). On triggered scenes, outcomes can include partial convergence toward the target pose, complete failure to return a valid pose, or incorrect/unpredictable pose estimates; in all cases the original correct pose is no longer reliably produced.",
      "source_document": "papers/2512.19058v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For backdooring 6DoF pose estimators in a way that still activates under different viewpoints, what makes a *3D object* trigger fundamentally more reliable than a 2D pixel-pattern trigger, and what are the two practical categories of 3D triggers that can be used (including why each category is attractive to an attacker)?",
      "answer": "A 3D object trigger is reliable for 6DoF pose backdoors because it has a well-defined 6DoF pose and therefore produces a view-dependent appearance/projection that remains geometrically consistent across different camera viewpoints; this lets the trigger be embedded and recognized in a way aligned with how 6DoF pipelines use geometry, rather than being distorted or washed out like naive pixel-level patterns through multi-stage feature extraction and projection-sensitive processing.\n\nTwo categories of 3D triggers are used:\n1) **Artificially modeled 3D triggers**: purpose-designed 3D models with controlled, distinctive geometry/texture. Their controlled shape/appearance enables precise and consistent embedding as a backdoor signal.\n2) **Real-world object triggers**: everyday physical items (e.g., cups/pen holders/vases, including objects from common pose datasets). They already carry 6DoF pose information and do not require extra synthesis or concealment, are physically authentic and easier to deploy in the real world, and can induce stealthy erroneous correlations.",
      "source_document": "papers/2512.19058v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "How does a training-free hallucination-mitigation decoder for large vision\u2013language models use (1) semantic segmentation and Jensen\u2013Shannon divergence and (2) a blank-image contrastive signal to choose a \u201cbest\u201d segmented view and produce the final next-token logits? Describe the role of the Div score, the adaptive mixing weight \u03b4, and the blank-image weighting factor \u03b1 in the final logit combination.",
      "answer": "The decoder first segments the original image V into two complementary views: v1 is formed by summing the masked regions from the largest N segmentation masks (default N=0.05\u00b7n masks), and v2 is the remainder (v2 = V \u2212 v1). It also introduces a completely blank image vn. All four images (V, v1, v2, vn) are fed to the LVLM to obtain next-token distributions.\n\nTo pick the more informative segmented view, it computes for each i\u2208{1,2} a divergence score\nDivi = JSD( p(yt | vi, x, y<t) || p(yt | vn, x, y<t) ).\nA larger Divi means the segmented image\u2019s distribution differs more from the blank-image (language-prior) distribution, so it is considered to contain more effective visual information; the selected view is i* = argmax Divi.\n\nAn adaptive mixing weight is then derived from the difference between these divergence scores: \u03b4 = Div1 \u2212 Div2, so the more informative segmented view provides stronger guidance while the other provides weaker influence. The visual-detail enhanced logits mix the original and the selected segmented view:\nlogitenh(V) = (1\u2212\u03b4)\u00b7logit(V) + \u03b4\u00b7logit(vi*).\n\nTo remove language-prior hallucinations, it performs contrastive decoding using the blank image: for any input vin (either V, v1, or v2), it subtracts the enhanced blank-image logits with a weight \u03b1,\nlogit*(vin) = (1+\u03b1)\u00b7logitenh(vin) \u2212 \u03b1\u00b7logitenh(vn),\nwhere vn acts as an \u201cimage placeholder\u201d that reduces the model to a language-only prior.\n\nFinally, it combines the contrastively corrected logits for the original image and the chosen segmented view using the same adaptive \u03b4:\nlogithdd = (1\u2212\u03b4)\u00b7logit*(V) + \u03b4\u00b7logit*(vi*).",
      "source_document": "papers/2512.19070v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "How can you empirically verify that an LVLM\u2019s visual encoder is less sensitive to small objects, and what quantitative relationship between an entity\u2019s image area and its corresponding next-token logit emerges from this test? Explain how this finding motivates a visual-hallucination mitigation strategy that increases the queried entity\u2019s proportion in the input via segmentation into two complementary images built from the top\u2011N largest masks.",
      "answer": "One verification is to hold the query fixed while varying only the target object\u2019s apparent size in the image: create multiple versions of a scene where an object of interest (e.g., a \u201ccar\u201d) becomes progressively smaller (as if farther away), while a comparison object (e.g., a \u201cperson\u201d) stays the same size. When extracting the LVLM\u2019s logits for the relevant entity tokens under the same question, the comparison object\u2019s logits change little, but the small-object (\u201ccar\u201d) logits change almost linearly with the object\u2019s size. This yields the observation that the visual encoder\u2019s sensitivity (reflected in the entity token logit) is correlated with entity area, approximately logit_i \u221d A_i / A_all (A_i is the entity\u2019s area and A_all the whole-image area). Because small entities contribute too little visual evidence, the model is more easily swayed by language priors, increasing hallucinations. To counter this, the image can be segmented into two complementary inputs v1 and v2 where v1 is formed by summing pixel-wise masked regions for the largest N segmentation masks and v2 is the remainder (v2 = V \u2212 v1). This increases the proportion of local details for entities within the segmented view, making the visual signal for the queried entity stronger and reducing visually induced hallucinations.",
      "source_document": "papers/2512.19070v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an open-vocabulary 3D instance segmentation system that avoids using SAM to generate masks, how can 2D open-vocabulary detector boxes be lifted into temporally consistent 3D instance proposals using depth/camera geometry and superpoints, and what fusion/filtering rules are used when combining these RGBD-derived proposals with a pretrained point-cloud segmenter\u2019s point-based masks to prioritize rare objects while avoiding redundant overlapping instances?",
      "answer": "2D detections are converted into 3D proposals by (1) taking each predicted 2D box in a frame and projecting all pixels inside the box into 3D using the corresponding depth map and camera intrinsics/extrinsics, then fitting an oriented 3D bounding box around the projected points (via Open3D). (2) To avoid duplicating objects already found by the point-based segmenter, a lifted 3D box is removed as redundant if it overlaps with a sufficient fraction of points from an existing point-based mask. (3) For each remaining lifted 3D box, superpoints from a graph-based segmentation of the point cloud are assigned to the box when a sufficient fraction of the superpoint\u2019s points lie inside the box; the selected superpoints form a coarse mask for that frame. (4) Coarse masks are merged sequentially across frames into object candidates: a new coarse mask is merged with an existing candidate when their IoU exceeds a merge threshold and they share the same predicted prompt label; otherwise it becomes a new candidate. (5) The merged superpoint sets are converted into binary RGBD-based masks. (6) Before fusing with point-based proposals, an additional filter discards any RGBD-based mask that has high IoU with any point-based mask\u2014this design explicitly prioritizes adding rare objects missed by the class-agnostic 3D segmenter; when overlap occurs the point-based mask is kept because it typically has better geometric quality. The final proposal set is the union of the retained RGBD-based masks and the point-based masks.",
      "source_document": "papers/2512.19088v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an open-vocabulary 3D instance segmentation pipeline that avoids CLIP by classifying 3D masks using 2D detector outputs, how can a multi-view \u201cprompt distribution\u201d be computed for each 3D candidate mask so it can be assigned a semantic label from the input query\u2014specifically, how are per-frame label maps built from overlapping 2D boxes, how is point visibility/occlusion handled when projecting the 3D mask into each frame, and how is the final class chosen across frames?",
      "answer": "For each RGB frame, build a dense label map L by initializing all pixels to \u22121 (no relevant label) and then rasterizing the 2D detector boxes into the map in descending box-size order, writing the box\u2019s predicted prompt label into its pixels; writing large-to-small makes smaller/nearer objects overwrite larger ones when boxes overlap.\n\nTo score a 3D mask Mj, project the whole point cloud into every frame to get per-point 2D coordinates P^2D_x, P^2D_y and projected depth P^2D_z. Compute a frame-visibility mask Vf that keeps only projections inside image bounds, and an occlusion/depth-consistency mask Vd that keeps only points whose projected depth agrees with the depth map value Dz within a threshold \u03c4depth (|P^2D_z\u2212Dz|<\u03c4depth). The visible part of the j-th mask in frame i is then Mji = Vd_i \u00b7 Vf_i \u00b7 Mj.\n\nSelect the top-k frames where the mask has the most visible points (Pk). Collect the prompt labels from the label maps at the projected pixel locations of the visible mask points across those frames, forming Dj. Convert Dj into a class histogram (occurrence counts per class id), and assign the 3D instance to the class with the highest probability / highest count.",
      "source_document": "papers/2512.19088v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In open-vocabulary 3D instance segmentation pipelines that generate additional object masks by merging superpoints inside 3D boxes lifted from 2D detections (rather than refining masks with a 2D segmenter), why can the method show larger performance gains at looser IoU thresholds (e.g., AP/mAP at 50% or 25% IoU) than at stricter IoU thresholds, when compared to a point-based-proposal baseline like Open-YOLO 3D?",
      "answer": "Because the extra instances are formed purely by grouping superpoints within lifted 3D boxes, the resulting novel masks can be noisy and have imperfect boundaries. Such boundary/geometry errors are penalized much more at high IoU thresholds, so improvements from discovering additional rare/novel objects tend to appear more strongly at lower IoU thresholds (e.g., 50% or 25%) than at higher, stricter thresholds.",
      "source_document": "papers/2512.19088v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a box-guided open-vocabulary 3D instance segmentation pipeline that forms extra 3D proposals by lifting 2D detector boxes into 3D, what part of the lifting/proposal-generation step is the main computational bottleneck, and what concrete implementation change is suggested to reduce the runtime without changing the overall method?",
      "answer": "The main runtime bottleneck is the 2D-to-3D uplifting step when computing oriented 3D bounding boxes for lifted detections using the Open3D library. A suggested way to reduce runtime (while keeping the same pipeline) is to replace this with a more efficient GPU-based implementation for obtaining the oriented 3D boxes.",
      "source_document": "papers/2512.19088v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a fast open-vocabulary 3D instance segmentation pipeline that uses a 2D open-vocabulary detector for box guidance and label-map construction, how should the 2D detector be scheduled across RGB frames on ScanNet200 versus Replica to balance runtime and fair comparison to Open-YOLO 3D, and what image downsampling is applied during the box-guided RGBD-based proposal-generation step for efficiency?",
      "answer": "Use the YOLO-World extra-large detector sparsely on ScanNet200 by running it only on the first frame of each 10-frame interval, but run it on every frame for Replica. During the box-guided RGBD-based proposal-generation stage, downsample each processed RGB frame by a factor of 5 to reduce computation.",
      "source_document": "papers/2512.19088v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In multi-contrast MRI reconstruction, when you first create a structurally aligned but modality-mixed target input by complementing missing target k-space samples with a fully sampled reference, how does a Mamba-based modality-disentanglement block suppress reference-specific information? Describe how its gating signal is computed and how it is used in the target feature update (including whether the reference contribution is added or subtracted), and summarize what the fusion-vs-disentanglement ablation concludes about which interaction strategy works better.",
      "answer": "The modality-disentanglement block builds a target-specific feature by explicitly gating and removing (not fusing) the reference information. At iteration i it forms a mixed-path feature by a residual connection (F_mix^i = F_mix^0 + F_tar^i), projects features with a lightweight linear layer, then applies depthwise convolution plus an SS2D (Mamba-style) block and normalization to both the mixed and reference paths to get \\tilde{F}_mix^i and \\tilde{F}_ref. A target-dependent gate is computed from the current target feature as G_tar^i = \\sigma(Linear(F_tar^i)). This gate modulates the reference feature and the gated reference contribution is subtracted from the mixed feature to obtain the refined target feature: F_tar^{i+1} = \\tilde{F}_mix^i \u2212 Linear(G_tar^i \\odot \\tilde{F}_ref). In the fusion baseline, the interaction is changed to feature addition instead of subtraction. The ablation comparing the two strategies shows that using k-space complementation followed by this disentanglement (gated subtraction) consistently yields better reconstruction quality than simple fusion across multiple backbone architectures, indicating that filtering out reference-specific components is more effective than injecting them by addition.",
      "source_document": "papers/2512.19095v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using a cascade of modality-disentanglement blocks for multi-contrast MRI reconstruction, how does an iterative refinement strategy propagate information across stages (i.e., how is the stage-i mixed feature formed from the initial mixed feature and the current target feature), how is a \u201cparallel blocks\u201d alternative constructed for ablation, and what do the ablations reveal about (a) iterative vs. parallel vs. single-block performance and (b) the failure mode observed when increasing the number of disentanglement blocks too far?",
      "answer": "Iterative refinement runs the disentanglement module sequentially, feeding the next stage with an updated mixed representation formed by a residual update: for stage i, the mixed feature is computed as F_mix^i = F_mix^0 + F_tar^i (with F_mix^i and F_tar^i initialized from F_mix^0). Each stage uses the preceding stage\u2019s intermediate output, enabling gradual suppression of cross-modal interference and progressively \u201cpurifying\u201d the target features.\n\nFor the \u201cparallel blocks\u201d ablation, multiple (e.g., 6) disentanglement modules are not chained; instead, they independently process the same input pair (F_mix^0, F_ref), and their outputs are averaged to produce the final target result.\n\nAblations show that the sequential/iterative design yields the best reconstruction metrics, while both a single block and parallel stacking reduce performance. Increasing the number of disentanglement blocks improves performance up to a point (peaking at 6 blocks), but adding more (e.g., 7) degrades results due to feature over-subtraction (removing too much information while trying to suppress reference-specific components).",
      "source_document": "papers/2512.19095v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a 2D Gaussian-splatting image representation that supports progressive densification under a fixed Gaussian budget M, how can the method decide (i) when to add new Gaussian primitives, (ii) where to place them, (iii) how many to add, and (iv) how to initialize their position and color attributes; and what criterion can be used to prune invalid Gaussians during training?",
      "answer": "A practical scheme is a distortion-driven densification loop:\n\n- **When to add (schedule):** Perform densification periodically during training (e.g., every 5000 iterations).\n- **Where to add:** Compute a per-pixel reconstruction distortion D(X, X\u0302) between the ground-truth image X and the current rendered image X\u0302 (the described implementation uses **L1** for simplicity), and select the **top\u2011k pixels** with the largest distortion.\n- **How many to add:** Set the number of new Gaussians k using the remaining budget, e.g. with a scheduler \\(\\tau(t, N_t, M)=\\frac{M-N_t}{2}\\), where \\(N_t\\) is the current number of Gaussians.\n- **How to initialize attributes:** For the newly added set \\(\\Psi\\), initialize **positions** by assigning their means to the coordinates of the selected top\u2011k distortion pixels, \\(\\mu_\\Psi = \\xi(\\mathrm{Topk}(D(X,X\u0302)))\\). Initialize **colors** by copying the ground-truth pixel colors at those coordinates, \\(c_\\Psi = X(\\xi(\\mathrm{Topk}(D(X,X\u0302))))\\). (Covariances can be initialized similarly to the sparse initialization stage, i.e., random sampling with constraints to ensure valid covariance.)\n- **Pruning invalid Gaussians:** Periodically check whether each covariance matrix \\(\\Sigma\\) is positive semidefinite and prune invalid Gaussians; an explicit check is that \\(\\det(\\Sigma)\\ge 0\\) and the diagonal entries satisfy \\(\\Sigma_{11}\\ge 0\\), \\(\\Sigma_{22}\\ge 0\\).",
      "source_document": "papers/2512.19108v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In 2D Gaussian-splatting image fitting with a sparse-to-dense training schedule, how can you design a per-primitive, content-aware Gaussian low-pass filter that (i) is applied during rasterization, (ii) requires no extra storage at encoding time, and (iii) automatically weakens as more Gaussians are added\u2014what is the functional form used to set each primitive\u2019s filter variance and how does this mechanism reduce early \u201choles\u201d/undersampling artifacts?",
      "answer": "Use a Gaussian resampling (low-pass) filter by inflating each primitive\u2019s screen-space covariance during rasterization: replace the footprint kernel Gi(x) with G\u2032i(x)=exp(-1/2 (x\u2212\u03bci)^T (\u03a3i+s_i I)^{-1}(x\u2212\u03bci)), i.e., convolve Gi with a zero-mean Gaussian h(x) whose variance is a per-primitive scalar s_i. The filter is \u201ccontent-aware\u201d via an adaptive variance vector s\u2208R^{N_t}: newly added Gaussians get s_i = HW/(\u03b1 N_t) (for i > N_{t\u22121}), while existing Gaussians keep their previous variance s_i = s_{i\u22121} (for i \u2264 N_{t\u22121}), so as densification increases N_t the assigned variance for new primitives decreases, weakening the filter over time. It adds no storage overhead because the method stores the filtered covariance \u03a3+sI rather than s separately. Large s_i early enlarges each Gaussian\u2019s footprint, increasing Gaussian\u2013pixel intersection area, mitigating undersampling and reducing large \u2018holes\u2019 when N_t \u226a HW; later, smaller s_i prevents the filter from dominating and lets new Gaussians focus on refining fine details.",
      "source_document": "papers/2512.19108v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When turning an explicit 2D Gaussian-splatting representation into an image codec, how can you do attribute-level quantization-aware training with learnable scalar quantizers: (i) what quantize/dequantize transform (including learnable parameters) is applied to an attribute vector in LSQ+, (ii) how are different bit-depths assigned to geometry (position/covariance) versus color attributes, and (iii) how is training staged so that Gaussians are \u201coverfitted\u201d before quantization and what happens to densification during the quantization-aware fine-tuning phase?",
      "answer": "(i) Use LSQ+ learnable scalar quantizers with a learnable offset \u03b2 and scale s. For an attribute vector v and bit-depth b, the forward quantize/dequantize with straight-through gradients is:\n\nv_q = \u230a clip((v \u2212 \u03b2)/s, 0, 2^b \u2212 1) \u230b,\n\u0175 = v_q \u00b7 s + \u03b2.\n\n(ii) Apply distinct bit-depths per attribute type to balance rate and distortion: geometry is quantized more finely because it is more sensitive\u201412-bit for positions \u03bc and 10-bit for covariances \u03a3\u2014while RGB color uses lower precision (6-bit).\n\n(iii) Stage training as: first run the normal image-representation optimization to obtain well-fitted (\u201coverfitted\u201d) continuous Gaussian attributes (a warm-up, e.g., thousands of iterations) so the subsequent quantization has a good starting point; then perform attribute quantization-aware fine-tuning with the LSQ+ quantizers to adapt the attributes to quantization. During this quantization-aware phase, pruning of invalid Gaussians is kept, but Gaussian growing/densification is paused (no further Gaussian additions).",
      "source_document": "papers/2512.19108v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When enhancing a 2D Gaussian-splatting image fitting pipeline that may parameterize/optimize each Gaussian\u2019s covariance either via a factorization (e.g., Cholesky or RS) or by directly optimizing the covariance matrix, what qualitative effect should you expect from adding (i) distortion-driven densification and (ii) content-aware Gaussian low-pass filtering on reconstruction quality across these parameterizations\u2014and which of the two components tends to contribute the larger gain under a fixed Gaussian budget?",
      "answer": "Across different covariance-optimization choices (factorized covariances like Cholesky or RS, or directly optimized covariances), adding distortion-driven densification and content-aware filtering consistently improves reconstruction quality under the same maximum number of Gaussians. The improvement is robust to the covariance parameterization/optimization strategy rather than being tied to a specific factorization. Among the two, distortion-driven densification is the dominant contributor: it provides the most significant quality increase (notably improving cases with fewer Gaussians), while adding the content-aware filter on top of densification yields additional\u2014but smaller\u2014gains by stabilizing early fitting (reducing holes/artifacts) and letting later-added Gaussians focus on fine details.",
      "source_document": "papers/2512.19108v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a 3-view relative pose solver that first estimates a trifocal tensor from minimal point correspondences (e.g., 3- or 4-point cases), how can you enforce the internal consistency constraints so the final trifocal tensor corresponds to valid camera projection matrices, and how is this enforced form used to refine the initial linear system solution?",
      "answer": "After obtaining an initial trifocal tensor estimate Ti from the minimal solver, enforce consistency by re-parameterizing Ti through epipoles and structured projection matrices that satisfy the trifocal constraint by construction. Specifically:\n\n1) For each slice Ti, compute its left and right null-vectors ui and vi (u_i^T Ti = 0^T and Ti v_i = 0). The epipoles e\u2032 and e\u2032\u2032 are orthogonal to the sets of left/right null-vectors, so they are obtained via SVD from e\u2032^T [u1,u2,u3] = 0 and e\u2032\u2032^T [v1,v2,v3] = 0.\n\n2) Use these epipoles to parameterize the second and third camera projection matrices with unknown scalars a\u2217:\nP2 = [[a1,0,a2,e\u20321],[0,a3,0,e\u20322],[\u2212a2,0,a1,e\u20323]] and\nP3 = [[a4,0,a5,e\u2032\u20321],[0,a6,0,e\u2032\u20322],[\u2212a5,0,a4,e\u2032\u20323]].\n\n3) Build the trifocal tensor slices T1,T2,T3 as explicit functions of the unknowns a\u2217 and the epipole components; this parameterization \u201cnaturally satisfies\u201d the trifocal constraint.\n\n4) The original linear system Aq = 0 (where q stacks the tensor parameters) is converted using the linear relation q = E a induced by the constrained parameterization. Then solve for a by minimizing the residual under a unit-norm constraint: a = arg min_a ||A E a|| subject to ||E a|| = 1. Recover the refined Ti from a, and then use it to obtain the pose parameters.",
      "source_document": "papers/2512.19110v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a 3-view relative pose problem where roll/pitch are known from an IMU (known vertical direction), how can a minimal solver use only 3 point correspondences to estimate the remaining yaw angles and translations, and what algebraic trick is used to reduce the polynomial system\u2019s degree to make a Gr\u00f6bner-basis solution practical before recovering the translation vectors?",
      "answer": "Use a yaw-only rotation parameterization together with unknown translations for views 2 and 3, then derive trifocal constraints that depend only on the two yaw parameters and the two translations. Specifically, represent each unknown rotation Rk (k=2,3) with Cayley parameterization for yaw: sk = tan(\u03b8k/2) and \nRk = (1/(1+sk^2))[[1\u2212sk^2,0,2sk],[0,1+sk^2,0],[\u22122sk,0,1\u2212sk^2]], while tk = [tkx,tky,tkz]^T. Substituting into the trifocal equations yields constraints of the form F(s2,s3) t = 0 (up to a scalar factor 1/((1+s2^2)(1+s3^2))), where t stacks the 6 translation components (t2 and t3). With 3 point correspondences, select independent rows to build a 9\u00d76 matrix bF(s2,s3) such that bF t = 0; enforcing existence of a nonzero t implies det of any 6\u00d76 submatrix is zero, giving two equations in (s2,s3). The direct determinant equations are degree 24, so multiply/divide by the omitted factor 1/((1+s2^2)(1+s3^2)) to reduce degree: take the quotient by (1+s2^2)^3(1+s3^2)^3, which reduces the degrees in s2 and s3 to 6 and the total degree to 12. Solve the resulting degree-12 system with an automatic Gr\u00f6bner-basis solver, yielding up to 6 complex roots and keep the real roots as candidate (s2,s3) solutions. Then substitute (s2,s3) back into bF and recover translation components by solving for the nullspace via SVD, fixing scale by imposing \u2016t\u2016=1.",
      "source_document": "papers/2512.19110v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a monocular relative-pose solver (2-view or 3-view) on KITTI, what rotation and translation error metrics can be used, and why is translation error often reported as an angular error instead of Euclidean distance?",
      "answer": "Rotation error can be measured as the geodesic angle between estimated and ground-truth rotations: \n\u03b5R = arccos((trace(R_gt R^T) \u2212 1)/2).\nTranslation error can be measured as the angle between the estimated and ground-truth translation directions:\n\u03b5t = arccos((t_gt^T t)/(||t_gt||\u00b7||t||)).\nTranslation is reported as an angular error because monocular translation is only recoverable up to an unknown scale, so Euclidean distance is not meaningful without scale alignment.",
      "source_document": "papers/2512.19110v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a 3-view relative-pose pipeline evaluated on real driving sequences with many mismatched feature tracks, how can RANSAC be set up consistently across different solvers (i.e., what residual is used to score inliers, how is the winning hypothesis selected, and is any post-RANSAC nonlinear refinement applied), and what summary statistic is used to report the final pose errors?",
      "answer": "Use a common RANSAC wrapper for all solvers where inlier residuals are computed with the Sampson error and the same inlier threshold. The final pose is taken as the *initial* hypothesis that yields the largest inlier set (no subsequent optimization/refinement is applied after RANSAC). Performance is reported using median pose errors (median rotation and median translation error).",
      "source_document": "papers/2512.19110v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "Suppose you decompose an MLLM\u2019s last-layer, mean-pooled retrieval embedding with a sparse autoencoder into a dictionary of \u201cconcept\u201d atoms, and you can compute a per-concept retrieval attribution score measuring how much each concept influences image\u2013text similarity. How can you turn the top 1% highest-attribution concepts into a *training-free* embedding transformation before doing nearest-neighbor retrieval (include how the subspace is constructed and removed), and what does the resulting recall improvement imply about the role of these similarity-dominant components in MLLM retrieval?",
      "answer": "Construct a sub-dictionary DR consisting of the dictionary atoms whose retrieval attribution scores are in the top 1%. To capture the subspace S spanned by these high-influence concepts, perform SVD on DR (DR = U\u03a3V\u1d40) and take the top-r right singular vectors Vr as an orthonormal basis for S. For an embedding h, compute its projection onto this subspace \u03a0S(h)=VrVr\u1d40h, then remove these components by forming the residual embedding h\u0303 = h \u2212 \u03a0S(h); normalize h\u0303 and use it directly for retrieval. Empirically, removing this high-attribution subspace substantially increases recall, implying that the features contributing most to the dot-product similarity in raw MLLM embeddings act as distractors\u2014boosting similarity magnitude regardless of semantic relevance and overshadowing the truly discriminative features needed for accurate multimodal retrieval.",
      "source_document": "papers/2512.19115v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using a mean-pooled last-layer embedding from a multimodal LLM for zero-shot image\u2013text retrieval, how can you test whether retrieval is actually driven by visual tokens versus textual prompt tokens, and what qualitative retrieval outcome should you observe when you (i) mask out the hidden states for image tokens before pooling and (ii) mask out the hidden states for the user-prompt text region before pooling?",
      "answer": "Do two counterfactual re-embeddings before the mean-pooling step: (1) set (mask out) the last-layer hidden states corresponding to the image-token positions to zero (or remove them) and then recompute the mean-pooled embedding; (2) similarly mask out the hidden states in the user-prompt text span and recompute the pooled embedding. If retrieval is text-dominated, masking image tokens should produce only marginal changes in retrieval performance, whereas masking the user prompt region should cause a sharp drop in retrieval performance\u2014indicating that the pooled embedding (and thus similarity) is primarily determined by textual semantics rather than visual information.",
      "source_document": "papers/2512.19115v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an end-to-end autonomous-driving planner that decomposes planning into (i) target region localization, (ii) spatial path planning, and (iii) temporal trajectory prediction, how can the \u201ctarget\u201d be modeled to capture uncertainty, what training loss is used for this target module, and how is the learned uncertainty subsequently used inside the local iterative refinement that fuses global and local features?",
      "answer": "Model the target as a probabilistic 2D region (not a single point) using a Laplace distribution parameterized by a predicted center and scale: (\u03bc,b)=MLP(Q_target), where \u03bc\u2208R^2 is the region center and b\u2208R^2 is the scale/extent capturing uncertainty/scene complexity. Train it with a Laplace negative log-likelihood loss: L_{Laplace-NLL}=log(2b)+||y\u2212\u03bc||_1 / b. The predicted scale b is then reused as an uncertainty conditioning signal in iterative refinement: b is embedded (e.g., via an MLP) and concatenated with sampled local features, global query features, and planning state, so feature fusion is modulated by the estimated uncertainty (larger b \u2192 more cautious/adaptive fusion).",
      "source_document": "papers/2512.19133v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a latent world-model driving planner that adds an RL fine-tuning stage for safety, how can a deterministic trajectory-regression head be reformulated into a stochastic policy suitable for GRPO, what collision-aware reward signal is used, how is the per-timestep relative advantage computed under the \u201ctrajectory increments cause error accumulation\u201d assumption, and what final RL loss is optimized (including the KL regularization term to a reference policy)?",
      "answer": "The RL stage first \u201cgaussianizes\u201d the trajectory output to turn a regression model into a stochastic policy: the predicted trajectory is treated as the Gaussian mean \\(\\mu_\\theta\\), and an auxiliary variance network predicts the trajectory covariance/variances \\(\\Sigma_\\theta\\), so the policy \\(\\pi_\\theta\\) samples trajectories from this Gaussian distribution.\n\nSafety is driven by a collision-aware reward defined from distances between the ego bounding box and nearby agents\u2019 bounding boxes along the trajectory: collisions (negative distances) receive a penalty, while non-collisions receive zero reward, i.e. \\(r=-1\\) if a collision happens and \\(r=0\\) otherwise.\n\nGRPO is applied by sampling a group of \\(G\\) trajectories and computing a normalized per-point relative reward within the group,\n\\[\\tilde r^{(i)}_j=\\frac{r^{(i)}_j-\\mathrm{mean}(r_j)}{\\mathrm{std}(r_j)}\\]\nwhere \\(r_j\\) denotes the set of rewards at trajectory point \\(j\\) across the group. Because the trajectory is represented as time-differential increments (so an increment affects the current and all later points), the relative advantage for point \\(j\\) is accumulated over future points:\n\\[\\mathrm{Adv}^{(i)}_j=\\sum_{t\\ge j}\\tilde r^{(i)}_t.\\]\n\nThe GRPO objective uses a PPO-style clipped ratio term with a KL penalty to a reference (the pretrained) policy. The KL is computed between the Gaussian policy and a deterministic reference mean \\(\\mu_{\\mathrm{ref}}=T_{\\mathrm{traj}}^{\\mathrm{ref}}\\) via the Gaussian NLL form,\n\\[D_{KL}=\\tfrac12\\big[\\log|\\Sigma_\\theta|+(\\mu_{\\mathrm{ref}}-\\mu_\\theta)^T\\Sigma_\\theta^{-1}(\\mu_{\\mathrm{ref}}-\\mu_\\theta)+2\\log(2\\pi)\\big].\\]\nFine-tuning minimizes the combined RL loss\n\\[L_{RL}=-J(\\theta)+\\lambda D_{KL},\\]\nwhere \\(J(\\theta)\\) is the GRPO clipped objective (with a KL term weighted inside the objective for stability) and \\(\\lambda\\) weights KL regularization in the final loss.",
      "source_document": "papers/2512.19133v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a camera-only latent world model for end-to-end driving that needs stronger 3D spatial awareness (without explicit depth supervision), how can a frozen vision\u2013geometry foundation model be integrated into the world encoder (what geometric tokens are extracted and how are they fused with 2D backbone features), how is semantic information injected using vision\u2013language pseudo-labels, and what pretraining loss terms supervise these geometric/semantic and world-modeling components?",
      "answer": "A frozen visual-geometry model (VGGT) is used as a 3D spatial encoder to provide multi-view-consistent geometric priors from the surround-view RGB inputs. From VGGT\u2019s final layer, the encoder extracts tokens {t_c, t_r, t_3D} (camera, register, and 3D tokens). The 3D tokens t_3D are then fused into the 2D image backbone features F_t using a lightweight single cross-attention layer, with F_t as queries and t_3D as keys/values, producing the unified spatial-aware latent representation W_latent = C-A(F_t, t_3D), improving spatial understanding without explicit 3D inputs.\n\nSemantic awareness is injected by running Grounded-SAM to generate pseudo semantic labels (semantic masks, derived from prompted object-of-interest boxes/masks with only high-confidence labels kept), and supervising the latent representation with a cross-entropy semantic loss L_sem.\n\nPretraining combines: (i) an MSE reconstruction/world-prediction loss L_rec that predicts the next-step latent world representation from the current latent and planned trajectory, (ii) the semantic cross-entropy loss L_sem from pseudo-labels, (iii) a target-region localization loss L_target (Laplace negative log-likelihood for the probabilistic target region), and (iv) an L1 trajectory loss L_traj aligning predicted spatial path and temporal trajectory to expert demonstrations; these are summed as L_total = \u03b1 L_sem + \u03b2 L_rec + \u03b3 L_target + \u03b7 L_traj.",
      "source_document": "papers/2512.19133v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In closed-loop end-to-end driving evaluation on NavSim, how is the PDM Score (PDMS) computed from its constituent metrics (include the weights), and which terms act as multiplicative \u201cgates\u201d versus being combined additively? Explain what those gated terms measure and how they penalize unsafe/infeasible trajectories.",
      "answer": "PDMS is defined as\n\nPDMS = NC \u00d7 DAC \u00d7 (5 \u00d7 EP + 5 \u00d7 TTC + 2 \u00d7 Comf.) / 12,\n\nwhere EP is ego progress, TTC is time-to-collision, and Comf. is ride comfort; these three are combined by weighted summation (with weights 5, 5, and 2 respectively, then normalized by 12).\n\nNC (no at-fault collision) and DAC (drivable area compliance) are multiplicative factors that gate the score: they explicitly penalize unsafe or infeasible trajectories by scaling down the entire score when collisions are likely or the predicted path leaves drivable areas. NC measures the likelihood of collisions between the ego vehicle and other agents, while DAC assesses whether the predicted path remains within drivable areas.",
      "source_document": "papers/2512.19133v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a planning-oriented latent world model that is pretrained without dense perception annotations, how can future latent-world prediction be set up as a self-supervised objective (what does the world decoder take as input and what does it predict), what reconstruction loss is used for this latent prediction, and how is this term combined with the planning and semantic supervision terms (name each loss and give the overall weighted total objective)?",
      "answer": "Future latent-world prediction is trained by using a world decoder to predict the next-step latent world representation \\(\\hat W^{t+1}_{\\text{latent}}\\) from the current latent representation \\(W^{t}_{\\text{latent}}\\) together with the predicted temporal trajectory \\(T_{\\text{traj}}\\). The self-supervised reconstruction term is an MSE loss between the predicted and true next latent representations: \\(L_{\\text{rec}} = \\mathrm{MSE}(\\hat W^{t+1}_{\\text{latent}}, W^{t+1}_{\\text{latent}})\\). This is trained jointly with (i) a semantic cross-entropy loss \\(L_{\\text{sem}}\\) from pseudo semantic masks, (ii) a target-region negative log-likelihood loss \\(L_{\\text{target}}\\) (Laplace NLL), and (iii) an \\(L_1\\) imitation loss \\(L_{\\text{traj}}\\) aligning both the spatial path \\(T_{\\text{path}}\\) and temporal trajectory \\(T_{\\text{traj}}\\) to expert demonstrations, using the weighted sum: \\(L_{\\text{Total}} = \\alpha L_{\\text{sem}} + \\beta L_{\\text{rec}} + \\gamma L_{\\text{target}} + \\eta L_{\\text{traj}}\\).",
      "source_document": "papers/2512.19133v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a teacher\u2013student setup for online HD map construction where the teacher has access to future temporal context but the deployed student only sees the current frame, how is the training objective constructed to transfer \u201cfuture priors\u201d at both the BEV-feature level and the query/decoder level (including what supervision generates the spatial mask, how queries are aligned, what divergence is used, and how these distillation terms combine with the base mapping losses)?",
      "answer": "The total objective is the sum of the standard mapping loss and two distillation losses. The base loss is the same as MapTracker: \n- Lbasic = LBEV + LVEC + Ltrans.\n\nDistillation is applied at two levels:\n1) BEV-feature distillation uses a ground-truth-guided spatial mask derived from the auxiliary segmentation ground truth to suppress background regions. A masked feature regression loss is used between teacher and student BEV feature maps (applied to both basic and refined BEV representations), i.e., a mask-weighted squared error over spatial locations.\n2) Query/decoder distillation handles temporal query asymmetry by first performing one-to-one matching from the student\u2019s fixed set of queries to the teacher\u2019s (larger, dynamically tracked) query set using the Hungarian algorithm; then it distills the matched pairs\u2019 output logits with KL divergence:\n- LlogitsKD = sum_i LKL(LogitsS[i] || LogitsT[\u03c3\u0302(i)]).\n\nThe distillation loss is Ldistill = Lfeat + LlogitsKD, and the overall loss is Lall = Lbasic + Ldistill (with all loss-term weights set uniformly to 1.0 in the experiments).",
      "source_document": "papers/2512.19150v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating online vector HD map construction for autonomous driving, how can you adapt the standard map mAP to explicitly measure the model\u2019s quality in the *road ahead* versus *behind* the ego vehicle, and what are the main computation steps that remain the same as global mAP once the region is split?",
      "answer": "Define two half-region metrics by splitting the region of interest into two halves relative to the ego vehicle\u2019s heading: an ahead region and a rear region. Compute Ahead-mAP (A-mAP) by evaluating the matching between predicted and ground-truth map elements only within the ahead half, and compute Rear-mAP (R-mAP) analogously within the rear half. Within each half-region, the calculation follows the same pipeline as global mAP: spatial alignment, vector matching, true-positive identification, precision\u2013recall calculation, and average-precision computation.",
      "source_document": "papers/2512.19150v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In query-based online vector HD map models, a temporal teacher may carry a larger, dynamically tracked query set while a single-frame student uses a fixed set of learnable queries, making direct query-to-query distillation ill-posed. What strategy can be used to transfer query-level knowledge in this asymmetric setting (how correspondence is established and what is distilled), and what do ablations show about why more naive query-matching/distillation choices (e.g., distilling embeddings via heuristic or Hungarian assignment) tend to hurt performance?",
      "answer": "Use a matching-based asymmetric query distillation: establish a one-to-one correspondence from the student\u2019s fixed NS queries to the teacher\u2019s larger NT query set via Hungarian assignment, then distill the teacher\u2019s *output logits* onto the matched student queries using a KL-divergence loss computed only over matched pairs. This design targets the temporally inconsistent/dynamic nature of the teacher\u2019s tracked queries while keeping the student\u2019s static queries trainable.\n\nAblations indicate that naive correspondence strategies fail to create correct student\u2013teacher pairing and can cause a large accuracy drop (e.g., applying MapDistill-style distillation to dummy queries or selecting Top-K teacher queries). Even when using Hungarian assignment, distilling *query embeddings* is sensitive to the embedding loss choice (cosine or MSE) and still degrades performance, whereas applying the distillation on logits with KL gives the best overall results among the compared query-level distillation variants.",
      "source_document": "papers/2512.19150v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using downstream motion forecasting/planning sensitivity to motivate an ahead-focused evaluation for online vector HD mapping, how can you design a controlled perturbation experiment that isolates errors in the map region ahead of the ego vehicle versus behind it (what perturbation is applied to the mapper\u2019s vector outputs, what downstream model/metrics are used, and what outcome demonstrates the forward region is substantially more critical than the rear region)?",
      "answer": "A controlled way is to take an online mapper\u2019s vectorized map output (e.g., MapTR) and apply directional masking along the ego vehicle\u2019s longitudinal axis: for a chosen mask ratio, remove/zero the predicted map elements in either the forward half of the ROI (ahead-mask) or the backward half (rear-mask) to simulate degraded map quality confined to that region. Feed the perturbed maps into a standard trajectory prediction model (HiVT) and evaluate forecasting quality with minADE, minFDE, and Miss Rate (MR). The key outcome is that increasing the forward-mask ratio causes a clear, monotonic deterioration in all downstream metrics (e.g., at full forward masking, errors and MR rise notably), while masking the rear region has negligible impact and can even slightly improve MR at some ratios\u2014showing the downstream task depends far more on accurate mapping ahead than behind.",
      "source_document": "papers/2512.19150v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a cycle-consistent framework that jointly trains chart generation (table+NL query \u2192 schema), chart parsing (image \u2192 schema and image+schema \u2192 visualization-level table), and ChartQA (image+question \u2192 answer), how is the overall training loss constructed across tasks, and what sampling schedule is used to balance consistency-oriented training against QA so the model learns stable generate\u2013parse alignment while still improving reasoning?",
      "answer": "All four tasks are trained with the same token-level cross-entropy supervision, and the overall objective is a simple unweighted sum of the task losses: L = L_NL2Chart + L_Schema + L_Data + L_QA (with L_NL2Chart = CE(s\u0302, s), L_Data = CE(T\u0303_vis, T\u0302_vis), and L_QA = CE(\u00e2, a), plus the analogous schema-parsing CE loss). Training iterations are sampled from two groups\u2014(i) consistency-related tasks (NL2Chart, schema parsing, data parsing) and (ii) ChartQA\u2014with roughly 80% of iterations allocated to the consistency tasks and 20% to QA; no extra loss weighting is applied.",
      "source_document": "papers/2512.19173v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a lifecycle-aligned chart benchmark that supports both chart generation and chart-to-table/QA supervision, how can you obtain the ground-truth visualization-level table (the transformed data actually encoded in the marks) from a Vega-Lite chart specification, and how is that table then used to supervise chart data parsing and to generate ChartQA pairs without relying on LLM-generated answers?",
      "answer": "Execute (compile and render) the Vega-Lite specification and intercept the transformed data produced by the Vega-Lite execution/runtime pipeline (implemented via Altair as a front-end to the Vega-Lite compiler). This extracted transformed dataset is treated as the visualization-level table T_vis (the exact values used for mark rendering), which provides precise supervision for the chart data parsing task (image + schema \u2192 T_vis). The same T_vis is then used to programmatically generate ChartQA question\u2013answer pairs: LLMs may be used only to produce natural-language question templates, but the answers are computed deterministically from T_vis to avoid hallucination.",
      "source_document": "papers/2512.19173v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating chart data parsing as chart-to-table extraction, how is the final per-example score computed from a predicted visualization-level table and the ground-truth table (including the normalization steps, how rows are aligned via key matching/fuzzy matching, the numeric vs text value-matching tolerances, and how these components are combined into a single dataset-level metric)?",
      "answer": "Both predicted and ground-truth visualization-level tables are treated as CSV-like text and normalized by (i) decoding escaped newlines, (ii) stripping accents and redundant whitespace, and (iii) removing duplicated header lines (treating the first line as the header). Non-header rows are then sorted by a key column (the first column), yielding predicted and gold key sets K_pred and K_gold. Key-level precision/recall/F1 are computed from the key overlap; exact key matches use normalized string equality, with remaining unmatched keys optionally aligned using fuzzy matching with Levenshtein similarity \u2265 0.85.\n\nValue comparisons are performed only for aligned rows. For each aligned cell pair (a, b), numeric values are counted correct if |a \u2212 b| \u2264 max(\u03b4_abs, \u03b4_rel\u00b7|b|), with \u03b4_rel = 0.10 and \u03b4_abs = 0.02 for plain numbers (and \u03b4_abs = 2.0 for percentages). If numeric parsing fails, values fall back to text matching using normalized equality or token-level Jaccard similarity > 0.90. Let Acc_cell be the fraction of cell pairs accepted under these rules.\n\nThe per-example score is s_i = 0.5\u00b7(F1_k,i + Acc_cell,i), and the dataset-level score is the mean of s_i over all evaluated examples.",
      "source_document": "papers/2512.19173v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When constructing supervision targets for chart schema parsing from an image, what parts of a chart specification should be omitted because they are not visually inferable, and what does the resulting target schema represent for training/evaluation?",
      "answer": "Omit the specification\u2019s data-transform operations (e.g., grouping and aggregation) because they change intermediate data but are not visually observable from the rendered chart and thus cannot be reliably inferred from the image alone. The target schema is therefore the minimal Vega-Lite visual structure\u2014i.e., the necessary chart encodings/structure needed to describe the chart for accurate parsing supervision.",
      "source_document": "papers/2512.19173v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking NL-to-chart models that output Vega-Lite JSON, what evaluation protocol can jointly measure (1) spec-level textual correctness, (2) rendered visual fidelity/semantic alignment, and (3) whether the output is executable\u2014and how are the JSON strings normalized and non-renderable outputs handled in scoring?",
      "answer": "Use three complementary metrics: (1) textual similarity computed as ROUGE on the raw specification strings, with both JSON specs normalized (e.g., key sorting) before scoring to remove formatting noise, and reporting ROUGE-L recall to check coverage of essential ground-truth components without penalizing extra descriptive content; (2) visual similarity by rendering both the generated and ground-truth Vega-Lite specs to same-size images and computing PSNR (pixel fidelity), MS-SSIM (multi-scale structural consistency), and CLIP (perceptual/semantic alignment), assigning a similarity score of zero if a spec cannot be rendered; and (3) validity as the percentage of generated specifications that can be successfully rendered into a chart without errors.",
      "source_document": "papers/2512.19173v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a benchmark where ImageNet-pretrained CNN backbones are fine-tuned to classify pavement obstacles at three label granularities (category, subcategory, and obstacle type), which architectures achieve the top test accuracy at each granularity when training with (a) all layers frozen except the last vs (b) no layers frozen, and what overall pattern is observed about the effect of freezing on performance across these taxonomy levels?",
      "answer": "Using the PEDESTRIAN balanced subset, the best-performing models differ by taxonomy level and by whether layers are frozen:\n\n- Obstacle type (29-way): with frozen layers, ConvNeXt-Small reaches 99.96% test accuracy; with no freezing, EfficientNetV2-S reaches 99.89%. Freezing has negligible impact at this level.\n- Category: with no freezing, EfficientNet-B0 reaches 99.90%; with frozen layers, ConvNeXt-Large reaches 99.61%. Allowing layers to remain unfrozen generally improves category accuracy.\n- Subcategory: with no freezing, EfficientNetV2-S reaches 99.92%; with frozen layers, ConvNeXt-Large reaches 99.86%. Allowing layers to remain unfrozen generally improves subcategory accuracy.\n\nOverall pattern: freezing vs not-freezing matters little for obstacle-type classification, but for the coarser category and subcategory tasks, leaving layers unfrozen typically yields better performance.",
      "source_document": "papers/2512.19190v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When constructing a class-balanced training set of still frames from obstacle videos (one class per obstacle type), what sampling rule is used to decide between uniformly random frame selection versus sampling at regular temporal intervals, and how are the per-class target count and total balanced-set size determined in this dataset\u2019s benchmark setup?",
      "answer": "Frames are sampled per obstacle type by targeting a fixed count c: if the available frames n for that obstacle satisfy 1 \u2264 n/c < 2, frames are selected uniformly at random; if n/c \u2265 2, frames are selected regularly at intervals of \u230an/c\u230b starting from the first available frame (index 0 across the concatenated frames for that obstacle type). The benchmark uses c = 500, chosen to match the minimum available frames among obstacle types (Crowded Pavement has n = 694), producing a balanced subset of 29 \u00d7 500 = 14,500 images.",
      "source_document": "papers/2512.19190v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking many ImageNet-pretrained backbones on a balanced egocentric pavement-obstacle frame subset across three label granularities (category/subcategory/obstacle type) and two layer-freezing regimes, how is the data split and run-to-run variability handled to enable fair, comparable results across architectures and settings?",
      "answer": "The benchmark uses a random split of the balanced frame subset into 70% training, 20% validation, and 10% testing before fine-tuning. To control variability and make comparisons fair, each (architecture \u00d7 taxonomy level \u00d7 freezing setting) combination is repeated 5 times with different random seeds for the split, and the same fixed sequence of random seeds is reused across all combinations to allow one-to-one comparisons.",
      "source_document": "papers/2512.19190v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a benchmark that fine-tunes many different ImageNet-pretrained backbones on the same egocentric pavement-obstacle dataset, how can you standardize the input pipeline so that each architecture sees images in the input format it was pretrained with, and is this preprocessing applied at training time, test time, or both?",
      "answer": "Use the ImageNet-1K pretrained weights from PyTorch for each backbone and apply the default pre-processing transforms specified by that backbone\u2019s PyTorch \u201cmodel recipe\u201d (i.e., the recipe-associated input transforms) to the images. These default recipe transforms are applied consistently during both training and testing.",
      "source_document": "papers/2512.19190v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a data-free continual self-supervised pre-training setup that uses masked image modeling (MIM), how can model inversion be formulated to synthesize replay images while (i) preserving the previous task\u2019s learned feature distribution, (ii) suppressing inversion artifacts, and (iii) preventing mode collapse without class labels\u2014and what generator design choice is used to better recover high-frequency details during this inversion?",
      "answer": "The inversion optimizes a randomly initialized generator (InvUNet) and its noisy latent codes to minimize a composite inversion objective:\n\n- **Task-oriented SSL loss (Ltask)**: uses the same SSL objective as pre-training, instantiated as MIM reconstruction error on masked regions, i.e., predicting the masked content from the unmasked input and measuring reconstruction error only inside the mask.\n- **Feature distribution regularization (Lnorm)**: aligns synthetic images with the original data by matching batch-wise feature statistics (mean/variance) extracted from the frozen previous model, constraining generated samples to replicate the learned feature distributions.\n- **Image prior loss (Limg)**: a total-variation prior that penalizes differences between neighboring pixels (and additionally depth-wise neighbors for 3D), which promotes spatial smoothness and suppresses high-frequency artifacts/noise.\n- **Repulsive representation learning (Lrep)**: enforces diversity by extracting features for a batch of synthetic images using the frozen previous encoder and minimizing their squared cosine similarity to features stored in a persistent synthetic feature pool; this pushes samples to uniformly cover feature space and mitigates mode collapse even without class guidance.\n\nThese terms are combined as: LInv = Ltask + \u03b1norm\u00b7Lnorm + \u03b1img\u00b7Limg + \u03b1rep\u00b7Lrep.\n\nTo better recover high-frequency details during inversion, the generator is **InvUNet**, a dual-stream U-Net\u2013inspired architecture that **injects the latent z at the network bottleneck** (creating an information bottleneck) and uses a **Memory Cache Branch** to produce multi-scale structural priors that are fed via **skip connections** to the main Inversion Branch, enabling multi-scale fusion and improved fine-detail reconstruction compared to direct bottom-to-top projection.",
      "source_document": "papers/2512.19213v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In continual multi-modal self-supervised pre-training with replay (real or synthetic), what qualitative trend occurs as the replay buffer size increases (e.g., 1% \u2192 5% \u2192 10%), and how do inversion-synthesized replay samples compare to cluster-selected real replay at very small buffer sizes\u2014what underlying explanation is given for this behavior?",
      "answer": "As the replay buffer grows (from 1% to 5% to 10%), performance does not monotonically improve; larger buffers tend to intensify modality conflicts and increase computational cost, which can hinder learning on the current modality. With very small buffers (1%), inversion-synthesized samples perform better than the cluster-selected real replay subset; at 5% and 10% they match the replay method. The explanation is that when only a few samples are kept, inverted synthetic images can better capture the original data distribution (and diversity) than clustered subsets, which may inadequately represent diversity at small buffer sizes.",
      "source_document": "papers/2512.19213v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In data-free continual self-supervised pre-training with a BatchNorm-free backbone (e.g., ViT), how can feature-distribution regularization for model inversion be implemented without relying on BN running statistics\u2014specifically, how are the per-task target statistics computed from Transformer features, over which dimensions are mean/variance taken, and how are these statistics used during inversion?",
      "answer": "Instead of using BatchNorm running statistics, per-task targets are computed as batch-wise feature statistics from the trained model at the end of the final epoch for each task. The model is run once over the entire task dataset in evaluation mode, and statistics are extracted from Transformer encoder block feature maps. For a feature tensor with shape (B, L, D), the mean and variance are computed across the batch dimension B (aggregated over the full dataset via a running/streaming update), yielding per-layer targets {\u03bct, \u03c3t}. During inversion, these stored per-layer statistics are used as targets in the feature-matching loss Lnorm to constrain the generator so that synthetic images match the learned feature distribution of the original task.",
      "source_document": "papers/2512.19213v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a skeletal-representation hippocampal morphometry pipeline, how can longitudinal shape change be modeled so that true tissue loss (and potential vanishing of local thickness) is captured instead of preserving a forced point-to-point surface correspondence across time, and how does the model distinguish atrophy from growth in terms of skeletal surfaces and spoke lengths?",
      "answer": "Longitudinal follow-up scans are modeled within the same individualized skeletal coordinate system established at baseline by directly deforming the baseline skeletal representation, rather than independently reparameterizing the full surface at each time point (which would preserve correspondences even in regions that have biologically atrophied). Atrophy is represented as a controlled inward erosion of the skeletal surfaces coupled with spoke shortening, allowing local thickness to shrink toward (and in severe cases reach) zero when a spoke degenerates to a single point. Growth is represented with skeletal surfaces remaining stable while spoke lengths increase, producing increased thickness within the same stable coordinate framework.",
      "source_document": "papers/2512.19214v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When fitting a skeletal representation (s-rep) to hippocampal subfield surfaces, how can spoke refinement be formulated to simultaneously enforce (i) boundary convergence, (ii) spoke orthogonality to the boundary, and (iii) non-intersection/geometric regularity\u2014specifically, what are the three penalty terms used, and in what sequence are they applied/iterated to produce a geometrically valid spoke field?",
      "answer": "Spoke refinement is posed with three penalties tied to the required s-rep constraints:\n\n1) Boundary convergence penalty (\u03c60): for each spoke, measure how far its implied boundary endpoint p lies from the closest surface vertex v on the subfield boundary, i.e., the minimum Euclidean distance \u2016p\u2212v\u2016 (with v chosen as the nearest boundary vertex). This term is enforced strictly by projecting any implied boundary points that fall outside the surface back onto the boundary so endpoints align to the surface.\n\n2) Orthogonality penalty (\u03c61): penalize angular deviation between the spoke unit direction u and the unit surface normal n at the nearest boundary vertex, defined as \u03c61 = 1 \u2212 cos(\u03b8) = 1 \u2212 (u\u00b7n). Spoke directions are optimized to reduce this deviation.\n\n3) Non-intersection / medial-geometry regularization (\u03c62): measure asymmetry between the superior and inferior spokes at each medial point as the absolute difference in their lengths (|\u2016s_sup\u2016 \u2212 \u2016s_inf\u2016|). This is applied to correct spokes whose directions became problematic during \u03c61 optimization, improving local non-intersection and geometric regularity.\n\nOptimization sequence: first enforce \u03c60 (align endpoints to the surface), then optimize directions to minimize \u03c61; iterate the \u03c60\u2192\u03c61 steps for k rounds (empirically k\u22482\u20133). After the last iteration, apply \u03c62 to correct large angular deviations and improve non-intersection, then apply a final \u03c60 correction to restore accurate surface alignment after orientation updates.",
      "source_document": "papers/2512.19214v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking hippocampal shape models for anatomically meaningful pointwise correspondence (both cross-subject and along a simulated atrophy trajectory), what ground-truth supervision and error metric can be used, and what quantitative correspondence-error patterns distinguish a skeletal-representation approach from cm-rep, ds-rep, and SPHARM-PDM\u2014including the typical failure mode observed for SPHARM-PDM and why some methods drift under localized atrophy?",
      "answer": "Use manually annotated anatomical landmarks as ground truth and measure correspondence/alignment error as the Euclidean distance between the method\u2019s predicted corresponding points and the manually placed landmarks on the reference (ground-truth) surfaces.\n\nCross-sectional evaluation: five anatomically identifiable landmarks are manually placed per case (anterior/posterior extremities, medial folding apex, lateral curvature center, etc.) on both ground-truth and reconstructed surfaces; errors are Euclidean distances at these landmarks. The skeletal-representation pipeline achieves the lowest errors, with a maximum mean landmark error of about 1.66 mm (at the anterior folding apex) and the other landmark locations staying below ~1.1 mm on average (SD < 1 mm). In contrast, cm-rep and ds-rep have larger maximum landmark errors (about 4.64 mm and 3.41 mm, respectively), and SPHARM-PDM shows the largest variability and error (maximum ~9.97 mm), attributed to pole flipping.\n\nLongitudinal correspondence evaluation: simulate progressive atrophy by iteratively eroding a localized anterior region to create multiple follow-up surfaces (t1\u2013t3) while preserving the posterior region; place three landmarks within the atrophic region at each time point and compute Euclidean landmark correspondence errors over time. The skeletal-representation method has the smallest drift (mean ~1.68 mm across four time points; maximum ~2.81 mm at the most severe time point), whereas cm-rep shows large drift (mean ~10.57 mm; max ~20.48 mm), ds-rep is intermediate but can still drift severely (mean ~4.65 mm; max ~20.07 mm), and SPHARM-PDM also drifts substantially (mean ~3.56 mm; max ~16.87 mm). Qualitatively, competing methods exhibit non-local displacement from global reparameterization\u2014failing to confine deformation to the eroded region\u2014while the skeletal-representation approach maintains spatial specificity and can represent advanced collapse via near-zero spoke lengths.",
      "source_document": "papers/2512.19214v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an AD biomarker study that uses hippocampal substructure morphometrics (e.g., lamella-wise thickness/width features) rather than volumes, what feature-selection and modeling pipeline can be used to (i) control for demographic covariates and multiple comparisons, (ii) select clinically relevant features, and (iii) evaluate both cross-sectional discrimination and longitudinal conversion risk in an external cohort?",
      "answer": "A robust pipeline is:\n\n1) **Covariate control + multiple-comparison control for initial screening**: apply w-score normalization while using **age, sex, and education** as covariates, then test for **significant group differences** (effect size via **Cohen\u2019s d**) with **permutation testing** and **FDR correction**.\n\n2) **Clinical relevance filtering**: retain only features that also show **significant (reported as positive) correlations with cognitive decline** (Pearson correlation).\n\n3) **Model-based importance filtering**: further retain features with **positive importance scores in a random-forest classifier**.\n\n4) **Cross-sectional evaluation**: quantify discrimination (e.g., A\u2212/CU vs A+/CI) using **ROC/AUC** across feature sets, and validate the selected features in an independent cohort via **bidirectional cross-testing and internal cross-validation**.\n\n5) **Longitudinal conversion prediction**: train a **regularized logistic regression** with an **elastic-net penalty** (\u03b1 = 0.5) and tune the regularization strength with **5-fold cross-validation**; assess prediction with **AUC** and evaluate prognostic stratification with **Kaplan\u2013Meier survival analysis** and **log-rank tests**, using the **median predicted risk** to split participants into high- vs low-risk groups.",
      "source_document": "papers/2512.19214v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a skeletal (s-rep) morphometry pipeline from voxel-wise hippocampal subfield segmentations, what mesh-refinement strategy can be used to convert irregular, topologically inconsistent label boundaries (e.g., holes and disconnected fragments) into watertight, numerically stable surfaces suitable for downstream diffeomorphic registration and spoke refinement\u2014and what are the main stages of this refinement procedure?",
      "answer": "A practical strategy is a three-stage boundary cleanup/refinement pipeline (here termed a Robust Boundary Optimization Scheme, RBOS) applied to each subfield surface extracted from the voxel labels:\n1) Connected-component filtering: decompose the raw mesh into connected components and keep only the largest anatomically plausible component (by vertex count) to remove spurious fragments.\n2) Shrink-wrapping to enforce valid topology: apply shrink-wrapping to obtain watertight surfaces while preserving local geometry, including automated hole filling and adaptive surface offsetting; control mesh resolution via a target edge length chosen proportional to subfield complexity (finer for dentate gyrus, coarser for subiculum).\n3) Uniform remeshing for numerical stability: remesh uniformly with a quadrilateral-based scheme to improve face regularity, choosing the target quad count per subfield to balance anatomical detail and computational cost, then export as triangulated faces for compatibility with subsequent morphometric processing.\nBecause independently refined subfield boundaries may not align perfectly, whole-hippocampus modeling can instead combine the subfield labels and shrink-wrap them into a single merged object.",
      "source_document": "papers/2512.19214v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When fine-tuning a transformer VLM for screen/UI grounding using Image-LoRA (i.e., adapting only the attention value path on visual tokens), what supervision signal and loss are used for the click-point output, how is accuracy computed at evaluation time, and what part of the sequence computation is avoided relative to standard LoRA that makes adapter-only training FLOPs scale roughly with the visual-token fraction (Tv/T)?",
      "answer": "The grounding target is formatted as a single-line string like `point 2d:[x, y]`, and training optimizes cross-entropy over this output string. Evaluation reports accuracy as the fraction of predicted (x, y) points that fall inside the ground-truth bounding box in the model\u2019s effective pixel space. Efficiency comes from restricting the LoRA updates/computation to the visual-token span (adapting V only on those tokens), which avoids adapter computation through the long input text prompt and the output answer tokens; thus adapter-only training FLOPs are reduced roughly in proportion to the visual-token fraction Tv/T (and scale with image-token count rather than total sequence length).",
      "source_document": "papers/2512.19219v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When doing parameter-efficient fine-tuning of a transformer VLM by adding LoRA-style adapters only on the value projections of a subset of attention heads, how can the \u201cmost useful\u201d heads be selected efficiently without per-head ablations, how is head diversity incorporated into the selection, and what normalization is applied so that layers selecting more heads don\u2019t receive disproportionately large update magnitudes?",
      "answer": "Heads are ranked using a first-order, single-pass influence estimate: attach a lightweight rank-1 Image-LoRA factorization to every head (initialize the shared A with Gaussian noise, set each head\u2019s B(h) to zero, use \u03b3=1, r=1), run one forward\u2013backward pass with the task loss \u2113, and score each head by an empirical-Fisher / Taylor sensitivity metric I(h)=\u2016\u2207_{B(h)}\u2113\u2016^2_F. A layer\u2019s aggregate importance is \u03a6_L=\u2211_{h\u2208L} I(h), which is converted into a per-layer allocation weight p_L=(\u03a6_L)^\u03c4 / \u2211_{L\u2032}(\u03a6_{L\u2032})^\u03c4 (\u03c4 controls how concentrated the allocation is), and then heads are selected within each layer under the allocated budget.\n\nTo encourage diversity beyond pure importance, a diversity factor is computed from the elementwise-squared gradients F(h)=(\u2207_{B(h)}\u2113)\u2299(\u2207_{B(h)}\u2113), and selection balances importance with this diversity signal.\n\nBecause different layers may end up with different numbers of chosen heads N_chosen, a selection-size normalization is applied to stabilize layerwise update magnitude: scale the summed head increments by \u03bd(N_chosen)=1/\u221aN_chosen, motivated by a variance-preserving argument so that the variance/energy of \u2211_{h\u2208H_chosen}\u0394V(h) does not grow with the number of selected heads.",
      "source_document": "papers/2512.19219v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a transformer VLM where you restrict LoRA-style updates to *only* the visual-token positions within each attention layer, which attention projections (Q, K, V, O) can still change the output representations of *text* tokens in that layer, and why? Also, what empirical finding motivates adapting V-only rather than updating K (or K+V) under this visual-token-only constraint?",
      "answer": "Within a single attention layer, modifying the Q or O representations at visual-token positions does not affect the layer outputs at text-token positions, because the output for a token i depends on that token\u2019s own query q_i together with the shared keys and values; changing q_j or o_j for a different position j in the visual span has no effect on o_i for a text position i. In contrast, updating K and/or V at visual-token positions can affect text-token outputs, since text tokens attend over the shared K and V from all positions, so changing the keys/values of visual tokens changes what text tokens can attend to and retrieve. Empirically, ablations found that adapting only the value path V on visual tokens gives the best performance, while applying the modification to K (changing the attention probability distribution) consistently degrades performance; this supports choosing V-only under the visual-token-only update design.",
      "source_document": "papers/2512.19219v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a vision\u2013language model across different text:image token ratios while keeping the text prompt/template fixed, how can the image preprocessing be adjusted to hit a target ratio \\(\\rho=T_{text}/T_v\\) under patch-grid image tokenization, and what practical constraints prevent achieving an exactly fixed ratio for every sample?",
      "answer": "Adjust the processed image resolution (not the text) so the number of visual tokens \\(T_v\\) changes monotonically with image area, then choose a resolution that makes \\(T_v\\approx T_{text}/\\rho\\). For Qwen2.5-VL, images are tokenized on a regular patch grid; with patch size 14 and merge size 2 the effective stride is \\(s=28\\) pixels, so \\(T_v\\approx \\lfloor H/s\\rfloor\\,\\lfloor W/s\\rfloor\\). Given a target \\(T_v^*\\), the method binary-searches the processed longest side (bounded by min/max pixels and quantized to multiples of \\(s\\)) until \\(|T_v-T_v^*|\\le\\epsilon\\), resampling the image while preserving the original aspect ratio as much as possible.\n\nAn exactly fixed ratio per sample is generally impossible because the processed resolution must be quantized to multiples of the stride (e.g., 28 px), so \\(T_v\\) changes in discrete steps; the procedure yields ratios that are very close across samples rather than perfectly identical.",
      "source_document": "papers/2512.19219v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "If you suspect a PEFT method for a vision\u2013language model is overfitting to language priors (answering from the question text rather than the image), how can you design an evaluation using a VQA dataset that provides \u201cprior-following\u201d vs \u201cprior-contradicting\u201d image variants, and what outcome would indicate that adapting only visual tokens (vs adapting all tokens like standard LoRA) helps reduce this shortcutting?",
      "answer": "Use a split where each question has (i) an image whose answer can be inferred from the text alone (\u201cfollow\u201d the language prior) and (ii) images where the correct answer contradicts that prior and therefore requires visual evidence (\u201cagainst\u201d). Fine-tune on the \u201cfollow\u201d split, then evaluate on the corresponding \u201cagainst\u201d images (same questions but out-of-distribution answers). If the visual-token-only adaptation is less prone to language-prior shortcutting, it should achieve higher accuracy on the \u201cagainst\u201d evaluation than standard LoRA that adapts all tokens; in the reported ViLP experiment, standard LoRA performs much worse (5.00%) while the visual-token-only method is notably better (12.67%), supporting reduced reliance on language priors.",
      "source_document": "papers/2512.19219v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a street-view urban-perception model that uses scene-graph embeddings and learns from pairwise human judgments (left-vs-right preferences), what is the preference probability formulation and training loss used for the ranking head, and what fairness controls are applied when comparing against image-only backbones like ResNet-50/ViT/CLIP?",
      "answer": "Each scene embedding is passed through a weight-shared MLP to produce scalar scores s_left and s_right; the probability that the left image is preferred is computed as P(left > right) = \u03c3(s_left \u2212 s_right), and the ranking head is trained with cross-entropy against the true pairwise labels. For fair comparison to image-only models (ResNet-50, ViT-B/16, CLIP ViT-B/32), the baselines use the same pairwise comparison formulation with identical train/val/test splits and loss functions, and their pretrained visual backbones are kept frozen during training.",
      "source_document": "papers/2512.19221v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "How does the method learn a compact scene-level representation from each extracted scene graph without using manual graph labels, and which component of the graph autoencoder is kept for downstream perception prediction?",
      "answer": "It uses GraphMAE for self-supervised representation learning: random portions of the input scene graph are masked and the model is trained to reconstruct the missing graph information, forcing the encoder to capture the underlying semantic/spatial structure. After this pretraining, the decoder is discarded and only the GraphMAE encoder is retained to output a fixed-dimensional scene-level embedding (128-D in the implementation) used for the downstream perception predictor.",
      "source_document": "papers/2512.19221v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using scene-graph embeddings to predict urban perception across cities, what scene-graph characteristics are associated with higher predicted perception scores, and how do these differ from low-scoring cases that may still have many nodes and edges?",
      "answer": "Higher-scoring scenes tend to have (1) richer relational topology (more edges) and, more importantly, (2) greater subject diversity (more node types) with heterogeneous, cross-category relations (e.g., people near fa\u00e7ades, trees aligned with sidewalks, people sitting on benches, bicycles parked beside buildings, plants growing along walls). In contrast, some low-scoring scenes can also contain many nodes and edges, but their relations are concentrated in repetitive, within-category triplets (e.g., car\u2013near\u2013car or road\u2013beside\u2013road), lacking heterogeneity and cross-type links; this indicates that relation variety and cross-type connectivity\u2014not raw graph density\u2014aligns more with favourable perceptions.",
      "source_document": "papers/2512.19221v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an urban-perception pipeline that predicts perceptual attributes from open-vocabulary scene graphs, how are the raw object and relation labels converted into model-ready node/edge features so the downstream graph encoder can handle novel object/predicate categories?",
      "answer": "Each street-view image is converted into a scene graph where nodes are urban entities and edges are semantic relations (object\u2013predicate\u2013object triplets). Because labels are open-set, the object and predicate *texts* are embedded with Sentence-BERT, which maps each textual description (e.g., \u201cbuilding\u201d, \u201con\u201d, \u201csidewalk\u201d) into a high-dimensional semantic vector; these text embeddings are used as the node and edge feature representations fed into the graph representation learner (GraphMAE).",
      "source_document": "papers/2512.19221v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "How can cross-city generalization be evaluated for an urban-perception predictor trained on Place Pulse pairwise comparisons, and what outcome indicates that the learned relational scene-graph embeddings transfer to new cities?",
      "answer": "Cross-city generalization is evaluated by taking a model trained on Place Pulse 2.0 and applying it directly to an independent street-imagery domain (Mapillary) from cities not seen during training\u2014here, Tokyo and Amsterdam were chosen to reduce cultural bias and because they differ in spatial layout, building typology, and streetscape. The Mapillary subsets are manually annotated with the same pairwise-comparison protocol as Place Pulse (annotators pick, for each image pair, which scene better matches a given perceptual dimension such as safety or liveliness). When tested this way, performance shows only a moderate drop (accuracy and AUC decrease slightly), which is interpreted as evidence that the relational embeddings capture higher-level spatial/semantic patterns that remain predictive across different cities.",
      "source_document": "papers/2512.19221v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In multi-center breast DCE-MRI tumor segmentation with nnU-Net, which combination of training-center selection and temporal phase input produced the best validation performance, and what experimental pattern indicates that naively adding more (lower-quality) training data can reduce generalization?",
      "answer": "The best validation performance came from a quality-filtered setup trained on the higher-quality DUKE+NACT cases while integrating early temporal information\u2014using multiple DCE phases 0000\u20130002 as input\u2014reaching a validation Dice of about 0.72. Across experiments, simply expanding from a small DUKE-only training set to mixed-center training (\u2248400 cases) gave minimal gains, and expanding further to the full \u22481200-case multi-center set degraded performance on some test sets; the document attributes this to lower-quality ISPY scans (motion artifacts, reduced contrast, blurred/inconsistent tumor boundaries) harming training stability and generalization, even when additional preprocessing like CLAHE was tried.",
      "source_document": "papers/2512.19225v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a 3D full\u2011resolution nnU\u2011Net for breast tumor segmentation in multi\u2011center DCE\u2011MRI, what loss is optimized, what two broad categories of augmentations are used during training (with examples), and what inference\u2011time strategy is used to improve robustness?",
      "answer": "The 3D full\u2011resolution nnU\u2011Net is trained with a batch\u2011wise Dice loss. Training uses both (1) spatial augmentations\u2014e.g., rotations, scaling, elastic deformations, flipping, cropping\u2014and (2) intensity augmentations\u2014e.g., gamma correction, noise injection, blur, and contrast adjustments. At inference, robustness is improved by ensembling the models from 5\u2011fold cross\u2011validation.",
      "source_document": "papers/2512.19225v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In multi-center breast DCE\u2011MRI tumor segmentation, what evaluation metrics can you use to separately measure (i) volumetric overlap, (ii) worst\u2011case boundary disagreement, and (iii) whether performance is equitable across demographic subgroups\u2014and how is the fairness metric computed and interpreted in this setting?",
      "answer": "The evaluation uses (i) Dice score for overlap between prediction A and ground truth B (Dice = 2|A\u2229B|/(|A|+|B|)), (ii) Hausdorff Distance for worst\u2011case boundary mismatch (H(A,B)=max(h(A,B),h(B,A)) with h(A,B)=max_{a\u2208A} min_{b\u2208B} ||a\u2212b||), and (iii) a Fairness Score based on demographic parity: DP = P(\u0176=1|G=group1) \u2212 P(\u0176=1|G=group2). A fairness score close to 0 indicates balanced (equitable) performance across groups, i.e., no subgroup is disproportionately under\u2011 or over\u2011segmented.",
      "source_document": "papers/2512.19225v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an nnU-Net\u2013based breast DCE-MRI tumor segmentation pipeline, what post-processing operation can be applied to suppress small false-positive regions, and what clinical assumption makes this operation reasonable for primary-tumor segmentation?",
      "answer": "Apply 3D connected-component analysis to the predicted mask and keep only the largest connected component (discard smaller components), then resample back to the original image space. This is motivated by the clinical observation that the primary breast tumor is typically the largest contiguous lesion in the breast region, so small scattered components are more likely noise/false positives.",
      "source_document": "papers/2512.19225v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When doing multi-center breast DCE\u2011MRI tumor segmentation, you might try CLAHE to compensate for low-contrast scans. What specific failure mode can CLAHE introduce that can *hurt* nnU\u2011Net training stability and segmentation accuracy (and on which center(s) was it most problematic), and what streamlined preprocessing steps were ultimately sufficient to standardize intensities without creating artificial features?",
      "answer": "Applying CLAHE can introduce structured, non-physiologic artifacts (notably banding and over\u2011sharpening) that destabilize training and degrade segmentation performance; this was particularly problematic for the lower-quality ISPY1 and ISPY2 scans. A simpler preprocessing setup\u2014keeping isotropic resampling plus z\u2011score intensity normalization (with the usual nnU\u2011Net cropping)\u2014was sufficient to improve intensity consistency while preserving the native contrast characteristics, so CLAHE was dropped from the final pipeline.",
      "source_document": "papers/2512.19225v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a closed-loop VLM \u201cdirector\u201d that plans iterative text/image edits, how can the planner be post-trained with Group Relative Policy Optimization (GRPO) to reduce the number of edit iterations\u2014specifically, what PPO-style objective is optimized, how are tokens masked so gradients only update the planner\u2019s textual actions (not tool outputs), what reward signal is used to score the final generated image, and what change in median editing rounds is achieved after this GRPO tuning?",
      "answer": "The planner is post-trained with a GRPO variant of PPO that optimizes a group-sampled trajectory objective: over G sampled edit trajectories per prompt, it uses the standard PPO clipped surrogate loss (Lclip) with a KL regularization term toward a reference policy (weighted by \u03b2), and a group-normalized advantage computed using the average group reward as the baseline. To ensure learning updates only affect the planner\u2019s own language actions, an indicator mask I(\u00b7) is applied to tokens so that tokens originating from external tools are excluded and policy gradients apply only to the planner\u2019s textual action tokens. Each trajectory replays the full director loop to produce a final edited image, which is then scored by a separate alignment VLM on a 0\u20135 scale for how well it satisfies the target instruction (covering attributes like typography, lighting, and spatial layout), and rewards are normalized within the group before policy updates. With this GRPO tuning, the median number of editing rounds drops from 4.2 to 3.1 (about a 26% reduction) without reducing verification accuracy.",
      "source_document": "papers/2512.19243v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating long multi-goal text-to-image and image-to-image editing systems with automated VLM-based verification, how can goal-level judgments be aggregated into a task-level outcome\u2014specifically, what inputs does the verifier compare for T2I vs. I2I, how are low-confidence verifier responses handled, and what thresholds define success vs. partial vs. failure at the task level?",
      "answer": "Use a multimodal verifier that (a) for image-to-image tasks compares the \u201cbefore\u201d and \u201cafter\u201d images, and (b) for text-to-image tasks inspects the generated image directly. Convert the long-form prompt into structured instructions so the verifier can reason goal-by-goal and output per-goal verdicts. Filter out verifier responses whose confidence is below 0.81. Then aggregate per-goal pass/fail into a task label: \u201csuccess\u201d if at least 80% of goals are satisfied, \u201cpartial\u201d if goal coverage is between 0% and 80%, and \u201cfailure\u201d if no goal passes (0% coverage).",
      "source_document": "papers/2512.19243v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a closed-loop, VLM-guided diffusion generation/editing controller, what two mechanisms can be used to (1) reduce per-step stochastic failures from diffusion randomness and (2) stop errors from accumulating across iterative edits, and what ablation evidence shows the impact of removing each mechanism on typography-related failures and overall goal coverage?",
      "answer": "(1) Use micro-grid execution: for each planned edit batch, sample multiple candidate outputs with different random seeds and use a lightweight VLM judge to pick the best candidate, which mitigates diffusion randomness without large latency. Ablating this micro-grid sampler increases stochastic failures on typography instructions by about 7%.\n\n(2) Use semantic verification with rollback: a verifier VLM checks the edited result against the remaining/pending goals after each edit; if the new edit degrades overall alignment, revert to the previous best image and reshuffle remaining goals to avoid repeating conflicts. Disabling this semantic rollback leads to accumulated hallucinations over iterations and reduces goal coverage to about 0.74.",
      "source_document": "papers/2512.19243v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a VLM-directed closed-loop image generation/editing system for long multi-goal instructions, what decision criteria are used to choose one-shot execution versus staged editing, how are goals scheduled in staged mode (batching and ordering), and what rules govern how the controller switches edit modes and decides to stop when some goals remain unresolved?",
      "answer": "The controller first extracts a set of explicit goals and estimates a one-shot feasibility score. It attempts a one-shot solution when this score is high and the affected area is small; otherwise it switches to staged execution. In staged mode it schedules goals in small batches (typically one or two goals at a time) and orders them from global constraints to local constraints to keep each edit focused. After each iteration, if goals remain, it adapts the editing operation based on what is failing: local adjustments use inpainting, while unsatisfied global constraints trigger scene-level regeneration. It terminates when all goals are satisfied, and it also supports early stopping via a periodic self-query (\u201cCan the image still improve?\u201d) that stops the loop when confidence in further improvement is low.",
      "source_document": "papers/2512.19243v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a unified image-driven generator that conditions on both high-level multimodal semantics (from an MLLM) and low-level visual latents (from a VAE), what hierarchical training strategy can prevent VAE features from hurting text\u2013image consistency, and which parameters are updated at each stage?",
      "answer": "A three-stage hierarchical schedule is used: (1) pretrain the adaptive gate first, updating only the gate parameters to initialize accurate condition-type discrimination; (2) train the generator conditioned only on the learnable semantic queries (with the MLLM kept frozen), optimizing the semantic queries and the connector to the diffusion backbone so the model converges on textual/cross-modal semantics; and then (3) perform joint training that includes both semantic queries and VAE latent features, which adds fine-grained visual detail without degrading the already-learned text\u2013image consistency.",
      "source_document": "papers/2512.19271v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a unified image-driven generator that uses an Adaptive Task-specific Memory (ATM) to modulate learnable semantic queries, how are task-specific memory items retrieved and then updated during training, and what is different about the memory behavior at inference time (include the role of the attention weights and the EMA coefficient)?",
      "answer": "Given semantic queries Q (l\u00d7c) and a task-specific memory item Mi (m\u00d7c), the model performs cross-attention retrieval to obtain (i) a retrieved representation R = Attn(Q, Mi) and (ii) an attention weight matrix W = Softmax(Q M_i^T / sqrt(c)), where W measures similarity between query tokens and memory slots. During training, Mi is updated by softly aggregating the current queries into the memory using W: the similarity-weighted query summary is fused with the existing memory via an exponential moving average, \nM\u0302_i = \u03b1 \u00b7 ( (W / \u03a3_j W_{j,:})^T Q ) + (1 \u2212 \u03b1) \u00b7 Mi,\nwhere \u03b1 is the EMA coefficient; this continuously stores task-specific priors into each memory item. At inference time, the memory items are fixed (no updates); only retrieval/modulation is performed.",
      "source_document": "papers/2512.19271v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a unified image-driven generation setting that supports subject, style, and structure conditioning, what set of quantitative metrics can be used to separately measure (i) subject consistency, (ii) style similarity, (iii) structure fidelity, (iv) text\u2013image instruction adherence, and (v) overall visual quality\u2014and how can a VLM-based automated evaluator be incorporated to score both condition similarity and prompt adherence?",
      "answer": "A suitable multi-dimensional evaluation uses: (i) subject consistency as cosine similarity between generated and reference images in CLIP-I and DINO embedding spaces; (ii) style similarity via the CSD score; (iii) structure fidelity via the L1 similarity between structure maps (e.g., edge/depth/pose maps) of the generated output and the provided structure reference; (iv) text fidelity via CLIP-T; and (v) overall visual appearance via FID. In addition, an automated VLM-based metric can be used by applying a vision-language model (Qwen2.5-VL) as a scorer to rate both task-specific similarity to the visual condition(s) and adherence to the textual instructions.",
      "source_document": "papers/2512.19271v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a unified image-driven generator that uses (i) learnable semantic queries from an MLLM, (ii) a task-specific memory module, and (iii) an adaptive gate that selects which memory to use for a given conditioning type, what distinct failure modes and metric degradations would you expect if you ablate each component (replace semantic queries with generic MLLM hidden states, remove the memory module, or remove the adaptive gate)? Explain which aspects of quality/consistency each ablation primarily hurts and how that shows up in CLIP-I, CLIP-T, and FID trends.",
      "answer": "Ablating each component causes different, diagnosable failures:\n\n- Replacing learnable semantic queries with the MLLM\u2019s final hidden states makes the conditional signal semantically deficient, leading to degraded image quality (e.g., artifacts) and it produces simultaneous declines in text fidelity and visual quality metrics (CLIP-T decreases and FID worsens).\n\n- Removing/bypassing the task-specific memory module eliminates task-specific conditional priors, which consistently reduces consistency with the visual conditions; quantitatively this ablation mainly induces a drop in subject consistency (CLIP-I decreases), rather than being primarily a text-metric failure.\n\n- Removing the adaptive gate (so memory items are effectively retrieved/used in a disordered way) causes semantic confusion across different conditioning types, harming both perceptual quality and conditional consistency; this shows up as simultaneous declines in CLIP-T and worsening FID.\n\nThese behaviors reflect the division of labor: semantic queries provide strong aligned semantic conditioning, the memory provides condition-specific priors that stabilize visual-condition consistency, and the gate prevents cross-condition interference by selecting the appropriate memory interactions.",
      "source_document": "papers/2512.19271v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a unified image-driven generator to handle *compositional* conditioning (e.g., subject+style and structure+style) but paired multi-condition supervision is scarce, what synthetic data construction pipeline can be used to create these multi-condition training triplets\u2014specifically, how are diverse style instructions produced and how are the edited target images generated?",
      "answer": "Multi-condition compositional data can be synthesized via instruction-guided style transfer: generate diverse style descriptions/instructions with an LLM (GPT-5), then create the corresponding edited images with an image-editing model (Gemini-Flash). This produces large paired sets (e.g., tens of thousands) for subject\u2013style and structure\u2013style conditioning.",
      "source_document": "papers/2512.19271v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a zero-shot identity-transfer evaluation where the reference image comes from one person and the driving video (gait) comes from a different person, what qualitative pattern in identification results distinguishes generators that are mainly copying appearance (i.e., acting like Re-ID) from generators that actually transfer gait dynamics, and which evaluated generator shows clear evidence of non-trivial gait transfer on a skeleton-only recognizer?",
      "answer": "Appearance-copying (Re-ID-like) behavior is indicated when the generated video matches the Reference identity with high accuracy but collapses to near-zero accuracy when evaluated against the Driving identity (the transferred motion), showing reliance on static texture/silhouette cues rather than temporal gait dynamics. Actual gait transfer is indicated by higher accuracy against the Driving identity on skeleton-based recognizers that ignore texture.\n\nAmong the evaluated generators, MagicAnimate shows clear non-trivial gait transfer: on the skeleton-only SkeletonGait model it reaches 22.72% Rank-1 when evaluated against the Driving Motion, while being much lower against the Reference identity (2.30% Rank-1), whereas most other generators are around ~0\u20132% Rank-1 on Driving Motion in this task.",
      "source_document": "papers/2512.19275v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking human-animation generators with pixel-aligned reconstruction metrics (e.g., SSIM/PSNR/LPIPS), what evaluation design choice can you use to ensure the scores reflect foreground human fidelity rather than background hallucinations, and which common identity-transfer setting cannot be evaluated with these metrics because there is no pixel-aligned ground truth?",
      "answer": "Apply a person segmentation mask to both the generated frames and the corresponding ground-truth frames so that SSIM/PSNR/LPIPS are computed only over the subject foreground, mitigating the effect of hallucinated or mismatched backgrounds. These pixel-aligned metrics cannot be used for the zero-shot identity-transfer setting (reference identity + different driving identity) because it produces novel videos without a pixel-aligned ground-truth target for comparison.",
      "source_document": "papers/2512.19275v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In pose-guided human video generation evaluated for gait restoration, what motion-conditioning design choice is shown to be most robust under in-the-wild conditions with large camera shifts, and what specific failure mechanism explains why a dense-pose\u2013conditioned generator can lose its advantage in that setting?",
      "answer": "The most robust design under in-the-wild conditions is using a unified-noise video diffusion conditioning strategy (as used by UniAnimate), which remains more stable when backgrounds vary and the camera moves. Dense-pose conditioning (as in MagicAnimate) can lose its controlled-setting advantage in the wild because its performance is bottlenecked by DensePose estimation quality; DensePose errors under extreme viewpoints/camera motion degrade the conditioning signal and lead to a collapse in structural/gait-preservation performance.",
      "source_document": "papers/2512.19275v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating GenAI human-animation outputs with multiple gait-recognition models that take different input modalities (RGB, silhouette, skeleton), what control can you apply to make sure any Rank-1/mAP differences are attributable to the generated inputs rather than to different feature-projection or metric-learning heads, and what shared embedding head is used under this control?",
      "answer": "Implement all gait-recognition models within the same framework (OpenGait) and force them to share a unified feature-embedding/projection head so the backbones see comparable metric learning; in this setup, all four recognizers use the same GaitBase embedding head.",
      "source_document": "papers/2512.19275v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating pose-guided human video generators for gait recognition, what recurring structural artifact can systematically degrade silhouette/shape-based identification even if RGB videos look plausible, and what underlying limitation of sparse guidance explains this artifact (including which type of guidance avoids it)?",
      "answer": "A common failure is an aspect-ratio/shape distortion where the generated subject becomes unnaturally \u201cvolumetrically thickened\u201d (rendered significantly wider than the ground truth), which harms silhouette- and shape-driven gait identification despite visually plausible RGB. This is attributed to sparse skeleton/keypoint guidance not sufficiently constraining the subject\u2019s volumetric boundaries during texture warping; dense pose\u2013based guidance (the dense-pose\u2013guided model is the notable exception) better constrains body volume and mitigates this artifact.",
      "source_document": "papers/2512.19275v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a hand-conditioned diffusion framework for egocentric full-body motion reconstruction, what combination of loss terms is used during training to (1) learn the denoising objective, (2) enforce sequence-consistent body shape, and (3) regularize motion with 3D physical constraints (including contact-aware foot skating), and how is that auxiliary physical regularizer weighted across diffusion timesteps?",
      "answer": "The training objective combines three parts:\n\n\u2022 Denoising loss (Lsimple): the standard DDPM objective, an L2 reconstruction between the clean canonicalized pose sequence X0 and the model\u2019s prediction \\hat{X0} (i.e., E[||X0 \u2212 \\hat{X0}||^2]).\n\n\u2022 Shape-consistency loss (Lshape): instead of directly penalizing SMPL shape coefficients \u03b2, it minimizes the squared error between 3D T-pose joint locations produced by forward kinematics, ||FK(0,\u03b2) \u2212 FK(0,\\hat{\u03b2})||^2, enforcing a single consistent body shape for the whole sequence.\n\n\u2022 Auxiliary physical regularizer (Laux): encourages 3D physical plausibility using (a) a joint position loss averaged over time, comparing FK(Mt,\u03b2) to FK(\\hat{Mt},\\hat{\u03b2}), and (b) a contact-aware foot skating loss that penalizes frame-to-frame FK motion when the ground-truth contact label ct indicates contact.\n\nThese are combined as L = Lsimple + \u03bbshape\u00b7Lshape + Laux, with Laux = \\bar{\u03b1}_n (\u03bbpos\u00b7Lpos + \u03bbskat\u00b7Lskat). The multiplicative \\bar{\u03b1}_n weighting applies the physical regularization primarily when the diffusion signal level is high (lower noise), and this auxiliary term is reported as crucial for satisfying the sparse hand condition H and producing stable/smooth motion.",
      "source_document": "papers/2512.19283v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training an egocentric full-body motion diffusion model that conditions on head trajectory plus sparse wrist observations, what kind of data augmentation can be used to realistically simulate intermittent hand visibility caused by camera field-of-view limits and occlusions\u2014and how does this augmentation model both (a) the spatial FoV boundary (including tilt/aspect/distortion) and (b) the temporal pattern of hand-track dropouts in a way that is more realistic than i.i.d. Bernoulli masking?",
      "answer": "A realistic strategy is a two-stage hand-visibility augmentation with separate spatial and temporal components.\n\n(a) Spatial FoV simulation: represent each wrist direction in head coordinates by its yaw/pitch angles, then declare the wrist \u201cvisible\u201d if those angles fall inside a parametric FoV boundary defined by a generalized ellipse. The boundary is controlled by five parameters: a 2D center offset for camera tilt, two half-angles setting FoV size and aspect ratio, and a power term that adjusts the boundary shape to mimic lens distortion/non-pinhole behavior. Sampling these FoV parameters from realistic distributions during training exposes the model to diverse camera configurations.\n\n(b) Temporal dropout simulation: instead of i.i.d. masking, generate bursty drop events with a two-stage stochastic process: first sample the number of dropout events from a Poisson distribution to control the overall drop rate; then sample each event\u2019s duration from a heavy-tailed Log-Normal distribution to capture both short transient losses (e.g., motion blur) and long occlusions. The final visibility mask is formed by the union of these event masks. This better matches real egocentric failures than Bernoulli masking (which implies geometric, typically too-short, dropout lengths).",
      "source_document": "papers/2512.19283v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a DDIM-sampled diffusion model that reconstructs full-body motion from head trajectory plus intermittent wrist observations, how can test-time guidance be formulated so the generated motion satisfies the sparse wrist constraints without drifting off the learned motion prior\u2014specifically, what are the two terms in the guidance objective, how are they weighted across diffusion timesteps, and which subset of joints is optimized during this refinement?",
      "answer": "Test-time guidance refines the current sampled motion by directly optimizing a pose variable (a refined motion \\(\\tilde M_0\\)) at each DDIM step to minimize an objective with two competing terms:\n\n1) A **prior/regularization term** that keeps the refined motion close to the denoiser\u2019s current prediction \\(\\hat M_0\\): it penalizes the deviation between refined and predicted wrist (hand-joint) positions via forward kinematics, weighted by \\(\\bar\\alpha_n\\) (high weight at low-noise/late steps).\n\n2) A **constraint term** that pulls the refined wrists toward the observed wrist targets \\(p_t^j\\) (in world coordinates), gated by the visibility indicator \\(v_t^j\\) and scaled by a guidance factor \\(s\\), with weight \\((1-\\bar\\alpha_n)\\) (high weight at high-noise/early steps).\n\nConcretely, the objective sums over time and both hands \\(j\\in\\{lHand,rHand\\}\\):\n\\[\\sum_t \\sum_j \\big( \\bar\\alpha_n\\,\\|FK_j(\\tilde M_{0,t},\\hat\\beta)-FK_j(\\hat M_{0,t},\\hat\\beta)\\|^2 + s\\,v_t^j\\,(1-\\bar\\alpha_n)\\,\\|FK_j(\\tilde M_{0,t},\\hat\\beta)-p_t^j\\|^2 \\big).\\]\n\nOnly the **arm joints** are optimized (not the full body), so the refinement is lightweight and stays on-manifold: the constraint term dominates early in sampling, while the prior term dominates late.",
      "source_document": "papers/2512.19283v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using reinforcement learning to fuse two object categories by mixing their text embeddings before conditioning a diffusion model, how can a CLIP-based visual reward be constructed so that the generated image both (i) preserves semantic attributes from each source category and (ii) avoids one concept dominating the other? Define the reward mathematically and explain the role of foreground segmentation and the balance term.",
      "answer": "A visual reward can be computed by comparing the generated fused image to two reference exemplar images\u2014one generated from each individual concept prompt\u2014using CLIP image embeddings on the foreground object region. First, extract foreground segments from the fused image and the two exemplar images (to focus the similarity on the object and reduce background/juxtaposition effects), then encode them with a pretrained CLIP image encoder to get embeddings. Let S1 be the cosine similarity between the fused foreground embedding and exemplar-1\u2019s foreground embedding, and S2 the cosine similarity to exemplar-2. The fusion reward is:\n\nR = (S1 + S2) \u2212 \u03b1 \u00b7 |S1 \u2212 S2|,\n\nwhere the (S1+S2) term encourages presence/preservation of both concepts (high similarity to each exemplar), and the penalty term \u03b1|S1\u2212S2| (with \u03b1>0) discourages imbalance by penalizing one similarity being much larger than the other, thereby promoting compositional balance.",
      "source_document": "papers/2512.19300v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an RL-based cross-category concept-fusion method that generates a single hybrid object by mixing two text-encoder embeddings e1 and e2, how can the fusion process be formulated as an MDP\u2014specifically, what are the definitions of the state, action, initial state, and transition (fusion) update if the policy outputs a column-wise interpolation vector rather than a single scalar mixing weight? Give the explicit update equation for the next fused embedding.",
      "answer": "Define a multi-step MDP over fused text embeddings. The state at step t is the current fused embedding s_t = e_f^(t) \u2208 R^{h\u00d7w}. The action is a column-wise (token-wise) interpolation coefficient vector a_t \u2208 R^w sampled from an MLP policy \u03c0_\u03b8(a_t|s_t). The initial state is the average of the two source embeddings: s_0 = e_f^(0) = 1/2 (e1 + e2). The transition applies the action as per-column weights via a diagonal matrix: e_f^(t+1) = f_fuse(a_t,e1,e2) = e1 \u00b7 diag(a_t) + e2 \u00b7 diag(1 \u2212 a_t).",
      "source_document": "papers/2512.19300v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When a concept-fusion diffusion pipeline uses a stochastic mixing policy (and stochastic diffusion sampling), what two-stage inference-time selection procedure can be used to pick a single representative fused image from many generated candidates? Define (i) the candidate filtering conditions in terms of per-concept similarity scores S1 and S2 and two thresholds that enforce \u201cdual concept presence\u201d and \u201cfusion balance\u201d, and (ii) the final ranking rule used to choose the output from the filtered set.",
      "answer": "Generate a pool of fused samples and select the final output in two stages:\n\n1) **Candidate filtering (fusion criteria).** For each generated image If, compute per-concept similarities **S1** and **S2** (CLIP-based similarities between the fused image\u2019s foreground segment and the two source-concept exemplar foreground segments). Keep If as a candidate only if it satisfies:\n- **Dual concept presence:** S1 > \u03c4presence and S2 > \u03c4presence (both concepts are sufficiently present).\n- **Fusion balance:** |S1 \u2212 S2| < \u03c4balance (neither concept dominates).\nThis yields a candidate set: Ican = {If | S1 > \u03c4presence, S2 > \u03c4presence, and |S1 \u2212 S2| < \u03c4balance}.\n\n2) **Top-1 ranking (concept preservation strength).** From Ican, choose the single image with the largest combined alignment score:\nIf* = argmax_{If \u2208 Ican} (S1 + S2).\n\nThis decouples balance enforcement (filtering) from maximizing total concept preservation (ranking).",
      "source_document": "papers/2512.19300v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking cross-category concept fusion (hybrid object) methods, why can CLIP-based average similarity to the input concepts\u2014and prompt-conditioned alignment metrics like HPSv2/VQAScore\u2014be misleading for some strong T2I baselines, and what evaluation choice(s) can make the comparison fairer and better reflect \u201ctrue fusion\u201d rather than simple compositing/splicing?",
      "answer": "Avg. Sim can be higher for methods that *don\u2019t actually fuse* because they can produce high-quality composites or partial splices that still score well under CLIP similarity: e.g., a diffusion baseline can make a clean composite of both categories (boosting Avg. Sim (I\u2192I)), and a prompt-driven system can splice in prompt keywords (boosting Avg. Sim (I\u2192T)) without synthesizing a single coherent hybrid. For HPSv2 and VQAScore, which measure image\u2013text alignment using the *input prompt*, models like SDXL\u2011Turbo and GPT\u2011Image\u20111 that generate directly from the evaluation prompt would get artificially inflated, non-comparable scores. A fairer evaluation therefore emphasizes fusion-sensitive metrics (e.g., balance between similarities to each concept and a combined reward that penalizes dominance) and excludes prompt-alignment metrics for baselines whose scores would be trivially boosted by being conditioned on the same prompt used for scoring.",
      "source_document": "papers/2512.19300v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a reinforcement-learning approach that fuses two concepts by iteratively mixing their text embeddings and scoring each generated image with a visual reward, how can PPO be adapted when (i) the goal is to obtain the best single-step fusion within a T-step episode (rather than optimize long-horizon discounted return) and (ii) you want to avoid training a separate critic? Specify how rewards are aggregated across steps, what replaces the advantage/critic term, and write the resulting clipped PPO surrogate objective (including the probability ratio and clipping operation).",
      "answer": "The episode objective is treated as \u201cbest-of-trajectory\u201d: while an episode produces rewards {R1,\u2026,RT}, the primary goal is to maximize the highest single-step reward (max_t R_t). For optimization, rewards are aggregated with no discounting, using \\(\\sum_{t=1}^T \\gamma^t R_t\\) with \\(\\gamma=1\\), and intermediate rewards at each step are retained.\n\nTo avoid a critic, the formulation removes the value/advantage estimator and directly uses the visual reward \\(R(s_t,a_t)\\) in the PPO surrogate. The clipped PPO objective is:\n\n\\[\nL_{\\text{PPO}}(\\theta)=\\mathbb{E}_{(s_t,a_t)\\sim \\pi_{\\theta_{\\text{old}}}}\n\\Big[ -\\min\\big( k_t(\\theta)\\, R(s_t,a_t),\\; \\text{clip}(k_t(\\theta),1-\\xi,1+\\xi)\\, R(s_t,a_t) \\big) \\Big],\n\\]\n\nwhere \\(k_t(\\theta)=\\frac{\\pi_{\\theta}(a_t\\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t\\mid s_t)}\\) is the probability ratio and \\(\\xi\\) is the clipping hyperparameter (set to 0.2).",
      "source_document": "papers/2512.19300v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a decoupled LVLM\u2192SAM2 reasoning-segmentation setup trained from mask annotations (no box/point labels), how is the reinforcement-learning objective designed to give the LVLM usable feedback, and what constraint is imposed on the prompt action space to keep GRPO exploration tractable\u2014compared with approaches that rely on pseudo box\u2013point supervision?",
      "answer": "The LVLM is optimized with mask-only GRPO using a simple result-oriented reward computed after executing the LVLM\u2019s generated geometric prompts with a frozen SAM2. The total reward is a weighted sum of (1) a format reward that checks the output is syntactically valid (correct <think>/<answer> tags and valid JSON that the segmenter can parse) and (2) a segmentation reward given by the IoU between the SAM2-produced mask and the ground-truth mask; for empty-target scenes the IoU reward is set to 1 if the LVLM outputs an empty prompt list and 0 otherwise. To make exploration tractable, the action space is constrained to a fixed prompt configuration per instance\u2014one bounding box plus two positive points\u2014so GRPO only needs to learn the geometry (placement/scale) rather than searching over prompt types and counts. This differs from pseudo box\u2013point supervised methods that train by mimicking a single handcrafted/converted prompt (introducing behavioral-cloning bias and noisy intermediate supervision), whereas mask-only GRPO aligns learning directly with final mask quality and permits broader exploration, which is reported to yield more complete/coherent masks (notably improving cumulative IoU).",
      "source_document": "papers/2512.19302v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a decoupled LVLM-to-frozen-SAM2 reasoning-segmentation pipeline, which geometric prompt configuration gives the best overall mask quality, and what is the underlying reason that adding more positive points or introducing negative points tends to degrade performance on high-resolution remote-sensing scenes?",
      "answer": "The most effective configuration is **one bounding box plus two positive points**: the box provides a global spatial prior while the two positives disambiguate heterogeneous subregions inside the box, yielding coherent semantic-region masks. Using **more positive points (e.g., four)** often places points on semantically different subregions (rooftops, shadows, adjacent textures), which makes the frozen segmenter merge inconsistent cues and produces overextended/irregular masks. Adding **negative points** further degrades results because \u201cnon-target\u201d regions in aerial imagery are intrinsically heterogeneous (vegetation, concrete, water, asphalt, etc.), so the LVLM cannot learn a stable placement strategy and the negatives inject contradictory signals against the positives, fragmenting or distorting the mask.",
      "source_document": "papers/2512.19302v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a decoupled LVLM\u2192frozen-SAM2 reasoning segmentation system trained with a mask-only, global-IoU reinforcement signal, why would you expect strong zero-shot transfer to datasets with semantic-level masks but a noticeable drop on instance-level referring segmentation benchmarks\u2014and what concrete changes to the reward and/or prompt vocabulary can help close that gap?",
      "answer": "Because optimizing a mask-only, global IoU objective biases the prompter toward semantic coherence and scene-level completeness (good when supervision/ground truth is semantic-level and may include multiple disjoint regions of the same concept), but it does not impose the explicit geometric priors needed to separate many small, adjacent, rotated, or thin instances required by instance-level benchmarks. To improve instance-level performance, the document proposes lightweight geometry-aware extensions: add orientation- and thin-structure/skeleton-consistency terms to the reward, expand the prompt vocabulary beyond axis-aligned boxes + points (e.g., rotated boxes and line or multi-point chains), and optionally include a small amount of referring-style supervision as curriculum calibration.",
      "source_document": "papers/2512.19302v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a decoupled LVLM\u2192frozen-segmenter prompting system trained with mask-only reinforcement learning, how can the reward be defined on images where the queried concept is absent so that the policy learns to abstain rather than hallucinate\u2014and how is this \u201crejection\u201d behavior quantitatively evaluated in the remote-sensing reasoning-segmentation setting?",
      "answer": "Absent-target (empty-target) images are handled by defining the segmentation (IoU) reward so that the model receives full credit only when it outputs no prompts: the IoU reward is set to 1 if the LVLM outputs an empty prompt list and to 0 otherwise. Rejection is then evaluated by running the model on designated empty-target cases (e.g., in the EarthReason validation/test splits) and reporting how many such samples are correctly classified as empty versus incorrectly predicted as non-empty (TRUE/FALSE counts and percentages for empty-target classification).",
      "source_document": "papers/2512.19302v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a decoupled LVLM\u2192frozen-segmenter reasoning-segmentation pipeline trained with semantic-level (coarse polygon) masks, why can a smaller SAM2 variant outperform larger SAM2 variants, and what characteristic failure mode do the larger segmenters exhibit that lowers IoU under this supervision style?",
      "answer": "Smaller SAM2 variants (e.g., Tiny/Small) can outperform larger ones because semantic-level remote-sensing annotations often label an entire facility/region as one coherent polygon, while high-capacity segmenters (Base-plus/Large) are overly sensitive to fine textures and microstructures. The larger models tend to over-segment by delineating small structural details, gaps, and background texture variations, producing fragmented, quasi-instance-level masks (e.g., splitting components and negative spaces inside a facility) that semantically mismatch the coarse ground truth and therefore reduce IoU. In contrast, smaller models have an implicit regularization effect that smooths over small gaps and texture noise, yielding spatially coherent masks better aligned with semantic-level labeling.",
      "source_document": "papers/2512.19302v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a diffusion/flow-matching model trained with MixFlow, what training objective is optimized when using a slowed-interpolation mixture, how are the training timestep t and the slowed timestep m_t sampled (including the role of the mixture-range coefficient \u03b3), and what sampling property over input pairs (x_{m_t}, t) is this design meant to achieve?",
      "answer": "MixFlow replaces the standard single-timestep interpolation input x_t with a slowed interpolation x_{m_t} while still conditioning the network on the original training timestep t. The optimized objective is\n\nE_{t,x0,x1,m_t} [ || u_\u03b8(x_{m_t}, t) \u2212 u*(x_t, t) ||^2_2 ],\n\nwhere x_{m_t} = \u03b2_{m_t} x0 + \u03b1_{m_t} x1 and x_t is the ground-truth interpolation at timestep t used to define the target u*(x_t,t). The slowed timestep is sampled from a uniform distribution over a t-dependent range determined by \u03b3:\n\nm_t ~ U[(1\u2212\u03b3)t, t],\n\nso \u03b3 controls the mixture width (range size \u03b3t) and restricts m_t to be a higher-noise (\u2264 t) timestep. The training timestep is sampled as\n\nt ~ Beta(2,1) (equivalently p(t)=2t on [0,1]),\n\nchosen so that the joint sampling over pairs is \u201ceven\u201d across the 2D space of (m_t,t) inputs: since p(m_t|t)=1/(\u03b3t), setting p(t) \u221d t makes p(m_t,t)=p(m_t|t)p(t) constant, yielding uniformly sampled input pairs (x_{m_t}, t) across all valid (m_t,t).",
      "source_document": "papers/2512.19311v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When post-training diffusion/flow-matching generators with MixFlow, how does the size of the quality gain change as you reduce the number of sampling steps at inference time, and what explanation links this trend to the Slow Flow / exposure-bias mechanism?",
      "answer": "The gain from MixFlow becomes larger when using fewer sampling steps. The explanation is that with fewer steps the sampler\u2019s numerical approximation is coarser, which makes the Slow Flow/exposure-bias mismatch between training inputs (ground-truth interpolations) and testing inputs (model-generated noisy states) more pronounced; MixFlow\u2019s slowed-interpolation training better matches these test-time states, so it helps more in the few-step regime.",
      "source_document": "papers/2512.19311v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When trying to reduce exposure bias in diffusion/flow models, how do inference-time fixes like Time-Shift sampling and Epsilon Scaling compare against a training-time fix like Input Perturbation and a slowed-interpolation training approach (MixFlow) on a standard class-conditional benchmark, and what mechanism explains why Time-Shift can actually degrade generation quality?",
      "answer": "On the ImageNet class-conditional setting evaluated with a SiT-B/2 backbone, the slowed-interpolation training approach (MixFlow) gives the strongest overall improvement across common generation metrics (e.g., better FID/IS/precision-recall) compared with both inference-time heuristics and Input Perturbation. Input Perturbation improves over the baseline but remains worse than MixFlow, while Epsilon Scaling provides some improvement yet is still inferior to MixFlow. Time-Shift sampling can even worsen results versus the baseline.\n\nThe degradation with Time-Shift is attributed to its heuristic timestep adjustment: the adjusted timestep may be inaccurate and can correspond to a higher-noise state than intended, which then effectively requires more sampling steps to reach the final data timestep; this mismatch can increase drift and hurt quality. Epsilon Scaling is also described as heuristic, so its adjustment may not accurately correct the training\u2013sampling mismatch, limiting gains relative to MixFlow.",
      "source_document": "papers/2512.19311v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When applying a slowed-interpolation\u2013based exposure-bias mitigation scheme as a post-training step to a class-conditional generator that was originally trained with an auxiliary alignment objective (e.g., REPA), how should the post-training loss be composed, and what inference-time evaluation settings should be held fixed to make a fair comparison to the original aligned model?",
      "answer": "Compose the post-training objective as a sum of (i) the original alignment loss used by the aligned training scheme (REPA\u2019s alignment loss) and (ii) the MixFlow slowed-interpolation-mixture loss. For a fair comparison, keep the evaluation protocol consistent with the original aligned model\u2014specifically the same guidance settings used for REPA, including guidance scale 1.8 and the same guidance interval [0, 0.7].",
      "source_document": "papers/2512.19311v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an implicit-network pipeline that reconstructs a patient-specific 3D cardiac mesh from only sparse 2D segmentation slices, what are the two MLP components (their inputs/outputs and shared latent code), how is the joint training objective defined (segmentation loss, regression loss, latent prior regularization and its weighting schedule), and what loss is minimized at test time to optimize the latent code under domain shift/misalignment (including the Mahalanobis latent regularizer and any BCE/Dice reweighting)?",
      "answer": "The pipeline uses two MLPs that share a per-shape latent vector h:\n\n\u2022 Segmentation MLP (MLPseg / SEG-NIF): takes Cartesian \u201ccardiac\u201d coordinates together with h and outputs occupancy probabilities over 5 semantic labels (BG, LV blood pool, RV blood pool, LVM, RVM).\n\n\u2022 Regression MLP (MLPreg / UVC-NIF): takes universal ventricular coordinates (UVCs) together with the same h and outputs 3D Cartesian cardiac coordinates (vertex positions), enabling mesh reconstruction by querying a fixed template\u2019s UVCs.\n\nJoint training optimizes both MLPs (and the latent codes) with a three-term loss:\nL = (1/\u03bbseg)\u00b7LSEG + (1/\u03bbreg)\u00b7LREG + \u03bbprior\u00b7Lprior,\nwhere LSEG is a combined Dice + binary cross-entropy loss, LREG is an MSE loss on the regression task, and Lprior regularizes the latent codes toward a standard Gaussian via an L2 penalty: Lprior = (1/B)\u2211||h_i||^2/2 over the batch. The latent prior weight is ramped up over the first 100 epochs: \u03bbprior = min(1, epoch/100)\u00b7\u03bbprior,max, with \u03bbprior,max = 1.0e\u22124. The task scaling factors are set to \u03bbseg = 1.0 and \u03bbreg = 1000.0.\n\nAt test time, only the segmentation network is used to infer h by backpropagating into h (network weights frozen) to minimize a segmentation objective:\nL = \u03bbr\u00b7Lr + \u03bbBCE\u00b7LBCE + \u03bbDice\u00b7LDice,\nwhere Lr is a latent-space regularizer using a Mahalanobis distance to the empirical latent distribution learned in training: (z\u2212\u03bc)^T \u03a3^\u22121 (z\u2212\u03bc), and its weight is set empirically to 10\u22122. To handle a domain gap, the BCE/Dice weights are rebalanced: for the synthetic (SYN) case \u03bbBCE=10 and \u03bbDice=1, while for the real aligned (SEG) case both are set to 1.",
      "source_document": "papers/2512.19316v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a neural implicit pipeline that reconstructs a patient-specific 3D biventricular mesh from sparse CMR slice segmentations, how can you design test inputs to separate (i) reconstruction error due to inconsistent/misaligned clinical segmentations from (ii) error intrinsic to the learned shape/coordinate mapping? Concretely, describe how the two evaluation contour sets are constructed (real vs synthetic), what key property differs between them (alignment/consistency), and what the observed performance gap implies about the dominant error source under real clinical inputs.",
      "answer": "Use two contour/segmentation inputs at test time:\n\n1) **Real contour set (SEG):** segmentation masks/contours extracted from the actual CMR slices (e.g., via cvi42). These contain real-world inconsistencies such as residual misalignment between short- and long-axis slices and segmentation errors.\n\n2) **Synthetic contour set (SYN):** generate segmentation masks directly from the reference meshes by sampling/slicing the meshes at the same image-slice locations. Because they are derived from the fitted reference meshes, these synthetic masks are internally consistent and *free from misalignment*.\n\nComparing performance on SYN vs SEG isolates the effect of contour inconsistencies: performance is highest on SYN (where slices are perfectly consistent with the underlying reference mesh), and the degradation on SEG indicates that a large fraction of the error under real inputs is driven by the domain gap introduced by misaligned/inconsistent clinical contours rather than by limits of the implicit mesh-decoder itself (the paper reports that roughly 62\u201365% of Euclidean-distance error is attributable to this domain gap).",
      "source_document": "papers/2512.19316v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When defining frame-accurate stroke events for supervision, how is the timestamp chosen if the exact ball\u2013racket impact moment is not captured by the video frame rate, and what is the rationale for using that specific choice?",
      "answer": "Strokes are annotated as a single instant: the exact frame of ball\u2013racket contact. If the true impact moment falls between frames (so it is not directly captured at 120 fps), the annotation could be placed on either the preceding or subsequent frame; the dataset uses the subsequent frame to match the convention already used in OpenTTGames. This keeps stroke timestamps consistent with the original dataset and simplifies annotation by avoiding ambiguous start/end boundaries for multi-frame stroke segments.",
      "source_document": "papers/2512.19327v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a side-view table-tennis dataset where you want to predict point outcomes from the final exchange, how can rally-ending labels be defined so they (1) remain consistent when swapping left/right table sides and (2) explicitly attribute responsibility (winner vs error) to a player? Describe the set of outcome categories and the prefixing convention used for each case.",
      "answer": "Define each rally-ending as a player-prefixed outcome category, where the semantics are specified relative to the left side of the table and mirrored for the right side so the same rules apply when sides swap. The dataset uses six outcome categories: out (ball fully passes the table), net (ball stopped by the net or hits the striker\u2019s own side; mere net-touch that still crosses is not labeled as net-ending), winner (ball becomes unreachable for the receiver, including hitting the receiver\u2019s body), not_hitting_ball (player swings at a reachable ball but misses), double_bounce (ball bounces twice on one side), and miss_on_own_side (ball drops below the table on the striker\u2019s own side; if it flies off due to an edge hit, the label may be placed a few frames after contact). Each label is prefixed with the player tied to the rally-ending action: out/net/not_hitting_ball/miss_on_own_side are prefixed by the player who commits the error; winner is prefixed by the player who hits the winner; double_bounce is prefixed by the player who hit the ball immediately before the double bounce (i.e., the player awarded the point).",
      "source_document": "papers/2512.19327v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When labeling foot stance at the instant of racket\u2013ball contact for a stroke (to support pose/technique modeling), what practical rules can be used to decide whether each foot is \u201clifted\u201d vs \u201cplanted\u201d, and in what situations should the foot label be set to \u201cunknown\u201d despite occlusion?",
      "answer": "Annotate each foot independently at the single impact frame and classify it as lifted/planted/unknown using these rules: (1) Treat \u201cslight contact\u201d with the floor as still lifted\u2014e.g., only marginal edge contact or the foot being dragged across the floor does not count as planted. (2) Mark a foot as planted when it is clearly pressed to the ground (e.g., toes/sole visibly contacting with weight). (3) If a foot/leg is occluded but the stance can be inferred from the rest of the body, still label it (e.g., inferred planted). (4) Use \u201cunknown\u201d when occlusion or subtle motion makes it impossible to reliably tell whether the foot is slightly lifted or planted.",
      "source_document": "papers/2512.19327v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "If you want to train and benchmark a stroke-type classifier on a table-tennis dataset whose labels concatenate multiple factors (e.g., stroke technique plus posture/stance attributes), what label-aggregation strategies can you use to avoid an explosion of rare classes, and what concrete examples of such aggregation are suggested?",
      "answer": "Because the full fine-grained taxonomy produces many highly specific labels that occur only a few times, the labels should be aggregated (consolidated) according to the needs of the task. Suggested strategies include pooling all strokes of a given technique regardless of posture/stance (e.g., pool all forehand loops even if lean and foot placement differ), grouping labels using only the lean categories, or otherwise consolidating/ignoring components of the concatenated label as appropriate for the analysis or model.",
      "source_document": "papers/2512.19327v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When extending a side-view table-tennis dataset that already provides racket\u2013ball impact timestamps but has no rally-ending timestamps, what annotation workflow can you use to efficiently and consistently label both stroke frames and rally-ending frames, and how can already-annotated strokes be leveraged to narrow down where each rally ending must occur?",
      "answer": "Use a two-step, code-assisted workflow: (1) first pass\u2014scan the video and tag all stroke frames with the temporary marker \"empty_event\" and all rally-ending frames with a temporary marker such as \"point\"; (2) second pass\u2014revisit those pre-marked frames and replace the temporary tags with the final, detailed stroke and rally-ending labels. If a video already contains stroke timestamps, you can skip step (1) for strokes by jumping directly to the provided impact frames, but rally endings still require both steps because they have no predefined timestamps. Once strokes are annotated, each rally ending can be found by exploiting the fact that it must occur after the final stroke of a rally and before the subsequent serve, which creates a short frame interval that the annotation script can jump between, greatly reducing the number of frames to inspect.",
      "source_document": "papers/2512.19327v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a MIL model for whole-slide images that uses a gated delta-rule key\u2013value memory to aggregate extremely long patch sequences, how is the global memory updated at each step to both (i) forget the previous contribution associated with the current patch and (ii) write the updated information back, and what distinct roles do the retention gate and update gate play in this update?",
      "answer": "At step t, the model computes an update gate and a retention gate from the current hidden state (both passed through a sigmoid): \u03b2t=\u03c3(W\u03b2ht) controls how much new information is integrated, and \u03b1t=\u03c3(W\u03b1ht) attenuates (retains/forgets) the existing memory. The previous memory contribution corresponding to the current key kt is first retrieved after attenuation as vold_t = \u03b1t S_{t\u22121} kt. The new value to store is then formed by blending the incoming value vt with the retained old contribution using the update gate: vnew_t = \u03b2t vt + (1\u2212\u03b2t) vold_t. Finally, the gated delta-rule updates the key\u2013value memory by explicitly removing the old contribution for kt and writing the new one: S_t = \u03b1t S_{t\u22121} \u2212 vold_t kt^T + vnew_t kt^T. This can be interpreted as a two-step operation\u2014\u201cremove old\u201d (subtract vold_t kt^T from the attenuated memory) and \u201cwrite new\u201d (add vnew_t kt^T).",
      "source_document": "papers/2512.19331v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a WSI MIL model that augments a gated delta-rule memory with an explicit locality-preserving branch, how are 1D patch embeddings converted into local spatial features and fused back into the sequence before computing Q/K/V, and what does the ablation evidence indicate happens to slide-level classification performance when this local module is removed?",
      "answer": "Locality is injected by first using patch spatial coordinates to reconstruct the 1D patch embedding sequence into a 2D slide-level feature map, filling background/out-of-bound positions with a learnable pad token. A depthwise 2D convolution is applied on this map to capture local spatial context; the convolved features are then extracted back at the patch coordinates to form a 1D local-feature sequence. The original embeddings are fused with these local features using a learnable gate via\nH = Z + tanh(\u03bb) \u00b7 Z_local_seq,\nand Q/K/V are computed from this fused representation (after a short convolutional preprocessing \u03d5).\nIn the BRACS classification ablation, removing the local module reduces performance from the full model\u2019s AUC/ACC of 0.8208/0.5313 to 0.8013/0.5073, showing the local branch materially improves discriminative accuracy.",
      "source_document": "papers/2512.19331v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When analyzing whole-slide images with MIL, how can you operationalize \u201cmost patches are irrelevant\u201d using attention-based patch sampling, and what qualitative accuracy trend do you expect to see when (i) retaining only the top-attended patches versus (ii) removing the most informative patches\u2014what does this imply about redundancy and where the decision signal resides?",
      "answer": "Compute a per-patch importance score \u03b1_i from the trained model\u2019s attention and define three sampling sets for a bag of N patches: random-k (uniformly choose k indices), top-k (choose patches whose \u03b1_i are at least the k-th largest attention value), and bottom-k (choose patches whose \u03b1_i are at most the k-th smallest attention value). Re-run the model using only the selected subset and vary the retained ratio k/N to track ACC. The observed trend is that keeping a small fraction of high-attention patches (on the order of only ~5\u201310% of patches) preserves accuracy close to using all patches, whereas discarding the most informative/high-attention patches causes accuracy to drop sharply. This indicates strong redundancy in WSIs: the slide-level decision is dominated by a tiny subset of discriminative regions while most patches contribute little and can be treated as task-irrelevant noise.",
      "source_document": "papers/2512.19331v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In weakly supervised MIL for whole-slide images, what trade-off does stacking more sequence-integration layers (e.g., multiple gated delta/memory blocks) introduce, and why can a shallow (single-layer) design outperform deeper (2\u20133 layer) stacks in terms of slide-level classification AUC/accuracy?",
      "answer": "Increasing the number of stacked gated-delta/memory blocks introduces additional iterative mixing of patch information that can amplify noise and blur the small set of discriminative patches. Empirically, performance peaks with a single layer and drops when using 2\u20133 layers. The explanation is that, under weak slide-level supervision and high WSI heterogeneity, deeper recurrent stacking becomes more sensitive to irrelevant regions and can over-smooth discriminative instances; it is also harder to optimize with limited supervision, which reduces generalization.",
      "source_document": "papers/2512.19331v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a MIL aggregator on whole-slide patch sequences with an effectively tiny batch size, what task-dependent optimization/regularization choices are applied differently for survival prediction versus slide-level classification, and what is the intended effect of those differences on training stability and overfitting?",
      "answer": "Training uses Adam with early stopping on a validation metric and batch size 1. For survival prediction specifically, dropout is enabled (rate 0.25) and gradients are accumulated over multiple steps (32-step accumulation), whereas for slide-level classification dropout and gradient accumulation are not used. The intent is to regularize the survival model and to effectively increase the batch signal / stabilize optimization under batch size 1 (via accumulation), while avoiding unnecessary regularization/complexity for classification training.",
      "source_document": "papers/2512.19331v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In CBCT-to-CT synthesis with a discriminator that has both an adversarial head and an auxiliary segmentation head, how are real and generated CT patches used differently during training so that the segmentation loss updates either the discriminator or the generator, and what specific segmentation loss formulation is used?",
      "answer": "Training uses both real CT and generated CT patches: when a real CT patch is fed to the discriminator, it outputs a segmentation map that is compared to the ground-truth segmentation, and the discriminator is updated using a segmentation loss (Lsegd). When a generated (fake) CT patch is fed in, the discriminator\u2019s weights are frozen; the segmentation map is still compared to the same ground truth, but the resulting segmentation gradients (Lsegg) are backpropagated through the frozen discriminator to update the generator, encouraging structurally/semantically coherent synthesis. The segmentation loss used for both Lsegd and Lsegg is a combination of Dice loss and cross-entropy.",
      "source_document": "papers/2512.19336v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a 3D U-shaped generator that replaces standard U-Net convolutions with ConvNeXt-style blocks for MRI/CBCT-to-CT synthesis, what are the key design choices inside the 3D ConvNeXt-inspired basic block (kernel size, normalization, and the convolutional FFN structure), and how do the dedicated downsampling and upsampling blocks change the first layer while preserving residual learning?",
      "answer": "The generator\u2019s basic unit starts with a 3D depthwise convolution using a compact 3\u00d73\u00d73 kernel (instead of the large 2D ConvNeXt kernels). It replaces ConvNeXt\u2019s layer normalization with instance normalization to better handle small batch sizes. After the depthwise conv it uses a transformer-inspired convolutional feed-forward network (convFFN): a 1\u00d71\u00d71 pointwise convolution expands the channel dimension by a stage-specific expansion ratio Ri, followed by a second 1\u00d71\u00d71 pointwise convolution that projects back to the original channel size; the block output is added to the block input via a residual connection.\nFor resolution changes, the downsampling block modifies the basic block\u2019s first layer into a stride-2 depthwise convolution; to keep the residual addition valid it uses a resize convolution on the shortcut path to match spatial and channel dimensions before addition. The upsampling block is analogous but replaces the first layer with a stride-2 transposed convolution for upsampling, while keeping the remaining convFFN and residual structure the same.",
      "source_document": "papers/2512.19336v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a 3D patch-based GAN for MRI\u2192CT synthesis with a multi-term objective, which added loss component tends to produce the largest performance jump beyond voxel-wise MAE, and what are the intended roles/design choices of (i) feature-matching loss and (ii) the perceptual loss backbone in improving synthesis quality?",
      "answer": "Adding the adversarial (conditional GAN) loss produces the largest single improvement beyond MAE-only training. The feature-matching loss is applied to intermediate discriminator activations to stabilize adversarial training and encourage convergence by aligning higher-level feature representations between real and synthetic CT. The perceptual loss is implemented as an LPIPS-style learned perceptual similarity, but with the usual pretrained VGG network replaced by a stronger ConvNeXt-B backbone to better capture structural and texture information.",
      "source_document": "papers/2512.19336v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a fully convolutional GAN for 3D MRI\u2192CT and CBCT\u2192CT synthesis against common convolutional and transformer backbones, which baseline backbone emerges as the strongest competitor, and how does the proposed model\u2019s performance gap differ between the MRI\u2192CT and CBCT\u2192CT tasks in terms of masked MAE/PSNR/MS-SSIM trends?",
      "answer": "The strongest competing baseline is MedNeXt (a modern convolutional backbone), which is the closest runner-up on both tasks.\n\n\u2022 MRI\u2192CT: the performance gap vs MedNeXt is relatively modest\u2014GANeXt achieves the best masked metrics and improves over MedNeXt by about 2.05 masked MAE, ~0.81 dB PSNR, and ~0.0014 MS-SSIM on the local validation set (and further improves on the official validation set, indicating good generalization).\n\n\u2022 CBCT\u2192CT: the gap is much larger\u2014GANeXt surpasses MedNeXt by about 21.73 masked MAE, ~3.26 dB PSNR, and ~0.0428 MS-SSIM on the local validation set (and again improves on the official validation set).\n\nOverall, the comparison shows the ConvNeXt-powered GAN consistently outperforming both conventional convnets (3D U-Net/U-Net++) and a transformer baseline (SwinUNETR), with especially pronounced gains for CBCT\u2192CT.",
      "source_document": "papers/2512.19336v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a 3D image-to-image GAN for volumetric CT synthesis, how does a conditional PatchGAN discriminator typically incorporate the conditioning input, what is the main convolutional stack design (kernel/stride, activations, normalization, and final prediction layer), what is the discriminator\u2019s patch-level output resolution relative to the input volume, and why is this patch-based formulation beneficial compared with a single global real/fake score?",
      "answer": "The discriminator is a 3D conditional PatchGAN that evaluates a (real or generated) CT patch while being conditioned on the corresponding input patch x (used as an additional condition). Architecturally, it uses a cascade of strided 4\u00d74\u00d74 convolution layers with LeakyReLU activations and instance normalization, progressively increasing channels (from 32 up to 512) while downsampling, and then applies a final 1\u00d71\u00d71 convolution to produce patch-wise real/fake logits. The prediction is a dense score map of shape 1\u00d7(D/16)\u00d7(H/16)\u00d7(W/16), i.e., one score per local 3D region rather than one global score. This patch-based scoring makes the discriminator focus on local volumetric realism\u2014high-frequency structure and texture\u2014because its receptive field covers a local region instead of the entire volume, which is well-suited for enforcing detailed anatomical fidelity in 3D synthesis.",
      "source_document": "papers/2512.19336v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In ReasonCD\u2019s end-to-end training (where the model must both generate a short text response and output a pixelwise change map), what is the full training objective\u2014i.e., which losses are used for (a) text generation and (b) change-mask supervision, how are they combined, and what mechanism is used to penalize generations that omit the special <CHG> token needed to trigger change-map decoding?",
      "answer": "ReasonCD optimizes a sum of a text-generation loss and a change-mask loss: \n\u2022 Text generation: a weighted cross-entropy loss L_text = \u03bb_ce \u00b7 L_ce between the LLaMA2-generated text and the annotated text. To ensure the downstream change-map features can be extracted, the model increases the penalty when the special token <CHG> does not appear: when <CHG> is missing, the loss weight for the <CHG> token class is set to 10\u00d7 its original value via \u03bb_ce.\n\u2022 Change mask: a pixel-level loss L_cd = \u03bb_bce \u00b7 L_bce + \u03bb_dice \u00b7 L_dice, using weighted binary cross-entropy plus Dice loss between the predicted change probability map M\u2032 and the ground-truth mask M.\n\u2022 Total objective: L = L_text + L_cd = \u03bb_ce L_ce(y_text, y\u2032_text) + \u03bb_bce L_bce(M, M\u2032) + \u03bb_dice L_dice(M, M\u2032).",
      "source_document": "papers/2512.19354v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multimodal remote-sensing change detection model where a SAM-style mask decoder must condition on both high-level semantics (from an LLM prompt) and bitemporal image features, what design is used to fuse high-level features from a deep transformer image encoder with lower-level visual cues, and why does this fusion primarily affect the mask-supervision loss/convergence rather than the text-generation loss? Describe the feature extractors involved, the fusion operations (including any attention/MLP blocks), and the observed training/accuracy behavior when the fusion is removed.",
      "answer": "The model adds a multi-scale feature fusion module fu(\u00b7) to produce the bitemporal visual feature fed into the SAM mask-decoder-style ChangeMap Decoder. The motivation is that SAM\u2019s image encoder (a deep transformer) yields strong high-level semantic features but is relatively weak at preserving low-level visual details needed for accurate pixel-level change masks.\n\nConcretely, the module extracts low-level/multi-scale visual features with a ResNet34 backbone plus an FPN, and concatenates these with the features from the SAM image encoder. A channel-attention mechanism then reweights the concatenated high-/low-level channels: it uses two pooling branches (average pooling and max pooling), passes both through a shared-weight MLP for nonlinear transformation, sums the two outputs, applies a sigmoid to obtain per-channel attention scores, and multiplies these scores back onto the original features. After concatenation, a small feed-forward mapping network (FFN) made of two 1\u00d71 convolution layers (each followed by BN and GELU) is used for channel transformation to produce the fused output fu(X_img_t1, X_img_t2) (reported as R256\u00d764\u00d764).\n\nBecause this module sits after the LLM\u2019s text generation and before the ChangeMap Decoder, it does not change the language-model text generation pathway; accordingly the text-generation cross-entropy loss is essentially unchanged with/without the module. Its impact is on the change-mask branch: removing the module degrades change-mask loss optimization and yields worse validation metrics, while including it improves change-detection accuracy across metrics and makes training converge faster and more stably (e.g., higher early-epoch validation performance and a lower optimized mask-loss value).",
      "source_document": "papers/2512.19354v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When adapting a large language model with LoRA for a multimodal change-detection system (where the LLM must reason over bitemporal image tokens and emit a special token that conditions a mask decoder), what is the exact LoRA setup: which weights remain frozen vs. trained, how are the LoRA A/B matrices initialized and why, which self-attention projection matrices are targeted for low-rank adaptation, and what does the rank-ablation reveal about the best-performing rank and its effect on training stability (oscillation/smoothness) versus the overall loss curve?",
      "answer": "The LoRA update is added to a pre-trained weight W0 as W = W0 + \u0394W with \u0394W = A\u00b7B; during fine-tuning only the low-rank matrices A and B are trained while the original W0 stays frozen. A is initialized with Kaiming initialization, while B is initialized to zeros so that the LoRA branch initially contributes nothing (A\u00b7B = 0), letting the model start from the pre-trained behavior and then gradually introduce low-rank adjustments for more stable early training; A cannot be zero-initialized because it would block effective gradient learning of the update. In the model\u2019s main training configuration, LoRA is applied primarily to the q and v projection matrices in each LLaMA2 self-attention layer. In rank ablation (testing multiple ranks), the intermediate rank (r=8) gives the best change-detection metrics overall (aside from precision) and exhibits stronger learning; lower ranks show larger metric-curve oscillations and worse stability, while higher ranks reduce oscillation and make loss reduction smoother, even though the loss curves across ranks are reported to be nearly identical in value overall.",
      "source_document": "papers/2512.19354v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a change-detection MLLM that performs reasoning over a single token sequence containing both text and two images, how are the two images converted into tokens and inserted into the LLM input (including the vision backbone used, which layer/token features are extracted and why, the image preprocessing, how feature dimensionality is matched to the LLM embedding size, and what operation actually injects the image features into the text token sequence)?",
      "answer": "The model embeds each timepoint image with a frozen CLIP image encoder (ViT-based). Rather than using the final-layer [CLS] embedding (which discards detail), it takes all patch token embeddings from the penultimate CLIP layer (index \u22122) and excludes the first [CLS] token (slice [1:]); with CLIP\u2019s patch size 14 this yields 16\u00d716=256 patch tokens per image, each 1024-D. Before encoding, CLIP\u2019s preprocessing normalizes RGB channels to mean [0.48145466, 0.4578275, 0.40821073] and std [0.26862954, 0.26130258, 0.27577711], and resizes the image to 224\u00d7224. A trainable linear projection then maps each 1024-D patch embedding to the LLM embedding dimension 4096, producing 256\u00d74096 tokens for each image. Finally, the token embeddings at the special placeholders <ImgT1> and <ImgT2> in the text embedding are replaced with these projected patch-token sequences, so the resulting multimodal input sequence has (num_tokens\u22122 + 2\u00d7256) tokens, each 4096-D.",
      "source_document": "papers/2512.19354v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "If you want to compare a multimodal, semantics-first change-detection system (that generates a short text plus a change mask) fairly against standard fully-supervised change-detection CNN baselines on a dataset like BCDD, how can you set up the text side of the task so the comparison isolates mask-decoding quality (rather than implicit-intent reasoning), and what qualitative outcome on the BCDD test metrics shows the multimodal model is competitive with supervised baselines?",
      "answer": "To isolate mask-decoding quality on BCDD (a single-category building-change dataset) and avoid confounding from implicit-CRoI reasoning, the text input is normalized to an explicit CRoI: the \u201cquestion\u201d field is set uniformly to \u201cBuilding\u201d, and the target \u201canswer\u201d text is set uniformly to \u201cIt\u2019s <CHG>\u201d, so training/evaluation reduces to a single-CRoI fitting task while still exercising the model\u2019s text\u2192<CHG>\u2192mask pipeline. Under this setup, the multimodal model is competitive with (and largely better than) fully supervised CD baselines: on the BCDD test set it surpasses all eight supervised baselines on the reported metrics except Precision (i.e., it leads on F1/Recall/OA/IoU/Kappa while lagging only on Precision).",
      "source_document": "papers/2512.19354v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When auxiliary feature-refinement modules are removed at inference in a drone\u2013satellite geo-localization model, how does a hierarchical re-ranking alignment learning (HRAL) scheme still train the backbone to be discriminative\u2014specifically, how are refined features constructed via re-ranking (neighborhood definition and feature diffusion), how is cross-batch information incorporated, and which losses are combined to align the backbone features to these refined representations?",
      "answer": "HRAL trains the backbone by turning re-ranking into an explicit training-time supervisory signal. (1) Refined-feature construction (improved re-ranking): for each iteration, query features qf and gallery features gf are concatenated into F=[qf;gf]. Pairwise distances are computed, normalized column-wise, and a top\u2011k neighbor list Rank(i) is built per sample. A k\u2011reciprocal neighborhood R(i) is formed as those j in Rank(i) for which i is also in Rank(j), then expanded with candidates whose reciprocal sets overlap sufficiently (overlap ratio > 2/3). Using these neighborhoods, a row-stochastic affinity matrix V is computed with Gaussian weights on the normalized distances; then a light query-expansion smoothing averages rows over the initial top\u2011k neighbors. Refined features are produced by residual diffusion and L2 normalization: F\u0302 = Norm\u21132( \u03b1F + (1\u2212\u03b1)VF ) (with \u03b1=0.7), and split back into refined q\u0302f and \u011df.\n(2) Cross-batch incorporation (hierarchical alignment): a memory mechanism stores query and gallery features from historical batches; the same improved re-ranking is applied to both the current batch and stored historical batches so neighborhood relationships span current + historical samples, extending re-ranking beyond the current batch distribution and enforcing cross-batch consistency.\n(3) Losses used to align backbone features: the supervisory effect of re-ranking is expressed by alignment losses between original features and their refined versions. For the current batch, cosine-similarity losses (1\u2212cos(qf,q\u0302f) and 1\u2212cos(gf,\u011df)) plus KL-divergence terms between similarity distributions p(\u00b7) computed with softmax are summed to form Lcurrent; the same formulation over historical samples gives Lhistorical. In addition, an InfoNCE contrastive loss directly aligns qf and gf across views. These are combined as L3 = Lcurrent + Lhistorical + LInfoNCE, and overall training jointly optimizes the SSA cosine-embedding loss L1 and SHS cross-entropy loss L2 with HRAL via Ltotal = \u03bb1 L1 + \u03bb2 L2 + L3.",
      "source_document": "papers/2512.19365v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a spike-driven cross-view geo-localization network, how can a selective-attention refinement block be implemented to mitigate information loss from sparse spiking activations\u2014specifically: (1) how convolutional positional encoding is applied before attention, (2) how two spike-driven gates provide separate local vs global modulation while preserving gradients via residual paths, and (3) how the resulting refined drone/satellite features are supervised with a cosine-embedding objective to reduce the cross-view representational gap?",
      "answer": "A selective-attention refinement can be implemented as the Spike-Driven Selective Attention (SSA) block:\n\n1) Convolutional positional encoding (CPE) before SSA: the backbone output feature F (shape B\u00d7C\u00d7L) is reshaped into a 2D grid P\u00d7P with P=\u221aL, a lightweight depthwise convolution is applied in a residual manner to inject local spatial/positional priors, and the result is flattened back to a 1D sequence. Formally: F\u207a = F + Fla(DWConv(Res(F,P,P))).\n\n2) Two spike-driven gates for local vs global modulation with residual compensation:\n\u2022 After LayerNorm, a Conv followed by DWConv and spiking activation produces a refined feature \u007fF\u207a = SN(DWConv(Conv(LN(\u03a0(F\u207a))))).\n\u2022 A \u201cglobal\u201d spike-driven gate A is produced by a linear projection and spiking activation: A = SN(F\u207a W_a).\n\u2022 A local branch builds Q = Conv(\u007fF\u207a) and a locally processed feature Q\u2032 = SN(DWConv(Conv(\u007fF\u207a))). In parallel, a \u201clocal\u201d gate G is generated by a linear projection on Q (with appropriate permutation): G = SN(\u03a0\u207b\u00b9(Q) W_b).\n\u2022 Local fusion is done by element-wise modulation and normalization: Q\u2033 = LN(\u03a0(Q\u2032) \u2299 G). Then global modulation is applied and combined with a residual term before an output projection: Q\u2034 = W_c( SN(Q\u2033) \u2299 A + Q\u2033 ). This is added back to the CPE feature through a residual path: F\u207a\u207a = F\u207a + \u03a0\u207b\u00b9(Q\u2034). Additional residual paths (a DWConv residual branch and an SNN-MLP main path) further stabilize gradients and preserve information.\n\u2022 Functionally, gate G modulates locally processed features (local-region selection), while gate A operates over the global region; multi-level residual connections are used to counter potential gradient vanishing and information loss from gating.\n\n3) Cosine-embedding supervision to reduce cross-view gap: after SSA, enhanced drone/satellite features (bF_d and bF_s) are trained with a cosine embedding loss that averages 1\u2212cosine-similarity over paired drone\u2013satellite samples: L1 = (1/N) \u03a3_k (1 \u2212 cos(bF_d, bF_s)).",
      "source_document": "papers/2512.19365v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a spike-driven transformer for drone\u2013satellite geo-localization, how can a hybrid state-space refinement block be structured to capture long-range dependencies without losing local spatial cues\u2014specifically: (1) how the feature tensor is alternately reshaped between spatial (B\u00d7C\u00d7H\u00d7W) and sequence (B\u00d7C\u00d7L) forms, (2) what operations are applied in each form (e.g., residual spike-driven depthwise convolutions, LayerNorm, an HSM-SSD state-space module, and a spike-driven feed-forward network), and (3) what supervision is used to enforce cross-view semantic consistency from the block\u2019s outputs?",
      "answer": "A spike-driven hybrid state space (SHS) block can be built by explicitly mixing local spatial priors with a linear-complexity state-space module through alternating spatial/sequence reshaping:\n\n1) Reshaping between spatial and sequence forms:\n- Start from backbone features Fi_d \u2208 R^{B\u00d7C\u00d7L}.\n- Reshape to a 2D spatial feature F^{\u03b7i}_d \u2208 R^{B\u00d7C\u00d7H\u00d7W} before applying spatial convolutions.\n- After spatial processing, flatten back to a 1D sequence (B\u00d7C\u00d7L) so the state-space module operates along the token/patch dimension L.\n- After the state-space step, reshape back to (B\u00d7C\u00d7H\u00d7W) for further spatial refinement.\n\n2) Operations applied in each form:\n- In spatial form, apply a spike-driven depthwise convolution with a residual connection to inject local spatial information:\n  F~^{\u03b7i}_d = F^{\u03b7i}_d + DWConv(SN(F^{\u03b7i}_d)).\n- Flatten (Fla(\u00b7)) to sequence and apply LayerNorm to stabilize the distribution.\n- Apply the hybrid state-space module HSM-SSD along L to capture long-range dependencies with linear complexity, then reshape back:\n  F~^{\u03b7i+}_d = Res(HSM-SSD(LN(Fla(F~^{\u03b7i}_d))), H, W).\n- Apply a second residual spike-driven depthwise convolution to further strengthen local spatial representations and reduce fine-grained information loss under sparse activations:\n  F~^{\u03b7i++}_d = F~^{\u03b7i+}_d + DWConv(SN(F~^{\u03b7i+}_d)).\n- Finish with a spike-driven feed-forward network (SFFN) implemented as two 1\u00d71 spike-driven convolutions with BatchNorm, again in residual form:\n  bF^{\u03b7i}_d = F~^{\u03b7i++}_d + SFFN(F~^{\u03b7i++}_d).\n\n3) Supervision for cross-view semantic consistency:\nAfter SHS, the drone-view and satellite-view refined features are passed through spike firing activation and a classification layer, and a cross-entropy loss is applied (denoted L2). This classification-based cross-entropy supervision is used to enforce discriminative learning and semantic consistency/alignment of cross-view features produced by the SHS block.",
      "source_document": "papers/2512.19365v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating an SNN-based drone\u2013satellite geo-localization model against ANN baselines, how can inference-time energy consumption be estimated in a hardware-agnostic way, and what condition guarantees that the SNN is more energy-efficient? In your explanation, include (i) how ANN FLOPs are converted into an SNN energy estimate, (ii) the roles of the simulation timesteps and spike firing rate, and (iii) why AC operations typically lead to lower energy than MAC operations.",
      "answer": "A hardware-agnostic estimate treats ANN compute as FLOPs and converts those FLOPs into an SNN energy proxy by assuming that the ANN\u2019s dense MAC-based FLOPs become sparse, spike-triggered AC operations in the SNN.\n\n(i) If an ANN layer has FLOPs FL (e.g., convolution/MLP FLOPs), ANN energy is approximated as:\nE_ANN \u2248 E_MAC \u00b7 FL,\nwhere E_MAC is the energy per multiply-and-accumulate.\n\nFor the SNN counterpart, energy scales with how often AC operations are actually activated over time and space:\nE_SNN \u2248 E_AC \u00b7 T \u00b7 R_f \u00b7 FL,\nwhere E_AC is the energy per accumulate-and-compare, T is the number of simulation timesteps, and R_f is the spike firing rate (the fraction of non-zero elements in the spike tensor).\n\n(ii) Thus, increasing timesteps T or firing rate R_f increases SNN energy linearly; sparse spiking (low R_f) is the key driver of energy savings.\n\n(iii) The SNN is more energy-efficient whenever\nE_AC \u00b7 T \u00b7 R_f < E_MAC.\nThis typically holds because AC operations are assumed to consume substantially less energy than MAC operations under common CMOS measurements, and SNN activations are sparse (R_f \u226a 1), making the left-hand side small even with multiple timesteps.",
      "source_document": "papers/2512.19365v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "How can an inference-time sparsification protocol be constructed to stress-test a drone\u2013satellite geo-localization model\u2019s ability to (i) preserve and use sparse \u201ccritical\u201d visual cues and (ii) integrate long-range, spatially separated context\u2014specifically, how are the retained regions parameterized (retention ratio vs number of patches), how is the new image formed from the original, what is done to unselected pixels, and why does permitting patch overlap/repetition matter for the difficulty of the test?",
      "answer": "A stress test can be created by deliberately sparsifying each input image via a \u201ckeep random patches\u201d procedure.\n\n\u2022 Parameterization: choose a retention ratio R that specifies the total fraction of the image area to preserve, then split that retained area into P small patch regions (so each patch has area proportional to H\u00d7W\u00d7R/P).\n\n\u2022 Image construction: sample the P patches at random spatial positions; copy only the original pixels that fall inside those selected patches into a new image; set all remaining (unselected) regions to 0, yielding highly sparse and fragmented inputs where continuous context is disrupted.\n\n\u2022 What it tests: under these conditions, solving drone\u2013satellite retrieval requires relying on limited remaining \u201ccritical information\u201d while also integrating cues that are now spatially separated, probing both discriminative cue preservation and long-range dependency modeling.\n\n\u2022 Overlap/repetition: allowing overlap and repetition makes the effective visible content unpredictable\u2014so even when R=100 the constructed image is almost never identical to the original\u2014and can increase confusion (especially when using larger P, which raises overlap risk), thereby making the test more extreme and less trivially equivalent to standard augmentations like padding/mirroring.",
      "source_document": "papers/2512.19365v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a dual-pathway surgical workflow recognition model that aims to reduce frame-to-frame prediction jitter while improving recognition of visually ambiguous phases, how can temporal stabilization and discriminative enhancement be implemented as separate modules, how are their outputs adaptively fused based on prediction confidence, and what training objective can jointly address class imbalance and encourage temporally smooth predictions?",
      "answer": "Temporal stabilization can be handled by a Reliable Memory Propagation (RMP) module that keeps a FIFO sliding-window memory bank of recent frame features and selectively aggregates only \u201creliable\u201d past features. Reliability is assessed per memory entry using multiple criteria: (i) feature similarity between the current feature and the stored feature (cosine similarity), (ii) class-consistency between their baseline predicted phase distributions (dot-product similarity of softmax outputs), and (iii) temporal proximity via an exponential decay that down-weights distant frames. Entries whose combined reliability score passes a threshold are retained and fused with normalized reliability weights to produce a refined temporal feature.\n\nDiscriminative enhancement can be handled by an Uncertainty-Aware Prototype Retrieval (UPR) module that maintains a class-specific prototype bank populated with the most uncertain training features for each phase, where uncertainty is measured as one minus the maximum baseline class probability. A lightweight policy decides whether to add/replace prototypes so each class bank focuses on hard samples. At inference, the current feature is refined by retrieving top-k prototypes using cosine similarity, reweighted by the baseline class probability of each prototype\u2019s class, and aggregating the selected prototypes with similarity-based softmax weights.\n\nThe two pathway outputs are fused with a confidence-driven gate derived from baseline prediction confidence c_t = max(p_base_t): gate weights for memory and prototype pathways are computed with sigmoid functions of (threshold \u2212 c_t), so when confidence is low the gates increase (using more temporal/prototype guidance) and when confidence is high they attenuate (preserving the original feature). The final feature is the sum of the original feature plus gated RMP and UPR refinements.\n\nA joint training objective combines (1) a class-balanced cross-entropy loss to mitigate phase imbalance and (2) a temporal smoothness regularization term defined as a KL-divergence penalty between adjacent-frame predictions to discourage abrupt changes, i.e., L = L_CE + L_KL.",
      "source_document": "papers/2512.19387v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an uncertainty-aware prototype retrieval module for surgical phase recognition, how are \u201cuncertain\u201d training features quantified and selected into class-specific prototype banks (including the decision policy inputs and the replacement rule when the bank is full), and during inference how are prototypes scored/filtered and aggregated to refine the current frame representation (including the role of class probabilities and the chosen top\u2011k setting)?",
      "answer": "Uncertainty is measured from the baseline per-frame prediction as u_t = 1 \u2212 max(p_t^{base}) (low confidence \u21d2 high uncertainty). For each class c, a prototype bank P_c stores the N most uncertain features seen for that class. Whether a feature f_t (with ground-truth class c) is added is decided by a lightweight policy network that takes a state vector encoding u_t, prediction entropy, confidence margin, and current bank occupancy and outputs add/skip; an action is sampled as a_t ~ Bernoulli(\u03c0_\u03b8(s_t)). If the action is add and the bank is not full, f_t is inserted; if the bank is full, f_t replaces the least-uncertain stored prototype when its u_t is higher.\n\nAt inference, each prototype p_j is scored by cosine similarity sim(f_t,p_j) and then weighted by the baseline class probability for that prototype\u2019s class label c_j: s_j = p_t^{base}[c_j] \u00b7 sim(f_t,p_j). The top\u2011k prototypes with highest s_j are selected and aggregated with similarity-softmax weights w_j = softmax(sim(f_t,p_j)) to form f_t^u = f_t + \u03a3_{j\u2208top-k} w_j p_j. The method uses top\u2011k = 8 (k=8 and 16 are comparable; k=4 causes an accuracy drop).",
      "source_document": "papers/2512.19387v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a surgical workflow recognition model that uses a short-term feature memory to reduce frame-wise prediction jitter, how can the model score the reliability of each historical feature using complementary cues (semantic alignment, prediction agreement, and recency), and once these scores are computed how are memories filtered and weighted to produce the temporally refined feature for the current frame?",
      "answer": "Reliability can be computed per memory entry f_i in a sliding-window FIFO bank M_t={f_{t\u2212K},\u2026,f_{t\u22121}} by combining three criteria: (1) semantic/feature similarity via cosine similarity s_sim(f_t,f_i)=(f_t\u00b7f_i)/(||f_t||\u00b7||f_i||); (2) class-consistency as agreement between the baseline predicted phase distributions, using the dot product s_cls(f_t,f_i)=(p_t^base)^T p_i^base where p^base=softmax(y^base); and (3) temporal proximity with exponential decay s_temp(t,i)=exp(\u2212|t\u2212i|/\u03c4) so nearer frames are favored. The overall reliability is the sum r_i=s_sim+s_cls+s_temp. Selective propagation keeps only entries with r_i>\u03b8, then assigns normalized weights over the retained entries with a softmax on r_i (w_i=exp(r_i)/\u2211_j exp(r_j) over selected j). The refined temporal feature is produced by aggregating the weighted selected memories together with the current feature through a convolution: f_t^m = Conv(f_t,{w_i\u00b7f_i | r_i>\u03b8}), so only confident, temporally coherent history contributes.",
      "source_document": "papers/2512.19387v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When ablating a dual-pathway surgical phase recognition system that separately targets temporal stability (memory propagation) and ambiguous-frame discrimination (prototype retrieval), what distinct changes in evaluation metrics would you expect from adding each module alone versus combining them, and what additional benefit indicates that a confidence-based integration gate is doing more than a simple additive fusion?",
      "answer": "Adding the temporal-stability pathway (reliable memory propagation) primarily improves overall correctness and temporal consistency, reflected by a clear gain in accuracy along with increases in recall and F1 compared with the backbone baseline. Adding the ambiguous-frame pathway (uncertainty-aware prototype retrieval) mainly strengthens discrimination on difficult/overlapping phases, showing its most notable improvement in overlap/segmentation-style quality (Jaccard) and also improving precision and recall over the baseline. Using both modules together yields further improvements beyond either module alone (i.e., complementary gains rather than redundancy), improving accuracy/Jaccard/recall compared with RMP-only and UPR-only configurations. Finally, introducing the confidence-driven integration gate produces an additional performance boost compared with the same model without gated integration, indicating the gate\u2019s adaptive weighting based on prediction confidence adds value beyond simply summing the two pathway features.",
      "source_document": "papers/2512.19387v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a surgical phase recognition model on a long, highly imbalanced workflow dataset with gradual phase transitions, what evaluation protocol and set of metrics can be used to ensure a fair comparison across methods and to capture both frame-wise correctness and temporal overlap/robustness\u2014and what additional diagnostic can help analyze inter-phase confusions and temporal consistency?",
      "answer": "Use a cross-validation protocol that evaluates on multiple held-out surgeries (here, three-fold cross-validation with 14 cases for training and 7 for testing per fold, reporting averages). Report a metric suite consisting of frame-level accuracy plus Jaccard index, precision, recall, and F1-score to capture classification correctness, temporal overlap quality, and robustness to class imbalance. Additionally, analyze confusion matrices to visualize inter-phase misclassifications and assess temporal consistency.",
      "source_document": "papers/2512.19387v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-organ CT model for grading esophageal varices on an ordinal scale (G0\u2013G3), how can you design the training objective so that it (i) enforces the ordinal nature of the grades at the fused prediction level and (ii) explicitly regularizes agreement between the esophagus branch and the liver/spleen branches? Describe the specific losses used, what network outputs they act on, and what each term is intended to achieve.",
      "answer": "Use a composite objective with two complementary terms:\n\n1) **Ordinal regression loss on the fused logits**: After fusing features/logits from the esophagus, liver, and spleen branches into a fused prediction vector \\(H_F\\), convert each ground-truth grade (G0\u2013G3) into an **ordinal (cumulative) binary encoding** \\(Y_{ord}\\). Apply an ordinal regression loss by penalizing the distance between \\(H_F\\) and \\(Y_{ord}\\) (formulated as a squared error), which encourages predictions that respect the ordered progression of severity rather than treating grades as unrelated classes.\n\n2) **Deep CCA (DCCA) regularization between organ branches**: Compute a DCCA loss that aligns the esophagus branch logits/features \\(H_E\\) with the liver and spleen branch logits/features (\\(H_L\\), \\(H_S\\)) after passing each through small projection networks. The DCCA term maximizes cross-correlation (equivalently minimizes the negative normalized trace of the cross-correlation matrix), acting as a regularizer that drives organ representations/logits to be more consistent across organs.\n\nThe overall loss is a weighted sum of the ordinal term plus the DCCA alignment terms between \\(H_E\\) and each of \\(H_L\\) and \\(H_S\\), so training simultaneously learns correct ordinal grading while enforcing multi-organ cohesion.",
      "source_document": "papers/2512.19415v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a multi-organ NCCT network for esophageal varices grading, how can you implement an inter-organ feature interaction block that lets liver/spleen features selectively refine esophagus features without the full cost of applying self-attention over concatenated multi-organ tokens? Describe the key steps in the interaction block (including any pooling/projection, attention formulation, and the \u201cswitching\u201d mechanism), and explain what empirical takeaway the ablation provides about this switching cross-attention design versus simpler fusion (add/concat) or non-switching attention variants.",
      "answer": "A workable design is to (1) extract organ-specific feature maps for esophagus, liver, and spleen, then (2) pool them to a shared spatial size so the organs can interact in a common latent grid. The interaction block then runs two parallel paths on these pooled features: an attention path and a direct (non-attention) projection path. In the attention path, the pooled features are reshaped into an attention-ready sequence and processed with scaled dot-product attention using learned query/key/value projections, i.e., SoftMax(QK\u1d40/\u221ad_k)V, followed by a learned linear projection back to the feature space. In the direct path, features are only linearly projected (no attention). Instead of always using attention (or always using direct fusion), the block alternates (\u201cswitches\u201d) between the attention and direct paths across multiple iterations, and then uses convolution and interpolation to return to the organ feature resolutions.\n\nThe ablation shows that this switching cross-attention interaction is the strongest feature-interaction choice: attention-based interactions improve over simple add/concat fusion, and among attention variants, the switching cross-attention strategy performs best overall while keeping computational complexity manageable compared with applying self-attention to concatenated features (and it also outperforms a query-swapped cross-attention alternative).",
      "source_document": "papers/2512.19415v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When extending a multi-organ NCCT model for 4-grade esophageal varices (G0\u2013G3) to incorporate *clinical priors* derived from organ segmentations, how can you turn the structured measurements (esophagus/liver/spleen volumes and liver-to-spleen volume ratio) into a multimodal embedding that improves grading performance? Describe (i) how the priors are represented and encoded (including how text prompts are formed and what CT signal is used for the image prior), (ii) how the image/text prior features are adapted before fusion, and (iii) what the ablation evidence implies about using categorical vs numeric text descriptions and about adding extra image\u2013text alignment losses (e.g., contrastive/InfoNCE) on top of the pre-trained CLIP-style encoders.",
      "answer": "(i) The priors are computed directly from the organ segmentation masks as physical volumes using voxel counts and voxel spacing, and the liver-to-spleen volume ratio (LSVR) is included as an additional clinically motivated indicator of fibrosis/portal hypertension severity. These structured priors are then represented in two complementary modalities: a full-volume NCCT \u201cimage prior\u201d (the whole CT volume, resized to a fixed size) encoded by a CLIP-like 3D image encoder, and a \u201ctext prior\u201d formed as natural-language prompts that describe the organ sizes and their relationship (including LSVR), encoded by a CLIP text encoder (both encoders coming from a medical CLIP-style MLLM, M3D). \n\n(ii) The high-dimensional image and text embeddings are each passed through simple adaptor networks whose main role is dimensionality reduction before fusion; the adapted image feature (H_img) and adapted text feature (H_text) are concatenated and fused with the multi-organ image-branch features for final 4-grade classification.\n\n(iii) Ablations show that adding clinical priors consistently improves grading, but categorical (bucketed) text descriptions of volumes/LSVR outperform feeding raw numerical values, especially when combining categorical volume descriptions with categorical LSVR\u2014suggesting that clinically meaningful thresholds are easier for the text encoder/fusion to exploit than precise numbers. Additional explicit alignment supervision on the CLIP features (e.g., contrastive learning/InfoNCE) yields only marginal or inconsistent gains and does not materially improve overall performance, implying the pre-trained CLIP-style encoders already provide sufficient CT\u2013text alignment for this task and that keeping training simpler is preferable (also noting contrastive learning typically benefits from large batch sizes).",
      "source_document": "papers/2512.19415v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a multi-organ, multimodal (image + clinical-prior) model for 4-grade esophageal varices classification, what are the practical trade-offs among common *post-fusion* choices at the classifier input\u2014simple concatenation, prediction-sum, low-rank fusion, FiLM modulation, and cross-attention\u2014and what observed validation-vs-test behavior would justify selecting concatenation as the default fusion operator? Summarize how these fusion strategies differ conceptually and how their performance trends vary across the \u2265G1/\u2265G2/G3 thresholds and overall multi-class accuracy.",
      "answer": "A practical way to choose a post-fusion operator is to compare (i) how much modeling capacity the fusion adds versus (ii) how well it generalizes across datasets and across clinically meaningful thresholds.\n\nConceptually:\n- **Concatenation** preserves each modality/organ feature vector intact and lets a lightweight classifier learn a linear/nonlinear combination; it adds minimal inductive bias and tends to be stable.\n- **Prediction-sum (pred-sum)** combines branch outputs by summing, enforcing that modalities contribute in the same \u201clogit space,\u201d but can under-use complementary information.\n- **Low-rank fusion** explicitly models multiplicative interactions between feature groups with a parameter-efficient factorization; it can capture cross-feature co-dependencies but may overfit.\n- **FiLM** uses one modality to generate feature-wise scale/shift for another, giving controlled conditioning; it can be sensitive to which modality is used as the conditioner.\n- **Cross-attention** learns content-dependent interactions across feature groups; it is the most flexible but can be harder to train robustly and may generalize worse when data are limited.\n\nEmpirically, the fusion strategies all show strong performance for **\u2265G1** on validation (roughly 92\u201396% accuracy), with **concatenation** giving the best \u2265G1 result (about 96% accuracy and ~0.98 AUC). As the task becomes harder (especially **\u2265G2**), performance varies more across fusion types (validation spanning roughly mid-70s to mid-80s), while **G3** is relatively stable on validation (about high-80s to ~90%).\n\nThe key justification for choosing **concatenation** comes from **generalization to the independent test set**: while \u2265G1 remains high for all strategies (roughly low-to-mid 90s), **\u2265G2 degrades more noticeably**, and for **G3** the gap widens\u2014concatenation maintains strong G3 performance (around ~91%), whereas other fusion strategies cluster much lower (around ~75%). Overall, concatenation yields the most robust multi-class accuracy across validation and test distributions (i.e., the most consistent performance across datasets and severity grades), motivating its selection as the default post-fusion operator.",
      "source_document": "papers/2512.19415v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a multi-organ non-contrast CT (NCCT) model for 4-grade esophageal varices severity, what end-to-end *input pipeline* choices (preprocessing, organ localization/cropping, and data augmentation) are used to make the ROIs learnable and computationally practical, and what observed accuracy\u2013memory trade-off determines how large the cropped input volumes should be?",
      "answer": "The pipeline standardizes CT appearance and then feeds organ-focused ROIs rather than the entire scan. NCCT volumes are first resampled to isotropic 1\u202fmm spacing and windowed with an abdominal CT window (width 400\u202fHU, level 50\u202fHU). A localization/segmentation network (nnUNet-based, trained/fine-tuned on manually annotated organ masks and followed by radiologist review/refinement) produces esophagus/liver/spleen masks, which are used to crop localized ROIs; these ROIs are then resized to fixed organ-specific input sizes (different shapes for esophagus vs liver vs spleen). During training, the cropped ROIs undergo data augmentation including random rescaling, flipping, and cutout. Separately, a resized full CT volume is also prepared for the image-prior embedding pathway.\n\nEmpirically, input volume size strongly affects both performance and GPU memory: best grading accuracy is achieved when the input volume is slightly smaller than or approximately equal to the average cropped organ size, whereas larger volumes reduce multi-class accuracy while substantially increasing memory consumption.",
      "source_document": "papers/2512.19415v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When applying test-time scaling to diffusion multi-modal LLMs for text-to-image generation, how can you replace an external vision-language verifier while also reducing the usual O(NT) cost of exploring N trajectories for T denoising steps? Describe (i) the internal text\u2013image alignment signal used for ranking trajectories, and (ii) the main stages of the hierarchical trajectory search procedure that yields near-linear O(N+T) sampling complexity.",
      "answer": "External verification can be avoided by using the dMLLM itself as an in-loop verifier via Self-Verified Feedback (SVF): given a prompt C and an intermediate token sequence Z_t, the model is asked in a fixed yes/no template whether the image shows the prompt, and the logit probability of \u201cYes\u201d is used as the text\u2013image alignment score, \u03a6_SVF = logit_yes(G_\u03b8(Z_t, C)).\n\nTo reduce O(NT) linear search, Hierarchical Trajectory Search (HTS) performs a coarse-to-fine allocation of compute in three stages: (1) Initial stochastic exploration: sample N trajectories from the initialization prior and denoise only for T_s steps to obtain coarse hypotheses (SVF is not emphasized at very high noise). (2) Progressive hierarchical thinning: over steps t, the active trajectory width decays (e.g., W_t = max(\u230aN d^{-(t\u2212T_s)}\u230b, K)); at each step trajectories are scored by \u03a6_SVF, top-K are selected as survivors, and each survivor branches into stochastic continuations, with branching and width decreasing geometrically until W_t = K. (3) Final refinement: once narrowed to K trajectories (at T_r), refinement continues independently on these K trajectories until the final step T. By exploring broadly early and concentrating refinement on SVF-high trajectories later, the total sampling cost becomes near-linear, approximated as O(N+T), instead of evaluating all N trajectories for all T steps.",
      "source_document": "papers/2512.19433v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking test-time scaling methods for text-to-image generation on an object-centric prompt-alignment suite, what specific sub-capabilities are evaluated, and what results would demonstrate that the scaling method improves alignment broadly enough to match or surpass strong standalone text-to-image baselines?",
      "answer": "The object-centric suite used here (GenEval) breaks prompt-alignment into sub-capabilities covering: single-object generation, two-object compositionality, counting, color recognition, spatial position, and attribute binding, along with an overall score aggregated from these dimensions. Broad, meaningful improvement is indicated when test-time scaling raises scores across all (or most) of these dimensions consistently for multiple dMLLM backbones, and when the resulting overall score is competitive with or higher than strong non-dMLLM text-to-image systems (e.g., the TTS-enhanced dMLLM achieving a higher overall GenEval score than leading baselines like Qwen-Image or GPT-4o).",
      "source_document": "papers/2512.19433v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking an efficient test-time scaling strategy for diffusion multi-modal LLM text-to-image generation, how can you set up a fair efficiency\u2013quality comparison against a linear trajectory search baseline\u2014specifically (i) what compute-cost metric should be reported and how is the self-verification cost accounted for, (ii) what shared sampling settings keep outputs comparable across models, and (iii) what kind of speedup does a hierarchical trajectory search achieve at similar GenEval performance compared to linear search?",
      "answer": "(i) Inference cost is measured by the number of function evaluations (NFE), consistent with prior test-time-scaling studies; the self-verification used to score candidates is implemented as a single model step that outputs a direct \u201cYes/No\u201d answer for alignment checking, and this is part of the inference procedure. (ii) For fair comparison, baseline performance is established with 8 sampling steps, and all methods/models generate 512\u00d7512 images using the same classifier-free guidance setting (CFG = 4.0). (iii) Compared with linear trajectory search, hierarchical trajectory search reaches equivalent GenEval scores with substantially less compute\u2014about 5\u00d7 faster on Lumina-DiMOO and about 6\u00d7 faster on MMaDA and Muddit\u2014and it also converges to a higher overall GenEval score than the linear baseline.",
      "source_document": "papers/2512.19433v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When scaling inference for diffusion multi-modal LLM text-to-image generation by increasing the number of sampled trajectories (trajectory exploration scaling), what qualitative relationship should you expect between (i) the model\u2019s starting/baseline GenEval score and (ii) the size of the improvement from exploring more trajectories\u2014and on what kinds of GenEval prompt categories does test-time scaling tend to yield the largest gains versus more modest gains?",
      "answer": "Increasing the number of explored trajectories leads to a clear, positive improvement in GenEval scores across evaluated dMLLMs. The size of the gain is inversely correlated with the model\u2019s initial (baseline) score: weaker baselines benefit more from trajectory exploration scaling, while stronger baselines still improve but by a smaller margin. Across GenEval prompt categories, test-time scaling yields the largest gains on more complex compositional prompts (notably Counting, Position, and Attribute), whereas improvements on simpler/common prompts (e.g., Single Object, Two Object, Colors) are more modest because baseline performance is already strong.",
      "source_document": "papers/2512.19433v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In diffusion multi-modal LLM image generation, if you scale test-time compute by increasing the number of iterative refinement (denoising) steps per trajectory, what trend should you expect in (i) generation stability and benchmark prompt-alignment scores, and (ii) how should you choose an appropriate step count in practice\u2014including the recommended upper range for 512\u00d7512 sampling and what mainly determines the optimum?",
      "answer": "Increasing the refinement/denoising step count T makes the denoising process more fine-grained, which stabilizes generation trajectories and generally improves final image quality and GenEval prompt-alignment scores (scores rise consistently when scaling from about 8 up to 64 steps). In practice, indefinite scaling is not worthwhile; the optimal number of refinement steps is mainly determined by the complexity of the text prompt. For 512\u00d7512 sampling, using up to ~64 steps is suggested as a good balance between performance gains and computational efficiency.",
      "source_document": "papers/2512.19433v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a collaborative deep image watermarking system that couples an embedder and an extractor during training, what composite loss can be used to jointly optimize robustness and imperceptibility, and how can bidirectional collaboration be implemented by having the two networks exchange AFMM-derived modulation states (i.e., what signals are extracted from AFMMs and what is their role in aligning embedding with extraction)?",
      "answer": "A suitable end-to-end objective is a three-term composite loss\n\nL = \u03bb1\u00b7BCE(\u0175, w) + \u03bb2\u00b7MSE(Iwm, I) + \u03bb3\u00b7LPIPS(Iwm, I),\n\nwhere BCE(\u0175, w) drives accurate recovery of the embedded bit string, while MSE and LPIPS between the watermarked image Iwm and the original image I jointly preserve visual fidelity; the weights (\u03bb1, \u03bb2, \u03bb3) balance robustness vs. imperceptibility.\n\nBidirectional collaboration can be implemented with a Collaborative Interaction Mechanism (CIM) that operates at feature level through the Adaptive Feature Modulation Modules (AFMMs) placed in both embedder and extractor. Each AFMM produces a modulation map that reflects how it is adjusting intermediate features; CIM compresses these maps with global average pooling and feeds them through a weight-shared MLP to produce scalar \u201cmodulation states\u201d. By exchanging and adapting these AFMM-derived states across the embedder and extractor, CIM provides mutual-teacher feedback that explicitly aligns embedding strategies with extraction behavior, rather than relying only on a final scalar loss to couple the two networks.",
      "source_document": "papers/2512.19438v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "If you want a deep image watermarking model to generalize to many unseen attacks without training on a long list of simulated distortions, what minimal pair of training-time distortions can serve as proxies for the geometric and signal-processing attack families, and how should they be sampled and composed during training to avoid overfitting to a fixed attack pattern?",
      "answer": "Use one geometric distortion and one signal-processing distortion: (1) an Affine transform as a proxy for geometric attacks (covering rotation, translation, scaling, and shearing), with parameters sampled from continuous ranges rather than fixed presets to encourage generalization; and (2) additive Gaussian noise as a fundamental, unstructured signal-processing corruption (applied per-pixel with zero mean). During training, apply both distortions to the watermarked image in a randomized order (order sampled randomly, e.g., via a Bernoulli draw) to prevent order-specific bias and encourage distortion-invariant feature learning that transfers to unseen degradations like blur or compression.",
      "source_document": "papers/2512.19438v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For a deep image watermarking system that must decode reliably under diverse, potentially compound distortions, what extractor design uses multi-scale inputs and AFMM-based refinement to suppress host interference, and how are the scale-specific features fused into the final per-bit watermark predictions (include the role of downsampling and the fusion head)?",
      "answer": "Use a multi-scale extractor that processes the distorted image at three resolutions (full, 1/2, and 1/4). At each scale s, the input I^s is passed through a stem layer and then through a cascade of four AFMM refinement stages; the first three AFMM stages are each followed by downsampling, and the final AFMM refines at the coarsest resolution. The AFMMs at each resolution explicitly suppress host-image interference and strengthen the residual watermark signal, while the interleaved downsampling progressively expands the receptive field to aggregate broader context, yielding a purified robust feature u_s = F_s(I^s). The three outputs are concatenated and fused nonlinearly by an MLP, then projected by a linear layer with a sigmoid to produce per-bit probabilities: \u0175 = sigmoid(W_out \u00b7 MLP([u_1; u_{1/2}; u_{1/4}]) + b_out). Cross-scale redundancy and the MLP fusion mitigate information loss when any single scale is degraded; ablating the extractor\u2019s multi-scale design reduces average extraction accuracy across distortions by about 6.7%.",
      "source_document": "papers/2512.19438v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a collaborative deep image watermarking system that uses feature-modulation modules on both the embedding and extraction sides, what does an ablation that (i) removes only the embedder-side modulators, (ii) removes only the extractor-side modulators, (iii) removes all modulators, or (iv) disables the bidirectional collaboration mechanism reveal about which components primarily drive imperceptibility versus robustness under combined distortions?",
      "answer": "The ablations show a clear division of labor and the need for explicit coordination. Removing all feature-modulation modules causes a large drop in both imperceptibility and robustness, indicating they are critical overall. Removing only the embedder-side modulation mainly hurts imperceptibility (watermarked images become visibly more distorted), while removing only the extractor-side modulation mainly hurts robustness (watermark recovery degrades), implying the embedder\u2019s modulation is key for perceptually aligned injection and the extractor\u2019s modulation is key for suppressing host interference and amplifying watermark cues during decoding. Disabling the bidirectional collaboration mechanism while keeping the modulators still reduces both imperceptibility and extraction accuracy, showing that the modulation modules alone are insufficient\u2014explicit cross-end coordination is needed to align embedding behavior with extraction objectives under compound distortions.",
      "source_document": "papers/2512.19438v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking an image watermarking system for *imperceptibility*, PSNR/SSIM and even LPIPS can fail to reflect subtle but noticeable color shifts. What additional perceptual metric can be used to specifically capture color-visibility effects, what aspect of the human visual system does it model, and why is it a more stringent complement to LPIPS for evaluating watermark embedding quality?",
      "answer": "Use CVVDP (Color-Vision Visual Difference Predictor). It models human sensitivity to color differences via a perceptual detection threshold (i.e., how visible color deviations are to the human visual system). It complements PSNR/SSIM/LPIPS because standard metrics\u2014and even LPIPS, which is strong for texture/structure changes\u2014may not fully capture subtle color shifts; CVVDP is designed to better reflect subjective color distortion after watermark embedding, making it a more stringent measure of high-fidelity color imperceptibility.",
      "source_document": "papers/2512.19438v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a training-free visual token pruning scheme for MLLMs that aims to preserve fine-grained localization, how can you construct a hybrid token graph that enforces both semantic and spatial diversity, and how is this graph used in a greedy Maximal Independent Set\u2013style selection to avoid keeping redundant tokens while still prioritizing important ones?",
      "answer": "Build a graph over the N visual token hidden states on the h\u00d7w patch grid by (1) computing a dense semantic similarity matrix with pairwise cosine similarity S_sem(i,j)= (h_i\u00b7h_j)/(||h_i||\u00b7||h_j||) and min\u2013max normalizing it to \u007fS_sem; (2) defining a sparse binary spatial proximity matrix S_spat using an 8-neighborhood rule on the image grid (S_spat(i,j)=1 if j is in i\u2019s 8-neighborhood, else 0); (3) fusing them as S_fused = \u03b1\u00b7\u007fS_sem + (1\u2212\u03b1)\u00b7S_spat, where \u03b1 balances semantics vs. spatial structure; and (4) binarizing with a threshold \u03b8_sim to get the adjacency matrix S (edge if S_fused(i,j)>\u03b8_sim), so neighbors are both semantically similar and spatially close.\n\nGiven token importance scores from debiased/relative attention A_rel, use this adjacency to do pivot-based greedy MIS selection: first choose n_p=\u230an\u00b7r_pivot\u230b top-scoring tokens as \u201cpivots\u201d while excluding their graph neighbors; then iteratively add the remaining n\u2212n_p tokens by repeatedly selecting the highest-A_rel available token and excluding its neighbors. This preserves high-importance tokens while preventing selection of redundant, nearby-and-similar tokens, improving structural diversity for localization compared to purely importance-based or purely feature-space diversity pruning.",
      "source_document": "papers/2512.19443v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In attention-based, training-free visual token pruning for MLLMs, how can you debias the per-token importance scores to remove systematic positional bias, and how do you keep this debiasing usable when the number of visual tokens changes with input resolution?",
      "answer": "Compute a model-specific positional-bias prior Abias by a one-time calibration: run the MLLM on a set of randomly selected COCO images paired with a generic prompt (e.g., \u201cPlease describe the provided image\u201d), extract the attention from the final text token to each of the N visual tokens at a chosen layer, and average the resulting attention vectors across the ~1,000 images. For an input instance, extract the content-specific attention vector Aori (final text token \u2192 visual tokens) and form a debiased \u2018relative attention\u2019 importance score by normalizing with the bias prior: Arel = Aori / (Abias + \u03b5), with a small \u03b5 for numerical stability. If the visual token count N differs (e.g., different image resolutions/patch grids), resize Abias to match Aori\u2019s length using interpolation before computing Arel.",
      "source_document": "papers/2512.19443v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an edge-friendly sign-language recognizer that uses MediaPipe hand keypoints with a parallel bidirectional echo-state-network (two bidirectional reservoirs in parallel), how are the forward/backward reservoir states fused into the final feature vector before classification, what optimization objective is used to learn the readout weights, and how does this design compare in accuracy and training time against common deep models (e.g., Bi-GRU, I3D, Pose-TGCN) on WLASL100?",
      "answer": "The parallel bidirectional reservoir computing (PBRC) setup runs two separate ESN reservoirs (A and B), each processed bidirectionally by feeding the normal input sequence u(t) forward and a time-reversed sequence U(t) backward. Within each reservoir, the forward and backward state vectors are concatenated; then the two reservoirs\u2019 bidirectional states are concatenated again to form the final state/features: x(t)=xA(t)\u2295xB(t)=(xA_f(t)\u2295xA_b(t))\u2295(xB_f(t)\u2295xB_b(t)). Classification is performed by a linear readout y(t)=Wout x(t), where Wout is trained with ridge regression, i.e., minimizing squared error with an L2 penalty term (a regularized least-squares objective).\n\nOn WLASL100, PBRC reaches 60.85% Top-1 accuracy (Top-5 85.86%, Top-10 91.74%) with a training time of 18.67 seconds on CPU. This is higher accuracy and dramatically faster training than a Bi-GRU baseline (50.01% Top-1 with ~55:41 training time on CPU). Compared to other deep baselines on WLASL100, PBRC exceeds Pose-TGCN in Top-1 accuracy (55.43%) while being much faster (Pose-TGCN ~38:18 on GPU), and it is competitive with I3D in Top-k accuracy while using far less compute/training time (I3D 65.89% Top-1 but ~20:13 on GPU).",
      "source_document": "papers/2512.19451v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an emotion-controlled diffusion model trained with preference pairs, how can you modify DPO so that the model becomes more sensitive to different emotions while keeping the same textual semantics, and how should the \u201cnegative\u201d conditioning be constructed from the prompt bank to enforce this during training (including which submodules are optimized versus kept frozen)?",
      "answer": "Add an extra DPO penalty term that contrasts the model\u2019s denoising under the correct conditioning against denoising under a negative conditioning that keeps the same text semantics but swaps in a visual prompt from a different emotion. Concretely, the standard diffusion DPO objective L(\u03b8) (built from chosen vs rejected latents under condition c) is extended to \n\n\u00eaL(\u03b8) = L(\u03b8) \u2212 E[ log \u03c3(\u2212\u03b2T \u03c9( (||\u03b5c\u2212\u03b5\u03b8(xc_t,c,t)||^2 \u2212 ||\u03b5n\u2212\u03b5\u03b8(xc_t,cn,t)||^2) )) ],\n\nso the model is penalized if the chosen sample is also well-explained under the negative condition cn.\n\nTo build cn, keep the same textual prompt pt to preserve semantics, then retrieve the most semantically related negative visual prompt pv_n from the prompt bank but taken from a randomly selected other emotion e_n (i.e., \u201csimilar semantic, different emotion\u201d). The noise term \u03b5n corresponds to that negative visual prompt branch.\n\nDuring training, only the added LoRA parameters applied to the cross-attention for integrating the visual prompt, the visual-prompt embeddings/bank features F^v_e and the linear mapping from F^v_e to the visual-token embeddings H_v are optimized; the text features F^t_e remain frozen and serve as the CLIP-encoded key for query-based retrieval in the prompt bank.",
      "source_document": "papers/2512.19479v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an emotion-oriented text-to-image diffusion system that aims to avoid treating emotions as mere semantic attributes, how can you build a semantics-adaptive, emotion-conditioned *visual prompt bank* and retrieve a visual prompt for a new rewritten text prompt, and how is that retrieved visual prompt injected into the diffusion model\u2019s cross-attention so that it complements (rather than replaces) the standard text guidance?",
      "answer": "A semantics-adaptive visual prompt bank can be built per emotion by pairing clustered *text* features with corresponding clustered *image* features from an image\u2013text emotion dataset. Concretely, for each image\u2013text pair you extract a textual feature using a CLIP text encoder and a visual feature using DINOv3. For each emotion e, you run K-means over the CLIP text features to form L semantic clusters, using the closest real CLIP feature to each cluster center as the stored text prototype (to avoid altering CLIP-extracted features). You then assign the DINOv3 visual features the same cluster labels as their paired text features and compute L cluster centers in the visual feature space. This yields an emotion-specific bank PV_e = {F^t_e, F^v_e}.\n\nGiven a new rewritten prompt p_t and emotion e, you encode p_t with the CLIP text encoder to get q_t, retrieve the top-k most similar stored text prototypes via I_\u2113 = Topk(q_t \u00b7 (F^t_e)^T), and select the corresponding visual tokens as the visual prompt p_v = F^v_e[I_\u2113] (with \u2113 set to match the text embedding length).\n\nTo inject p_v into the diffusion model without discarding the pretrained text-to-image guidance, the model keeps the standard text cross-attention path for p_t and adds a second cross-attention path for p_v (implemented with LoRA on cross-attention). The two cross-attention branches are computed independently and their outputs are combined additively (text branch output + visual branch output), so the visual prompt provides complementary emotion-relevant guidance beyond semantics while preserving the original text-conditioning behavior.",
      "source_document": "papers/2512.19479v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "If you want to fine-tune a text-to-image diffusion model with DPO for *emotion faithfulness* (not just generic image quality), how can you construct preference pairs so that the \u201cchosen\u201d image is simultaneously better in aesthetics, text\u2013image alignment, and the intended emotion\u2014i.e., what generation/annotation pipeline is used and what constraints are enforced when selecting each (chosen, rejected) pair?",
      "answer": "Construct preference pairs via an emotion-oriented text-to-image dataset pipeline:\n- Start from a diverse, safety-filtered set of source prompts (filtered by NSFW score and cosine similarity), then rewrite each prompt into a fixed emotion set (eight Mikels emotions) with a multi-agent rewriting system to obtain emotion-highlighted yet semantically consistent prompts.\n- For each rewritten prompt, generate multiple candidate images using several strong generators/backbones (SD v1.5, SDXL, SD v3, and FLUX; 16 candidates per prompt).\n- Annotate each candidate with (i) an aesthetic score, (ii) text\u2013image consistency, (iii) emotion score, plus human preference feedback. Emotion labeling is first filtered automatically using a ViT-based emotion classifier and GPT-4o, then the retained images are scored by 11 experts.\n- Form a preference pair (chosen xc, rejected xr) only when the chosen image is better than the rejected one on *all* the tracked metrics (aesthetics, text\u2013image consistency, and emotion), and also has a clearly higher emotion score (the paper uses a minimum margin of 0.2 in emotion score).\nThis yields a set of contrastive preference pairs (20,848 pairs) suitable for DPO fine-tuning targeted at emotion faithfulness under fixed semantics.",
      "source_document": "papers/2512.19479v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building an LLM-based prompt-rewriting subsystem for emotion-oriented text-to-image generation, how can you structure a multi-agent workflow so that (i) the rewritten prompt preserves the source prompt\u2019s semantics, (ii) the emotion is expressed in visually depictable terms rather than template-like phrases, and (iii) low-quality rewrites are automatically filtered before being used to train a distilled rewrite model? Describe the stages, the roles/number of agents, the feedback signals used for checking, and the accept/retry criteria.",
      "answer": "Use a multi-agent \u201cchain-of-concept\u201d workflow with four stages: (1) Visual concept extraction: multiple agents (three) extract salient visual concepts from the source prompt (e.g., humans/animals/landscapes) to serve as concrete cues for rewriting. (2) Emotion attribution: randomly initialize three agents with different user backgrounds (e.g., age/gender/profession) to provide subjective, multi-view reasoning\u2014each explains why the specified visual concept in the source prompt would evoke the target emotion, reducing template-like outputs. (3) Textual prompt rewriting: generate an emotion-highlighted prompt conditioned on the extracted visual concept and candidate attributions, with an explicit constraint that the rewrite must not conflict with or omit the original semantics. (4) Checking with combined textual and multimodal feedback: an LLM-judge evaluates semantic consistency with the source prompt and logical coherence; emotion faithfulness is evaluated by a model fine-tuned on affective-image captions; and visual expressiveness is checked by generating four images per rewritten prompt (with SDXL) and filtering out prompts with weak visual relevance using the average prompt\u2013image CLIP score. The system attempts up to three rewriting rounds per source prompt; if no rewrite passes checking after all attempts, discard the source prompt. Accepted rewrites are collected (e.g., tens of thousands) and then used to fine-tune a single LLM to distill the rewriting capability.",
      "source_document": "papers/2512.19479v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "How can an emotion-oriented text-to-image generator be used to augment training data for downstream image emotion recognition, and\u2014when comparing different post-training strategies (e.g., SFT, diffusion-DPO, SPO vs a cross-modal prompt-based method)\u2014what qualitative pattern should you expect in the recognition gains on a small dataset versus a larger dataset, and which approach provides the largest improvement?",
      "answer": "Use the generator to synthesize additional labeled training images from the recognition dataset by first producing captions for the original images (captions are generated with GPT-4o), then generating synthetic images from those captions and adding them to the training set (expanding the dataset by a chosen factor). This synthetic augmentation improves emotion recognition performance, with larger gains on the small-scale dataset (Emotion6) than on the larger FI dataset. Among the compared generation/post-training methods, the cross-modal method (Emotion-Director) yields the highest improvement, attributed to stronger emotional expressiveness of its generated images.",
      "source_document": "papers/2512.19479v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a bidirectional (symmetric) deformable medical image registration network that estimates both forward and backward deformation fields, how is the overall symmetric registration objective formulated using the two warped images, and what two terms make up the per-direction registration loss (including what property the smoothness term is designed to preserve)?",
      "answer": "The symmetric objective sums the registration loss in both directions:\n\nLbireg = Lreg(xb, xa\u2218\u03d5a2b, \u03d5a2b) + Lreg(xa, xb\u2218\u03d5b2a, \u03d5b2a),\n\nwhere xa\u2218\u03d5a2b and xb\u2218\u03d5b2a are the source images warped by the forward/backward deformation fields, respectively. The per-direction loss Lreg is composed of (1) a smoothness loss Lsmo based on the Jacobian matrix to preserve deformation topology (i.e., encourage physically plausible, fold-free deformations), and (2) a similarity loss Lsim defined as the mean Dice similarity coefficients to align corresponding regions.",
      "source_document": "papers/2512.19486v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In DySNet\u2019s dynamic feature-modeling block, how is a per-pixel dynamic receptive field constructed from the fixed and moving feature maps, and how are the moving-image key/value features sampled at the dynamically shifted locations (including what role the interpolation operator plays for training stability and deformation smoothness)?",
      "answer": "A dynamic receptive field is built by (1) concatenating fixed and moving features along channels to form a fused tensor X = [f^a, f^b] \u2208 R^{B\u00d72C\u00d7H\u00d7W}; (2) feeding X to a small offset-prediction CNN \u03b8_offset to predict per-location offsets for each sampled point in a static receptive field U_{N_d}(i), giving \u0394_i = \u03b8_offset(X) (reshaped to match |U_{N_d}| points and d spatial dimensions); and (3) shifting the static receptive field by these offsets to obtain the dynamic field D_{N_d}(i) = U_{N_d}(i) + \u0394_i. The moving-image key/value features K,V are then evaluated at the (generally non-grid) positions D_{N_d}(i)_j via an interpolation sampler: K^m_j = I(K, D_{N_d}(i)_j) and V^m_j = I(V, D_{N_d}(i)_j). The interpolation sampling preserves continuity and differentiability of sampling, which supports stable training and encourages smooth, coherent deformations.",
      "source_document": "papers/2512.19486v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a deformable registration block that makes both the receptive-field locations and the aggregation weights input-dependent, how is a standard kernel decomposed into (i) a receptive-field point set, (ii) spatial weights, and (iii) channel weights, and then recombined into a per-position dynamic kernel used to aggregate sampled value features? Specify how the spatial attention weights and channel weights are fused, and how the final output feature at position i is computed from the deformed values.",
      "answer": "The kernel is decomposed in two stages: first into a receptive-field point set and its weights, and then the weights into spatial and channel components. Concretely, a basic kernel of size Nd with parameters \u03b8^i_{Nd,C} is decomposed as\nK_{Nd,\u03b8(i)} \u2192 {U_{Nd}(i), \u03b8^i_{Nd,C}}, and \u03b8^i_{Nd,C} \u2192 {\u03b8^i_{Nd,1}, \u03b8^i_{1,C}},\nwhere U_{Nd}(i) is the (static) receptive-field point set, \u03b8^i_{Nd,1} are spatial weights (Nd\u00d71), and \u03b8^i_{1,C} are channel weights (1\u00d7C). A dynamic receptive field D_{Nd}(i) is then obtained by adding learned offsets, and DySA produces dynamic spatial attention weights \u03c1^i_{Nd,1}(j) over points j in D_{Nd}(i). The per-position dynamic kernel weights are formed by fusing the dynamic spatial weights with the learnable channel weight to obtain \u03b8^i_{Nd,C}:\n{\u03c1^i_{Nd,1}, \u03b8^i_{1,C}} \u2192 \u03b8^i_{Nd,C}, and together with {D_{Nd}(i), \u03b8^i_{Nd,C}} defines the dynamic kernel K_{Nd,\u03b8(i)}.\nFeature aggregation at position i is then computed as a weighted sum over the deformed value features V^m_j sampled at the dynamic receptive-field points:\nA_i = \\sum_{j=1}^{|D_{Nd}|} \u03b8^i_{Nd,C}(j) \u00b7 V^m_j,\nwith multi-head outputs summed to produce the final attention feature.",
      "source_document": "papers/2512.19486v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In deformable medical image registration benchmarks where you care about both alignment accuracy and deformation plausibility, what two metrics can be used to quantify these (including what deformation pathology one of them detects), and what kind of trade-off does DySNet demonstrate relative to strong baselines like XMorpher and other CNN/Transformer/hybrid methods?",
      "answer": "Alignment accuracy is evaluated with the Dice similarity coefficient (DSC) computed between warped structures and targets, while deformation plausibility/smoothness is evaluated by the percentage of locations with negative Jacobian determinant (|J\u03d5|<0), which indicates folding/non\u2013topology-preserving transformations. Across 3D cardiac CT and 3D/2D brain MRI tasks, DySNet achieves consistently higher DSC than prior CNN, Transformer, and hybrid baselines while keeping low negative-Jacobian volume (physically plausible, topology-preserving deformations). In contrast, some baselines show higher negative-Jacobian rates and even registration failures, and although XMorpher attains extremely low negative-Jacobian volume, its registration accuracy (DSC) is substantially lower than DySNet\u2019s\u2014so DySNet provides a better accuracy\u2013smoothness balance.",
      "source_document": "papers/2512.19486v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When designing a dynamic-kernel feature aggregation module for unsupervised deformable medical image registration, what trend should you expect as you increase the kernel (receptive-field) size in terms of (i) alignment accuracy and deformation plausibility, and (ii) computational cost (FLOPs) versus parameter count\u2014and what does this imply about whether dynamic attention still needs large spatial contexts to capture useful cross-image feature relationships?",
      "answer": "Across kernel sizes, alignment accuracy and deformation plausibility remain essentially stable (DSC and the negative-Jacobian-based smoothness measure change only slightly), indicating the method does not rely on a large fixed spatial context to find good correspondences. In contrast, computational cost grows with kernel size: FLOPs increase significantly as the kernel expands, while the number of parameters stays nearly constant. This supports the conclusion that the dynamic attention mechanism can capture key feature relationships without large spatial contexts, so small kernels can retain performance while being more computationally efficient.",
      "source_document": "papers/2512.19486v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an LMN block-term coupled-tensor formulation for hyperspectral super-resolution (HSI\u2013MSI fusion) with known spatial and spectral degradations, what full objective is minimized (data-fidelity terms plus regularizers), and what specific priors are enforced by the regularizers on the spatial factors, spectral factors, and the core tensor of each material component?",
      "answer": "The method minimizes a sum of two Frobenius-norm data-fidelity terms\u2014one for matching the observed low-spatial-resolution HSI and one for matching the observed MSI\u2014plus smoothness and stabilization regularization on each LMN component:\n\nData fidelity (loss):\n\u2022 HSI term: (1/2)\u2016Y^H \u2212 \u03a3_{r=1}^R D_r \u00d71 (P1 A_r) \u00d72 (P2 B_r) \u00d73 C_r\u2016_F^2\n\u2022 MSI term: (1/2)\u2016Y^M \u2212 \u03a3_{r=1}^R D_r \u00d71 A_r \u00d72 B_r \u00d73 (P_M C_r)\u2016_F^2\n\nRegularization added to the loss:\n\u2022 Spatial/spectral smoothness on the factor matrices via \u03d5_r(A_r,B_r,C_r) = \u03d5_{p,\u03b5}(H1 A_r) + \u03d5_{p,\u03b5}(H2 B_r) + \u03d5_Tik(H3 C_r), weighted by \u03bb and summed over r.\n  \u2013 \u03d5_{p,\u03b5}(X)=\u2211_{i,j}(x_{i,j}^2+\u03b5)^{p/2} is a nonconvex total-variation\u2013type penalty; H1 and H2 are first-order finite-difference matrices along the two spatial dimensions, promoting small spatial total variation (spatial smoothness).\n  \u2013 \u03d5_Tik(H3 C_r)=\u2016H3 C_r\u2016_F^2 uses a second-order finite-difference (Tikhonov) matrix H3 along the spectral dimension, promoting second-order spectral smoothness.\n\u2022 A core-tensor energy penalty \u03c6_r(D_r)=(1/2)\u2016D_r\u2016_F^2, weighted by \u03b7 and summed over r, to mitigate scaling/counter-scaling ambiguities in the factorization.\n\nOverall objective: minimize over all factors \u03b8 the loss + \u03bb\u2211_r \u03d5_r(A_r,B_r,C_r) + \u03b7\u2211_r \u03c6_r(D_r).",
      "source_document": "papers/2512.19489v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an LMN-based coupled-tensor formulation for hyperspectral\u2013multispectral fusion solved via an alternating accelerated gradient (block coordinate) method, how is the Nesterov-style extrapolation/momentum parameter scheduled across iterations, and what iteration-complexity guarantee is given for convergence to a stationary point of the nonconvex objective?",
      "answer": "The extrapolation (momentum) parameters are set using a Nesterov recursion per block: for each material/component r and each updated block i, set\n\n\u2022 \u03bc^(t)_{r,i} = (\u03b3^(t)_{r,i} \u2212 1) / \u03b3^(t+1)_{r,i},\n\u2022 \u03b3^(t+1)_{r,i} = (1 + sqrt(1 + 4(\u03b3^(t)_{r,i})^2)) / 2,\nwith initialization \u03b3^(0)_{r,i} = 1.\n\nWith proper choices of step sizes \u03b1^(t) and these \u03bc^(t) parameters, the alternating accelerated gradient algorithm generates sequences that converge to a stationary point, and it achieves an O(1/T) iteration complexity in the sense that after T iterations it reaches an O(1/T)-stationary point of the (nonconvex) objectives (both the known-degradation and semi-blind variants).",
      "source_document": "papers/2512.19489v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In LMN block-term (rank-(L,M,N)) coupled-tensor hyperspectral super-resolution with known spatial/spectral degradation operators, what explicit dimensional/rank conditions guarantee (i) essential uniqueness of the LMN decomposition and (ii) exact recoverability of the super-resolution image from the coupled HSI/MSI observations, and what rank conditions are required on the degradation matrices?",
      "answer": "(i) Essential uniqueness (identifiability) of an LMN tensor decomposition holds almost surely (for factors drawn from absolutely continuous distributions) when the tensor dimensions and multilinear ranks satisfy\n- I \u2265 L R and J \u2265 M R, and\n- L M \u2265 N \u2265 max{\u2308L/M\u2309 + \u2308M/L\u2309, 3}.\nThis removes earlier unrealistic requirements such as L = M or N > L + M \u2212 2.\n\n(ii) For the coupled HSR problem with known spatial degradation, exact SRI recovery from any feasible solution of the coupled formulation is guaranteed almost surely when, in addition to the LMN identifiability condition above,\n- the coupled-observation spatial size satisfies I_H J_H \u2265 L M R,\n- the target SRI spatial sizes satisfy I_M \u2265 L R and J_M \u2265 M R,\n- and the degradation matrices/operators satisfy full-row-rank conditions: P1, P2, and P_M are full row rank.\nUnder these conditions, the ground-truth SRI is uniquely recovered as Y^S = \u03a3_{r=1}^R D_r^\u22c6 \u00d71 A_r^\u22c6 \u00d72 B_r^\u22c6 \u00d73 C_r^\u22c6.",
      "source_document": "papers/2512.19489v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In semi-real hyperspectral super-resolution experiments that use a real hyperspectral image as the ground-truth SRI (Wald-style protocol), how are the low-resolution HSI and the MSI synthesized (including the spatial blur/downsampling and the spectral degradation model), what noise model is applied during evaluation, and which quantitative metrics are used to compare reconstructed SRIs against the ground truth?",
      "answer": "The semi-real protocol treats a clean real HSI cube as the ground-truth SRI and generates observations as follows:\n\n\u2022 HSI synthesis (spatial degradation): the SRI is blurred band-by-band with a 9\u00d79 Gaussian kernel, then downsampled by a factor of 8 in each spatial dimension.\n\n\u2022 MSI synthesis (spectral degradation): the MSI is produced by applying a spectral degradation matrix PM that performs band selection and aggregation according to a multispectral sensor\u2019s spectral responses; two sensors are considered (LANDSAT and QuickBird).\n\n\u2022 Noise model: unless otherwise specified, zero-mean white Gaussian noise is added to both the simulated HSI and MSI at SNR = 35 dB, and results are averaged over 10 trials with different noise realizations.\n\n\u2022 Evaluation metrics: RSNR (reconstruction SNR), SSIM, CC (cross-correlation), ERGAS, RMSE, and SAM (spectral angle mapper).",
      "source_document": "papers/2512.19489v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In real hyperspectral\u2013multispectral image fusion where both the spatial and spectral degradation operators are unknown, what is the evaluation-time strategy for obtaining/handling the degradation operators across different classes of baselines (methods that need explicit spatial/spectral degradations vs matrix-factorization baselines vs tensor CTD baselines), and how is the fusion rank selected in that setting?",
      "answer": "In the real-data fusion setting with unknown degradations, spatial degradations (P1, P2) and the spectral degradation (PM) are estimated from the observed HSI and MSI using an external degradation-estimation procedure for methods that require these operators (CBSTAR, BTDvar, NPTSR, and NLSTF). The matrix-factorization baselines (CNMF and FUSE) instead use their built-in degradation-estimation routines. For the tensor CTD baselines (SCOTT, STEREO, SCLL1), their blind/semi-blind variants are used (BSCOTT, BSTEREO, BSCLL1), which do not assume known degradations. The rank is chosen by visual inspection (set to R=4 in the experiment).",
      "source_document": "papers/2512.19489v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In GRPO-style reinforcement learning for multimodal anatomy VQA, how is the group-relative advantage computed from the per-sample rewards, why can this advantage (and thus the gradient signal) vanish when the model outputs are homogeneous within a rollout group, and how does a diversity-focused augmentation strategy modify the image\u2013question prompt to restore reward variance? Also, what does the component ablation imply about the roles of question rewriting vs. image augmentation in that strategy?",
      "answer": "The group-relative advantage is computed by normalizing each response\u2019s reward against the reward statistics of the whole rollout group: for a group of responses with rewards {Ri}, the advantage for response i is (Ri \u2212 mean({Ri})) / std({Ri}). This signal can vanish when rewards become homogeneous within a group\u2014e.g., for \u201ceasy\u201d anatomies the policy quickly produces all-correct responses (uniformly high rewards) and for \u201chard\u201d anatomies it produces all-wrong responses (uniformly low rewards). In both cases the group-relative differences shrink toward zero, so the advantage approaches zero and the advantage gradient provides little to no learning signal, stalling updates and contributing to non-convergence on hard structures.\n\nTo restore reward variance, the diversity augmentation strategy expands a single multimodal prompt Q0={I,T} into a set of semantically consistent variants by (1) rewriting the textual question with a transformation operator that keeps intent/answer but changes phrasing or focus, and (2) applying controlled visual transformations to the image (e.g., noise injection) that preserve critical cues. Running the policy on these prompt variants increases diversity across the response group, yielding a broader (more informative) reward distribution and reducing \u201call-wrong\u201d invalid groups.\n\nThe ablation indicates that both components matter: removing either the text rewriting or the image augmentation causes consistent performance drops, implying the two are complementary and jointly needed to maximize the benefit of the diversity augmentation.",
      "source_document": "papers/2512.19512v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In multiple-choice surgical anatomy VQA, how can a curriculum be constructed so the model sees \u201ceasy\u201d questions first and \u201chard\u201d ones later using only the answer-option texts\u2014specifically, how is per-question difficulty computed from the correct option and distractors, and how is that score used to schedule training progression?",
      "answer": "Compute an embedding for each answer option text (correct option and each distractor) with a medical text encoder (MedCLIP\u2019s text encoder), then define option similarity by cosine similarity. For a question q, its difficulty score S(q) is set to the maximum cosine similarity between the correct option embedding and any distractor embedding (the most confusable distractor). Low S(q) means distractors are semantically distinct (easier); high S(q) means at least one highly similar distractor (harder). Training data are partitioned into bins by S(q): training starts on the lowest-difficulty bin and progressively moves to higher-difficulty bins once the model\u2019s performance reaches predefined thresholds, so the model learns coarse distinctions before fine-grained ones.",
      "source_document": "papers/2512.19512v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a multimodal LLM on high-stakes medical anatomy VQA, how can you operationalize quality and reliability using multiple-sample decoding\u2014specifically, how are Avg@5, Major@5, and Pass@k computed from k=5 generated answers, and why would you report Pass@1 (top-1 accuracy) instead of Pass@5 in this setting?",
      "answer": "Generate k=5 responses for each test case. Avg@5 is the mean of expert scores assigned to each of the 5 responses on a predefined correctness/relevance scale. Major@5 takes the most frequent answer among the 5 responses via majority voting and checks whether that consensus answer matches the ground truth. Pass@k is the fraction of test cases for which at least one of the k generated answers equals the ground truth (an indicator over existence of a correct answer among the k samples). In medical applications, Pass@5 can be misleading because having one correct answer among several incorrect ones is insufficient and potentially hazardous; clinicians need a single most reliable prediction. Therefore Pass@1 (equivalent to standard accuracy) is adopted to ensure the model\u2019s top-ranked answer is correct and trustworthy.",
      "source_document": "papers/2512.19512v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a PPO-style Group Relative Policy Optimization (GRPO) setup for fine-tuning a multimodal LLM, what objective is maximized to update the policy when you have group-normalized advantages\u2014specifically, how do the clipped probability-ratio term and the KL regularization term enter the objective, and which hyperparameters control the clipping range and the KL penalty strength?",
      "answer": "The policy is updated by maximizing a PPO-like clipped objective with an added KL penalty:\n\n- First compute a group-relative, normalized advantage for each sampled response using the group mean and standard deviation of rewards:  \u00c2_i = (R_i \u2212 mean({R_i})) / std({R_i}).\n\n- Then maximize, over questions and sampled responses, the average of a clipped importance-sampling term minus a KL regularizer:\n\nJ_GRPO(\u03b8) = E[ (1/G) * \u03a3_i min( (\u03c0_{\u03b8,new}(o_i|q)/\u03c0_{\u03b8,old}(o_i|q)) * A_i,\n                               clip(\u03c0_{\u03b8,new}(o_i|q)/\u03c0_{\u03b8,old}(o_i|q), 1\u2212\u03b5, 1+\u03b5) * A_i )\n              \u2212 \u03b2 * D_KL(\u03c0_{\u03b8,new} || \u03c0_ref) ].\n\nHere \u03b5 controls the clipping range (1\u2212\u03b5 to 1+\u03b5) for the probability ratio, and \u03b2 controls the strength of the KL-divergence penalty to the reference policy \u03c0_ref.",
      "source_document": "papers/2512.19512v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-modal transformer for soccer scene understanding that fuses per-player trajectories, team identity, and image-crop features to jointly predict (i) ball 2D trajectory, (ii) ball state per frame, and (iii) ball possessor, how are the learned CLS tokens and the two-stage temporal/social attention encoder used to produce task-specific predictions, and what loss is used to supervise each of the three tasks in the combined training objective?",
      "answer": "The model first projects each modality (crop appearance, (x,y) trajectory, and one-hot player type/team) into a shared latent space, concatenates them per player and frame, and refines the fused embedding with a small MLP stack. To support the two scene-level tasks, it appends two learned CLS tokens to the player set at every timestep, yielding a T\u00d7(N+2)\u00d7d tensor; these CLS tokens aggregate global information across agents and time and are later used by the ball-trajectory and ball-state heads.\n\nThe fused sequence (with CLS tokens) is processed by a two-stage transformer encoder: a coarse encoder followed by a fine encoder. Each encoder applies temporal positional encoding, then uses two Temporal Set Attention Blocks (attending along time independently per agent) to gather past/future context, followed by a Social Set Attention Block that attends across all agents within each frame (including the CLS tokens) to model inter-player interactions.\n\nTask heads and losses:\n\u2022 Ball position inference uses the output embedding of the \u201cball\u201d CLS token (shape T\u00d71\u00d7d), maps it through fully connected layers to a T\u00d72 sequence of (x,y) coordinates, and is supervised with Average Displacement Error (mean \u21132 distance between predicted and ground-truth ball positions over time).\n\u2022 Ball state classification uses the \u201cstate\u201d CLS token, maps it with an MLP to T\u00d7S logits, and is supervised with framewise cross-entropy over the S discrete ball-state classes.\n\u2022 Ball possessor identification uses the player tokens (shape T\u00d7N\u00d7d), applies a prediction head to produce T\u00d7N logits (a distribution over players each frame), and is supervised with framewise cross-entropy over the N players.\n\nThe overall objective is a weighted sum of these three losses, with task weights (\u03bb1, \u03bb2, \u03bb3) tuned to balance task performance and convergence.",
      "source_document": "papers/2512.19528v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using masked pre-training to regularize a multi-modal transformer that fuses (i) player trajectories, (ii) player/team identity, and (iii) per-player image-crop features for ball trajectory/state/possession prediction, how should the masking be applied to the visual stream to avoid shortcut learning, and why does masking trajectories (or mixing trajectory+visual masking) hurt compared to masking only the visual modality? Describe the concrete masking granularity/implementation used and the reasoning behind the observed performance differences.",
      "answer": "Mask only the visual crop modality, and do it at a per-player, full-sequence granularity: before modality fusion, randomly choose a subset of players in each input sequence and replace their crop-feature embeddings for all frames with a fixed placeholder value outside the valid feature range (so dimensionality is preserved while explicitly signaling \u201cmissing visual information\u201d). This prevents a shortcut where the model over-relies on the few crop embeddings that differ because the ball is visible near/with a player, and instead forces reliance on cross-modal patterns (especially structured trajectory context), improving robustness and generalization under realistic long occlusions/out-of-view periods. Full-sequence masking is more effective than sparse frame-wise masking because adjacent frames are highly redundant at dense frame rates and real failures are continuous over seconds. In contrast, masking trajectories (alone or together with visual crops) degrades performance because it removes essential structural information needed for stable learning; trajectory masking makes learning unstable/ineffective, while visual-only masking preserves the consistently available structured signal and trains the model to compensate for missing visual cues, which then benefits fine-tuning with full visual data.",
      "source_document": "papers/2512.19528v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a multi-task model to predict (i) ball 2D trajectory, (ii) ball state, and (iii) ball possessor from player-centric inputs, how can ball-trajectory ground truth be obtained if the dataset only provides per-frame ball possessor labels, what discrete ball-state taxonomy is used, and which portions of match footage should be filtered out as non-informative for learning coordinated play (and why)?",
      "answer": "Ball-trajectory supervision can be derived by using the per-timestep ground-truth ball possessor annotations to interpolate the ball\u2019s 2D trajectory in pitch coordinates. Ball state is treated as a 4-class label set: pass, possession, uncontrolled, and out of play. Sequences containing frames labeled \u201cout of play\u201d are filtered out because play is paused in these intervals (players may be walking/repositioning or even handling the ball), so coordinated ball-oriented team behavior breaks down, making those segments uninformative for modeling ball dynamics and interactions.",
      "source_document": "papers/2512.19528v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-modal soccer scene model that uses per-player image crops as one input stream, what end-to-end pipeline can be used to produce the per-frame crop embeddings\u2014starting from how player boxes are obtained and modified, through the choice of CNN backbone\u2014and what auxiliary supervision is used to make those embeddings emphasize ball-related cues?",
      "answer": "Player bounding boxes are obtained by tracking players over time with MixSort, then each box is enlarged by adding a fixed margin to keep contextual cues. The resulting per-player crops are passed through a CNN feature extractor using a ResNet backbone (chosen for being lightweight/efficient and reproducible). That ResNet is trained on an auxiliary action-recognition task that classifies whether the cropped player is performing a ball-related action (e.g., passing, receiving, dribbling) or not, providing an inductive bias toward ball-related visual cues; its output features are then used as the crop/appearance embeddings per player per frame.",
      "source_document": "papers/2512.19528v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When ablating input modalities in a multi-modal soccer scene model that predicts ball trajectory, ball state, and ball possessor from player-centric inputs, what role does each modality (player trajectories, player-type/team identity, and player image crops) play in the three tasks, and why does adding player-type and then visual crops to a trajectory-only model improve performance\u2014culminating in the best results when all three are fused?",
      "answer": "Player trajectories provide the essential structural signal for reconstructing ball motion over time; without them, the other modalities are insufficient to recover the dynamic (x,y) ball trajectory reliably. Player image crops, while weaker for trajectory reconstruction, still contribute meaningful information for the semantic classification tasks (ball state and possessor) by capturing subtle visual indicators such as ball control and other local interaction cues that are not obvious from positions alone. Player-type/team identity adds semantic context that helps the model differentiate between the two teams, reducing ambiguity in interactions and improving performance across tasks compared to using trajectories alone. Fusing trajectories with crops yields further gains because appearance complements structure in disambiguating complex scenes and subtle inter-player interactions; using all three modalities together is best due to this synergy in a shared representation space, improving both physical inference (trajectory) and semantic understanding (state and possession).",
      "source_document": "papers/2512.19528v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a preoperative virtual planning workflow that compares multiple preformed orbital plates across vendors, what two complementary plate-to-orbit distance metrics can be used to quantify plate conformity, and how does each metric support global vs region-specific assessment and plate ranking (including how edge measurements are sampled and can emulate trimming/bending decisions)?",
      "answer": "Two complementary conformity metrics are used:\n1) Plate-wide distances: pointwise distances from the plate model\u2019s surface vertices to the nearest point on the reconstructed orbit. These dense vertex-to-surface distances can be visualized as heatmaps to inspect global and regional conformity patterns over the entire plate.\n2) Edge-specific regional distances: deviations between points sampled along manually placed curves on key undersurface plate edges and their corresponding projected points on the reconstructed orbit. In the pilot workflow, five curves are placed (anterior floor, anterior medial wall, lateral floor, superior medial wall, and the floor\u2013wall junction), and points are resampled evenly along each curve; the curves can be repositioned to simulate alternative placements or the effects of bending, trimming, and repositioning.\nFor quantitative comparison and ranking, mean distances over well-defined regions (e.g., edges) or over the full plate are used, and the system exports raw distances and ranking summaries for analysis.",
      "source_document": "papers/2512.19534v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When reconstructing a fractured orbit by mirroring the contralateral side for preoperative plate-fit evaluation, what registration options can be used to align the mirrored anatomy to the injured side, what specific source of error are they intended to reduce, and what is the reported qualitative impact of this asymmetry correction on downstream plate-fit rankings?",
      "answer": "The mirrored-orbit reconstruction workflow aligns the mirrored contralateral anatomy to the fractured side using rigid alignment (e.g., of the mirrored hemiskull or full skull), with an optional Coherent Point Drift (CPD) deformable registration refinement step. These options are intended to reduce errors caused by natural left\u2013right orbital asymmetry. Qualitatively, minor asymmetry (and CPD refinement) was observed to have little impact on the relative plate-fit rankings, though the effect has not yet been quantified and is noted as a topic for future study.",
      "source_document": "papers/2512.19534v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In virtual preoperative planning for orbital fracture repair, how can you register a preformed orbital plate to a patient-specific reconstructed orbit in a way that mimics real intraoperative placement constraints, and what anatomical landmark is used as the key anchoring point during refinement (including how collision checking is used to enforce realistic contact)?",
      "answer": "A practical registration workflow is: (1) perform an initial coarse alignment using landmark-based positioning of the plate near the fracture site; (2) apply a posterior-stop alignment step to anchor the plate at the posterior orbital stop; then (3) refine the placement by manually rotating the plate around this anchored posterior stop (with only small optional translations), mirroring how surgeons adjust the plate while keeping the posterior stop engaged. Collision detection/visualization is used during this manual refinement to ensure the plate sits on the unfractured peripheral orbital bone surface without intersecting it, i.e., to simulate realistic plate\u2013bone contact. The key anchoring point is the posterior stop corresponding to the orbital process of the palatine bone, which usually serves as the posterior fixation point.",
      "source_document": "papers/2512.19534v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using closest-point plate-to-orbit distance fields to compare how well preformed orbital implants conform to a reconstructed orbit, what are the main failure modes that can make these distances an imperfect proxy for true anatomic fit, and what summary/aggregation strategy is recommended to make plate-to-plate comparisons more robust despite those limitations?",
      "answer": "Closest-point plate-to-orbit distances are only a geometric, algorithmic approximation of conformity: they depend on choosing the nearest orbital surface point for each sampled plate point, which may not perfectly reflect true anatomic relationships. In addition, the measured distances can be biased by operator-dependent plate placement decisions (different users may position the same plate slightly differently). To make comparisons more robust, the recommended approach is to summarize distances using mean values over well-defined regions (for example along plate edges) or over the entire plate, rather than relying on individual pointwise distances.",
      "source_document": "papers/2512.19534v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using vertex-wise plate-to-orbit distance heatmaps to judge how a virtually registered orbital plate conforms to a reconstructed orbit, how should the sign of the distances be interpreted (negative vs. positive relative to the reconstructed surface), and what are the default heatmap display conventions and exported artifacts that support downstream analysis (including how users can tune the visualization range)?",
      "answer": "Plate-wide conformity is computed as pointwise distances from plate surface vertices to the nearest points on the reconstructed orbit and displayed as a heatmap. By default, the heatmap uses a red\u2013green\u2013blue color scale over a \u22125 mm to +5 mm distance range; negative distances indicate plate regions lying beneath the reconstructed orbit surface, while positive distances indicate regions above it. Users can adjust the scalar display range in Slicer\u2019s Models module to optimize visualization. The workflow saves the heatmap models along with the underlying raw scalar distance values and corresponding histograms to the output directory (fit_output/fit_metrics) for further analysis.",
      "source_document": "papers/2512.19534v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a shot-by-shot long video diffusion framework that conditions each new shot on a bank of past keyframes, how does using a negative RoPE shift for the memory frames resolve the positional-encoding mismatch between past keyframes and the current shot, and what strategy is used to update the memory bank so it stays informative without growing unbounded?",
      "answer": "Negative RoPE shift treats stored memory keyframes as temporally preceding the current shot by assigning them negative temporal indices (spaced by a fixed offset S), while the current shot\u2019s frames start at index 0. This preserves the pretrained model\u2019s original temporal encoding for the generated clip but still lets attention operate over memory and current frames in a unified RoPE coordinate system.\n\nAfter each shot, a small set of keyframes is extracted and filtered: frames are selected to be semantically distinct using CLIP embeddings with an adaptive (dynamic) cosine-similarity threshold, then low-quality/unstable frames are removed using an aesthetic preference filter (HPSv3). The resulting keyframes are compared against existing memories in CLIP space and only semantically distinct ones are added. To prevent unbounded growth and preserve long-range anchors, the memory bank uses a hybrid \u201cmemory-sink + sliding-window\u201d policy: early keyframes are kept as fixed long-term anchors, while a recent-window of short-term memories is maintained and the oldest short-term frames are discarded once capacity is reached.",
      "source_document": "papers/2512.19539v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating multi-shot story video generators where some shots intentionally depict different characters/scenes, how can cross-shot consistency be quantified so it doesn\u2019t unfairly penalize such scripts, and how are the \u201cTop-k\u201d shot pairs selected for the focused consistency score? Also, how is prompt-following split into global vs per-shot scoring in the same evaluation protocol?",
      "answer": "Cross-shot consistency is computed with ViCLIP by taking the mean similarity across all shot pairs in the generated multi-shot video. Because many shot pairs may be semantically unrelated (different characters/scenes), a focused \u201cTop-10 Consistency\u201d is also reported: select the ten most relevant shot pairs using similarity between their *prompt* features, then average the ViCLIP similarities over just those pairs. Prompt following is also measured with ViCLIP: a global score is the cosine similarity between the entire multi-shot video and the story overview, while a single-shot score compares each shot to its corresponding per-shot prompt.",
      "source_document": "papers/2512.19539v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-shot video generator that maintains a compact keyframe \u201cmemory bank\u201d for conditioning future shots, what concrete procedure can be used to extract a small, diverse set of memory frames from each newly generated shot using CLIP features (including how the similarity threshold adapts to enforce a per-shot frame budget), and why is an additional aesthetic filtering step applied afterward?",
      "answer": "Compute CLIP embeddings for all frames in the generated shot and select keyframes sequentially: keep the first frame as a keyframe, then iterate through subsequent frames and compare each frame to the most recently selected keyframe using cosine similarity; add a new keyframe whenever this similarity falls below a dynamic threshold. The threshold is initialized low and is increased if the number of selected keyframes would exceed a preset upper bound, so selection becomes stricter and redundancy is suppressed while retaining semantically distinct moments. After semantic selection, apply aesthetic preference filtering with an aesthetic reward model (HPSv3) to remove low-quality candidates (e.g., blurred/noisy frames from large motion) so the stored memories are visually reliable and provide useful conditioning for later shots.",
      "source_document": "papers/2512.19539v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "If you want to adapt a pretrained single-shot image-to-video diffusion transformer into a multi-shot storyteller using an explicit keyframe memory, how can you train the memory-conditioned generator without any true multi-shot long-video training set\u2014specifically, how are \u201cmemory\u2013video coherent\u201d training examples constructed from single-shot clips, and what diffusion/flow objective is optimized so the model learns to reconstruct the target shot conditioned on sampled memory frames?",
      "answer": "Construct training groups from single-shot data so that the conditioning frames and the target clip are semantically coherent: (1) collect visually related short clips by grouping cinematic single-shot videos using shot-level similarity, and (2) include high-quality single-subject multi-scene short videos that naturally preserve identity. For each group, pick one clip as the target shot and randomly sample frames from the other coherent clips to serve as the keyframe memory. Encode those memory frames with the same 3D VAE as the base model, concatenate the memory latents with zero-filled frames to form the conditional latent, and use a binary mask so the DiT is trained to generate only the unmasked (target) region. Optimize the same rectified-flow velocity-prediction loss as the underlying I2V model (an L2 loss between the predicted velocity field and the target (z0\u2212\u03b5) under the linear interpolation zt=(1\u2212t)z0+t\u03b5). At inference, the memory part is treated as conditioning and only the newly generated segment is decoded as the shot.",
      "source_document": "papers/2512.19539v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When adapting a mask-guided image-to-video diffusion transformer to condition each new shot on a bank of past keyframes, how can the memory frames be represented and injected in latent space so they are preserved while the model only denoises/generates the new shot frames (i.e., what gets encoded, how the conditional latent is formed, and how the binary mask is used during diffusion and decoding)?",
      "answer": "Encode the selected memory keyframes with the same 3D-VAE encoder as the base model into memory latents z_m, with each latent corresponding to a single memory frame (no temporal compression for the memory). Form the conditioning latent z_c by concatenating these memory latents with latents obtained from zero-filled placeholder frames for the to-be-generated segment along the temporal dimension. Build a binary mask M over the (memory + target) latent frames that marks the memory frames as preserved (1) and the rest as generative regions (0). During diffusion, concatenate the noisy latent z_t, the conditional latent z_c, and the mask M along the channel dimension and feed them to the DiT so it generates only the unmasked frames while keeping the masked memory frames fixed. At inference, discard the memory portion and decode only the newly generated segment as the current shot.",
      "source_document": "papers/2512.19539v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a talking-avatar diffusion transformer that must follow temporally anchored action text while keeping accurate lip-sync, how can a two-stage training scheme be set up to avoid catastrophic forgetting of the base text-to-video capability? Describe (i) what is optimized vs frozen in each stage, and (ii) the training objective used in each stage (including what conditioning signals are fed to the velocity/flow predictor).",
      "answer": "A capability-preserving two-stage scheme is:\n\n(i) Parameters optimized vs frozen\n- Stage 1 (audio-visual correspondence): freeze the base text-to-video backbone parameters \u03b8_base and train only an audio adapter \u03b8_audio (audio projection that maps audio encodings to frame-aligned tokens + audio cross-attention layers).\n- Stage 2 (temporally aware action control): start from the base image-to-video model, inject the pretrained audio adapter from Stage 1, then do full fine-tuning of \u03b8_base, \u03b8_audio, and the Phase-Aware Cross-Attention (PACA) components \u03b8_PACA that use phase-position embeddings for hierarchical, temporally anchored prompts.\n\n(ii) Training objective and conditioning\nBoth stages use a Flow Matching objective on the optimal-transport linear path x_t=(1\u2212t)x_0+tx_1 (x_1 is Gaussian noise), training the model v_\u03b8(\u00b7) to predict the target velocity v_target=x_1\u2212x_0 with an L2 loss:\n- Stage 1 loss: L_stage1 = E[ || v_\u03b8(x_t, t, C_brief, A) \u2212 (x_1\u2212x_0) ||_2 ] where C_brief is a brief caption (e.g., \u201cA woman speaking\u201d) and A are audio embeddings (from Wav2Vec 2.0).\n- Stage 2 loss: L_stage2 = E[ || v_\u03b8(x_t, t, C_PACA, A) \u2212 (x_1\u2212x_0) ||_2 ] where C_PACA is the hierarchically structured prompt encoding (base block + phase blocks with temporal anchors) with phase position embeddings, and A is the same type of audio embedding.\n\nThis setup keeps the backbone\u2019s pre-learned text grounding intact during Stage 1 while learning robust lip synchronization via the adapter, then adds temporally grounded action control in Stage 2 without losing audio-visual alignment or text-following ability.",
      "source_document": "papers/2512.19546v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a text-conditioned diffusion transformer for talking-avatar video generation, how can you enforce phase-level (time-windowed) action control using only text prompts\u2014i.e., make the model attend to the tokens for \u201cPhase k\u201d primarily during frames whose normalized time falls inside that phase\u2019s window? Describe (i) how the prompt is represented to separate global scene context from temporally localized actions, and (ii) what change is made to the text tokens fed into cross-attention (including any initialization choice) so the network can learn phase-conditioned attention dynamics.",
      "answer": "(i) Represent the structured prompt hierarchically as a global base block plus K phase blocks with explicit temporal anchors: P = {P_base, {P_k, T_k}_{k=1..K}}, where P_base encodes time-invariant scene/identity/style semantics, and each phase block P_k describes a localized action tied to a normalized time window T_k = [\\tau_k^start, \\tau_k^end].\n\n(ii) After encoding the whole structured prompt into a token sequence C = {c_i}, add a learnable phase position embedding e_k to every token c_i that belongs to phase k: c'_i = c_i + e_k. The phase embeddings are zero-initialized so the module starts as identity behavior. Cross-attention then uses the phase-augmented tokens (C') as keys/values, allowing training on temporally annotated data to make attention concentrate on phase-k tokens when the current frame\u2019s normalized time \\tau falls within T_k.",
      "source_document": "papers/2512.19546v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a diffusion-transformer talking-avatar generator that is conditioned on both text (for body/hand actions) and audio (for lip motion), how can you implement a depth-aware \u201cprogressive audio-visual alignment\u201d mechanism inside the transformer blocks to reduce modality interference? Specify (i) the exact residual update applied to each block\u2019s audio cross-attention output, including the scaling function over layer depth, and (ii) why this scheduling helps preserve text-driven coarse action structure while still achieving accurate lip synchronization.",
      "answer": "(i) For transformer block \u2113\u2208{1,\u2026,L}, scale the audio cross-attention residual before adding it back to the block activations:\n\nx_\u2113 \u2190 x_\u2113 + f(\u2113) \u00b7 r_audio^\u2113,\n\nwhere r_audio^\u2113 is the residual from the audio cross-attention in block \u2113 and the depth-aware scaling is\n\nf(\u2113) = (\u2113 / L)^\u03b3 with \u03b3>1,\n\nso audio influence is small in early layers and progressively amplified toward deeper layers.\n\n(ii) Diffusion transformers learn features in a coarse-to-fine hierarchy: early layers set global structure (pose/gesture/hand trajectory) and later layers refine high-frequency details. Making f(\u2113) small early lets text dominate action semantics without being overridden by audio, while increasing f(\u2113) in deeper layers lets audio primarily refine mouth/lip articulation after the action scaffold is formed, preventing text\u2013audio competition and improving lip-sync without degrading action control.",
      "source_document": "papers/2512.19546v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a text+audio conditioned talking-avatar generator that must follow phase-anchored action prompts (e.g., \u201cPhase-1 [0\u20132s] \u2026, Phase-2 [2\u20134s] \u2026\u201d), how can you set up an automated benchmark using a multimodal LLM to score phase-level action control? Specify (i) the per-video/per-phase scores you would query (including how each is scaled), (ii) how to aggregate them into a single phase-hit metric, and (iii) what procedure can reduce evaluation variance across LLM runs.",
      "answer": "Use an MLLM-based evaluation that returns, for each prompted phase in a generated video: (1) Action Occurrence (AO): whether the described action appears (binary); (2) Action Accuracy (AA): how well the action matches the description, scored 0\u201310; (3) Temporal Correctness (TC): whether the action happens within the specified time window, scored 0\u201310; (4) Action Quality (AQ): overall naturalness of the execution, scored 0\u201310; and (5) Hand Clarity (HC): hand quality, scored 0\u201310. Aggregate occurrence into Hit@Segment (H@S), defined as the percentage of phases with AO = 1, and report mean AA/TC/AQ/HC over phases/videos. To reduce variance, run the MLLM evaluation multiple times per video (e.g., 5 runs) and report the averaged scores.",
      "source_document": "papers/2512.19546v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a fine-tuning dataset to inject *temporally grounded* action control into a talking-avatar diffusion/flow-matching model (without using extra control signals like pose trajectories at inference), how can you construct the dataset from raw talking videos so that the supervision emphasizes both \u201cwhat action\u201d and \u201cwhen it happens\u201d? Describe (i) how you filter/select clips so they contain meaningful body/hand motion, and (ii) how you produce phase-structured text prompts with explicit temporal anchors for training.",
      "answer": "Construct the action-control dataset in two steps:\n\n(i) Motion-based clip selection: run a pose estimator (DWPose) on candidate videos, compute a motion-magnitude measure from the estimated poses, and keep clips with significant movement so the data actually contains learnable actions (beyond lip motion).\n\n(ii) Phase-structured temporal prompting: use a multimodal large language model to generate hierarchical, structured prompts consisting of a global \u201cbase block\u201d plus multiple phase-specific descriptions, each paired with an explicit temporal anchor/window. These structured annotations provide phase-level temporal supervision (what to do in each phase and when), yielding a dataset of samples with phase-level temporal annotations for Stage-2 fine-tuning.",
      "source_document": "papers/2512.19546v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When fitting a disentangled identity\u2013expression 3D infant face model to an unseen expressive scan, what objective is minimized to estimate the latent codes, and how does the prior term regularize identity and expression using the learned variances/eigenvalues?",
      "answer": "The latent parameters z = (zid, zex) are estimated by non-linear optimization (alternating updates of identity and expression), minimizing a weighted sum of a vertex reconstruction term and a latent prior:\n\nE(z) = \u03b31\u00b7Everts(z) + \u03b32\u00b7Eprior(z), with \u03b31 + \u03b32 = 1.\n\n\u2022 Everts measures reconstruction error of the mesh vertices, i.e., the squared \u21132 difference between the model-generated shape and the aligned target scan (after subtracting the model mean):\nEverts(z) = \u2016(C \u2297 zid \u2297 zex) \u2212 (x \u2212 \u03bcx)\u2016\u00b2.\n\n\u2022 Eprior regularizes z to remain in a statistically plausible region by assuming a multivariate Gaussian in the latent space and penalizing deviations via a Mahalanobis/quadratic form using the identity and expression eigenvalues (variances):\nEprior(z) = z\u1d40\u039b\u207b\u00b9z = \u03a3_i (zid,i\u00b2/\u03bbid,i) + \u03a3_j (zex,j\u00b2/\u03bbex,j).\n\nThis constrains solutions to lie within a learned hyperellipsoid in the identity and expression spaces.",
      "source_document": "papers/2512.19560v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a RealNVP-style normalizing flow to model non-Gaussian identity/expression latent codes from a bilinear 3D face model, what exact optimization objective is used (including any regularization term), and what dequantization/noise injection is applied to the latent inputs during training?",
      "answer": "The flow parameters are trained by maximum likelihood, i.e., minimizing the negative log-likelihood (NLL) given by \u2212log pZ(f(w)) \u2212 log|det Jf(w)| via the change-of-variables formula. To stabilize training and avoid excessive local deformation, the objective adds Jacobian Frobenius-norm regularization, yielding\nL = NLL + \u2016Jf\u2016F = \u2212log pZ(f(w)) \u2212 log|det Jf(w)| + \u2016Jf\u2016F.\nIn addition, the (pre-flow) latent vector w is dequantized by adding uniform noise during training: \u007fw = w + \u03b5 with \u03b5 drawn from \u221a\u03c3w\u00b7U(0,1), where \u03c3w is the variance of pW(w), to smooth the distribution and improve likelihood estimation/robustness under limited data.",
      "source_document": "papers/2512.19560v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When doing cross-age 3D facial expression transfer from an adult expression dataset to an infant 3D face model with a different mesh topology, how can you construct the infant mesh with a target expression starting from an infant scan that already has some source expression, and what strategy can reduce dependence on any single adult subject\u2019s deformation field?",
      "answer": "First estimate the infant\u2019s current/source expression and pick an adult subject that has both the same source expression s and the desired target expression e. Compute an adult deformation field as the vertex-wise difference between the adult meshes in expressions e and s (DFW = YFW,e \u2212 YFW,s). Use the precomputed barycentric-coordinate linear mapping M that re-parameterizes FaceWarehouse meshes into the infant (BabyFM) triangulation to transport that deformation to the infant topology (DFW\u2192BabyFM = DFW \u00b7 M). Generate the infant target-expression mesh by adding a scaled version of this mapped deformation to the infant mesh in expression s (XBabyFM,e = XBabyFM,s + \u03b4\u00b7DFW\u2192BabyFM, with \u03b4 controlling expression intensity). To reduce dependence on any single adult identity, repeat the transfer with an ensemble of randomly selected adult subjects and average the resulting infant meshes.",
      "source_document": "papers/2512.19560v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using a diffusion model to synthesize photorealistic 2D infant portraits that stay faithful to a given 3D face mesh\u2019s geometry and expression, what conditioning signals can be extracted from the mesh, how are they computed, and how are they injected into the diffusion pipeline (including any text prompt used to steer appearance)?",
      "answer": "The 2D generator is Stable Diffusion 1.5 conditioned with a multi-ControlNet setup that takes two geometric cues derived from the input 3D mesh: (1) a depth map obtained by rendering the 3D face in a random pose and taking the z-buffer (via PyTorch3D), and (2) a Canny edge map computed by applying a Canny filter to the grayscale version of the same render. The depth and edge maps are provided to separate ControlNet branches to guide generation to align with the mesh geometry. Visual appearance is further steered with a structured prompt of the form \u201cRealistic frontal view portrait of an/a [ethnicity] infant\u201d, where the ethnicity token is manually set (e.g., Black, Chinese, Indian, Filipino, White, or Hispanic).",
      "source_document": "papers/2512.19560v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When you only have one (often non-neutral) 3D infant scan per subject but need to identify which facial action units are present so you can factor out the source expression before transferring a target expression, how can you recognize AUs from the mesh geometry (what local spectral features are computed, around which facial locations, and what classifier setup outputs the AU activations)?",
      "answer": "Recognize AUs using local shape spectral analysis on landmark-centered mesh patches: take the 19 central anatomical landmarks on the infant template and, for each landmark, form a 1-ring neighborhood patch mesh (V,E). Build the patch graph Laplacian (a discrete Schr\u00f6dinger-operator form with \u22121 on edges and vertex valence on the diagonal), eigen-decompose it, and keep a \u03c4-dimensional low-frequency embedding using the eigenvectors associated with the \u03c4 smallest eigenvalues (\u03c4=50) for robustness to local noise. Project the patch vertex coordinates into this spectral basis to get a \u03c4-D feature vector per landmark, then concatenate across landmarks into a feature matrix for the whole face. Train 46 one-vs-rest binary SVM classifiers (LIBSVM) on these features; the output is a 46-dimensional binary vector indicating which AUs are detected (1) or absent (0) in the infant scan.",
      "source_document": "papers/2512.19560v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a diffusion-prior\u2013regularized Patlak parametric imaging method, how can the kinetic-model data term and the diffusion prior be combined into a single objective and then optimized so that kinetic fitting is decoupled from diffusion sampling? Describe the HQS variable-splitting formulation (including what x and v represent) and the two alternating subproblems that are solved each outer iteration.",
      "answer": "The Patlak parameters are stacked into x (containing the Patlak slope and intercept images), with the dynamic PET frames y modeled as y = A x + e under Gaussian noise. The reconstruction is posed as minimizing a data-fidelity term plus a diffusion-model prior via a RED-Diff regularizer:\n\n\u2022 Objective:  x\u0302 = arg min_x  (1/2)||y \u2212 A x||_2^2 + R_RED(x), where R_RED(\u00b7) is a diffusion-based regularization defined as an expectation over diffusion time t and injected noise \u03b5_t, using a pre-trained score/noise predictor \u03b5\u0302_\u03b3(x_t,t) with a weighting \u03c9_t and a stop-gradient term to penalize correlation between predicted noise residual and the image component.\n\n\u2022 Decoupling via HQS: introduce an auxiliary variable v and a quadratic penalty to split the problem:\n  x\u0302 = arg min_{x,v} (1/2)||y \u2212 A x||_2^2 + R_RED(v) + (\u03bb/2)||x \u2212 v||_2^2.\n  Here, x is the current estimate constrained by the Patlak forward model (data consistency), while v is the variable on which the diffusion prior acts (the \u201cdenoised/prior\u201d image).\n\n\u2022 Alternating subproblems each outer iteration:\n  (1) Data-consistency / kinetic fitting step: x^{n+1} = arg min_x (1/2)||y \u2212 A x||_2^2 + (\u03bb/2)||x \u2212 v^n||_2^2 (a quadratic problem, implemented with an optimization-transfer surrogate to update voxels independently).\n  (2) Diffusion-prior step: v^{n+1} = arg min_v R_RED(v) + (\u03bb/2)||x^{n+1} \u2212 v||_2^2, solved with the same RED-Diff/diffusion-guided optimization procedure (using the pre-trained score function), thereby separating the kinetic-model fitting iterations from the diffusion denoising iterations.",
      "source_document": "papers/2512.19584v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a RED-Diff\u2013style diffusion-prior regularization step for Patlak parametric imaging (where a pre-trained score model predicts noise), what per-time-step loss is minimized to update the current estimate during the reverse process, including (i) how the current estimate is \u201cnoised\u201d before being fed to the score model, (ii) how the regularization term uses a stop-gradient on the noise residual, and (iii) how the time-dependent weight \u03c9t is chosen from the signal-to-noise ratio?",
      "answer": "The diffusion-prior update optimizes, for each reverse diffusion time t (from the chosen final diffusion step down to 0), a loss composed of a quadratic penalty that keeps the denoised variable close to the current data-consistency estimate plus a RED-Diff regularizer based on the score model\u2019s noise residual:\n\n\u2022 The variable is first \u201cnoised\u201d before being passed into the pre-trained score function by forming\nv_scaled = \u221a\u03b1t \u00b7 v + \u221a(1\u2212\u03b1t) \u00b7 \u03f5t,  with \u03f5t ~ N(0, I).\n\n\u2022 The loss includes a penalty term and a RED-Diff term of the form\nLoss = (\u03bb/2) ||x \u2212 v||^2_2  +  \u03c9t \u00b7 sg( \u03f5\u0302\u03b3*(v_scaled, t) \u2212 \u03f5t )^T v,\nwhere \u03f5\u0302\u03b3*(\u00b7,t) is the pre-trained score model\u2019s predicted noise and sg(\u00b7) denotes stop-gradient so the residual direction is treated as fixed when differentiating w.r.t. v.\n\n\u2022 The time weight is set as the inverse of a time-dependent SNR:\n\u03c9t = 1/SNRt, with SNRt := \u03b1t/\u03c3t (\u03c3t is the diffusion noise standard deviation at time t).",
      "source_document": "papers/2512.19584v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an HQS/penalty-based Patlak fitting setup where the data-consistency subproblem minimizes 1/2\u2016y\u2212Ax\u2016\u00b2 + (\u03bb/2)\u2016x\u2212v\u2016\u00b2, what optimization-transfer (majorize\u2013minimize) surrogate is used to decouple A from x so each voxel can be updated independently, and what is the resulting multiplicative voxel-wise update for x_j (including how the surrogate weights \u03b1_ij are defined in terms of A_ij, x_j, and (Ax)_i)?",
      "answer": "The quadratic term \u2016y\u2212Ax\u2016\u00b2 is majorized by a separable surrogate using convex-combination weights \u03b1^k_{ij} so that each measurement i is written as a weighted sum over voxels and the resulting surrogate becomes a sum of independent 1D quadratics in each x_j plus the penalty term (\u03bb/2)(x_j\u2212v_j^k)\u00b2. The weights are\n\n\u03b1^k_{ij} = (A_{ij} x^k_j) / (\\sum_{j=1}^{2J} A_{ij} x^k_j) = (A_{ij} x^k_j) / (A_i x^k),\n\nwhere A_i x^k denotes the i-th element of Ax^k.\n\nWith this surrogate, the voxel-wise iterative update (inner iteration n) becomes\n\nx^{k,n+1}_j = x^{k,n}_j \\; \\frac{\\sum_i A_{ij} y_i + \\lambda v^k_j}{\\sum_i A_{ij} (A_i x^{k,n}) + \\lambda x^{k,n}_j},\n\n(i summed over all MJ measurement entries), which updates each voxel independently given the current forward projection Ax^{k,n}.",
      "source_document": "papers/2512.19584v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "If you want to apply a score-based diffusion prior that was trained on cropped, SUV-normalized static total-body PET volumes to denoise/regularize Patlak parametric estimation from uEXPLORER dynamic PET, what concrete preprocessing and patching strategy can be used so the dynamic volumes match the score model\u2019s expected input and still fit in GPU memory, and how should the patch-wise outputs be recombined into a whole-body parametric image?",
      "answer": "Crop the uEXPLORER dynamic PET volumes to remove the leg portion and extraneous background so the image grid matches the score model\u2019s cropped static-PET field of view (192\u00d7288\u00d7520), and rescale the voxel size to the same voxel size used when training the score function. For memory/sampling efficiency, split each 3D frame into eight patches of size 192\u00d7288\u00d7136 with 8 axial slices overlapped between neighboring patches, run inference patch-wise (e.g., in parallel across multiple GPUs), then reconstruct the full-volume parametric image by merging patches with weighted averaging in the overlapping regions.",
      "source_document": "papers/2512.19584v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a dynamic-PET Patlak parametric-imaging study, if you want to fairly compare a diffusion-prior\u2013regularized estimator against classic denoising baselines, how should Gaussian smoothing, anatomically guided nonlocal means (NLM), and HYPR be inserted into the reconstruction/estimation pipeline (i.e., applied to parametric images vs. to dynamic frames), and what quantitative evaluation protocol should be used to compare normal-dose and low-dose results (including what reference images are used for PSNR/SSIM and how CNR is computed)?",
      "answer": "Gaussian filtering and anatomically guided NLM are applied as post-processing to the Patlak parametric images produced by a baseline iterative Patlak estimation method. In contrast, HYPR is applied earlier: it denoises the full set of dynamic PET frames first, and the denoised dynamic frames are then used as input to the same baseline iterative Patlak estimation to obtain parametric images.\n\nFor evaluation, comparisons are performed on both normal-dose and simulated 1/10 low-dose datasets using PSNR, SSIM, and lesion-to-reference CNR. For low-dose quantitative metrics, the normal-dose parametric images obtained from the baseline iterative method are used as the reference for computing PSNR/SSIM. CNR is computed as (\u03bc_lesion \u2212 \u03bc_ref)/\u03c3_ref, where \u03bc_lesion and \u03bc_ref are mean intensities in lesion and reference regions and \u03c3_ref is the voxel-wise standard deviation in the reference/background region; background noise is estimated using multiple (20) spherical ROIs placed in the liver.",
      "source_document": "papers/2512.19584v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a vision\u2013tabular model that must work from 0% to 100% tabular availability at inference, how can a \u201cmore vs. fewer\u201d ranking loss be adapted to missing tabular attributes during downstream fine-tuning\u2014specifically, how are the two attribute subsets sampled/related, what inequality is the loss enforcing, and how is it combined with the task loss?",
      "answer": "Use a Tabular More-vs.-Fewer objective (TabMoFe) that compares two different subsets of attributes for the same sample. At each iteration, sample two tabular-attribute sets t+ and t\u2212 such that t\u2212 \u2282 t+ \u2286 tfull, with |t+| between 1 and the maximum available number of attributes Na, and |t\u2212| between 0 and |t+|. The loss enforces that the model should not perform worse when given more attributes, i.e., the task loss with t+ should be \u2264 the task loss with t\u2212. This is implemented as a hinge on the loss difference:\n\nLT abMoF e = max( Ltask(i, t+) \u2212 Ltask(i, t\u2212), 0 ).\n\nThe multimodal fine-tuning objective adds task losses for both subsets plus a weighted ranking term:\n\nLmulti = Ltask(i, t+) + Ltask(i, t\u2212) + \u03bb \u00b7 LT abMoF e,\n\nwhere \u03bb weights the TabMoFe penalty.",
      "source_document": "papers/2512.19602v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a vision\u2013tabular network that must remain reliable when the tabular side has an arbitrary number of missing attributes, how can a gated cross-attention fusion block be set up to adaptively control tabular influence\u2014specifically: which modality provides the queries vs. keys/values in cross-attention, how is a scalar gate computed from the tabular representation, and how is that gate used to form the final multimodal embedding passed to the task head?",
      "answer": "Use separate pretrained image and tabular encoders to produce features v_i and v_t, then linearly project them into a shared latent dimension d (\\tilde v_i, \\tilde v_t). Apply multi-head self-attention on the image tokens to get \\hat v_i. Perform cross-attention with image tokens as queries and tabular tokens as keys and values. Compute a scalar gate w\\in[0,1] by feeding the tabular [CLS] token (from \\tilde v_t) through an MLP ending with a sigmoid. Form the multimodal embedding as v_m = \\hat v_i + w \\odot CrossAttention(\\hat v_i, \\tilde v_t), then pass it through a feed-forward layer and downstream task head(s).",
      "source_document": "papers/2512.19602v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a vision\u2013tabular model that uses a trainable multimodal fusion block, how can disentangled gradient learning be applied during downstream fine-tuning to avoid the multimodal objective degrading unimodal encoders\u2014specifically, what losses are computed in each of the two backward passes, which parameters are updated in each pass, and what is done to prevent gradients from the multimodal loss flowing back into the modality encoders?",
      "answer": "Use two separate backward passes that decouple encoder optimization from fusion optimization. (1) First compute a unimodal loss with separate task heads attached to each encoder, \\(L_{\\text{unimodal}} = L_{\\text{task}}(i) + L_{\\text{task}}(t)\\), and backpropagate it to update the image encoder and tabular encoder independently. Any gradients that accumulate on the multimodal fusion parameters during this step are removed, and the modality features are detached. (2) Then compute the multimodal loss \\(L_{\\text{multi}}\\) (the downstream multimodal task loss, including both tabular-subset task losses and the TabMoFe term) and backpropagate it only through the multimodal fusion module. Detaching the modality features ensures the second-pass gradients do not propagate into the encoders, preventing gradient interference between unimodal and multimodal objectives.",
      "source_document": "papers/2512.19602v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In multimodal contrastive pretraining for a vision\u2013tabular model that must tolerate arbitrary missing tabular entries at inference, what tabular augmentation strategy can be used to simulate missingness (how is the subset of attributes sampled from the full table for each sample), how does this differ from a \u201ctabular corruption\u201d augmentation that replaces values, and what kind of unimodal vs. multimodal downstream evaluation can be used to check that the missingness augmentation improves robustness without hurting the image encoder?",
      "answer": "Use missingness simulation as tabular augmentation during contrastive pretraining: for each training pair, start from the sample\u2019s full attribute set t_full with N_full entries and randomly select a subset t_sub \u2286 t_full whose size is sampled between 1 and N_full; feed only that subset to the tabular encoder, effectively simulating random entry missingness and regularizing the tabular encoder to produce meaningful representations from partial attributes.\n\nThis differs from tabular corruption (as in MMCL-style augmentation), where a fraction of tabular entries are randomly selected and their values are replaced by alternative values drawn from the empirical marginal distribution (i.e., the attribute remains present but may be wrong/noisy rather than missing).\n\nTo verify robustness gains without degrading the image encoder, evaluate pretrained backbones on (i) a multimodal task that benefits from tabular data (e.g., CAD classification using both image and tabular inputs) and (ii) an image-only task whose target is image-derived and does not require tabular inputs (e.g., LVEF regression from images only). Improved multimodal performance\u2014especially under low tabular availability\u2014while maintaining strong image-only performance indicates the missingness augmentation helps without harming the visual encoder.",
      "source_document": "papers/2512.19602v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a vision\u2013tabular model that fuses modalities with cross-attention, how can you compute an interpretable \u201cimportance score\u201d for each tabular attribute from the attention maps\u2014specifically, which cross-attention layer\u2019s scores are used, how are they aggregated across heads and tokens, and how is the aggregation performed differently for classification versus regression?",
      "answer": "Use the raw attention scores from the final cross-attention layer (image-to-tabular). Convert them to per-attribute scores by aggregating the attention weights assigned to each tabular attribute: first average over all attention heads, then average over the image tokens, yielding a single scalar score per tabular attribute. For classification, perform this aggregation per class; for regression, aggregate over the full test set.",
      "source_document": "papers/2512.19602v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When regularizing Euclidean SSL embeddings toward a non-Gaussian target distribution like a Laplace prior, what makes a Kernel Stein Discrepancy (KSD) regularizer (computed in the full embedding space) preferable to an MMD-style regularizer that matches the Laplace prior via sliced/spectral quadrature, and how is this reflected in downstream classification behavior?",
      "answer": "A full-space KSD regularizer is preferable because it can target a broad class of priors (including Laplace) directly through the prior\u2019s score function (\u2207x log Q(x)), yielding closed-form gradients and avoiding both sampling from the prior and numerical approximations of expectations under the prior. By contrast, adapting MMD to a non-Gaussian prior like Laplace requires approximating a spectral integral (e.g., of the characteristic function) using finite slicing plus quadrature, introducing extra hyperparameters (number of slices/knots) and approximation error that can degrade the regularizer. Empirically, the Laplace-prior version implemented via quadrature is less accurate/competitive on downstream classification than the corresponding unsliced KSD that matches the Laplace geometry directly.",
      "source_document": "papers/2512.19605v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a synthetic map route-tracing pipeline that starts from a binary traversability mask, how can the mask be converted into a graph suitable for shortest-path computation, and what edge-weighting scheme can be used to discourage paths from cutting through low-confidence/low-density traversable regions?",
      "answer": "Convert the validated traversability mask into a weighted, undirected pixel-graph by (1) discretizing the mask into non-overlapping b\u00d7b blocks and creating a node for each block that contains at least one traversable pixel; (2) connecting two nodes with an edge when the Euclidean distance between their block centers is within a neighborhood threshold \u03b4_max; and (3) weighting each edge by Euclidean distance multiplied by a density penalty based on the traversable-pixel densities \u03c1_i and \u03c1_j of the incident nodes: w(v_i,v_j)=||c_i\u2212c_j||_2\u00b7(1+\u03bb((1\u2212\u03c1_i)+(1\u2212\u03c1_j))). Shortest paths between sampled start/end nodes can then be found with Dijkstra\u2019s algorithm (optionally followed by Ramer\u2013Douglas\u2013Peucker simplification).",
      "source_document": "papers/2512.19609v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When fine-tuning a multimodal model to trace routes by emitting an ordered list of (x, y) points, which coordinate encoding is most effective for generalizing to real-world map queries (e.g., absolute pixel coordinates vs normalized coordinates vs delta steps), and what does the ablation conclude about how much numeric precision the coordinates need before performance degrades substantially?",
      "answer": "Normalized coordinates work best for route tracing, likely because MLLMs have prior exposure to normalized coordinate formats from object-detection-style pretraining. The precision ablation shows that keeping roughly four decimal places (or full precision) maintains performance, while reducing precision below that (e.g., to three or two decimals) causes significant degradation.",
      "source_document": "papers/2512.19609v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a map route-tracing model with both normalized dynamic time warping (NDTW) and success rate (SR), why can fine-tuning produce an apparent regression in average NDTW even though the model is actually better overall, and what kind of evaluation adjustment is suggested to better capture the improvement?",
      "answer": "Fine-tuning can substantially increase SR, meaning the model now outputs parseable, valid paths for many queries it previously failed to answer at all (where NDTW would be undefined). Those newly-solved cases tend to be harder, so their path\u2013ground-truth alignment errors are larger, which can raise the average NDTW even while overall capability improves. A suggested adjustment is to use a hybrid evaluation strategy that jointly accounts for both SR (whether the model produces a valid output) and path-alignment quality (e.g., NDTW) rather than interpreting NDTW alone in isolation.",
      "source_document": "papers/2512.19609v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When generating synthetic supervision for pixel-accurate route tracing on wayfinding maps, how can you use two LLM-based \u201ccritics\u201d to filter training data\u2014one for candidate traversability masks and one for candidate shortest-path polylines\u2014and what concrete acceptance criteria does each critic apply to reject masks/paths that include non-walkable areas or leave the map region?",
      "answer": "A practical way is to (1) extract multiple candidate binary path masks (e.g., from dominant-color / k-means color clusters) and pass each mask to a **Mask Critic**, then (2) compute shortest-path candidates on the validated mask and pass the rendered path overlay to a **Path Critic**.\n\n**Mask Critic criteria (mask filtering):** It judges whether the white region mostly corresponds to *pedestrian-traversable* surfaces on the map\u2014e.g., paved sidewalks, marked crosswalks, pedestrian-only paths, public plazas, indoor walkways. It should flag masks as poor if they include substantial *non-target* regions such as vehicle road lanes, grass/dirt, buildings, cars, or other non-pedestrian surfaces; a heuristic is that if \u201cthe majority of the image is white,\u201d it is likely low quality. The critic produces a structured analysis (composition breakdown and major errors) and a final one-word grade (Good/Fair/Poor), where \u201cGood\u201d requires the mask to be mostly target areas (>60%) and \u201cPoor\u201d is predominantly non-target (<40% correct).\n\n**Path Critic criteria (path filtering):** It validates a drawn route polyline using two checks: (i) a **boundary check** that the entire line stays within the main geographic area of the map and does not touch/cross background or non-map elements like the title/legend/scale bars, and (ii) a **traversability check** that the line follows traversable routes (roads/streets/marked trails/walkways) and does not cut across impassable obstacles such as buildings, solid land without a path, or bodies of water (unless on a bridge/ferry route). The final judgment is binary: GOOD or BAD.",
      "source_document": "papers/2512.19609v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a fine-tuned multimodal model that outputs pixel-level routes as coordinate sequences, what failure modes tend to dominate among the worst-aligned predictions, and what underlying spatial reasoning weaknesses do these errors suggest (e.g., local traversability confusion vs long-horizon planning)?",
      "answer": "The most common failure cases among the worst (highest-NDTW) predictions fall into four main categories: (1) invalid detours into non-traversable regions\u2014paths drift into clearly non-walkable areas like buildings or fenced regions, often because visually similar textures (e.g., large green areas) are mistaken as walkable; (2) overly long but still valid paths\u2014the route stays on traversable regions but chooses an inefficient longer alternative instead of the shortest path; (3) early off-path drift followed by recovery\u2014initial segments slip slightly outside walkable areas before realigning; and (4) correct start but failure in long-horizon planning\u2014the path begins correctly but the model commits to a wrong branch/dead-end and then compensates by going off-path to reach the goal, indicating difficulty maintaining global directionality over long routes.",
      "source_document": "papers/2512.19609v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a preference-alignment setup for a latent diffusion model where experts score generated plant images for realism, how can the scoring signal be converted into a training objective that directly fine-tunes the diffusion U-Net (without RL), including (i) what representation the reward model operates on and its learning loss, and (ii) how reward values are turned into per-sample weights that modify the standard denoising loss during fine-tuning?",
      "answer": "A practical approach is a two-stage pipeline: (i) train a separate reward model to predict expert realism scores from the diffusion model\u2019s latent representation, then (ii) use the predicted rewards to reweight the diffusion denoising loss during supervised fine-tuning.\n\n(i) Reward model: images are encoded into the Stable Diffusion VAE latent space (a 4\u00d764\u00d764 tensor), and a CNN reward predictor is trained to map these latents to a scalar reward. The reward model is optimized with mean-squared error between predicted and annotated expert scores: \n\\(\\mathcal{L}_{\\text{reward}}=\\frac{1}{N}\\sum_{i=1}^N(\\hat r_i-r_i)^2\\).\n\n(ii) Reward-weighted diffusion fine-tuning: for each prompt, multiple candidate images are generated and scored by the reward model; the top-k candidates are selected. The U-Net is then fine-tuned with a reward-weighted denoising objective\n\\(\\mathcal{L}_{\\text{SFT}}=\\sum_{i=1}^k w_i\\,\\lVert \\epsilon_\\theta(z^i_t,t)-\\epsilon\\rVert^2\\),\nwhere weights are a softmax over predicted rewards with temperature \\(\\tau\\): \n\\(w_i=\\exp(r_i/\\tau)/\\sum_j\\exp(r_j/\\tau)\\).\nThis biases updates toward expert-preferred (higher-reward) samples while still training via the standard diffusion noise-prediction loss.",
      "source_document": "papers/2512.19632v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In DreamBooth-style indoor-to-outdoor plant translation with a rare identifier token (e.g., \u201csks\u201d), how is the fine-tuning objective constructed to both preserve the specific plant\u2019s identity and avoid overfitting to that plant, which parts of the latent diffusion pipeline are updated versus frozen during this adaptation, and how is the learned token used in prompts at inference time to keep plant morphology while changing the outdoor environment (soil/lighting/background)?",
      "answer": "The adaptation uses DreamBooth with a rare identifier token (e.g., sks) bound to the target plant\u2019s visual identity. Training optimizes a composite loss that adds a subject reconstruction term and a class prior preservation term:  \\(\\mathcal{L}_{\\text{DreamBooth}} = \\mathcal{L}_{\\text{subj}} + \\lambda\\,\\mathcal{L}_{\\text{prior}}\\). The subject loss enforces that generations conditioned on the identifier retain the specific subject (the target plant), while the prior preservation loss regularizes the model to maintain generalization to the broader class (e.g., \u201ccanola plant\u201d), mitigating overfitting/catastrophic forgetting; \\(\\lambda\\) trades off subject fidelity vs generalization.\n\nFor the translation fine-tune, the diffusion U-Net denoiser and the subject-specific token embedding associated with sks are updated, while the VAE and the (CLIP) text encoder are kept frozen.\n\nAt inference, the learned identifier token is inserted into environment-describing prompts (e.g., \u201ca sks canola plant taken in the outdoor TerraByte field\u201d), so the token anchors the indoor plant\u2019s structure/morphology, and additional descriptors control the rendered outdoor context such as background, lighting, and soil texture (and can also specify arrangement and developmental stage).",
      "source_document": "papers/2512.19632v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When adapting a pretrained latent diffusion model (e.g., SD-v1.4) to a niche agricultural domain with captioned indoor/outdoor plant images, what parts of the pipeline are typically updated versus frozen and what denoising training objective is optimized; and, when benchmarking against large text-to-image baselines on indoor vs. outdoor sets, which way do FID and Inception Score tend to break (including any indoor\u2013outdoor trend) and why?",
      "answer": "The domain adaptation updates the U-Net denoising backbone while keeping the VAE (encoder/decoder) and the CLIP text encoder frozen to reduce compute and avoid forgetting general representations. Training optimizes the standard DDPM/LDM denoising objective: the U-Net is conditioned on the text embedding and learns to predict the Gaussian noise added to the latent at a sampled timestep (i.e., an MSE between true noise and predicted noise).\n\nFor benchmarking, realism/diversity are evaluated with FID (lower is better) and Inception Score (higher is better). The fine-tuned diffusion model achieves the best FID on both indoor and outdoor sets compared with Imagen, DALL\u00b7E, and GigaGAN; for IS it is best on indoor images, while on outdoor images Imagen is slightly higher. Across models, indoor imagery tends to have higher IS and lower FID than outdoor imagery because indoor images are captured under controlled, less complex conditions, whereas outdoor field images have greater variability and clutter, making the distribution harder to model and driving FID up (and typically IS down).",
      "source_document": "papers/2512.19632v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When translating controlled indoor plant imagery into realistic outdoor field scenes for downstream multi-object weed detection, why is an image-guided diffusion (img2img) strategy preferable to purely text-conditioned DreamBooth, and how is the image-guided input constructed and used during denoising? Also, what evaluation protocol and detection metrics are used to quantify whether the translated images help YOLOv8, and what overall performance trend is observed as the synthetic/translated ratio increases?",
      "answer": "DreamBooth-style text conditioning works well for single-plant translations but struggles to maintain spatial consistency in multi-object field scenes; text prompts alone are insufficient to preserve the spatial structure of multiple plants/objects. For downstream detection they therefore use Stable Diffusion\u2019s image-to-image (img2img) pipeline, which refines an existing composited image rather than generating from scratch: the denoising trajectory is initialized from a partially noised version of the input image. The input is built by segmenting/cropping indoor soybean plants into patches, compositing multiple patches into row-like multi-plant scenes on real outdoor soil/background images, and then running img2img to translate the composite into a realistic outdoor field appearance (with realism sensitive to segmentation/compositing artifacts).\n\nTo test utility for detection, YOLOv8 is fine-tuned on mixed real+translated soybean\u2013weed datasets at different synthetic ratios and evaluated on a held-out test set containing only real soybean\u2013weed images. Standard detection metrics\u2014precision, recall, and mAP50\u2014are reported. As more translated/synthetic data is added, precision and recall improve consistently while mAP50 stays high (\u22480.96 on average across weeds), and gains tend to saturate at high synthetic ratios, implying balanced augmentation matters more than simply adding more translated data.",
      "source_document": "papers/2512.19632v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When optimizing a continuous-time 4D Gaussian splatting model where Gaussian states are advanced by integrating a learned velocity field, what is the full training objective (name the loss components), and what role does each non-photometric term play in improving temporal stability/coherence during long rollouts?",
      "answer": "The objective is a weighted sum\n\nL = L_photo + \u03bb_coh L_coh + \u03bb_anchor L_anchor + \u03bb_tv L_tv.\n\n\u2022 L_photo: standard photometric reconstruction loss (L1, optionally combined with SSIM/LPIPS) on rendered images at target times.\n\u2022 L_tv: temporal smoothness on the spatiotemporal feature planes (plane total variation and time-smoothing) to smooth the 4D feature fields and reduce drift/instability.\n\u2022 L_coh: a velocity-coherence regularizer that encourages nearby Gaussians to move consistently (promoting coherent motion).\n\u2022 L_anchor (optional): an anchor-consistency term that pulls the integrated Gaussian state toward stored \u201canchor\u201d waypoint snapshots (sparse pseudo-observations) to stabilize long-horizon integration and suppress accumulated drift.",
      "source_document": "papers/2512.19648v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a continuous-time 4D Gaussian splatting model that represents motion as a learned velocity field, how can you *locally inject or blend* an external motion (e.g., a user-defined rotational flow) into only part of the scene at inference time, and what additional recomposition/geometry-completion procedure is needed to prevent artifacts when the moved object is only partially observed in the training views?",
      "answer": "Local motion injection is done by composing the learned velocity field v\u03b8 with an external field vext using a spatial mask \u03bb(x)\u2208[0,1], producing a mixed field\nvmix(x,t)=\u03bb(x)\u00b7v\u03b8(x,t)+(1\u2212\u03bb(x))\u00b7vext(x,t)\n(or equivalently v\u2032(x,t)=\u03bb(x)\u00b7vinj(x,t)+(1\u2212\u03bb(x))\u00b7v\u03b8(x,t) for an injected field vinj). Gaussians are then evolved by integrating this composed vector field, which yields smooth spatiotemporal transitions because evolution is continuous-time.\n\nBecause object-associated Gaussians are often incomplete (unseen surfaces due to limited viewing angles), moving/rotating them exposes missing geometry and causes severe artifacts. To address this, the object is first isolated with a 3D Gaussian segmentation mask \u03bb(x). Segmented ground-truth images from the original cameras are rendered and fed to Zero123 to synthesize novel, previously unobserved viewpoints. Both the real and synthesized views supervise a second-pass 3D Gaussian optimization applied only to the object\u2019s Gaussians Gobj to densify/complete its geometry. The refined object Gaussians are then reinserted into the full scene, and the injected velocity field is applied to them during continuous-time evolution.",
      "source_document": "papers/2512.19648v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a dynamic 3D Gaussian model where motion is produced by numerically integrating a learned continuous-time velocity field, what integration method is used to advance the Gaussian states, and how does swapping it for a first-order Euler integrator affect trajectory quality during forward/backward rollouts and long-horizon extrapolation?",
      "answer": "Gaussian states are advanced with a higher-order ODE solver\u2014specifically a 4th\u2011order Runge\u2013Kutta (RK4) integrator. Replacing RK4 with a first\u2011order Euler integrator is cheaper but quickly accumulates numerical drift, leading to temporally incoherent motion across Gaussians during both forward and backward rollouts. RK4 yields smoother, more stable trajectories and better preserves Gaussian structure (though the structure can still degrade under extremely long-horizon extrapolation).",
      "source_document": "papers/2512.19648v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a continuous-time dynamic 3D Gaussian splatting model where each Gaussian\u2019s parameters evolve according to an ODE, what is the per-Gaussian state vector, how is rotation parameterized/updated, which state components does the neural dynamical law actually predict time-derivatives for, and how are the local spatiotemporal conditioning features computed from a factorized 4D representation (name the planes and the interpolation used)?",
      "answer": "Each Gaussian i is modeled with a state vector\nx_i(t) = [p_i(t), R_i(t), S_i(t), c_i(t), \u03b1_i(t)],\nwhere p_i is 3D position, R_i is rotation (handled via an exponential-map update), S_i is anisotropic scale, c_i is color, and \u03b1_i is opacity. The continuous-time dynamics are defined by d x_i / dt = v_\u03b8(x_i(t), f_i(t), t), where v_\u03b8 is a lightweight MLP that predicts the instantaneous derivatives for position, rotation, and scale (not for color/opacity as described). The conditioning feature for each Gaussian center p_i at time t is obtained by differentiable bilinear interpolation (\u201clookup\u201d) on six factorized space\u2013time planes {P_xy, P_xz, P_yz, P_xt, P_yt, P_zt}: f_i(t) = \u03a6(p_i, t). These local 4D features encode geometry/motion cues and condition the velocity field used for the dynamical update.",
      "source_document": "papers/2512.19648v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a continuous-time dynamic 3D Gaussian splatting method that evolves Gaussians by integrating a learned velocity field, how can you stabilize long-horizon rollouts using a small set of stored Gaussian \u201cwaypoints\u201d? Describe (i) how anchors are chosen/used during rendering of an arbitrary target time, and (ii) the optional anchor-consistency loss term (give its form and what states it compares).",
      "answer": "Stabilization is done by keeping a small set of temporal anchor snapshots A={t(a)1,t(a)2,\u2026} that store the full Gaussian states at fixed times. For rendering/supervising any target frame at time t, the system finds the nearest past anchor time t(a), reinitializes the ODE state with the stored Gaussian parameters at that anchor, and then integrates forward only from t(a) to t. This shortens the effective integration horizon and reduces drift accumulation.\n\nOptionally, an anchor-consistency penalty is added that measures deviation between the integrated state at an anchor and the stored anchor snapshot:\n\nL_anchor = \\sum_{t(a)\\in A} \\| x(t(a)) - \\hat{x}(t(a)) \\|_2^2,\n\nwhere \\hat{x}(t(a)) is the stored anchor state and x(t(a)) is the state obtained by integrating from the preceding anchor. This encourages consistency with waypoint snapshots while still allowing smooth continuous-time evolution between them.",
      "source_document": "papers/2512.19648v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a diffusion-based video inpainting system to add environmental interaction effects (e.g., shadows/splashes) to a foreground-over-background composite, how can a single model be trained to work both with explicit effect masks and with no masks (including the case where only a few keyframes are annotated)? Describe the specific mask-augmentation scheme used during training and what it represents.",
      "answer": "Train with a \u201ctri-mask\u201d scheme that exposes the model to both masked and unmasked conditions: during training, the effect mask is randomly replaced by a uniform gray mask for some frames. This gray mask represents frames where the effect regions are unknown/unannotated (the frame may or may not contain effects, or their locations are uncertain). Seeing both true binary masks and gray \u201cunknown\u201d masks lets the same model run in supervised mask-guided mode, fully mask-free mode, or mixed guidance where only some keyframes have masks while the rest are left unannotated.",
      "source_document": "papers/2512.19661v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "If you want a video inpainting diffusion model for \u201caugmented compositing\u201d to keep strong text-to-video / prompt-editing ability after fine-tuning on limited paired (Iover,Igt) effect data, how can you incorporate additional *unpaired* text-to-video samples that have only an effect video Igt and a caption T but no corresponding composite Iover or effect mask Meffect? Describe how these unpaired samples are represented in the model\u2019s conditional inputs during training, and how classifier-free guidance is used at inference to support prompt-conditioned effect generation.",
      "answer": "Unpaired T2V clips are added as extra training examples by generating diverse captions (via LLM caption augmentation) and synthesizing additional effect videos Igt with a pretrained T2V model. During training on these unpaired examples, the model is still trained in the same conditional inpainting framework but only Igt and the caption T are provided; the missing conditioning signals are handled by zeroing out the latent codes of the composite input video Iover and the effect mask Meffect. Mixing these unpaired examples into fine-tuning helps preserve the base model\u2019s intrinsic T2V and prompt-editing capabilities and strengthens CFG-based prompt editing. At inference, classifier-free guidance (CFG) is applied so generation can be steered by the text prompt (and optionally by Meffect as well), enabling prompt-conditioned effect generation rather than drifting away from language control after fine-tuning.",
      "source_document": "papers/2512.19661v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a video model whose goal is to add subtle, spatially localized interaction effects (e.g., shadows/splashes) while preserving the original composite, how can you evaluate whether the *edit itself* matches the ground-truth effect rather than just scoring overall similarity to the target frame? Define the directional CLIP-based metric used for this purpose, including what embedding differences are compared and what the score represents.",
      "answer": "Use a directional embedding metric (CLIPdir) that compares the *change induced by the edit* relative to the original input-without-effects.\n\nLet EIover, EIgt, and EI be L2-normalized CLIP image embeddings for (i) the input composite without effects Iover, (ii) the ground-truth video with effects Igt, and (iii) the generated output frame I. Compute the cosine similarity between the two \u201cedit directions\u201d in CLIP space:\n\nCLIPdir = 100 \u00d7 ((EIgt \u2212 EIover) \u00b7 (EI \u2212 EIover)) / (||EIgt \u2212 EIover||2 \u00b7 ||EI \u2212 EIover||2).\n\nThis measures how well the semantic/visual change from Iover to the generated result aligns with the change from Iover to the ground truth\u2014i.e., whether the model applied the correct effect edit even when the effect is small or localized and might not strongly affect standard CLIP similarity scores.",
      "source_document": "papers/2512.19661v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an effect-generation benchmark where the source composite video contains no interaction effects, how can you fairly adapt video-editing baselines that (a) require an edited first frame for guidance and (b) rely on a mask to learn/propagate a localized edit (e.g., per-video LoRA tuning)? Describe what you use as the \u201cedited\u201d first-frame reference and what mask you provide, and explain why these choices are necessary in this setting.",
      "answer": "To make such baselines comparable when the input composite lacks any effects, you provide the first frame of the ground-truth target video (the version *with* effects) as the required \u201cedited first frame\u201d reference. For a mask-driven per-video tuning baseline (like LoRA-Edit), you use a full-frame mask (rather than a tight box) so the method can learn effect-related changes over the entire frame/video despite there being no pre-existing localized effect region in the source composite; this enables knowledge learning across the whole clip (i.e., effectively I+V2V) and avoids handicapping the baseline due to the missing effect region in the input.",
      "source_document": "papers/2512.19661v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When fine-tuning a diffusion-transformer video inpainting model to add semi-transparent interaction effects while preserving the original composite, what training-time choices help retain scene context and reduce hallucinations\u2014specifically (i) which model components are updated vs frozen during fine-tuning, and (ii) how is the input composite encoded inside the masked region compared with the common practice of zeroing masked pixels/latents?",
      "answer": "The approach fine-tunes a base video inpainting diffusion transformer (DiT) by updating the transformer attention blocks while keeping the VAE encoder/decoder frozen. For conditioning, it uses the fully encoded latent of the entire input composite video (together with the effect-mask latent and text via attention), and crucially does not zero out the masked region of the composite; passing through the full composite latents preserves scene context and helps suppress hallucinations.",
      "source_document": "papers/2512.19661v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multimodal transformer that fuses retinal fundus images, clinical text, and structured patient metadata into a joint embedding for diabetic retinopathy grading and cross-modal retrieval, what combination of training objectives can be used to (i) align modalities, (ii) preserve modality-specific information, and (iii) maintain grading performance\u2014and how are these objectives dynamically balanced during optimization?",
      "answer": "Use six objectives: (1\u20133) three pairwise contrastive alignment losses between image\u2013text, image\u2013structured, and text\u2013structured embeddings; (4\u20135) two reconstruction losses that decode back the image (from the image CLS embedding) and the text sequence (from the text CLS embedding); and (6) a supervised DR severity classification loss averaged over the SDRG and ICDR five-class logits. The overall loss is a weighted sum L_total = \\sum_{i=1}^6 w_i L_i where the weights are learned and softmax-normalized, w_i = exp(\\alpha_i)/\\sum_j exp(\\alpha_j), with \\alpha_i trainable so the model adaptively rebalances tasks during training.",
      "source_document": "papers/2512.19663v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multimodal transformer that learns a shared embedding from retinal fundus images, clinical text, and structured patient features, how is the final 256\u2011D joint embedding constructed from the three token sequences (include what is concatenated, how modality identity is encoded, the fusion transformer configuration, and which output representations are pooled), and how is cross-modal retrieval evaluated and quantified against a fine-tuned CLIP baseline on the same dataset?",
      "answer": "Joint embedding construction: each modality is first encoded into a token sequence and projected to a common 256\u2011D space (ViT-B/16 patch tokens + CLS for images; Bio-ClinicalBERT CLS plus truncated token embeddings for text; an MLP embedding for the 6 structured variables with a learnable structured CLS token). The three encoded sequences are concatenated into one long sequence and augmented with learnable modality-type embeddings indicating whether a token comes from image, text, or structured data. This composite sequence is passed through a multimodal fusion transformer encoder with 6 layers, 8 attention heads, and model dimension 256, enabling token-level cross-modal attention. From the fusion output, the CLS token corresponding to each modality segment (image CLS, text CLS, structured CLS) is extracted; these three CLS vectors are concatenated and then linearly projected back down to a shared 256\u2011D joint embedding.\n\nRetrieval evaluation: text-to-image retrieval is evaluated using Recall@K (K \u2208 {1,5,10}), i.e., the fraction of text queries whose correct paired image appears in the top\u2011K retrieved items in the joint embedding space. On BRSET, the proposed model achieves 99.94% Recall@1, compared to 1.29% Recall@1 for CLIP fine-tuned on BRSET image\u2013text pairs (zero-shot CLIP is 0.00% Recall@1).",
      "source_document": "papers/2512.19663v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "If you want to train a multimodal diabetic-retinopathy model on a fundus dataset that provides images and structured metadata but no free-text clinical notes, what preprocessing pipeline can you use to (i) construct a text modality, (ii) preprocess/augment the retinal images, and (iii) prepare the structured features and data split so evaluation avoids patient leakage?",
      "answer": "A workable pipeline is to synthesize the missing text modality from the dataset\u2019s pathology indicators, while applying standard image preprocessing/augmentation and patient-level splitting:\n\n(i) Text modality construction: create a pseudo-clinical note by turning each binary disease/pathology label marked \u201cpresent\u201d into a short declarative sentence (e.g., \u201cDiabetic retinopathy is present.\u201d) and concatenating these sentences into a single narrative; then tokenize the resulting string and truncate it to a fixed maximum length (128 tokens) before feeding it to a clinical-language encoder (Bio-ClinicalBERT).\n\n(ii) Image preprocessing/augmentation: resize each macula-centered fundus image to 224\u00d7224, normalize pixel values to [0,1], and standardize using ImageNet statistics; during training only, apply light augmentations\u2014random horizontal/vertical flips, \u00b115\u00b0 rotations, and mild brightness/contrast jitter\u2014leaving validation/test images unaugmented.\n\n(iii) Structured features and split: form a 6-D structured vector from clinical/demographic attributes (age, sex, laterality/exam eye, diabetes duration, insulin use, diabetes diagnosis/type). Standardize continuous variables (zero mean/unit variance), numerically encode categorical variables, and impute missing values with median/mode as appropriate. Split the dataset at the patient level into 80% train, 10% validation, and 10% test so that no patient appears in more than one split, preventing information leakage.",
      "source_document": "papers/2512.19663v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When fine-tuning a multimodal transformer for diabetic-retinopathy that uses large pretrained encoders for fundus images and clinical text, what partial-freezing strategy and token-sequence construction can be used to (i) preserve pretrained representations, (ii) still allow domain adaptation, and (iii) keep fusion computation tractable\u2014specifically for the ViT image stream and the Bio-ClinicalBERT text stream?",
      "answer": "A compute- and data-efficient adaptation strategy is to fine-tune only the upper parts of the pretrained encoders while keeping most layers frozen, and to limit the number of tokens passed to the fusion transformer.\n\n\u2022 ViT image stream: resize each fundus image to 224\u00d7224, tokenize it into non-overlapping 16\u00d716 patches, and append a CLS token (yielding a 197-token sequence). Freeze the first 10 transformer blocks of the pretrained ViT-B/16 to preserve its pretrained visual features, and fine-tune only the remaining blocks for domain adaptation. Project the resulting token embeddings to the shared 256-D space with a linear layer, and prepend a learnable image-specific CLS token for fusion.\n\n\u2022 Bio-ClinicalBERT text stream: tokenize each clinical note with a maximum length of 128 tokens, run it through Bio-ClinicalBERT, and project hidden states from 768 to 256 dimensions. Freeze the embedding layer and the first 10 transformer blocks to retain domain-specific linguistic priors, and keep only the CLS representation plus a truncated subset of token embeddings (the first 50 tokens) to reduce fusion-time cost while retaining key textual content.",
      "source_document": "papers/2512.19663v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multimodal transformer trained to (a) grade diabetic retinopathy severity (SDRG/ICDR) and (b) support text-to-image retrieval by aligning fundus images, synthetic clinical notes, and structured patient features, what does an ablation over loss components show about which objectives are essential for retrieval versus classification? Summarize how performance changes when training with (i) classification loss only, then adding (ii) pairwise contrastive losses, then adding (iii) reconstruction losses, and finally using (iv) the full dynamically weighted multi-loss setup.",
      "answer": "The loss ablation shows that supervised grading alone is sufficient to obtain reasonable DR classification accuracy but is almost useless for cross-modal retrieval, whereas explicit alignment (contrastive) is the key driver of retrieval and reconstruction further refines the joint embedding without hurting classification.\n\n\u2022 Classification only: achieves high grading accuracy (about 95.11% SDRG and 94.92% ICDR) but retrieval essentially fails (R@1 \u2248 0.19%), indicating the model can discriminate severity from the fused representation without learning a modality-aligned embedding space.\n\n\u2022 + Contrastive losses (image\u2013text, image\u2013structured, text\u2013structured): retrieval jumps dramatically to R@1 \u2248 98.33%, with classification improving to ~96.03% SDRG and ~97.03% ICDR, showing that explicit cross-modal alignment objectives are necessary to make the embedding usable for retrieval.\n\n\u2022 + Reconstruction losses (image and text decoding): retrieval improves further to R@1 \u2248 99.60%, while classification stays similar (~96.10% SDRG, ~96.96% ICDR), consistent with reconstruction acting as a regularizer that encourages embeddings to retain fine-grained modality information useful for matching.\n\n\u2022 Full model with learnable softmax-normalized loss weights over all six components: gives the best overall balance, reaching R@1 \u2248 99.94% while also improving classification to ~97.05% SDRG and ~97.97% ICDR, suggesting dynamic weighting helps simultaneously satisfy alignment, reconstruction, and grading objectives.",
      "source_document": "papers/2512.19663v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an MRI super-resolution network that optimizes both pixel-level fidelity and perceptual quality, what composite training loss is used when combining an \u21131 reconstruction term with an LPIPS perceptual term, and how is the loss weighted to emphasize \u21131 over LPIPS?",
      "answer": "The training objective is a weighted sum of an \u21131 reconstruction loss and an LPIPS perceptual loss:\n\nL = \u03bb \u00b7 ||x\u0302_HR \u2212 x_HR||_1 + \u2113_p(x\u0302_HR, x_HR),\n\nwhere the weighting coefficient is set to \u03bb = 4 to put more weight on the \u21131 term than on the LPIPS term.",
      "source_document": "papers/2512.19676v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In Vision Mamba\u2013style MRI super-resolution, what hybrid selective scanning directions can be used to serialize 2D patches, and what specific limitation of using only horizontal+vertical scans does adding diagonal scans address?",
      "answer": "A hybrid scanning scheme uses vertical, horizontal, and diagonal scans to form the patch/token sequences. Diagonal scanning addresses the limitation of purely horizontal/vertical tokenization that separates a central pixel from its diagonal neighbors (leading to \u201cpixel forgetting\u201d and weaker modeling of long-range spatial dependencies); diagonal scans preserve that diagonal adjacency so global context is better retained.",
      "source_document": "papers/2512.19676v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-head selective state-space module for image super-resolution, how is numerical stability of the per-head state transition enforced, and how are the per-token adaptive step sizes generated before running the parallel recurrent scans?",
      "answer": "Stability is enforced by parameterizing each head\u2019s state-transition matrix as a negative diagonal exponential: for head i, A(i)=diag(\u2212e^{a(i)}), which keeps the dynamics stable. The per-token step sizes \u0394_t^(i) are predicted by a lightweight channel-MLP and passed through a softplus reparameterization to ensure valid (positive) step sizes before the module runs the m parallel recurrent scans; outputs from all heads are then merged, layer-normalized, optionally gated, and linearly projected back to the model dimension.",
      "source_document": "papers/2512.19676v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an efficient Vision-Mamba MRI super-resolution block that uses a lightweight channel MLP instead of a full dense two-layer MLP, how does the channel-mixing module transform an input feature map (including the expand\u2013split\u2013gate\u2013project steps), and why is this design more parameter-efficient while keeping two-layer MLP expressivity?",
      "answer": "The channel MLP performs channel mixing using pointwise (1\u00d71) convolutions with multiplicative gating: it first applies a 1\u00d71 convolution to expand the channel dimension by a factor \u03b1, then splits the expanded tensor into two channel groups (x1, x2) and computes an elementwise product x1 \u2299 x2 as a gate, and finally applies a second 1\u00d71 convolution to project the gated features back to the original channel dimension. Because it uses 1\u00d71 convolutions and a split-and-gate interaction rather than a fully connected MLP over channels, it preserves the expressivity of a two-layer MLP while reducing parameter overhead (pointwise convolutions are cheaper and avoid large dense matrices).",
      "source_document": "papers/2512.19676v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking multiple MRI super-resolution models across a test cohort using metrics like PSNR/SSIM and perceptual scores (LPIPS/GMSD), what statistical testing workflow can you use to determine whether methods differ significantly overall and which pairs differ\u2014especially when metric distributions are not normal\u2014and how do you control for multiple comparisons?",
      "answer": "Use a non-parametric workflow: (1) for each metric, run a Kruskal\u2013Wallis omnibus test to detect overall group differences across methods; (2) if significant, run post-hoc pairwise Dunn\u2019s tests; (3) apply Holm correction to the Dunn p-values to control for multiple comparisons. Non-parametric tests are motivated by failed normality (Shapiro\u2013Wilk p < 0.05), with significance set at p < 0.05.",
      "source_document": "papers/2512.19676v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a chunk-based novel view synthesis system that conditions a video diffusion model on forward-warped priors with occlusions and geometric distortions, how is the spatio-temporal *spatially adaptive* noise schedule constructed and used differently for (i) valid warped regions and (ii) blank/occluded regions during training and inference, and what denoising target and loss are used to train the diffusion model under this schedule?",
      "answer": "The method first builds a latent-space composite that separates \u201cwarped\u201d (valid) and \u201cfilled\u201d (invalid/occluded) regions using a validity mask. Given warped priors x_{s\u2192t} and ground-truth frames x_t, both are VAE-encoded to z_{s\u2192t} and z_t, and a downsampled latent mask M_latent,t indicates which pixels were successfully rendered by warping. For each frame t, a clean composite latent is formed by taking warped features where M_latent,t=1 and ground-truth features where M_latent,t=0:\n\n- z_{c,t} = M_latent,t \u2299 z_{s\u2192t} + (1\u2212M_latent,t) \u2299 z_t.\n\nTraining noise is then applied with two coupled variations:\n1) temporal variation: each frame gets its own independently sampled noise level; and\n2) spatial variation: within each frame, different noise levels are applied to warped vs filled regions.\n\nConcretely, for each frame t, a pair (\u03c3_warped,t, \u03c3_filled,t) is sampled and combined into a spatial noise map using the mask:\n\n- \u03a3_t = M_latent,t \u2299 \u03c3_warped,t + (1\u2212M_latent,t) \u2299 \u03c3_filled,t,\n- z_noisy,t = (1\u2212\u03a3_t) \u2299 z_{c,t} + \u03a3_t \u2299 \u03b5_t, with \u03b5_t ~ N(0,I).\n\nBecause \u03a3_t varies per spatial token and per frame, it is broadcast to the full latent tensor and passed through the time-embedding network so the diffusion transformer receives a per-token noise/timestep embedding.\n\nAt inference, the reverse diffusion is initialized from a spatially varying noise level: blank/occluded regions (M_latent=0) are set to pure noise (\u03c3_filled = \u03c3_TN \u2248 1.0), while valid warped regions (M_latent=1) are only partially noised using an intermediate \u03c3_start determined by a strength parameter \u03c4 (mapped to an intermediate timestep T_start). This yields an initial latent\n\n- z_start,t = (1\u2212\u03a3_start,t) \u2299 z_{s\u2192t} + \u03a3_start,t \u2299 \u03b5_t,\n\nso occluded areas trigger generation while warped areas are revised/refined rather than regenerated from scratch. (With chunk overlap, the context frames can be enforced as hard constraints by keeping them at zero noise.)\n\nThe diffusion model G_\u03b8 is trained to denoise the entire noisy composite sequence conditioned on the noise maps (and other conditions such as text/camera) by predicting a velocity v_{\u03b8,t}; the target is defined from the noise and the ground-truth latent as (\u03b5_t \u2212 z_t). The training objective is an L2 loss summed over time:\n\n- \ud835\udcdb = E[ \u03a3_{t=1}^T || v_{\u03b8,t} \u2212 (\u03b5_t \u2212 z_t) ||_2^2 ].",
      "source_document": "papers/2512.19678v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a chunk-wise autoregressive novel-view video generation pipeline that conditions a diffusion model on forward-warped priors, how is an *online* 3D Gaussian Splatting (3DGS) cache constructed and used at inference time to provide geometric conditioning for the next chunk, and what do ablations reveal about the failure modes when this cache is replaced by (i) no cache at all or (ii) a simple RGB point-cloud cache with respect to long-horizon visual quality and camera-pose stability?",
      "answer": "At inference time, the method rebuilds a short-term, online 3D geometric cache at every generation iteration using only the most recent history (the initial views for the first chunk, or the previously generated chunk thereafter). It first runs a 3D geometry model (TTT3R) on the history frames to estimate their camera poses and an initial 3D point cloud. This point cloud is then used to initialize a 3D Gaussian Splatting (3DGS) scene representation, which is optimized for a few hundred steps on the history frames and their estimated poses. The resulting online-optimized 3DGS serves as a high-fidelity cache that can render forward-warped prior images (and corresponding validity/occlusion masks) at the novel camera poses for the next chunk; these warped priors are then encoded and fed as dense geometric conditioning to the spatio-temporal diffusion model during chunk generation.\n\nAblations show that removing caching entirely causes long-range generation to break down (the system cannot sustain long-horizon synthesis). Using a simple RGB point-cloud cache improves over having no cache but remains significantly worse than the online-optimized 3DGS cache: the warped priors are lower fidelity, which degrades long-term image quality and leads to larger camera-pose errors/drift. In contrast, the online optimized 3DGS cache provides a much more robust and accurate geometric scaffold, improving both long-horizon visual fidelity and pose stability, despite the training/inference representation gap (point clouds during training vs 3DGS at test time).",
      "source_document": "papers/2512.19678v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using a video diffusion model with spatially- and temporally-varying noise (i.e., different noise levels per frame and also per region within each frame), what key architectural modification is required relative to a standard diffusion model that uses a single timestep embedding, and how is the resulting noise map incorporated to condition the network at the token level?",
      "answer": "Instead of conditioning the whole image/video chunk on a single timestep embedding, the model is adapted to accept a unique noise level for every latent token. Concretely, the per-frame spatial noise maps (the sequence \u03a3V={\u03a3t}) are broadcast to the full latent sequence shape (B\u00d7T\u00d7H\u2032\u00d7W\u2032) and fed through the time-embedding network, producing a distinct time/spatial embedding for each corresponding token; the denoiser then takes the noisy latent sequence together with these token-wise noise/time embeddings as input.",
      "source_document": "papers/2512.19678v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking long-range novel-view video synthesis, how is *geometric alignment / camera control* quantified from the generated frames\u2014specifically, which pose-estimation method is used to recover camera poses from generated views, and how are rotation and translation errors computed and normalized to be comparable across sequences?",
      "answer": "Camera-control accuracy is evaluated by first recovering camera poses (R_gen, t_gen) from the generated views using DUST3R. These are compared to ground-truth poses (R_gt, t_gt) using (i) a rotation distance computed from the relative rotation via\nRdist = arccos(0.5*(tr(R_gen R_gt^T) \u2212 1)), and (ii) a translation distance tdist = ||t_gt \u2212 t_gen||_2. For consistency across sequences, poses are expressed relative to the first frame, and translation is normalized by the furthest frame.",
      "source_document": "papers/2512.19678v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a post-training scheme that aligns an autoregressive image token generator with pixel-space quality without retraining the tokenizer, how can you (1) define an intrinsic reconstruction reward from teacher-forced sampled tokens, and (2) impose a token-level regularizer that mitigates exposure bias and keeps the updated policy close to the pretrained prior? Describe the specific loss terms (including any perceptual component) and how contextual noise is used.",
      "answer": "(1) Treat the AR generator as a policy and compute an intrinsic reconstruction reward from teacher-forced samples: encode the reference image I into ground-truth tokens x* = Q(E(I)), sample a token sequence x under teacher forcing from the model, decode the sampled tokens to an image \u00ce = D(x), and set the reward to the negative reconstruction loss\nR(x, x*) = \u2212(L_MSE(\u00ce, I) + \u03bb_p L_p(\u00ce, I)),\nwhere L_p is a perceptual reconstruction loss (e.g., LPIPS).\n\n(2) Add a prior/token regularizer implemented as next-token prediction under a corrupted (noisy) context to address exposure bias and preserve the pretrained token distribution: corrupt the ground-truth context tokens with a kernel K_\u03be to get \u007fx* ~ K_\u03be(\u00b7|x*), then minimize the cross-entropy\nL_prior(\u03c0_\u03b8, x*, \u007fx*) = \u2212(1/N) \u2211_{t=1}^N log \u03c0_\u03b8(x*_t | \u007fx*_<t).\nThis acts as the ELBO\u2019s prior regularization (a KL-style constraint between the teacher-forced distribution and the free-running model) and is combined with the reward in a GRPO-style policy optimization objective with a weight \u03b2 on L_prior.",
      "source_document": "papers/2512.19680v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In monocular hand\u2013object manipulation reconstruction, how can the approach phase be completed when the hand is partially or fully outside the camera\u2019s field of view, and what specific guidance signals are used to constrain the generated motion to remain consistent with the observed frames?",
      "answer": "The approach phase is completed using a learned human motion prior (EgoAllo), which generates a full hand pose-and-translation trajectory that is smooth and kinematically plausible. It is conditioned on the reliable estimates from the visible frames: discrete hand poses and wrist (hand) translations for the set of frames where the hand is detected (i \u2208 I), along with a prescribed initial pose; the prior outputs a completed motion (\u03b8\u0303, \u03c4\u0303) that matches these guidance signals.",
      "source_document": "papers/2512.19684v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a two-stage hand\u2013object reconstruction pipeline that optimizes interaction after detecting 2D fingertip contact candidates, how can you lift those 2D contacts into consistent 3D hand\u2013object correspondences when the object mesh is watertight and a camera ray can intersect the surface multiple times? Describe the criterion used to decide whether the object-side contact should be taken from the first vs. last ray\u2013mesh intersection, and how this choice enters the contact loss used during interaction-stage optimization.",
      "answer": "2D contact candidates are sampled from hand mesh vertices (115 fingertip-distributed vertices) and projected with camera intrinsics to image-plane candidates. Intersecting these projected candidates with a rendered object mask yields the 2D contact set. Each 2D contact pixel is associated with (i) a specific 3D hand vertex via its vertex index, and (ii) an object surface point obtained by casting a ray from the camera through that pixel and intersecting it with the posed object mesh.\n\nBecause the object is assumed watertight, the ray intersects the surface an even number of times, so the method must choose which intersection corresponds to the physical contact. It uses the posed hand\u2019s per-vertex normal at the candidate vertex: with the viewing direction defined as +z, vertices whose normal has z<0 (facing toward the camera) are treated as front-surface hand contacts, and their object contacts are taken as the first ray\u2013mesh intersection; vertices whose normal has z>0 (facing away from the camera) are treated as back-surface hand contacts, and their object contacts are taken as the last ray\u2013mesh intersection.\n\nThe contact loss then penalizes squared Euclidean distances between each contacting hand vertex and its chosen object surface point, computed separately for front-surface and back-surface contact sets and summed.",
      "source_document": "papers/2512.19684v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When reconstructing scene-aligned hand\u2013object manipulation from monocular video, how can the system automatically detect the grasp-to-interaction transition time from the object\u2019s 6-DoF pose trajectory, and how are the object poses preprocessed on each side of this transition before optimizing hand\u2013object motion?",
      "answer": "The grasping frame t\u22c6 is inferred from the reconstructed object trajectory O={(o0,r0),\u2026,(oT,rT)} by thresholding both translation and rotation changes over a sustained window: choose translation and rotation thresholds (\u03b5o, \u03b5r) and a window length m, then find the earliest time t where, over the last m frames, the per-frame change exceeds the corresponding threshold (t\u22c6o for \u2016\u0394ok\u20162>\u03b5o and t\u22c6r for \u2016\u0394rk\u20162>\u03b5r), and set t\u22c6=min(t\u22c6o,t\u22c6r). For frames before grasping, the object is treated as stationary and its pose is initialized by averaging the object poses over frames 0\u2026t\u22c6. For frames after grasping, the object pose sequence O[t\u22c6+1:T] is smoothed with a One-Euro filter to provide a stable trajectory for subsequent optimization.",
      "source_document": "papers/2512.19684v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When refining a monocular hand\u2013object manipulation sequence in the interaction stage, what objective terms are optimized (and what does each term enforce), and what qualitative trade-offs occur if you ablate (i) the contact term, (ii) the SDF penetration term, (iii) the pose regularizer, or (iv) the smoothness term?",
      "answer": "The interaction stage jointly optimizes hand pose \u03b8 and translation \u03c4 with an objective of the form \u03bb1 Lcon + \u03bb2 Lsdf + \u03bb3 Lsm + \u03bb4 Lreg.\n\n\u2022 Contact term (Lcon): enforces 3D alignment between hand and object by matching corresponding hand\u2013object contact points. Contact candidates are sampled from fingertip-area hand vertices, projected to 2D, intersected with the rendered object mask to get 2D contact pixels, then lifted back to 3D by pairing each pixel with its hand vertex and the ray\u2013object-surface intersection point; the loss is the summed squared distances between the paired 3D hand vertices and object surface points.\n\n\u2022 Penetration term (Lsdf): discourages finger\u2013object interpenetration by penalizing signed distances of hand vertices to the object surface using the object\u2019s signed distance field.\n\n\u2022 Smoothness term (Lsm): encourages temporally smooth motion by regularizing per-frame changes in hand pose rotations and fingertip joint positions.\n\n\u2022 Regularization term (Lreg): keeps the optimized motion close to the initial hand pose estimates from the hand-pose foundation/baseline model by penalizing deviation from those poses.\n\nAblation trade-offs reported:\n(i) Without Lcon there is little/no hand\u2013object contact; interpenetration metrics can look deceptively low because the hand is not aligned to touch the object.\n(ii) Removing Lsdf eliminates collision penalties: pose/trajectory errors can decrease, but penetration volume and depth increase markedly.\n(iii) Removing Lreg lets optimization drift toward physically plausible poses while deviating from the input observations.\n(iv) Removing Lsm can improve per-frame accuracy but introduces pronounced temporal jitter across the sequence; using all constraints gives the best overall balance.",
      "source_document": "papers/2512.19684v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a monocular, scene-aligned hand\u2013object manipulation reconstruction system, what set of metrics can be used to jointly measure (i) hand pose accuracy, (ii) global hand trajectory accuracy, (iii) physical plausibility of hand\u2013object interaction, and (iv) motion smoothness\u2014and over which frames/stages should each metric be computed to ensure a fair evaluation under hand visibility/occlusion?",
      "answer": "A suitable evaluation combines:\n- Hand pose accuracy: Mean Per Joint Position Error (MPJPE).\n- Global trajectory accuracy: Mean Root Position Error (MRPE), defined as the mean distance between predicted and ground-truth wrist positions over the sequence after aligning the first frame.\n- Physical plausibility of interaction: interpenetration Volume (IV) and interpenetration Depth (ID), each reported as mean and maximum.\n- Motion smoothness: jerk magnitude (JM).\nTo handle occlusion fairly, MPJPE and MRPE are computed only on frames where the hand is visible, while IV, ID, and JM are computed over all frames in the interaction stage.",
      "source_document": "papers/2512.19684v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When reinforcing a unified text-to-image model to better preserve visual context from reference images, how can a composite \u201cvisual-consistency\u201d reward be constructed so it evaluates both (i) object/identity preservation and (ii) prompt alignment, including what similarity signals are used for identity and style; and in an ablation over reward-term choices, which reward-term combination works best and what is the observed effect of adding an aesthetic preference term?",
      "answer": "A composite reward can be defined as a sum of a visual-consistency term and a text-consistency term, i.e., R_total(Y_final,V,T)=R_visual(Y_final,Z_plan,V)+R_text(Y_final,T). The visual term evaluates each checklist item with type-specific similarity: for identity preservation it localizes the referenced target using GroundingDINO and then scores identity consistency using DINO feature similarity between the cropped reference and generated regions; for style consistency it uses CSD-Score. The text-consistency term uses a CLIP score between the generated image and the prompt. In reward-term ablations, combining the object similarity score (ObjSimScore, based on the GroundingDINO+DINO pipeline) with CLIPScore gives the best overall performance; adding PickScore degrades performance, attributed to introducing conflicting optimization signals.",
      "source_document": "papers/2512.19686v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a checklist-guided iterative image-generation pipeline that alternates (i) evaluating the current image against a visual plan and (ii) applying editing instructions, how does the maximum allowed number of refinement iterations affect benchmark performance, and what iteration budget is typically sufficient before additional iterations show diminishing returns or even degrade results (and why)?",
      "answer": "Allowing iterative refinement improves performance versus single-pass generation, but gains saturate quickly: most images that can be fixed are successfully corrected within about 3 iterations. Increasing the maximum iteration count beyond this point yields little benefit (it only helps a few especially difficult cases) and can even reduce overall performance because the model may over-correct or introduce new errors in later rounds.",
      "source_document": "papers/2512.19686v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an autoregressive diffusion model for long human\u2013human interaction generation, how can a \u201cmixed memory\u201d context buffer be constructed so the denoiser has both high-resolution recent history and a long-range history without a proportional increase in tokens/frames, and how does this buffer determine the total number of stored frames used for conditioning?",
      "answer": "The context is split into (1) a short-term memory Ms that keeps the ms most recent previously generated frames at full frame rate and (2) a long-term memory Ml that covers a much longer window of ml frames but is temporally downsampled by keeping only every \u03b4-th frame from that history. At generation step k, Ms contains the recent frames x0_{kn\u2212ms:kn}, while Ml contains {x0_{kn\u2212ml+i\u03b4} | i = 0,\u2026,\u230aml/\u03b4\u230b}. The denoiser conditions on the concatenation Mk = {Ml_k, Ms_k}, giving broad temporal coverage to reduce repetition artifacts while remaining efficient. The total number of stored frames used in the mixed buffer is m = ms + ml/\u03b4 (equivalently ms plus the number of long-term samples).",
      "source_document": "papers/2512.19692v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a diffusion model that generates two-person SMPL-X motion, how can the training objective explicitly encourage realistic inter-person contact/spacing (e.g., handshakes and hugs) beyond per-person pose accuracy\u2014what distance-based loss is used, how is it computed from forward-kinematics joint positions, and why is a binary mask applied when forming this loss?",
      "answer": "Use a pairwise joint distance-map loss between the two interactants computed on global 3D joint positions obtained via differentiable forward kinematics from predicted/ground-truth SMPL-X parameters. For each frame, form a matrix D(pa,pb) whose (i,j) entry is the Euclidean distance between joint i of person a and joint j of person b, and penalize the difference to the predicted distance map D(p\u0302a,p\u0302b), i.e., ||(D(pa,pb) \u2212 D(p\u0302a,p\u0302b)) \u2299 M||^2. The binary mask M activates the loss only for joint pairs that are close in the ground-truth, so supervision focuses on the spatial relationships most critical for interaction/contact rather than all joint pairs.",
      "source_document": "papers/2512.19692v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For evaluating text-conditioned full-body human\u2013human interaction generators, how can you make an automatic text\u2013motion evaluator sensitive to global positioning errors (e.g., swapped trajectories) while also reporting separate quality/alignment scores for body motion vs hand motion\u2014what motion representation should the evaluator be trained on, how is that representation computed from SMPL-X outputs, and how are the full/body/hand evaluators differentiated in training?",
      "answer": "Train the evaluator on **global 3D joint positions** (not rotation-based or relative representations) so that degradations in the **global placement/trajectories between interactants** are reflected in the embedding and downstream metrics. The global joint coordinates are obtained by applying a **forward-kinematics (FK) function** to the predicted SMPL-X rotations/parameters to recover 3D joint positions in global coordinates.\n\nUsing the standard contrastive text\u2013motion evaluator design (a motion feature extractor and a text feature extractor trained with contrastive learning, then using the resulting embeddings to compute metrics like R-Precision/FID/MM-Dist/Diversity/MultiModality), train **three separate evaluators** that differ only in which joints they see:\n- **Full evaluator:** uses **all SMPL-X joints**.\n- **Body evaluator:** uses only the **base SMPL body joints**.\n- **Hand evaluator:** uses the **additional 30 hand joints**.\n\nAll three are trained to output fixed-size motion/text embeddings (512-D in the described setup), enabling part-specific evaluation while remaining sensitive to global inter-person configuration.",
      "source_document": "papers/2512.19692v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating an interaction generator\u2019s ability to do temporal motion composition (and recovery from state perturbations), what jerk-based metrics can be used to quantify transition smoothness, how are they computed over composed sequences, and how can you make the comparison fair for baselines that cannot natively concatenate sub-motions?",
      "answer": "Use Peak Jerk (PJ) and Area Under the Jerk (AUJ) to quantify smoothness of transitions. Compute them over full composed sequences by generating multiple long sequences formed by temporally concatenating several motions (the setup described uses 64 sequences, each made of 8 concatenated motions) and then measuring PJ and AUJ across the complete concatenated trajectory. For methods that lack native temporal composition, implement the concatenation step via inpainting so all models are evaluated on the same kind of composed sequences.",
      "source_document": "papers/2512.19692v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a diffusion model trained to generate full-body SMPL-X interaction motions, how can you formulate a foot-contact loss that specifically reduces artifacts like foot skating/floating\u2014what quantities are multiplied, which joints are included, and how does the binary contact signal gate the penalty?",
      "answer": "Use a foot-contact loss that penalizes the velocity of foot joints only when they are (predicted/ground-truth) in contact with the ground. Compute per-frame joint velocities from global FK joint positions (v_t = p_t \u2212 p_{t\u22121}), then for the set of foot joints sum an \u21132 penalty on the elementwise product of velocity and a binary contact indicator: \n\nL_foot(f, f\u0302) = \\sum_{i\\in\\{feet\\}} \\|v_i \\odot f_i\\|_2^2 + \\|\\hat v_i \\odot \\hat f_i\\|_2^2,\n\nwhere f_i \u2208 {0,1}^T indicates contact (1 when the foot is in contact, 0 otherwise) and \u2299 is elementwise multiplication. This gates the loss so foot motion is penalized during contact, discouraging skating/floating while not over-constraining swing phases.",
      "source_document": "papers/2512.19692v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a unified tokenizer that factorizes latents into frequency bands to combine a pretrained semantic encoder with pixel-faithful reconstruction, how can the training objective be designed so that the model inherits the teacher\u2019s semantic structure without constraining its ability to learn high-frequency visual details? Describe the specific alignment loss, which frequency bands it is applied to, and the intended effect on the remaining bands.",
      "answer": "Use a selective semantic-alignment loss computed after decomposing both the frozen semantic-teacher features and the unified-encoder features into band-wise representations via the same frequency decomposition. Define a semantic-wise loss as an L2 penalty between corresponding band features, but apply it only to the first K_base low-frequency bands (those encoding global structure/object semantics):\n\nL_sem = (1/K_base) * sum_{k=0}^{K_base-1} || f_u^k \u2212 f_s^k ||_2^2.\n\nRestricting supervision to low-frequency bands transfers the teacher\u2019s semantic layout/category-level organization into the unified encoder while leaving higher-frequency bands unconstrained so they can learn modality-specific, pixel-level detail (improving fidelity without collapsing into purely pixel features). In practice K_base is set to 1 by default.",
      "source_document": "papers/2512.19693v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a method that generates a sharp video from a single motion-blurred image, how can evaluation be designed to handle the inherent ambiguity that both a sequence and its time-reversed version may be valid\u2014especially when motion direction can vary locally across the image\u2014and what metrics are computed under this bidirectional evaluation scheme?",
      "answer": "A bidirectional evaluation can be used. First, time-reversal ambiguity is handled by evaluating both the predicted video and its temporally reversed version against the ground-truth sequence and taking the better score; to address local (spatially varying) ambiguity, the same idea is applied patch-wise: for each spatial patch, compute the metric on the patch from the forward prediction and on the corresponding patch from the temporally reversed prediction, then take the minimum (best) of the two and average over patches (with PSNR computed by first selecting forward vs reverse MSE per patch, averaging patch-wise MSE to a frame-level MSE, then converting to PSNR and averaging over frames).\n\nUnder this scheme, the reported metrics are bidirectional patch versions of PSNR, SSIM, and LPIPS. In addition, Fr\u00e9chet Video Distance (FVD) is computed at full resolution on videos played forward, and optical-flow end-point error (EPE) is computed using RAFT by selecting per pixel the temporal direction (forward or backward) that minimizes flow error between the first and last frames.",
      "source_document": "papers/2512.19817v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When fine-tuning a video diffusion model to recover sharp frame sequences from a single motion-blurred image, how can the training pipeline synthesize realistic motion blur from high-FPS videos\u2014specifically, what steps are taken to ensure that frame averaging matches the physical blur integral in terms of color space and \u201cdead time\u201d between frames, and what temporal upsampling strategy is used to address that dead time?",
      "answer": "Motion blur is synthesized by averaging consecutive video frames (approximating the exposure-time integral). Two key adjustments are made so the averaging better matches the physical model: (1) the averaging is performed in linear sRGB (to avoid errors from gamma-compressed, non-linear RGB values), and (2) to reduce errors when there is significant \u201cdead time\u201d between recorded frames (i.e., when the per-frame exposure is shorter than the frame-to-frame interval), the source videos are temporally upsampled before averaging using a frame interpolation method, producing 1920 FPS sequences that provide denser samples of the exposure interval.",
      "source_document": "papers/2512.19817v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When extending a latent video diffusion transformer so it can generate frames at user-specified times (including outside the blurred image\u2019s exposure) despite temporal compression in the VAE (one latent frame representing multiple output frames), how can the per-frame exposure windows be represented, normalized, and injected into the transformer\u2019s conditioning signal?",
      "answer": "Encode the blurry input image and the target video in the model\u2019s latent space, and condition the transformer on both the latent blurry image and explicit exposure intervals for the output frames. First normalize time by assigning the input motion-blur exposure to a canonical interval T = [\u22120.5, 0.5], and express every desired output-frame exposure interval T1\u2026TF relative to that normalized interval (allowing intervals inside it for \u201cpresent\u201d and outside it for \u201cpast/future\u201d). Because the VAE temporally compresses by 4\u00d7, each latent video frame corresponds to four output frames, so represent the exposure timing for latent frame i as an 8D vector \\tilde{T}_i containing the start/end times of those four exposure intervals (e.g., [T1[1], T1[2], \u2026, T4[1], T4[2]]). Inject this timing by applying a sinusoidal positional encoding \u03b3 to each scalar coordinate of \\tilde{T}_i, concatenating the resulting embeddings, and projecting with a linear layer to the latent channel dimension \\tilde{D}; add this projected vector to the corresponding latent frame features (analogously replicate T four times for the latent representation of the input blurred image and encode it the same way) before patchifying and passing tokens through the diffusion transformer.",
      "source_document": "papers/2512.19817v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When adapting a large pre-trained latent video diffusion transformer to generate a sharp frame sequence from a single motion-blurred image, what training objective is used in the latent space, which network components are actually fine-tuned, and how is classifier-free (unconditional) conditioning implemented during fine-tuning so guidance can be used at inference?",
      "answer": "Training is done by encoding the motion-blurred image and the corresponding sharp video frames with the model\u2019s VAE, adding diffusion noise to the latent video frames, conditioning the transformer on the latent blurred image plus the (position-encoded and exposure-interval\u2013encoded) noisy latent video frames, and optimizing an L2 denoising objective: minimize the expected squared error between the model\u2019s denoised latent video output and the clean latent video (E_{V,\u03b5}||V\u0302\u2212V\u0303||^2). The fine-tuning updates all parameters of the diffusion transformer as well as the linear projection layer used for the exposure-interval encoding (i.e., the whole model components involved in denoising and time-interval conditioning). Classifier-free conditioning is trained by additionally performing unconditional generation with the conditioning latent image set to zero (\u0128=0) using the same 20% conditioning dropout as the base model, enabling classifier-free guidance at inference.",
      "source_document": "papers/2512.19817v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "If a method generates a short video (past/present/future) from a single motion-blurred photo, how can you check whether the generated camera/scene motion is geometrically plausible in 3D without having any explicit 3D supervision\u2014what pair of frames do you analyze, what classical vision estimates do you compute from feature matches, and how do you use a homography-vs-epipolar explanation to tell true viewpoint change (parallax) from pure panning?",
      "answer": "Analyze the first and last generated frames from cases where the blur is caused by viewpoint motion. Detect feature correspondences with SIFT and use RANSAC to robustly estimate (1) the fundamental matrix, then inspect whether the implied epipolar lines are consistent with the apparent camera motion; and (2) the best-fit 2D homography, then examine how well it aligns the two frames (e.g., residual keypoint motion and absolute difference after warping). If a homography aligns the background but produces large residuals on foreground objects, that mismatch indicates parallax due to 3D structure (true viewpoint change). If the camera motion is a pan with no viewpoint change, the motion should be well explained by a single homography, and residuals should be small aside from independently moving objects.",
      "source_document": "papers/2512.19817v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When adapting a latent video diffusion model to synthesize an entire focal stack from a single defocused input, how should the conditioning and classifier-free guidance be modified so the network can infer the correct focal position\u2014and what typically happens if you instead use the original scheme that replicates the input latent across all frames (or conditions via the model\u2019s motion-profile ID)?",
      "answer": "Use position-dependent conditioning: encode the input image to a latent corresponding to its focal position p, and build the conditioning tensor as a per-frame sequence that is zero everywhere except at index p, where the input latent is inserted. Then apply classifier-free guidance by combining (i) the conditional noise prediction given this sparse, position-tagged conditioning sequence with (ii) an unconditional prediction given an all-zero conditioning tensor, i.e., e\u0302 = (1+w)\u00b7\u03b5\u03b8(z_t, [0,\u2026,f_p,\u2026,0], t) \u2212 w\u00b7\u03b5\u03b8(z_t, [0,\u2026,0], t). If the input latent is replicated across all frames (or the motion-profile ID is used while replicating the latent, as in the original SVD conditioning), the model sees inconsistent focal-position supervision, cannot learn a clear mapping from input to output focal positions, and in the reported ablation it fails to refocus\u2014collapsing to reproducing the input image in every generated frame.",
      "source_document": "papers/2512.19823v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a diffusion-based refocusing system that reconstructs an entire focal stack in VAE latent space, which perceptual metrics are used to judge refocused image quality (and why), how does its performance typically differ from pixel-domain deblurring baselines as the refocus distance increases versus near-identity refocus, and what underlying modeling factor is identified as a main cause of the near-identity/high-frequency weakness?",
      "answer": "Perceptual quality is evaluated primarily with LPIPS and FID, using FID because refocusing is treated as a generative task and FID measures how realistic the generated image distribution is relative to the ground-truth distribution; distortion metrics like PSNR are not optimized by generative methods and the diffusion approach can be ~2\u20134 dB worse in PSNR. The latent video-diffusion (SVD-based) approach achieves superior perceptual quality for large focal-position shifts (e.g., refocusing from the ends of the stack), but at smaller refocus distances\u2014especially around the middle focal position where scenes are often naturally in focus and rich in high-frequency detail\u2014pixel-domain deblurring networks can perform better because they preserve fine details more directly. A main reason for the diffusion model\u2019s weakness in the near-identity/high-frequency regime is VAE latent-space compression, which makes it struggle to reconstruct the input focus (identity case) and fine-grained details.",
      "source_document": "papers/2512.19823v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When fine-tuning a pre-trained latent video diffusion model to generate a full focal stack from a single defocused input, what is an efficient training recipe in terms of (i) which submodules are updated versus frozen, (ii) what denoising objective/timestep sampling is used, and (iii) why train on full resized images instead of patches for this task?",
      "answer": "An efficient recipe is to fine-tune only a small part of the diffusion backbone\u2014specifically, retrain just the UNet transformer blocks while leaving the rest of the pre-trained video diffusion model frozen\u2014and optimize the standard diffusion denoising/noise-prediction objective using an L2 loss at a randomly sampled diffusion timestep each iteration. Training is done on full frames (resized to 896\u00d7640) rather than patches because even after the dataset\u2019s alignment/distortion-correction pipeline there are slight residual radial-distortion shifts across the focal stack; patch training would not capture these global, stack-dependent geometric shifts as reliably as full-image training. (EMA is also applied during training.)",
      "source_document": "papers/2512.19823v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training and evaluating a model that generates an aligned focal stack from a single defocused smartphone image, what preprocessing steps can be used to remove focal-breathing\u2013induced geometric changes across the stack, and why is doing this correction important for both learning and benchmarking refocusing quality?",
      "answer": "A practical pipeline is: (1) convert each captured RAW frame to a common rendered color space (sRGB) using a consistent RAW developer; (2) apply a calibrated radial distortion correction specific to each focus setting to remove focus-dependent lens aberrations; (3) perform alignment by applying a precomputed scale transformation so all focal-stack frames match the first frame\u2019s field of view (the smallest FOV), compensating remaining focal-breathing misalignment after distortion correction; and (4) optionally generate a high-quality all-in-focus (AiF) reference image from the stack (e.g., with a focus-stacking tool in depth mode) to evaluate defocus deblurring baselines.\n\nThis correction is important because uncorrected focal breathing introduces frame-to-frame changes in field of view and distortion, so a refocusing model would have to learn geometric warps in addition to defocus changes; with misalignment, both the proposed diffusion model and competing baselines degrade and refocusing comparisons become less meaningful.",
      "source_document": "papers/2512.19823v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In single-image refocusing, especially when the desired output focus is several \u201cpositions\u201d away from the input, why do deterministic pixel-regression/deblurring networks tend to produce overly smooth (blurred) refocused results, and what property of a diffusion/score-based focal-stack generator helps avoid this \u201cregression-to-the-mean\u201d behavior?",
      "answer": "For large focus shifts, there are often multiple plausible sharp refocused images consistent with the same blurred input. Deterministic, non-generative refocusing/deblurring networks trained with regression losses tend to average these plausible solutions, which produces overly smooth/blurred outputs (a regression-to-the-mean effect). A diffusion-based focal-stack generator is trained with a score/noise-prediction objective and leverages strong generative priors, allowing it to sample a perceptually realistic solution and to synthesize (hallucinate) high-frequency detail and realistic defocus instead of averaging modes.",
      "source_document": "papers/2512.19823v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When refining a geometric model inside a RANSAC pipeline using a Gaussian-inlier / uniform-outlier residual mixture parameterized by a decision threshold \u03c4, how does marginal-likelihood optimization connect Expectation\u2013Maximization (EM) and Iteratively Reweighted Least Squares (IRLS), and what is the interpretation of the per-correspondence weights used in the update step?",
      "answer": "For the Gaussian\u2013uniform (GaU) mixture, maximizing the marginal likelihood leads to EM and IRLS taking the same form: each iteration computes a soft inlier weight for every correspondence and then updates the model by solving a weighted least-squares problem. The weight for correspondence i is the posterior probability that it is an inlier under the current model, q_i = p(k_i = 1 | r(x_i,\u03b8_t)) (equivalently expressible as a sigmoid/logistic function of log p_in(r_i(\u03b8_t)) relative to log p_in(\u03c4)). Using these weights, the parameter update minimizes the weighted sum of squared residuals, \u2211_i q_i\u00b7r(x_i,\u03b8)^2 (which can be implemented with Gauss\u2013Newton/Levenberg\u2013Marquardt in an IRLS-LMA scheme).",
      "source_document": "papers/2512.19850v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When learning a residual-based, additive scoring function for RANSAC from ground-truth geometric models under an inlier\u2013outlier mixture with uniform outliers, how can the inlier residual density be parameterized and constrained to prevent the mixture from explaining almost everything as inliers, and once learned how is this density converted into the per-residual-bin score weights used to rank hypotheses?",
      "answer": "Use a 1D residual mixture model p(r)=\u03b3\u00b7p_in(r)+(1\u2212\u03b3)\u00b7p_out(r) with p_out(r) uniform on [0,r_max] (so p_out(r)=1/r_max). Because the uniform-outlier assumption fits poorly, if \u03b3 is learned jointly with a flexible p_in the fit tends to label nearly all data as inliers; to avoid this \u201cmixture collapse\u201d, fix \u03b3 to a deliberately lower value (treat it as a hyperparameter).\n\nParameterize p_in(r) as a discretized (piecewise-constant) density over residual bins via a softmax of free bin weights w_in,k, i.e., p_in(r)= (1/Z)\u00b7exp(w_in,k(r)) where k(r) is r\u2019s bin and Z normalizes. To reduce noise/overfitting and encode that smaller residuals should be more probable, enforce that p_in(r) is monotone non-increasing in r by representing the bin logits as a reverse cumulative sum of nonnegative terms: w_in,k = \\sum_{l>k} log(1+e^{\u03b7_l}), which guarantees w_in,k decreases with k.\n\nAfter learning the discretized inlier logits, form the scoring look-up weights for each residual bin using the marginal-likelihood score of the mixture: w_k = log( \u03b3\u00b7(1/Z)\u00b7e^{w_in,k} + (1\u2212\u03b3)\u00b7(1/r_max) ). These w_k are then normalized (min 0, max 1) and used as the additive per-residual-bin contributions when scoring candidate models.",
      "source_document": "papers/2512.19850v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking different RANSAC scoring functions that each depend on a single \u201cthreshold\u201d hyperparameter, how can threshold sensitivity be defined in a way that is invariant to any monotone reparameterization of that hyperparameter, and how is this sensitivity estimated in practice using repeated small validation sets and precomputed per-threshold test errors?",
      "answer": "Define sensitivity as the variance of the test performance induced by selecting the hyperparameter from a small validation set: measure \\(\\mathrm{Var}[e(\\tau(V))]\\), where \\(V\\) is a randomly drawn validation subset, \\(\\tau(V)\\) is the threshold chosen to minimize (e.g., median) validation error over a dense grid of candidate thresholds, and \\(e(\\tau(V))\\) is the resulting test error at that selected threshold. To estimate it efficiently, precompute for each image pair (and method) the model error obtained for every threshold value (by scoring a fixed pool of minimal models, picking the best-scoring model at each threshold, and evaluating its pose error). Then repeatedly (e.g., many trials) sample a small validation set, select \\(\\tau(V)\\) using the precomputed validation errors, compute the corresponding test error using the precomputed test errors, and compute both \\(\\mathbb{E}[e(\\tau(V))]\\) and \\(\\mathrm{Var}[e(\\tau(V))]\\) across trials.",
      "source_document": "papers/2512.19850v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In robust homography or relative-pose estimation, MAGSAC++ is often configured with four degrees of freedom for its chi-based residual model and uses a single \u201cthreshold\u201d hyperparameter. What parameter mapping between this threshold and the (marginalized) inlier noise scale makes the resulting IRLS weights (and hence the integrated score) numerically equivalent to scoring under a simple Gaussian-inlier / uniform-outlier residual mixture, and what practical consequence does this equivalence have for comparing MAGSAC++ to that Gaussian\u2013uniform (GaU) score?",
      "answer": "When MAGSAC++ uses a chi model with \u03bd=4, it ties its maximum inlier scale to the threshold via \\(\\bar\\sigma=\\tau/\\kappa\\), where \\(\\kappa\\) is the 99th-quantile of the \\(\\chi_\\nu\\) distribution. With this reparameterization, the scale-marginalized inlier density used as the MAGSAC++ IRLS weight, \\(\\tilde w(r)=p_{in}(r;\\bar\\sigma)\\), can be closely approximated by the posterior inlier probability of a Gaussian\u2013uniform mixture; under the same threshold parameter \\(\\tau\\) this GaU posterior is well matched by choosing a Gaussian scale \\(\\sigma\\) approximately equal to \\(\\tau\\) (fit reported as \\(\\sigma\\approx0.96\\tau\\)). Integrating the weight to obtain the robust loss/score then yields a MAGSAC++ score that aligns almost perfectly with the GaU marginal-likelihood score.\n\nPractically, for these settings (\u03bd=4), MAGSAC++ and GaU become indistinguishable in scoring behavior and therefore should not be expected to differ meaningfully in performance; any observed gains in full pipelines must come from other components rather than the score itself.",
      "source_document": "papers/2512.19850v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a probabilistic RANSAC model where each observed correspondence x is generated by adding spherical (radially symmetric) noise around an unknown true correspondence x\u0304 constrained to lie on the model manifold M\u03b8, why does marginalizing out x\u0304 make the inlier likelihood depend only on the distance r(x,\u03b8) to M\u03b8 (i.e., a 1D \u201cray density\u201d pin(r)) rather than on the full noise norm \u03c1=\u2225x\u2212x\u0304\u2225, and what inlier residual distributions does this imply for (i) relative pose estimation and (ii) homography estimation when the underlying observation noise is Gaussian (identify dg and the resulting \u03c7-distribution for r)?",
      "answer": "Because the observation noise pin(x|x\u0304) is spherical about x\u0304 and x\u0304 is assumed locally uniform on the manifold, integrating (sum-projecting) pin(x|x\u0304) over the dM manifold dimensions produces a density pin(x;\u03b8) that is constant along the tangent directions of M\u03b8 and depends only on the orthogonal distance r(x,\u03b8) to the manifold. This 1D profile along a normal ray is the \u201cray density\u201d pin(r).\n\nFor relative pose, the correspondence lives in 4D and the epipolar manifold has dimension dM=3, so the co-dimension is dg=1; the ray density pin(r) coincides with the marginal distribution of residuals r, and with Gaussian observation noise it is 1D Gaussian on [0,\u221e), i.e. \u03c71.\n\nFor homography, the co-dimension is dg=2 (spherical in 2 orthogonal dimensions); with Gaussian observation noise the ray density is still 1D Gaussian along the normal direction, but the residual magnitude r has a \u03c72 distribution. (By contrast, the full noise norm \u03c1=\u2225x\u2212x\u0304\u2225 for Gaussian 4D noise would be \u03c74, which is a different quantity than the Sampson residual r.)",
      "source_document": "papers/2512.19850v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a BEV-based 3D panoptic occupancy model that fuses discretized depth-bin lifting with a continuous Gaussian depth representation, what set of loss terms are jointly optimized end-to-end, which module/branch does each loss supervise, and how are the pseudo edge labels for the boundary loss generated?",
      "answer": "The end-to-end objective combines four losses: (1) a discretized Lift-Splat-Shoot occupancy loss LLSS, implemented as a binary cross-entropy between predicted and ground-truth occupancy volumes, supervising the discrete depth-bin unprojection branch; (2) a Gaussian-branch multi-task loss LG = \u03bb1\u00b7Lseg + \u03bb2\u00b7Lcenter_g + \u03bb3\u00b7Loffset_g, where Lseg is focal loss for segmentation and the auxiliary centerness and offset maps are supervised with L1 and L2 losses respectively, supervising the continuous Gaussian-based unprojection branch; (3) a panoptic occupancy loss Locc = \u03bbsem\u00b7Lsem + \u03bbcenter\u00b7Lcenter + \u03bboffset\u00b7Loffset, where Lsem is voxel-wise cross-entropy for semantic classes and center/offset fields are trained with L1 losses, supervising the panoptic prediction head (semantic, instance center heatmap, and offset branches); and (4) an edge loss Ledge = BCE(Epred, Egt) supervising an edge prediction module that predicts a BEV-level edge probability map from intermediate BEV features. The total loss is Ltotal = \u03bbLSS\u00b7LLSS + \u03bbG\u00b7LG + \u03bbedge\u00b7Ledge + Locc. Pseudo edge labels Egt are computed from the semantic/panoptic ground truth by applying Sobel filtering: Egt = sqrt((Sx * Ygt)^2 + (Sy * Ygt)^2), where Sx and Sy are Sobel kernels and * is convolution.",
      "source_document": "papers/2512.19871v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a BEV-based 3D panoptic occupancy system that combines discretized Lift-Splat-Shoot depth lifting with a continuous Gaussian-splatting depth representation, how are the two BEV feature tensors fused into a single hybrid BEV embedding before volumetric decoding (including how multi-view features are combined), and what do ablation results imply about using a learned spatial gating fusion versus a fixed \u03b1-blending fusion in terms of accuracy/stability trade-offs?",
      "answer": "The discretized branch produces BEV features Bd_i per camera view via LSS-style unprojection, and the continuous branch produces BEV features Bg_i per view via Gaussian-based unprojection. These are fused per view by a linear \u03b1-blending to form a hybrid BEV feature: Bh_i = \u03b1\u00b7Bg_i + Bd_i (with \u03b1 in [0,1]). Hybrid BEV features from all camera views are then aggregated with a standard BEV fusion operator (Fuse) to obtain a single multi-view BEV embedding Bagg that is fed to the volumetric decoder.\n\nAblations comparing fusion strategies indicate that a learned gating mechanism (predicting a spatially varying fusion weight map) provides flexibility but adds parameters and makes training more sensitive, while simple fixed \u03b1-blending achieves comparable (and slightly better overall) accuracy with a simpler, more stable formulation\u2014suggesting that linear fusion is sufficient to integrate complementary cues from continuous and discretized depth features without complex adaptive weighting.",
      "source_document": "papers/2512.19871v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using a continuous, Gaussian-parameterized depth representation to lift multi-view image features into BEV for 3D occupancy/panoptic occupancy, how is each pixel\u2019s predicted depth distribution converted into a 3D Gaussian primitive (what parameters define the primitive), how are many such primitives accumulated into a BEV feature field, and what auxiliary predictions and loss functions are used to train this Gaussian-based unprojection branch beyond the discretized occupancy objective?",
      "answer": "Each pixel predicts a continuous depth distribution with mean \u00b5 and variance \u03c3\u00b2, which is turned into a 3D anisotropic Gaussian primitive centered at \u00b5 with covariance \u03a3 = diag(\u03c3x\u00b2, \u03c3y\u00b2, \u03c3z\u00b2) and an opacity \u03b1\u2208[0,1] controlling visibility/transparency. The primitive defines a volumetric kernel Gi(x)=\u03b1i\u00b7exp(\u2212\u00bd(x\u2212\u00b5i)\u1d40\u03a3i\u207b\u00b9(x\u2212\u00b5i)). Many Gaussians are then \u201csplat-rendered\u201d into the BEV plane by differentiable accumulation into a continuous BEV feature field: Fgauss(x)=\u2211i wi\u00b7Gi(x), where wi is a learned feature weight derived from the image backbone.\n\nBeyond occupancy, this branch also predicts a centerness map \u0109 (to emphasize object centers and suppress noise) and an offset map \u00f4 (to regress shifts to ground-truth centers). It is trained with a multi-task loss LG=\u03bb1Lseg+\u03bb2Lcenter_g+\u03bb3Loffset_g, using focal loss for Lseg (class imbalance), an L1 loss for centerness (\u2016\u0109\u2212c\u2016), and an L2 loss for offsets (\u2016\u00f4\u2212o\u2016\u00b2).",
      "source_document": "papers/2512.19871v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In camera-only 3D (panoptic) occupancy prediction, why are ray-based metrics used in addition to voxel-wise mIoU, and how are RayIoU and RayPQ computed in terms of (i) casting rays, (ii) what consistency they measure, and (iii) the role of depth thresholds (e.g., 1 m / 2 m / 4 m)?",
      "answer": "Ray-based metrics are used because voxel-only scores can miss how well the predicted 3D scene stays consistent along the viewing direction, especially at long range; ray-based evaluation more faithfully assesses long-range scene consistency and panoptic quality along camera rays.\n\n\u2022 mIoU: voxel-wise semantic accuracy, computed as IoU averaged over semantic classes.\n\u2022 RayIoU: extends mIoU by casting query rays into the predicted 3D volume and evaluating geometric consistency along each ray\u2019s viewing direction, reported within depth error thresholds (1 m, 2 m, 4 m).\n\u2022 RayPQ: adapts Panoptic Quality to a ray-based setting, jointly evaluating semantic and instance-level consistency along rays (again reported over the same depth-thresholded ray queries), providing a comprehensive metric for panoptic 3D occupancy prediction.",
      "source_document": "papers/2512.19871v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a BEV-based camera-only 3D panoptic occupancy network that represents instances via a center heatmap and per-voxel offset vectors, what are the three prediction branches in the panoptic head, what supervision (loss type and target signals) is applied to each branch, and how are these terms combined into the overall panoptic occupancy loss used during training?",
      "answer": "The panoptic occupancy prediction head has three branches: (1) a semantic occupancy branch that outputs per-voxel class probabilities and is supervised with a standard voxel-wise cross-entropy loss against the ground-truth semantic class label for each voxel; (2) an instance center branch that predicts an instance-center heatmap and is supervised with an L1 loss to the ground-truth center field derived from 3D instance annotations; and (3) an instance offset branch that predicts a 3D offset vector per voxel pointing to the corresponding instance center and is supervised with an L1 loss to the ground-truth offset field (also derived from 3D instance annotations). These are combined into a single panoptic occupancy loss as L_occ = \u03bb_sem L_sem + \u03bb_center L_center + \u03bb_offset L_offset, where the \u03bb terms weight the semantic, center, and offset losses, respectively.",
      "source_document": "papers/2512.19871v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a Widget-to-Code system that compiles an intermediate widget DSL to executable front-end code, how can an adaptive rendering step enforce the widget\u2019s original aspect ratio while also preventing element overflow/occlusion, and what concrete layout-feasibility signal is used to drive the search over the widget\u2019s size?",
      "answer": "The adaptive rendering treats geometry as a constrained layout-feasibility problem driven by feedback from the rendering engine. It first computes the input widget\u2019s target aspect ratio r = w/h, then searches over the rendered widget width w while setting the height to h = w/r. A violation function \u03a8(w) summarizes whether the rendered content fits inside the widget container without overflow: \u03a8(w) = max( Cw(w)/Vw(w) \u2212 1, Ch(w)/Vh(w) \u2212 1, max_i \u0394_i(w) ), where Cw and Ch are the rendered content extents, Vw and Vh are the container viewport extents, and \u0394_i(w) measures how much each child element exceeds the container boundary beyond padding. The layout is feasible when \u03a8(w) \u2264 0. Starting from the natural layout size, the system iteratively adjusts w with a feedback-guided binary search, expanding or contracting based on \u03a8(w_t) until the feasibility error is within a small tolerance |\u03a8(w_t)| < \u03b5 (\u03b5 = 1/Vw, roughly one pixel). This yields a stable (w*, h*) that preserves the aspect ratio and avoids occlusion, after which a final resizing step can match the original widget\u2019s exact pixel dimensions.",
      "source_document": "papers/2512.19918v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a widget-to-code pipeline where small UI icons are hard for a multimodal LLM to synthesize reliably, how can an icon module replace direct generation with retrieval while still enforcing both visual and semantic alignment\u2014i.e., what offline representations are built for the icon library, what is the two-stage (coarse\u2192refined) retrieval procedure, and what is ultimately passed to the code/DSL generator?",
      "answer": "Use deterministic retrieval from a large SVG icon library instead of asking the model to draw icons. Offline, each SVG icon is (1) rendered to an image, (2) captioned by a vision-language model to get a text description, and (3) encoded into paired visual and textual embeddings using SigLIP\u2019s visual encoder Fv and text encoder Ft, storing {SVGn, f_v^n, f_t^n} for all icons.\n\nAt inference, for each detected icon element e=[r,b,t,c] (cropped region r, bbox b, text description t, category c), compute the query embeddings f_v^e=Fv(r) and f_t^e=Ft(t). Then:\n1) Coarse retrieval: rank all icons by cosine similarity between visual embeddings (f_v^e)^T f_v^n and take the top-K candidates (K=50).\n2) Refinement: re-rank those candidates by cosine similarity between text embeddings (f_t^e)^T f_t^n.\n\nKeep the top-5 re-ranked SVGs; this retrieved set is provided to the generator for contextual exploration and insertion into the produced WidgetDSL/code, using visual similarity as a coarse filter and caption similarity to refine semantic correctness.",
      "source_document": "papers/2512.19918v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an image-only widget-to-code benchmark (no ground-truth markup), how can layout fidelity be evaluated in a way that is invariant to color/texture/style and reported as a bounded similarity score\u2014specifically, how is the geometry-only \u201cstructural mask\u201d constructed, how are the four margins and the margin-symmetry (padding balance) metric computed from it, and how are raw difference values converted into a [0,100] similarity score?",
      "answer": "Layout is evaluated on a geometry-only representation derived from the rendered widget: a binary \u201cstructural mask\u201d is extracted by running an edge detector on the widget image and dilating the detected edges, which preserves the visible structure outline while suppressing interior appearance (color/texture/style) details. From the mask support S(M)={(y,x)|M(y,x)=1}, the four margins are defined by the extremal mask coordinates: m_top=min_{(y,x)\u2208S(M)} y, m_bottom=H\u22121\u2212max_{(y,x)\u2208S(M)} y, m_left=min_{(y,x)\u2208S(M)} x, m_right=W\u22121\u2212max_{(y,x)\u2208S(M)} x, giving margin vector m(M)=(m_top,m_bottom,m_left,m_right). Padding balance is measured by comparing ground-truth vs generated margins via \u0394m=|m(M_gt)\u2212m(M_gen)|, then computing mean \u03bc and standard deviation \u03c3 of the four entries; with \u03b5=1e\u22126, margin asymmetry is 0 if \u03bc<\u03b5 else \u03c3/\u03bc (larger when deviations are uneven across sides). Difference-based layout measures (including margin asymmetry and others) are mapped to bounded similarity with an exponential transform Sim(v)=exp(\u2212v/s) (s is a scale parameter), and then multiplied by 100 so scores fall in [0,100].",
      "source_document": "papers/2512.19918v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an image-only Widget-to-Code setting (no ground-truth markup), what practical procedure can be used to (a) extract a widget\u2019s dominant color theme so an MLLM/DSL generator can reuse it, and (b) quantitatively score whether the reconstructed widget preserved the input\u2019s color style\u2014covering hue-theme similarity, saturation/vibrancy similarity, and foreground\u2013background polarity consistency?",
      "answer": "(a) A widget\u2019s global palette can be extracted with an image-processing module that (after preprocessing such as filtering out transparent colors) converts the widget image to a perceptually uniform color space, then clusters the pixel colors with K-means to obtain the top\u2011K dominant colors and their proportions. The palette is represented as P = {(\u03bck, wk)} for k=1..K, where \u03bck is the cluster centroid color and wk is its relative weight, and this ranked palette is injected to guide WidgetDSL/code generation for style consistency.\n\n(b) Style fidelity can be scored with three complementary metrics:\n\u2022 Palette Distance (hue-theme similarity): compute the 1D Wasserstein/Earth Mover\u2019s Distance between the normalized HSV hue histograms h_gt and h_gen, EMD_hue = W1(h_gt, h_gen), then convert deviation to a similarity via an exponential mapping PaletteDistance = exp(\u2212EMD_hue/\u03b1).\n\u2022 Vibrancy Consistency (saturation similarity): compute the 1D Wasserstein distance between normalized HSV saturation histograms s_gt and s_gen, EMD_sat = W1(s_gt, s_gen), then Vibrancy = exp(\u2212EMD_sat/\u03b2).\n\u2022 Polarity Consistency (foreground\u2013background relationship): in grayscale, estimate background luminance bg(L) as the median luminance and foreground fg(L) as the mean luminance of the darkest 10% pixels; define the polarity sign p(L)=sign(bg(L)\u2212fg(L)). If p(L_gt)\u2260p(L_gen), the score is 0; otherwise measure the magnitude gap \u0394=((bg\u2212fg)_gt\u2212(bg\u2212fg)_gen) and map it with PolarityConsistency = exp(\u2212\u03b3|\u0394|).",
      "source_document": "papers/2512.19918v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using a synthetic widget generator as supervision to fine-tune a multimodal LLM for Widget-to-Code, what aligned artifacts should each synthesized training instance include to make the supervision usable end-to-end, and what four instruction-style tasks can be constructed from that metadata to train layout reasoning and visual grounding?",
      "answer": "Each synthetic widget instance should be produced as an aligned quadruplet (I, L, D, C): the rendered widget Image (I), spatial Layout metadata (L), the structural WidgetDSL specification (D), and the executable front-end Code (C), so supervision is traceable from specification through to rendering. From this metadata, four instruction\u2013response tasks can be formed: (1) Layout Generation\u2014output a hierarchical UI component tree with explicit spatial relationships; (2) General Grounding\u2014detect all visible UI elements and localize them with bounding boxes; (3) Category Grounding\u2014given a target type (e.g., Icon, Text), identify/localize all elements of that type; and (4) Referring Expression Comprehension\u2014given a bounding box, generate a textual description of the corresponding element including its type/label and visual attributes.",
      "source_document": "papers/2512.19918v1.pdf",
      "mode": "textual",
      "content_refs": [
        "lines 1991-1995",
        "lines 2245-2256"
      ]
    },
    {
      "question": "In a joint brain MRI registration approach that couples volumetric (3D) alignment with cortical surface (spherical) alignment, how can you enforce that both pathways produce geometrically consistent cortical correspondences, and what full training objective (including similarity, regularization, and any auxiliary structural supervision) implements this coupling?",
      "answer": "Geometric consistency is enforced with a cortical consistency loss that directly compares the 3D displacement predicted by the volumetric warp on cortical mesh vertices to the 3D displacement implied by the spherical (2D) warp composed back into 3D. For cortical mesh C1 with vertices v, the loss is\n\nL_cons(\u03c6, \u03c8, C1) = (1/N_v) \u03a3_{v\u2208C1} f( \u03c6(v), \u03c0^{-1}( \u03c8( \u03c0(v) ) ) ),\n\nwhere \u03c0 maps a cortical vertex to the 2D stereographic parameterization of the sphere, \u03c8 is the learned 2D spherical deformation (from an integrated SVF), and \u03c0^{-1} maps the warped 2D coordinate back to 3D (implemented via trilinear interpolation over the planar parameterization); f is taken as mean squared error.\n\nThe overall joint training objective combines (i) an image/descriptor similarity loss, (ii) this consistency term, (iii) smoothness regularization on both deformation fields, and (iv) an auxiliary structural alignment term when label maps are available:\n\nL(\u03c6, \u03c8) = L_sim(\u03c6, \u03c8) + \u03b3 L_cons(\u03c6, \u03c8) + \u03bb L_reg(\u03c6, \u03c8) + \u03ba L_struc(\u03c6, \u03c8).\n\nL_sim uses local normalized cross-correlation (NCC) to match MRI intensities in 3D and also matches spherical cortical geometric descriptors (sulcal depth and mean curvature), with weighting to correct stereographic area distortion; L_reg is a smoothness penalty \u2225\u2207\u03c6\u2225\u00b2 + \u2225\u2207\u03c8\u2225\u00b2; and L_struc is a soft Dice overlap loss on available volumetric and rasterized cortical ribbon segmentations.",
      "source_document": "papers/2512.19928v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a deformable brain MRI registration model with an auxiliary soft-Dice structural supervision term weighted by \u03ba (in addition to similarity and smoothness regularization), what qualitative trade-off emerges as \u03ba increases in terms of (i) cortical vs. subcortical Dice accuracy and (ii) deformation-field regularity, and how should \u03ba be chosen for a target application?",
      "answer": "Increasing the Dice-loss weight \u03ba produces consistent gains in cortical Dice, while subcortical Dice improves less and begins to saturate once \u03ba is sufficiently large (reported as saturating for \u03ba > 5). These accuracy gains come at the expense of deformation-field regularity: higher \u03ba leads to more irregular warps, reflected by increases in folding (% voxels with detJ<0) and in SD(log|detJ|). Thus \u03ba should be selected based on the downstream need to prioritize structural alignment versus smoothness/topology regularity; users can tune \u03ba to control this balance, and the method can still yield accurate matchings with regular fields over a broad range of \u03ba values.",
      "source_document": "papers/2512.19928v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When implementing a learning-based spherical cortical registration module as a standard 2D CNN by stereographically projecting the cortical sphere to a planar grid, what specific corrections are needed to (i) compensate for non-uniform sampling / area distortion introduced by the spherical parameterization and projection, and (ii) avoid artifacts at the left\u2013right seam and at the poles during convolution and warping?",
      "answer": "Use distortion-aware weighting in the surface losses and explicit padding/unwrap handling in the planar parameterization: (i) apply distortion correction to all surface-based losses by weighting samples by sin(\u03b8) (\u03b8 = elevation angle) to account for stretching near the poles, and also use an area-distortion weighting term w(\u03c1) in the surface similarity loss to correct distortion from stereographic projection; (ii) handle boundary discontinuities using circular padding along the left\u2013right axis, and at the poles use a 180\u00b0 circular shift together with reflection padding.",
      "source_document": "papers/2512.19928v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a vehicle-centric MAE pre-training setup that injects contour and vision\u2013language priors, what is the full pre-training objective (list all loss terms and how they are combined), and for each non-reconstruction term specify exactly what features or probability distributions are being matched (including how the text\u2013image similarity distribution is computed and regularized)?",
      "answer": "The full pre-training loss is a weighted sum of six terms:\n\nL = \u03bbr Lr + \u03bbmim Lmim + \u03bbcls Lcls + \u03bbcf Lcf + \u03bbcs Lcs + \u03bbvt Lvt.\n\n\u2022 Reconstruction loss (Lr): pixel-space L2/MSE over masked pixels between the original RGB image V and the decoder reconstruction Vr:\nLr = (1/Nm) \u2211_{t\u2208Pm} ||Vt \u2212 Vt^r||2.\n\n\u2022 Contour-guided masked-image-modeling loss (Lmim): a cross-entropy / masked-token distribution matching objective between (i) \u201cskeleton\u201d contour features Fs (obtained by running an edge detector BDCN to produce a contour map, patchifying it, and encoding it with the same Transformer encoder used for the raw image) and (ii) reconstructed invisible-token features Ft from the decoder. Both are projected into K-dimensional patch-level probability distributions via separate projection heads P^{patch}_{\u03b8\u2032}(Fs_i) and P^{patch}_{\u03b8}(Ft_i):\nLmim = \u2212\u2211_{i=1}^{Np} (P^{patch}_{\u03b8\u2032}(Fs_i))^T log P^{patch}_{\u03b8}(Ft_i).\n\n\u2022 Contour-guided CLS distribution loss (Lcls): similarly matches the contour-map CLS-token distribution to the reconstructed CLS-token distribution via separate projection heads:\nLcls = (P^{cls}_{\u03b8\u2032}(Fs_cls))^T log P^{cls}_{\u03b8}(Ft_cls).\n\n\u2022 CLIP visual feature distillation/similarity loss (Lcf): matches the MAE decoded visual feature Ft to the frozen CLIP visual feature Vc using squared distance between L2-normalized embeddings:\nLcf = ( Ft/||Ft||2 \u2212 Vc/||Vc||2 )^2.\n\n\u2022 Similarity-distribution consistency + regularization (Lcs): uses frozen CLIP text embeddings W = [f_{w1}, \u2026, f_{wm}] (from unpaired attribute text descriptions) and forms two similarity distributions over texts: one from CLIP visual features and one from MAE decoded features. For j-th text,\nsj(f_Vc, f_wj) = exp((f_Vc * f_wj)/\u03c4) / \u2211_{n=1}^{m} exp((f_Vc * f_wn)/\u03c4),\nwith temperature \u03c4 set to 1; then S(f_Vc,W)=[s1,\u2026,sm]. The analogous distribution S(f_Ft,W) is computed by replacing f_Vc with f_Ft. The loss enforces distribution agreement via KL divergence plus an entropy regularizer on the decoded-feature distribution:\nLcs = KL( S(f_Vc,W), S(f_Ft,W) ) + H( S(f_Ft,W) ).\n\n\u2022 Vision\u2013text contrastive loss (Lvt): for images where bounding-box (and optionally angle) labels can be predicted, content-relevant text descriptions are generated and encoded by a frozen CLIP text encoder to get Fw_i. The encoder output image features P\u0304_i (from the MAE encoder) and text features are L2-normalized and optimized with cosine embedding loss:\nLvt = (1/Nb) \u2211_{i=1}^{Nb} CEL( P\u0304_i/||P\u0304_i||2 , Fw_i/||Fw_i||2 ),\nwhere CEL is Cosine Embedding Loss and Nb is batch size.\n\nThe CLIP visual and language encoders are kept fixed while these losses train the MAE-based model.",
      "source_document": "papers/2512.19934v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a vehicle-centric MAE pre-training pipeline that uses a detector to exploit bilateral symmetry, how is the masking strategy defined for (i) images with no detector labels, (ii) images with only a predicted bounding box, and (iii) images with both bounding box and viewpoint/angle labels\u2014and in the symmetry-guided case, what exact rule is enforced on each pair of symmetric patches and how is the overall masking ratio kept unchanged after enforcing that rule?",
      "answer": "The pre-training uses three masking strategies depending on which labels the vehicle angle detector produces:\n\n1) No predicted labels: apply the standard MAE random masking strategy.\n\n2) Only a predicted bounding box: apply a box-guided masking strategy that prioritizes masking inside/outside the vehicle differently by setting different mask ratios for the vehicle region and the background.\n\n3) Predicted bounding box + angle: apply a symmetry-guided masking strategy that extends the box-guided strategy using the symmetry implied by the target\u2019s angle and box. It first computes the symmetric block (patch) indices within the target region from the box and angle. It then enforces that for every symmetric pair of patches, at least one of the two must be masked; if both symmetric patches are currently unmasked, it randomly masks one of them. To keep the global masking rate unchanged after these additional symmetry constraints, it randomly removes (undoes) a mask from one patch selected from a candidate replacement queue (i.e., unmasking another patch to compensate). In the described setup, 75% of patches are masked and the remaining 25% are fed to the encoder.",
      "source_document": "papers/2512.19934v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a vehicle-focused vision\u2013language guided MAE pre-training setup where text prompts are generated from detector-predicted metadata (e.g., bounding box and viewpoint/angle), what prompt design choices were evaluated for constructing those texts, what overall effect did template-based prompts have on downstream performance compared to using raw parameters directly, and why can drawing templates at random from a template pool fail to improve over using a single fixed template?",
      "answer": "Three prompt formulations are compared: (1) a template-free prompt that constructs texts directly from the raw predicted parameters (box/angle metadata), (2) a randomized-template prompt that samples from a pool of LLM-generated templates to increase text diversity, and (3) a fixed-template prompt that uses a single structured template.\n\nAcross downstream vehicle tasks, template-based prompts (randomized-template or fixed-template) yield clear performance improvements over template-free prompts, indicating that structured semantic text helps cross-modal alignment and discriminative representation learning. However, randomized-template prompting is only comparable to, and sometimes slightly worse than, fixed-template prompting because randomly chosen templates introduce semantic perturbations that weaken semantic consistency across samples and hinder learning stable semantic structures; therefore, the fixed-template prompt is chosen as the default for better balance between consistency and performance.",
      "source_document": "papers/2512.19934v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When pre-training a masked autoencoder with several auxiliary objectives (e.g., MIM-style feature matching and classification/semantic losses), what failure mode can occur if all loss terms are given equal weight, and how can re-weighting the auxiliary losses versus the reconstruction loss restore stable optimization? Explain which losses need to be down-weighted, why, and what adjustment re-establishes reconstruction as the dominant training signal.",
      "answer": "If all losses are weighted equally (all weights set to 1), training degrades across downstream tasks because some auxiliary terms\u2014specifically the MIM loss (Lmim) and the classification-related loss (Lcls)\u2014take on excessively large values and dominate the optimization, preventing effective representation learning. A stable fix is to suppress these dominant auxiliary losses by setting their weights to small values (\u03bbmim = 0.02 and \u03bbcls = 0.02), balancing contributions across terms. Under that setting, the reconstruction objective can become relatively underweighted later in training, so increasing the reconstruction weight to \u03bbr = 4 restores reconstruction as the main optimization target and yields additional downstream performance gains.",
      "source_document": "papers/2512.19934v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In multimodal pre-training for vehicle-centric perception, how can you build and use an *unpaired* text modality that still injects vehicle semantics into a masked autoencoder\u2014specifically, what vehicle attributes are collected to form those texts, and how are these attribute-based descriptions represented and consumed during pre-training?",
      "answer": "The unpaired text modality is created by collecting a fixed set of vehicle-model attributes and turning them into attribute-based textual descriptions (not necessarily one-to-one paired with each image). The attributes collected are: brand, color, energy type, level, length, width, height, number of doors, number of seats, wheelbase, and years. During pre-training, each description is embedded by a frozen CLIP text encoder into a 512\u2011D text feature/token, and the (frozen) CLIP visual encoder embeds the vehicle image into a 512\u2011D visual feature; these CLIP features are then used as semantic targets/priors for the MAE\u2019s decoded/reconstructed features within the semantics-guided representation learning objectives.",
      "source_document": "papers/2512.19934v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a recurrent, weight-tied surrogate of a pretrained Vision Transformer that aims to reproduce the teacher\u2019s *entire depth trajectory* (not just final logits), how is the activation-matching objective defined across layers, and what training strategy is used to reconcile stable optimization with the need for closed-loop autoregressive inference?",
      "answer": "The surrogate is trained to match the teacher\u2019s intermediate activations at every depth: for teacher activations a_\u2113(x) and surrogate activations \u007f\u00e3_\u2113(x), an autoregressive trajectory-fidelity loss sums per-layer Frobenius errors across depth, e.g. L_AR,h(x)=E_x[\u2211_{\u2113=1}^h ||\u007f\u00e3_\u2113(x)\u2212a_\u2113(x)||_F]. Training uses a hybrid objective that mixes teacher forcing and autoregressive rollout to avoid the train\u2013test mismatch: L_total(x)=\u03bb L_TF(x)+(1\u2212\u03bb) L_AR,H(x)+\u03a9(\u03b8), with \u03bb annealed down. Concretely, Stage 1 trains each recurrent block on its assigned contiguous layer segment using teacher-forced inputs (with the AR term present and \u03bb gradually reduced), then Stage 2 connects all blocks and fine-tunes end-to-end using only the autoregressive loss (\u03bb=0) so the model learns self-consistent closed-loop dynamics; teacher forcing alone is insufficient and collapses to very low ImageNet-1k accuracy (~3%).",
      "source_document": "papers/2512.19941v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a frozen ViT/Raptor backbone on NYUv2 monocular depth estimation with a linear probe, how are the final-layer token outputs transformed into a per-pixel depth map (including how cls/patch tokens are used, the discretization scheme, and upsampling), and what training loss is used for the depth head?",
      "answer": "The probe uses the backbone\u2019s final predicted Layer-12 tokens. For NYUv2, images are processed at 480\u00d7640 (center-padded so dimensions are multiples of 14). From the final layer, the cls token is concatenated with the patch tokens and the spatial token grid is upsampled by 4\u00d7; both cls and patch representations are upscaled, then the (upscaled) cls token is concatenated to each patch token to form per-location \u201clogits.\u201d Depth is obtained by applying a softmax over 256 evenly spaced depth bins and taking the weighted average of the bin-center values, followed by upsampling the resulting depth map to 480\u00d7640. The depth head is trained with the loss introduced by Bhat et al. (2021).",
      "source_document": "papers/2512.19941v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "If you want to turn a pretrained ViT into a k-block weight-tied recurrent surrogate, how can you automatically choose the k contiguous \u201cphases\u201d (repetition ranges) from a layer\u2013layer similarity matrix, and what objective and dynamic-programming recurrence define the optimal contiguous partition?",
      "answer": "Compute a symmetrized layer\u2013layer similarity matrix S\u2208R^{L\u00d7L} (e.g., cosine similarity between same tokens at different depths) and seek a partition \u03a0={[b1,e1],\u2026,[bk,ek]} of layers 1\u2026L into k contiguous segments that maximizes within-segment similarity (equivalently minimizes cross-block cut). For any candidate segment [i,j] with length n=j\u2212i+1, define sum(i,j)=\u2211_{p=i}^j\u2211_{q=i}^j S_{pq} and use an additive segment score g(i,j)=sum(i,j)/n^2 (a weighted mean similarity). The best k-segment partition maximizes \u2211_{t=1}^k g(bt,et). Using a minimum segment length m, solve via DP: dp[1,j]=g(1,j) for j\u2265m, and for t=2\u2026k, j\u2265t\u00b7m,\n\ndp[t,j]=max_{i\u2208{t\u00b7m\u22121,\u2026,j\u2212m}} dp[t\u22121,i] + g(i+1,j),\n\nkeeping backpointers to recover boundaries. Submatrix sums sum(i,j) can be computed in O(1) using a 2-D prefix-sum (\u201cintegral image\u201d) table, yielding an overall O(kL^2) time, O(kL) memory solver.",
      "source_document": "papers/2512.19941v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When trying to make a pretrained Vision Transformer more accurately representable as a small set of weight-tied recurrent blocks, what training-time regularizer can be applied to promote contiguous phase/block structure, and how can you verify that it actually improves recurrent approximability in terms of (i) within-model layer similarity and (ii) teacher\u2013student hidden-state reconstruction quality?",
      "answer": "Apply stochastic depth during ViT training: each transformer layer is independently dropped with probability p, using a uniform drop rate across depth. To verify it promotes block/phase structure and recurrent approximability, (i) measure the ViT\u2019s layer\u2013layer representational similarity via cosine similarity matrices and observe that increasing p increases layer\u2013layer similarity (stronger within-block similarity), and (ii) train an autoregressive Raptor student to reconstruct the teacher\u2019s hidden activation states at every layer, then quantify teacher\u2013student alignment as the R\u00b2 of matched token embeddings (reported for both cls and patch tokens, including last-layer 1\u2212R\u00b2 error). As p increases, the student\u2019s layerwise reconstruction fidelity improves and correlates positively with the teacher\u2019s increased intra-block similarity; CIFAR-100 classification accuracy also improves for both the ViT teacher and the Raptor student under higher stochastic depth (before very high p causes aberrant training).",
      "source_document": "papers/2512.19941v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "Suppose you hypothesize that a pretrained Vision Transformer\u2019s depth consists of contiguous phases of reusable computation (so layers within a phase are functionally similar). What concrete intervention can you perform on the teacher model itself to test whether layers are actually interchangeable within a phase, and what qualitative difference in downstream accuracy would you expect between swapping a layer with another layer from the same phase versus from a different phase if the hypothesis is true?",
      "answer": "A direct test is to do a layer-swapping experiment: replace a layer in the ViT with another layer\u2019s parameters/operation, comparing swaps drawn from within the same discovered block/phase (\u201cintra-block\u201d) to swaps drawn from a different block/phase (\u201cinter-block\u201d). If the phases reflect genuine functional reuse, intra-block swaps should preserve classification accuracy, whereas inter-block swaps should cause the model to collapse (severe accuracy degradation).",
      "source_document": "papers/2512.19941v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a text-guided 360\u00b0 panorama editing model, how can you apply rotation-based data augmentation without invalidating prompts that mention object location? Describe the orientation-decoupled augmentation strategy and how it treats prompts with absolute position descriptions versus relative position descriptions, and note any additional standard augmentations applied.",
      "answer": "Use an orientation-decoupled augmentation that first classifies training prompts by whether they contain absolute or relative position language. For prompts with absolute position descriptions, the panorama is only rotated latitudinally within a small range so the stated absolute location remains approximately true. For prompts with relative position descriptions, the panorama can be rotated latitudinally by any angle because relative spatial relationships are preserved under global rotation. Standard augmentations such as scaling and rotation are also applied.",
      "source_document": "papers/2512.19943v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a 360\u00b0 panorama editing dataset built from automatic object erasure, how can you refine the erased image\u2013original image pairs to (i) remove near-no-op edits and (ii) ensure the left\u2013right panorama seam remains visually continuous? Describe a cost-effective two-stage filtering pipeline that uses a global structural similarity check followed by a region-constrained feature-similarity test, including what regions are compared and what kinds of cases each filter is meant to discard.",
      "answer": "A practical refinement is to generate cleaner erasures with a stronger editor (replacing the basic inpainter) and then filter the resulting pairs in two steps:\n\n1) **Global SSIM filtering:** Compute SSIM between the erased panorama and the original panorama and discard pairs whose SSIM is too high (i.e., the erasure barely changes the image), which typically indicates an insignificant/failed edit.\n\n2) **Region-constrained feature-similarity filtering (DINOv2 features):** Instead of a single global feature comparison, compute feature similarity separately in (a) the **masked region** (using the mask\u2019s bounding box) and (b) a **thin border band** around the panorama (a 10%-width border region) to explicitly check seam/boundary continuity. Discard samples if the masked-region similarity is too high (again indicating an insignificant edit) or if the border-region similarity is too low (indicating boundary/seam discontinuity introduced by the erasure).",
      "source_document": "papers/2512.19943v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When adapting a diffusion Transformer (pretrained on perspective images) to instruction-guided editing of 360\u00b0 equirectangular panoramas, what architectural changes can be used to (i) eliminate visible artifacts at the left\u2013right longitudinal seam during VAE encode/decode and (ii) give the Transformer a positional prior that respects spherical geometry rather than 2D image-grid distance? Describe the seam-handling procedure and the spherical positional embedding construction, including how ERP pixels are mapped to a unit sphere and how the embedding is formed from the 3D coordinates.",
      "answer": "(i) Seam handling can be done with a symmetric \u201cPad-and-Unpad\u201d strategy around the frozen VAE: during encoding, apply circular padding by prepending and appending content from the opposite horizontal edges so the encoder sees adjacent 360\u00b0 context; then crop the latent back to the original width. During decoding, apply the same circular padding in latent space before decoding, reconstruct a slightly wider image with smooth boundary transitions, and finally crop the central portion to obtain a seamless panorama (removing boundary artifacts caused by standard convolutional kernels that ignore spherical topology).\n\n(ii) A spherical positional prior can be provided with 3D Spherical Positional Embedding (SPE): for an ERP image of size H\u00d7W and pixel coordinates (i,j), convert to spherical coordinates longitude \u03bb and latitude \u03d5, then map to 3D Cartesian coordinates on the unit sphere p=(x,y,z)=(cos\u03d5 cos\u03bb, cos\u03d5 sin\u03bb, sin\u03d5). Extend standard sinusoidal positional encoding to 3D by splitting the embedding dimension d equally across x, y, and z and applying the sinusoidal functions independently to each axis, which preserves proximity on the spherical manifold while remaining compatible with Transformer blocks.",
      "source_document": "papers/2512.19943v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When probing frozen video foundation model features for feedforward 3D reconstruction, what probe architecture is used (including how temporal information is mixed and what outputs are decoded), how are input frames sampled, and what multi-task training losses/normalizations are applied to the point-map, depth, and camera-pose predictions to handle uncertainty and scale ambiguity?",
      "answer": "The probe is a shallow VGGT-like transformer that takes spatial feature maps from a frozen backbone for S=4 frames (the first frame as a reference plus three additional frames sampled with a minimum temporal gap of 5 frames). It applies four alternating-attention blocks, where each block has (1) within-frame (\u201cframe\u201d) attention to mix tokens spatially inside each frame and (2) cross-frame (\u201cglobal\u201d) attention to mix tokens across time. Three readout heads decode 3D outputs: two dense DPT-style heads predict a per-frame 3D point map in the coordinate system of the first frame and a dense depth map, and a camera head predicts each frame\u2019s pose relative to the first frame.\n\nTraining uses a VGGT-style multi-task objective L = \u03bb_pmap L_pmap + \u03bb_depth L_depth + \u03bb_cam L_cam with \u03bb\u2019s set to 1 by default. L_pmap and L_depth are confidence-weighted \u21132 losses between predicted and ground-truth point/depth maps, and ground-truth scenes are normalized before loss computation to remove global scale ambiguity. The camera-pose loss is a Huber loss between predicted and ground-truth relative poses.",
      "source_document": "papers/2512.19949v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a histology-wide association study that links WSI-derived topological (point-pattern) features to a continuous clinical endpoint, what is the end-to-end statistical testing setup\u2014specifically (i) how are heterogeneous features preprocessed to make regression effect sizes comparable, (ii) what univariate regression form is fit per feature, (iii) how are multiple-hypothesis significance thresholds controlled, and (iv) which edge-effect correction is applied to pair-counting functions versus nearest-neighbor/empty-space functions?",
      "answer": "(i) All extracted feature columns are z-score standardized (mean 0, standard deviation 1) so beta coefficients are comparable across features with different units and ranges. (ii) Each feature is tested with a separate mass univariate linear regression of the form: Phenotype ~ \u03b20 + \u03b21 \u00d7 Feature_i. (iii) Multiple testing is handled with both a Bonferroni threshold (\u03b1 = 0.05 / number of tests) and an FDR threshold using the Benjamini\u2013Hochberg procedure with Q = 0.05. (iv) Edge effects are corrected using Ripley\u2019s isotropic correction for the L- and g-functions (pair-counting statistics derived from K/g), while nearest-neighbor/empty-space based functions use a Kaplan\u2013Meier estimator for censoring\u2014applied to G, F, and consequently J (as a ratio of G and F).",
      "source_document": "papers/2512.19954v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When computing density-dependent spatial point-pattern features from histology object centroids, how can the foreground tissue area be estimated automatically in the presence of slide artifacts, and what geometric construction is used to turn detected tissue clusters into an area estimate?",
      "answer": "Treat all object centroids in a WSI as a point cloud, run DBSCAN to separate coherent tissue clusters from artifacts/noise, then (for each cluster with at least three points) compute its convex hull; the WSI\u2019s foreground tissue area is estimated by summing the areas of these convex hulls across all retained clusters.",
      "source_document": "papers/2512.19954v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using multi-scale spatial point-pattern functions (e.g., L-, g-, G-, F-, and J-functions) as pathology \u201ctopology\u201d descriptors, how can these non-scalar curves be converted into fixed-length numeric features suitable for mass univariate regression, and what does each curve-summary statistic represent in terms of deviation from complete spatial randomness (CSR)?",
      "answer": "Convert each distance-dependent function into scalar \u201ccurve summary\u201d features computed from the centered curve\u2019s overall shape. For each function (L, g, G, F, J), compute: (1) AUC (area under the centered curve), which measures the total cumulative deviation from randomness across all distances; (2) Max/Min, the maximum positive (clustering) and minimum negative (dispersion) peaks of the centered curve; and (3) DistAtMax/DistAtMin, the distances r at which the maximum and minimum deviations occur, identifying the characteristic spatial scales where clustering or dispersion is strongest.",
      "source_document": "papers/2512.19954v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In weakly supervised whole-slide image (WSI) classification with MIL, what mechanisms can be used to (1) make Transformer-style inter-patch dependency modeling scalable to tens of thousands of patches and (2) adapt attention to tumor regions that vary greatly in spatial extent? Describe how WSD-MIL implements both goals, including how patches are reduced before attention, how global attention is approximated, how the local attention window size changes across layers, and how region importance is reweighted before final slide prediction.",
      "answer": "WSD-MIL makes dependency modeling scalable and multi-scale by combining a patch-reduction step with a coarse-to-fine attention scheme and a region-gating module:\n\n1) Patch reduction before attention (scalability): patch embeddings from a frozen pretrained encoder are clustered with K-means, then a fixed proportion \u03b1% of features is kept from every cluster via stratified random sampling. This reduces the number of instances while preserving feature diversity/heterogeneity.\n\n2) Global attention approximation (scalability): after projecting the sampled feature matrix to queries/keys/values, WSD-MIL uses a Nystr\u00f6m attention layer (with a Moore\u2013Penrose pseudoinverse) to approximate global attention instead of full quadratic self-attention.\n\n3) Progressive window-scale decay (multi-scale/local modeling): after Nystr\u00f6m global attention, it applies multi-head self-attention within local windows at progressively smaller scales\u20144k\u00d74k, then 2k\u00d72k, then k\u00d7k (with k=4)\u2014so attention moves from coarser regional context to finer-grained local correlations.\n\n4) Region reweighting before prediction (global modeling/class separability): the SERG module partitions features into L\u00d7L windows (L=8), applies per-window global average pooling, then a squeeze-and-excitation style two-layer gating (ReLU then sigmoid with reduction ratio r) to produce region weights G. The feature map is region-wise reweighted (Hout = H\u2032 \u2299 G) and then passed to a WSI-level attention aggregator and linear classifier for the slide label.",
      "source_document": "papers/2512.19982v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a weakly supervised whole-slide image (WSI) classification pipeline that uses frozen patch encoders plus MIL aggregation, how is the raw gigapixel slide preprocessed into a bag of patch embeddings (from tissue/foreground detection through patch extraction), and what pretrained encoders/embedding dimensionalities are used before the MIL model consumes the instances?",
      "answer": "Each raw WSI is first processed with a simple foreground/tissue detection step using threshold-based segmentation to delineate the tissue region; background pixels are removed to form a foreground mask. Only the foreground area is then tiled into non-overlapping patches, and each patch is encoded by a frozen pretrained backbone to produce instance embeddings for MIL. Two encoders are used: ResNet-50 to produce 1,024-dimensional patch features, and the Virchow pathology foundation model to produce 1,280-dimensional patch features.",
      "source_document": "papers/2512.19982v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a Transformer-style MIL aggregator for whole-slide classification that combines (i) a progressive, multi-scale decaying window self-attention block and (ii) a squeeze-and-excitation region-gating block, what do ablations that (a) remove the decaying-window attention, (b) replace it with a single fixed window scale (e.g., small or large fixed windows), and (c) remove the region gate reveal about each component\u2019s contribution? Summarize the typical direction and approximate magnitude of the accuracy/F1 changes on CAMELYON16 and TCGA-BRCA, and interpret what this implies about modeling local multi-scale correlations vs global region reweighting.",
      "answer": "The ablations show that the progressive decaying-window attention is the main driver of the gains, while the SE-style region gate provides a smaller but consistent boost by improving global discriminative weighting:\n\n- Removing the decaying-window attention causes the largest drop. On CAMELYON16, accuracy/AUC/F1 fall by about 3.0/1.3/4.1 percentage points relative to the full model; on TCGA-BRCA the F1 drop is larger (about 5.6%), indicating this component is critical for capturing local correlations across varying tumor scales.\n\n- Replacing the decaying-window attention with fixed-scale window self-attention (both smaller and larger fixed windows) is better than removing it entirely but still underperforms the full model: it trails by roughly 0.8\u20131.3% in accuracy and about 0.6\u20131.0% in F1 on CAMELYON16. This supports the claim that a single fixed attention scale cannot adequately model multi-scale tumor associations.\n\n- Removing the SE region gate produces a smaller degradation (e.g., on CAMELYON16 F1 drops only slightly, from 87.89% to 87.69%, and accuracy decreases by ~0.7%; on TCGA-BRCA removing the gate costs about 1.5% F1). This suggests the gate\u2019s role is mainly to enhance class separability by globally reweighting regions (emphasizing discriminative regions and suppressing uninformative ones), complementing the local multi-scale modeling from the decaying-window attention.\n\n- Removing both components degrades performance most (e.g., CAMELYON16 F1 drops to ~81.8%; TCGA-BRCA F1 drops by ~7.0%), indicating the two modules provide synergistic benefits when combined.",
      "source_document": "papers/2512.19982v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When doing weakly supervised MIL classification of TCGA-BRCA whole-slide images with frozen patch encoders, what change in performance is observed when swapping a standard CNN encoder (ResNet-50) for a pathology foundation model (Virchow) for patch embeddings, and what does this indicate about whether the proposed MIL aggregator benefits from higher-dimensional pretrained representations compared to common MIL baselines?",
      "answer": "Using Virchow patch embeddings (higher-dimensional than ResNet-50) lifts performance on TCGA-BRCA across metrics: accuracy and AUC increase by a few points and F1 improves by roughly 6\u20137 points (about 90.4/91.1/77.7 with ResNet-50 to about 93.4/94.8/84.2 with Virchow for Acc/AUC/F1). Despite the stronger features, the proposed aggregator still ranks best among the compared pooling-, attention-, and fixed-window transformer MIL baselines, indicating it has strong synergy with large-scale pretrained pathology representations rather than relying on a specific backbone.",
      "source_document": "papers/2512.19982v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a two-stage CNN-feature-extractor + classical-ML hybrid ensemble for multi-class plant disease recognition, how is prediction confidence quantified and used to route samples between the first-stage classifier and the fallback model, and what threshold and fallback classifier are used?",
      "answer": "Confidence is defined as the maximum class probability output by the first-stage (base) classifier, \u03b3(x)=max_c p_base(y=c|x). If \u03b3(x)\u2265\u03c4 (with \u03c4=0.8), the base classifier\u2019s predicted class is accepted; if \u03b3(x)<0.8, the sample is treated as uncertain and is instead passed to a CatBoost classifier, whose prediction becomes the final decision for that sample.",
      "source_document": "papers/2512.19989v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training an image-based disease classifier on a dataset that already contains augmented images and separate train/val/test folders, what concrete dataset-splitting and preprocessing choices can be used to (i) prevent train\u2013test leakage and (ii) avoid inadvertently introducing additional synthetic variation, and how is class imbalance handled in this setup?",
      "answer": "A leakage-avoiding setup is to first merge the provided train/validation/test folders into a single pool for uniform processing, then create a fresh split that allocates 80% of the pooled data to training and 20% to validation, while keeping the originally provided test set untouched and using it only for the final evaluation. To avoid adding extra synthetic variation beyond what is already in the released dataset, no additional preprocessing/normalization/augmentation is applied other than basic resizing (to 224\u00d7224 for the model input) and pixel rescaling (1/255) during loading; specifically, augmentation is not re-applied and oversampling/new-image generation is not performed. Class imbalance is addressed by balancing the training data via random undersampling: use the smallest class size as the reference and randomly downsample the other classes to match it (and experiments are run on both balanced and unbalanced variants).",
      "source_document": "papers/2512.19989v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When choosing a CNN-feature-extractor + gradient-boosting hybrid for real-time guava disease detection on limited compute, which ensemble variant provides the best accuracy\u2013efficiency trade-off, and how does its accuracy and compute cost compare qualitatively to (i) the highest-accuracy AdaBoost-based hybrids and (ii) CatBoost-based hybrids?",
      "answer": "The Random Forest + LightGBM (RF\u2011LGBM) hybrid is identified as the most practical accuracy\u2013efficiency trade-off for real-time/resource-constrained use: it keeps accuracy in the mid\u201196% range while requiring only on the order of a few dozen seconds of training and around a few hundredths of a second per inference. In contrast, AdaBoost-based hybrids (e.g., Ada\u2011LGBM/Ada\u2011XGB/Ada\u2011Cat) reach near-perfect performance (~99.99% accuracy/precision/recall/F1) but at higher training cost, and CatBoost-based hybrids impose very large computational overhead (hundreds of seconds of training and noticeably slower inference) despite competitive accuracy.",
      "source_document": "papers/2512.19989v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a CNN-based feature extractor setup that uses EfficientNet-B0 for a 3-class fruit-disease classifier, what mechanism is used to convert the final convolutional feature maps into a fixed-length feature vector (instead of flattening), how are class probabilities produced from that vector, and what loss function and optimizer are used to train the CNN classifier head?",
      "answer": "The final convolutional tensor is aggregated with Global Average Pooling (GAP), which averages each of the d feature maps over its h\u00d7w spatial locations to produce a condensed feature vector z\u2208R^d. That vector is fed to a fully connected layer to produce logits, and class probabilities are obtained with a softmax over the K classes. Training minimizes categorical cross-entropy loss using the Adam optimizer.",
      "source_document": "papers/2512.19989v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using a pretrained CNN purely as a fixed feature extractor before training downstream classical/boosting classifiers, what training/configuration choices help keep the pipeline reproducible (e.g., what is frozen vs manually set), and what does this imply about how to interpret the reported results and where additional accuracy might come from?",
      "answer": "The pipeline keeps EfficientNet-B0 as a frozen backbone for feature extraction (transfer learning with ImageNet weights, without training the backbone), while only a small set of core training settings are manually specified for the feature-extraction stage (e.g., batch size, number of epochs, input resolution, and a train/validation split). In contrast, most downstream classical and gradient-boosting classifiers (Logistic Regression, SVM, Random Forest, XGBoost, LightGBM, CatBoost) are run largely with default configurations and only minimal adjustments for convergence/reproducibility. As a result, the reported performance should be interpreted as largely reflecting baseline/default parameter settings rather than heavily tuned models, and it leaves clear headroom for improvement via systematic hyperparameter tuning and more advanced optimization strategies.",
      "source_document": "papers/2512.19989v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a weakly supervised cross-resolution land-cover segmentation model that combines a diffusion-based local branch with a transformer-based global branch, how can pseudo-label noise be reduced using a confidence mechanism derived from diffusion features, and what losses are used to supervise each branch and the final joint objective?",
      "answer": "Pseudo-label noise is reduced by using the diffusion branch\u2019s frozen denoising prior to produce more noise-robust features and then filtering supervision for the transformer branch. First, the diffusion branch outputs an initial prediction Y1 and is directly supervised by the low-resolution labels T with pixel-wise cross-entropy L1. Next, diffusion features F are used to compute class-wise prototype features Fc by averaging diffusion features at pixels predicted as class c (i.e., over pixels where Y1=c). For each pixel, its diffusion feature is compared to its corresponding class prototype with cosine similarity; pixels whose similarity exceeds a threshold are marked as high-confidence, forming a binary mask M. The transformer branch prediction Y2 is then supervised by the filtered pseudo labels Y1\u2032 using a masked pixel-wise cross-entropy L2 that ignores low-confidence regions (only pixels with M=1 contribute). The overall training objective combines both supervisions as Ltotal = \u03bb L1 + (1\u2212\u03bb) L2 (with \u03bb=0.5).",
      "source_document": "papers/2512.19990v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a weakly supervised cross-resolution segmentation framework that uses a pretrained diffusion model for local refinement and a Transformer for global context, how are the diffusion branch\u2019s multi-scale features constructed from the denoiser/decoder and then fused with the Transformer features to produce the final high-resolution prediction (include the roles of upsampling, concatenation, and the ConvBR blocks)?",
      "answer": "The diffusion branch does not use the diffusion model for generation; instead a pretrained DDPM denoising network is used as a frozen multi-scale feature extractor. For an input image x, the DDPM decoder outputs intermediate feature maps at multiple scales (five levels, f1\u2026f5) controlled by the diffusion step (noise level). Each fi is upsampled to the original image resolution (f\u2032i), then the upsampled features are concatenated across scales and passed through a ConvBR block (convolution + batch norm + ReLU) to normalize/adjust channels, producing a local-detail feature representation.\n\nThe Transformer branch encodes the image with patch embeddings plus positional encodings through a stack of self-attention blocks, then its encoded features are progressively upsampled back toward the high-resolution spatial scale. At each scale during this upsampling, the Transformer features are fused with the corresponding diffusion features by concatenating them and applying another ConvBR block for channel adjustment/normalization. This hierarchical, scale-by-scale fusion combines fine-grained local textures/boundaries from the diffusion features with long-range contextual information from the Transformer, yielding the final fused feature representation used for pixel-wise prediction.",
      "source_document": "papers/2512.19990v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "On the Chesapeake Bay cross-resolution land-cover mapping benchmark, how does the proposed dual-branch local\u2013global framework compare to the strongest weakly supervised baseline in terms of where it improves vs. where it lags (i.e., across the six U.S. states), and what overall conclusion does this support about its robustness under coarse supervision?",
      "answer": "It achieves the best overall performance on average and improves over the strongest weakly supervised baseline (Paraformer) in five of the six states\u2014Delaware, Maryland, Pennsylvania, Virginia, and West Virginia\u2014while Paraformer is slightly better only in New York. This pattern supports the conclusion that the method is generally more robust to cross-resolution/coarse supervision and transfers well across regions, with only limited regional cases where the baseline has an edge.",
      "source_document": "papers/2512.19990v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a diffusion-based image-to-video model that jointly generates frames and a subject mask sequence to improve robustness, how is the predicted mask sequence converted into an attention bias that encourages within-foreground/background attention while suppressing cross-region attention, and what training strategy is used early on to prevent noisy intermediate mask predictions from destabilizing learning?",
      "answer": "The mask sequence is turned into an additive attention mask M that biases the softmax in attention (softmax(QK\u1d40/\u221ad + M)V). For a token at spatial location p in frame i with subject confidence S\u1d62_p and a token at location q in frame j with confidence S\u02b2_q, the mask entry is\n\nM^(i,j)_(p,q) = log( S\u1d62_p S\u02b2_q + (1\u2212S\u1d62_p)(1\u2212S\u02b2_q) + \u03b5 ),\n\nso same-region pairs (both foreground or both background) get higher values and cross-region pairs get large negative values, suppressing their attention. Because the intermediate predicted masks can be very noisy early in training, a dropout scheme is used: with probability p (following a predefined decreasing schedule), the predicted proxy mask sequence is replaced with the ground-truth mask sequence, stabilizing attention modulation until predictions improve.",
      "source_document": "papers/2512.20000v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a diffusion-based image-to-video animation setup that adds cross-frame attention to improve temporal coherence, how can the self-attention block be augmented to explicitly model dependencies on (i) the first frame and (ii) the previous frame, and how are the relative contributions of these cross-frame terms adaptively weighted as a function of the diffusion time step?",
      "answer": "Augment each self-attention (SA) layer with two cross-frame attention (CFA) layers: one attending from the current frame tokens f_i to the first-frame tokens f_1 (to enforce identity/appearance consistency) and one attending to the previous-frame tokens f_{i\u22121} (to enforce temporal smoothness). Each CFA has the form Att(f_i W_Q, f_j W_K, f_j W_V) W_O with j\u2208{1,i\u22121}. The SA block output is then a weighted combination SA*(f_i; f_1,f_{i\u22121}) = \u03bb1\u00b7SA(f_i) + \u03bb2\u00b7CFA_{i,1} + \u03bb3\u00b7CFA_{i,i\u22121}. Instead of manually choosing (\u03bb2,\u03bb3), use an adaptive weighting module \u03c6 attached to each SA layer that takes the diffusion timestep embedding c_t and outputs a 2D vector for (\u03bb2,\u03bb3): \u03c6(c_t;W_\u03c6)=\u03c3(SiLU(c_t)W_\u03c6), while \u03bb1 is fixed to 1.",
      "source_document": "papers/2512.20000v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an image-to-video diffusion setup that wants motion control without user-provided text prompts, how can you still exploit the base model\u2019s cross-attention prior by conditioning on a learnable \u201cimplicit prompt\u201d embedding: specifically, how is the original cross-attention term reparameterized into a small MLP, and which projection(s) are kept fixed versus optimized to keep the base model\u2019s semantics while making the adapter parameter-efficient?",
      "answer": "Use a learnable embedding c (the implicit prompt) as the conditioning signal in place of natural language, and rewrite cross-attention so it becomes a 2-layer MLP applied after a fixed video projection. Starting from CA(f,c)=Att(fWQ, cWK, cWV)WO = softmax(fWQ (WK c)\u1d40 /\u221ad) \u00b7 (cWV) WO, keep WQ fixed (to preserve the base model\u2019s video-side semantic prior) and optimize the remaining conditioning-dependent terms. Define A = (1/\u221ad) WK\u1d40 c\u1d40 and B = cWV WO, so CA(f,c) = softmax((fWQ)A) B. This is equivalent to a lightweight 2-layer perceptron following the frozen projection WQ, reducing the CA adapter size to roughly LoRA-level without losing conditioning capacity.",
      "source_document": "papers/2512.20000v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When combining multiple motion-specific adapters in a modular image-to-video diffusion system (to produce a video with multiple atomic motions), how are different adapters\u2019 outputs merged at (i) cross-attention and temporal self-attention layers versus (ii) spatial self-attention layers augmented with cross-frame attention, and what property of diffusion-model residual learning makes this additive composition stable (i.e., avoids domain deviation)?",
      "answer": "(i) At cross-attention (CA) and temporal self-attention (t-SA) layers, the per-adapter outputs are merged by a weighted summation across adapters.\n\n(ii) Spatial self-attention (SA) layers require special handling because the base SA layer is shared; only the adapters\u2019 CFA residual branches are combined. For frame i, the CFA-infused SA output is computed as the base term plus a weighted sum over adapters j:\n\u03bb1\u00b7SA(f_i) + \\sum_{j=1}^n w_j( \u03bb2^(j)\u00b7CFA^(j)_{i,1} + \u03bb3^(j)\u00b7CFA^(j)_{i,i\u22121} ),\nwhere w_j is the user-specified adapter weight and \u03bb2^(j), \u03bb3^(j) are that adapter\u2019s CFA weights.\n\nThis works because diffusion models use residual learning via skip connections: each layer is guided to predict a residual of the target signal rather than the full signal, so residual outputs from multiple adapters can be summed without causing domain deviation.",
      "source_document": "papers/2512.20000v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For evaluating an image-to-video diffusion animation method where the key goal is temporally coherent motion (not framewise text faithfulness), what quantitative metrics can be used to measure (i) subject consistency, (ii) background consistency, (iii) motion smoothness, (iv) temporal flickering, and (v) motion intensity, and how is each metric computed (including the feature backbones or reconstruction procedure)? Also, why are standard text\u2013image alignment metrics like CLIP alignment considered unsuitable in this setting?",
      "answer": "A suitable evaluation suite uses the following objective metrics (from VBench, plus one from EvalCrafter):\n\n- Subject Consistency: computed as the average cosine similarity (scaled to [0,100]) between DINO features of consecutive frames, to detect identity/appearance shifts of the foreground subject.\n- Background Consistency: computed as the average cosine similarity (scaled to [0,100]) between CLIP features of consecutive frames, measuring background stability across time.\n- Motion Smoothness: computed via a reconstruction procedure where alternate frames are dropped, then re-predicted using a video frame interpolation model; the mean absolute error (MAE) between the interpolated and original frames is taken, and the score is the negative MAE normalized to [0,100] (smaller MAE \u2192 smoother motion \u2192 higher score).\n- Temporal Flickering: computed as the negative MAE between consecutive frames, normalized to [0,100], intended to capture content-independent frame-to-frame fluctuations (e.g., lighting/exposure changes, shaky artifacts).\n- Motion Intensity (Average Flow): computed as the average optical-flow magnitude between consecutive frames using RAFT; higher average flow indicates stronger dynamics.\n\nText\u2013image alignment metrics (e.g., CLIP alignment) are considered unsuitable because they match each frame independently to the text prompt and therefore can only attend to static visual content; they cannot capture dynamic information that is only discernible by observing multiple frames. As a result, dynamics quality is instead assessed via a user study.",
      "source_document": "papers/2512.20000v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training object detectors on a large, multi-source pavement-distress benchmark to improve robustness to real-world capture variability, what image preprocessing/augmentation operations are applied, and how is class imbalance handled during training?",
      "answer": "Images are resized to a standardized 640\u00d7640 resolution while preserving aspect ratio, then augmented with random cropping, rotation, horizontal flipping, brightness/contrast adjustments, and added Gaussian noise; pixels are normalized to a [0,1] scale. To address class imbalance (some distress types appearing much more frequently), weighted sampling is used during training so rarer classes are seen more often.",
      "source_document": "papers/2512.20011v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When unifying multiple pavement-distress datasets that use different annotation formats and inconsistent class naming/ID conventions, what standardization procedure can be used to make the merged benchmark compatible across sources, and how should ambiguous or under-represented distress categories be handled to improve training stability?",
      "answer": "Use a dataset harmonization pipeline that (1) standardizes distress class names and assigns a single, consistent class-ID scheme across all sources, (2) keeps a consistent class mapping while supporting multiple annotation serializations (e.g., retaining Pascal VOC/XML, COCO/JSON, and YOLO/TXT in parallel so users can choose a format), and (3) resolves labeling inconsistencies by merging synonyms (e.g., \u201cAlligator\u201d vs \u201cAlligator Cracking\u201d), removing duplicates, and eliminating ambiguous/unwanted labels. To reduce imbalance and improve training stability, drop distress classes that have too few samples after merging.",
      "source_document": "papers/2512.20011v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking pavement-distress detectors across one-stage YOLO variants, a two-stage region-proposal model, and a transformer-based detector, what model-design features are emphasized as helping with (i) fine-grained/small defects and (ii) overlapping/complex road textures, and what qualitative class-level performance patterns emerge across the different architectures on the unified benchmark?",
      "answer": "Fine-grained/small-defect sensitivity is tied to stronger feature extraction and localization mechanisms: YOLOv9\u2019s programmable gradient backpropagation and GELAN are highlighted for improving detection of fine-grained cracks; YOLOv11\u2019s strengthened multi-scale feature aggregation (e.g., C3k2/SPPF) plus spatial attention is described as improving robustness to defect scale variation; and Faster R-CNN\u2019s two-stage design (RPN proposals followed by iterative box/class refinement with a ResNet-50 feature extractor) is characterized as particularly effective for small or densely packed defects such as microcracks and texture inconsistencies. For overlapping/complex road textures, DETR\u2019s self-attention and direct set prediction (eliminating the need for NMS) is emphasized as well-suited to complex textures and overlapping pavement defects.\n\nQualitatively in the benchmark results, different architectures excel on different distress types: YOLOv8 is described as strong on large, easily recognizable deformations like rutting and shoving; YOLOv9 shows higher recall on manhole and rutting; YOLOv10 is noted for balancing precision and recall consistently across classes; YOLOv11 attains particularly high precision on complex patterns such as alligator cracking; YOLOv12 is solid overall but has weaker recall on more irregular/subtle classes. Beyond YOLO, Faster R-CNN and DETR are described as competitive on manhole and rutting; Faster R-CNN is relatively balanced across defects, while DETR tends toward high recall for classes with distinct shape signatures but slightly lower precision on subtler categories such as bumps and sags.",
      "source_document": "papers/2512.20011v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In pavement-distress object-detection benchmarking, how can you report mean Average Precision in a way that reflects localization quality beyond a single IoU cutoff, and what IoU threshold range and step size define the stricter mAP variant compared with mAP@50?",
      "answer": "Use two mAP metrics: (1) mAP@50, which computes Average Precision at a fixed IoU threshold of 50%, and (2) mAP@50\u201395, which averages AP over multiple IoU thresholds from 50% to 95% in 5% increments, providing a stricter assessment of localization accuracy.",
      "source_document": "papers/2512.20011v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When merging multiple pavement-distress detection datasets into a single standardized benchmark, how can you quality-check the converted bounding-box annotations without manually reviewing every image, and what iterative steps help ensure the standardized labels remain faithful to the original annotations across distress types and collection conditions?",
      "answer": "Use a stratified sampling\u2013based validation rather than exhaustive review: select a representative subset of images stratified to cover different distress types, environmental conditions, and geographic locations. For the sampled images, visually overlay both the standardized (converted) annotations and the original-source annotations on the same images to check alignment. Any mismatches are corrected and then re-checked, repeating this overlay\u2013fix\u2013re-evaluate loop iteratively until the standardized annotations accurately match the true pavement conditions and stay consistent with the originals.",
      "source_document": "papers/2512.20011v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For a remote-sensing vision\u2013language segmentation model that generates masks from multiple [SEG] token queries, what unified training loss is used (name each component and what it supervises), and how is the spatial attention supervision term computed from the [SEG]-to-patch attention maps and a downsampled ground-truth mask to encourage sharper localization of small/part-level targets?",
      "answer": "The model is trained with a single objective\n\nL = Lt + Lb + Ld + \u03bbS LS,\n\nwhere (1) Lt is the standard autoregressive cross-entropy loss for text generation; (2) Lb is a per-pixel binary cross-entropy loss for mask supervision; (3) Ld is a DICE loss for mask supervision; and (4) LS is a spatial attention supervision loss applied to the model\u2019s internal attention, weighted by coefficient \u03bbS.\n\nTo compute LS, the model takes the attention maps from each transformer block and head corresponding to attention from the [SEG] token to image patch tokens, A(m,n) \u2208 R^{d\u00d7d}, and averages them across all M blocks and N heads to form a unified attention grid AS = (1/(MN)) \u03a3_m \u03a3_n A(m,n). Using a downsampled ground-truth mask \u011c \u2208 {0,1}^{d\u00d7d}, it computes the average background attention a = (\u03a3_{i,j} AS(i,j)\u00b7(1\u2212\u011c(i,j))) / (\u03a3_{i,j} (1\u2212\u011c(i,j))). The supervision then maximizes foreground\u2013background separation via\n\nLS = \u2212log( (\u03a3_{i,j} (AS(i,j) \u2212 a)^2 \u00b7 \u011c(i,j)) / (\u03a3_{i,j} \u011c(i,j)) ),\n\nwhich encourages the [SEG] attention to concentrate on the target region, providing a more localized learning signal that improves grounding/localization for challenging small or fine-grained parts.",
      "source_document": "papers/2512.20013v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In language-guided segmentation systems that generate multiple mask proposals per instruction, how does varying the number of segmentation queries k change the inference procedure (i.e., how are [SEG1]\u2026[SEGk] used and when is a matching step required), and what is the observed net effect of switching from a large k (propose-then-select) to k=1 on both efficiency (runtime/compute) and segmentation accuracy (gIoU)?",
      "answer": "When k>1, the model replaces the single [SEG] token with a sequence of k independent query tokens ([SEG1]\u2026[SEGk]), producing k proposal masks; a Mask2Former-style matching algorithm is then used to select one mask from the k candidates. When k=1, the model outputs a single mask directly and bypasses the matching step entirely. Moving from a large k (e.g., typical propose-then-select settings with many queries) to k=1 is reported to be both more efficient (lower inference time and computational cost by avoiding redundant proposals and matching) and to improve gIoU on the evaluated remote-sensing benchmark (RRSIS-D), consistent with the observation that matching can pick a non-optimal proposal.",
      "source_document": "papers/2512.20013v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For language-guided segmentation in remote-sensing imagery, what practical evidence supports using a hierarchical, multi-scale visual backbone paired with a Mask2Former-style pixel+transformer mask decoder instead of coupling a monolithic ViT-H backbone with a SAM/SAM2 decoder\u2014both in terms of (i) the resulting gIoU trends on a reasoning-oriented benchmark (EarthReason) and a referring benchmark (RefSegRS), and (ii) the architectural reason this choice better fits remote-sensing scale variation?",
      "answer": "Evidence comes from swapping the segmentation head/backbone combinations and evaluating on EarthReason and RefSegRS: the Swin-B (hierarchical, multi-scale) + Mask2Former (pixel decoder + transformer decoder) setup achieves the best gIoU on both benchmarks (EarthReason: 72.3 val / 73.5 test; RefSegRS: 82.4 val / 71.8 test), outperforming ViT-H paired with SAM (62.7 / 65.4 on EarthReason; 73.9 / 60.3 on RefSegRS) and ViT-H paired with SAM2 (64.3 / 64.9 on EarthReason; 74.2 / 64.2 on RefSegRS).\n\nThe architectural rationale given is that remote-sensing scenes contain targets across a wide range of scales, so a hierarchical backbone that captures multi-scale features is better suited than a monolithic vision transformer backbone; this better aligns with a Mask2Former-style decoder that fuses multi-scale features for mask prediction.",
      "source_document": "papers/2512.20013v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a language-guided remote-sensing segmentation model that pairs a pretrained MLLM with a Mask2Former-style segmentation head, what parameter-efficient fine-tuning recipe can be used to reduce GPU memory\u2014i.e., which visual components are kept frozen, which segmentation-head components are fully fine-tuned, and how is LoRA applied (where and with what rank)?",
      "answer": "To conserve GPU memory, all visual encoders are kept frozen (the hierarchical visual encoder/backbone). The LLM is fine-tuned using LoRA applied to the LLM with rank 8. The smaller Mask2Former segmentation-head components\u2014the Pixel Decoder and the Transformer Decoder\u2014are fully fine-tuned.",
      "source_document": "papers/2512.20013v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a remote-sensing segmentation dataset from noisy SAM/SAMRS masks, how can an automatic geometric-feature filter be set up to discard implausible masks for a category like \u201cairplane\u201d\u2014specifically, how is a \u201cgold standard\u201d reference set used to derive acceptable ranges, and which geometric shape properties are computed and thresholded to decide whether a candidate mask is retained?",
      "answer": "A practical filter is to (1) first remove obvious count errors by discarding samples whose number of connected components in the binary mask does not exactly match the number of associated bounding boxes (to eliminate fragmented or merged masks). Then (2) form a \u201cgold standard\u201d category reference set by manually selecting and verifying a small set of high-quality masks (e.g., 50 airplane masks). (3) Compute geometric properties on the primary connected component of each reference mask (implemented with OpenCV), summarize their distribution (e.g., mean/standard deviation), and use this to define an acceptable range per metric. (4) For every remaining candidate mask, compute the same properties and retain the mask only if all metrics fall within the predefined bounds. The filtered geometric properties include eccentricity, circularity (4\u03c0\u00b7Area/Perimeter\u00b2), solidity (area / convex-hull area), a bilateral-symmetry score, and extent (mask area / bounding-box area, to filter overly sparse masks).",
      "source_document": "papers/2512.20013v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a dual-camera distracted-driving classifier that uses naive early fusion by stacking the road-facing view with the driver-facing view into a single composite video, how do different spatiotemporal backbones (SlowOnly-R50, SlowFast-R50, and X3D-M) typically change in performance relative to a driver-only input, and what architectural reason explains why SlowFast can degrade while SlowOnly improves?",
      "answer": "With stacked dual-view (early-fusion) inputs, SlowOnly-R50 improves noticeably over its driver-only version, X3D-M changes very little (slightly worse), and SlowFast-R50 drops in performance compared to driver-only. The explanation given is architectural: SlowOnly\u2019s single-pathway design and simplified temporal modeling have capacity to exploit the additional spatial/contextual cues from the road-facing view without internal conflicts, whereas SlowFast\u2019s dual-pathway design\u2014especially the fast pathway tuned for fine-grained driver-motion cues\u2014can be disrupted by the extra motion patterns in the road scene, creating representational interference/conflicting signals when the two views are simply stacked rather than fused with a dedicated mechanism. X3D-M is comparatively robust to the input change due to its progressive, multi-axis optimization design.",
      "source_document": "papers/2512.20025v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When fine-tuning Kinetics-pretrained spatiotemporal backbones for a 6-class distracted-driving classifier under driver-only vs stacked dual-view inputs, what unified training protocol (optimizer choice, objective/loss, model-head modification/initialization, and checkpoint selection strategy) can be used to ensure a controlled, apples-to-apples comparison across architectures?",
      "answer": "Use the same supervised fine-tuning setup for all backbones: replace the final fully connected classifier with a 6-way output head, initialize the network from Kinetics-400 pre-trained weights, and train with cross-entropy loss using the Adam optimizer (with a fixed learning rate and batch size). Train for up to a fixed maximum number of epochs and apply early stopping based on validation accuracy; keep the checkpoint that achieves the best validation performance for final test evaluation. This shared protocol keeps differences attributable to the backbone and input modality rather than differing optimization settings.",
      "source_document": "papers/2512.20025v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When adapting a SlowOnly (single-pathway) video backbone with sparse temporal sampling for distracted-driving classification, how should temporal convolutions be allocated across network stages, and what failure mode does this design choice mitigate when the temporal stride is large and motion is fast?",
      "answer": "Use essentially 2D (non-temporal) convolutions in the early stages (conv1 through res3), and introduce non-degenerate temporal convolutions only in the later stages (res4 and res5). This placement is motivated by the observation that adding temporal convolutions too early can degrade accuracy when objects move quickly under a large temporal stride; deeper layers have a larger spatial receptive field that supports more reliable temporal correlation modeling.",
      "source_document": "papers/2512.20025v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a patient-specific multimodal diagnosis pipeline that builds multiple \u201cactivation graphs\u201d from disentangled semantic subspaces, how are the per-dimension graphs constructed\u2014specifically, what defines the nodes, how are edges chosen, and how are edge weights computed from semantic feature-influence scores?",
      "answer": "Each activation graph corresponds to one learned semantic dimension m. All graphs share the same node set V with C nodes, where each node represents one feature dimension of the concatenated (compressed) multimodal feature vector x\u2208R^C. For a given semantic dimension m, the model first selects \u201cactivated\u201d features as those with the highest influence scores Cm(i) (computed by perturbing/ablating feature i and measuring the change in the discriminator\u2019s m-th output). The edge set Em is then formed by connecting each activated node to its k-nearest activated neighbors using Euclidean distance in the initial feature space. Edge weights are defined from feature importance as the average influence of the two endpoints: w_ij^(m) = 1/2 \u00b7 (Cm(i) + Cm(j)).",
      "source_document": "papers/2512.20026v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a two-stage multimodal diagnosis GNN that (i) learns disentangled semantic dimensions to build multiple patient-specific activation graphs and (ii) performs intra-graph encoding followed by inter-patient classification on a global graph, what composite training objective is used to jointly train the whole system end-to-end, and what does each loss term supervise or regularize (including how semantic disentanglement and representation fidelity are enforced)?",
      "answer": "The model is trained end-to-end with a weighted sum of three losses: L = \u03bb_cls L_cls + \u03bb_rep L_rep + \u03bb_sd L_sd. \n\u2022 L_cls is the primary task loss: standard cross-entropy between ground-truth patient labels Y and predictions \u0176 from the final classifier operating on the inter-patient (global) graph.\n\u2022 L_rep regularizes the planar (activation-graph) encoders to keep graph embeddings informative by reconstructing the initial node features from the final node embeddings: a mean-squared reconstruction error between each graph\u2019s initial node features and a decoder output applied to the encoder\u2019s final-layer embeddings, averaged over patients and activation graphs.\n\u2022 L_sd trains the multi-dimensional feature discriminator to produce robust, disentangled semantic dimensions: it combines a reconstruction loss for the discriminator\u2019s autoencoding (L_AE) with a regularizer L_reg on discriminator weights that includes L1 and L2 penalties plus an orthogonality constraint on linear-layer weights (to encourage semantic independence across dimensions).",
      "source_document": "papers/2512.20026v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multimodal diagnosis GNN that builds patient-specific activation graphs by selecting \u201cactivated\u201d features per semantic dimension based on feature-influence scores from perturbations, what hyperparameter controls the sparsity of each activation graph, what trade-off does it impose, and which activation proportion and perturbation strategy were found to work best for computing those influence scores?",
      "answer": "Graph sparsity is controlled by the Proportion of Activated Features (PAF), i.e., the fraction of top-influence features retained as \u201cactivated\u201d per semantic dimension. Increasing PAF makes graphs denser (retains more signal but also more noise), while decreasing PAF yields sparser graphs (filters noise but can discard useful information). Empirically, selecting 5% of features (PAF = 0.05) gave the best balance. For computing influence scores via feature perturbation, a zeroing-out (mask-to-zero) strategy outperformed alternatives like setting features to 1 or halving them, giving the strongest diagnostic performance.",
      "source_document": "papers/2512.20026v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a hierarchical multimodal diagnosis GNN that first builds multiple patient-specific \u201cactivation graphs\u201d and then performs intra-sample fusion before inter-sample classification, how are the multiple activation graphs encoded into a single patient vector for the global (inter-patient) graph\u2014i.e., what graph encoder and readout are used per activation graph, how do predefined sparse edges and edge weights interact with learned attention during message passing, and how are the per-graph embeddings combined with the original patient feature vector for downstream GCN classification?",
      "answer": "Each activation graph is fed to a planar graph encoder implemented with a Graph Attention Network (GAT), producing node embeddings whose neighbor aggregation uses learned attention coefficients but only over the predefined sparse edge set (so attention is constrained to a semantically meaningful subset of neighbors). The learned attention coefficients are additionally modulated by the predefined edge weights w^(m)_{ij}, so both learned patterns and prior feature-importance weights guide aggregation. A graph-level embedding g_m is then obtained by a readout that averages the final-layer node representations. For inter-sample reasoning, all M graph embeddings are concatenated together with the original patient feature vector x_p to form the patient representation F_p = Concat(g_1,\u2026,g_M,x_p), which becomes the node feature for that patient in the global Fusion-Relation Graph processed by a GCN and an MLP classifier.",
      "source_document": "papers/2512.20026v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When a multimodal diagnosis GNN is decomposed into (i) a feature-salience module that projects raw multimodal features into disentangled dimensions, (ii) a module that uses those salience scores to build multiple activation graphs, and (iii) a hierarchical fusion module that performs intra-sample and inter-sample reasoning, what do cross-dataset ablation outcomes reveal about which module is most critical for (a) multi-parametric MRI classification versus (b) heterogeneous CT + structured clinical diagnosis, and what does this imply about the dominant bottleneck in each modality setting?",
      "answer": "Ablations show that the \u201cmost critical\u201d module depends on the modality mix: for the mpMRI task (PI-CAI), removing the multi-activation graph construction module causes the largest accuracy degradation, indicating that modeling diverse inter-feature relationships (via multiple, semantically distinct graphs) is the dominant bottleneck for mpMRI fusion. For the heterogeneous CT + clinical task (CHD), removing the feature-discriminator/salience module is most detrimental, implying that filtering/selecting salient features and producing a more discriminative embedding is the primary bottleneck when modalities are structurally and semantically heterogeneous. In both datasets, the full model achieves the best overall accuracy and F1, supporting that the modules are complementary and work synergistically rather than any single component being sufficient.",
      "source_document": "papers/2512.20026v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a hyperbolic compositional zero-shot learning setup that aligns image embeddings with text embeddings for state\u2013object compositions, how can a discriminative alignment objective incorporate hard-negative mining to better separate semantically ambiguous compositions (e.g., those sharing a state or an object), and what exact set of hard negatives is used and how are they weighted in the contrastive loss?",
      "answer": "Hard negatives are defined as compositions that share at least one primitive (state or object) with the positive composition, so for a training instance with composition (s_i,o_i) the hard-negative set is H_i = { (s_j,o_k) \u2208 C | (s_j = s_i) OR (o_k = o_i) } \\ {(s_i,o_i)}. The discriminative alignment loss is a weighted InfoNCE-style contrastive objective over hyperbolic (Lorentz) geodesic distances between the image composition embedding v_c^L and the refined text composition embedding t_{c_i}^{\\prime L}: it uses D_{c}=d_L(v_c^L,t_{c}^{\\prime L})/\u03c4, and computes L_DA = \u2212log( exp(D_{c_i}) / ( \u2211_{c_k\u2208C\\H_i} exp(D_{c_k}) + w \u2211_{c_j\u2208H_i} exp(D_{c_j}) ) ), where w>1 up-weights the penalty for hard negatives. This forces larger geodesic separation for compositions that are semantically similar because they share a primitive, improving fine-grained discrimination.",
      "source_document": "papers/2512.20029v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a CLIP-based compositional zero-shot recognition model that embeds images and text in Lorentz hyperbolic space, how can an entailment-cone regularizer be constructed to simultaneously encode (i) conceptual hierarchy between a composition and its primitives and (ii) semantic hierarchy between a primitive and its hypernym parent? Specify the cone-violation penalty used for a parent\u2013child pair and which parent\u2013child pairs (across visual and text branches) are summed to form the overall hierarchy loss.",
      "answer": "Use a taxonomic entailment loss built from hyperbolic entailment cones: for each directed parent\u2013child edge (p,q) in a predefined taxonomy, penalize the child if it falls outside the parent\u2019s cone via\nLe(p,q)=max(0, \u03c0 \u2212 \u03b1(p,q) \u2212 \u03c9(p)),\nwhere \u03b1(p,q)=\u22200pq is the exterior angle of q w.r.t. the cone at p (computed from Lorentz inner products / space-like norms) and \u03c9(p) is the cone aperture (set as \u03c9(p)=arcsin(2\u03b3/(\u221a\u03ba\u2016p\u0303\u2016)) with boundary constant \u03b3=0.1 and learnable curvature \u03ba). The overall hierarchy loss sums cone penalties for both hierarchy types and both modalities:\n\u2022 Conceptual hierarchy (composition \u2282 primitive): Le(v_s^L, v_c^L)+Le(v_o^L, v_c^L) on the visual embeddings, and Le(t\u2032_s^L, t\u2032_c^L)+Le(t\u2032_o^L, t\u2032_c^L) on the refined text embeddings.\n\u2022 Semantic hierarchy (primitive \u2282 hypernym parent): Le(t\u2032_{sp}^L, t\u2032_s^L)+Le(t\u2032_{op}^L, t\u2032_o^L) on the refined text embeddings.\nThese terms together form the taxonomic entailment loss that enforces the dual hierarchy in hyperbolic space.",
      "source_document": "papers/2512.20029v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a CLIP-based compositional zero-shot recognition system that projects both image and prompt embeddings into the Lorentz model of hyperbolic space, how can the text embedding for a class (state/object/composition) be refined in an instance-aware way using a residual hyperbolic cross-modal attention module while keeping all operations on-manifold? Specify (i) the residual update rule in hyperbolic space, (ii) how multi-head hyperbolic *linear* attention is computed from the space-like components (including the feature map used), and (iii) how the time-like component of the attended output is reconstructed to satisfy the Lorentz constraint.",
      "answer": "The text embedding is first mapped to the Lorentz manifold via the exponential map at the origin, and then refined with a residual Hyperbolic Cross-Modal Attention (HCA) update that uses M\u00f6bius addition to stay on-manifold.\n\n(i) Residual refinement (for any text embedding t^L, using image patch tokens V^L_patch as context):\n\n\u2022 t\u2032^L = t^L \u2295_\u03ba \u03bb \u00b7 HCA(t^L, V^L_patch, V^L_patch)\n\nwhere \u2295_\u03ba is M\u00f6bius addition in curvature \u03ba and \u03bb is a learnable fusion scalar.\n\n(ii) HCA definition and hyperbolic linear attention on space-like components:\n\n\u2022 HCA(q^L, K^L, V^L) = q^L \u2295_\u03ba FFN_L( MHA_L(q^L, K^L, V^L) )\n\nMHA_L applies a hyperbolic linear attention mechanism by operating on the space-like parts (denoted with tildes). For head h, the output space-like component is:\n\n\u2022 \\tilde Z_h = \u03d5(\\tilde Q_h) ( \u03d5(\\tilde K_h)^T \\tilde V_h ) / ( \u03d5(\\tilde Q_h) ( \u03d5(\\tilde K_h)^T 1 ) )\n\nwhere 1 is an all-ones vector. The feature map is:\n\n\u2022 \u03d5(x) = ( \\|\\bar x\\| / \\|\\bar x^p\\| ) \\bar x^p,  with  \\bar x = ReLU(x)/t\n\np is a power parameter and t is a learnable scaling factor.\n\n(iii) Reconstruction of the time-like component to satisfy the Lorentz constraint:\n\nAfter computing \\tilde Z_h, the full Lorentz vector for the head is rebuilt as Z^L_h = [Z_{h,0}, \\tilde Z_h], with the time-like coordinate set to\n\n\u2022 Z_{h,0} = sqrt( \\|\\tilde Z_h\\|^2 + 1/\u03ba )\n\nwhich ensures the output lies on the Lorentz manifold. The FFN_L uses hyperbolic linear layers of the form HFC(x) = [ sqrt(\\|Wx+b\\|^2 + 1/\u03ba), Wx+b ] followed by a nonlinearity, preserving on-manifold representations throughout.",
      "source_document": "papers/2512.20029v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a two-stage Mandarin visual speech recognition pipeline that uses an LLM as a second-stage corrector, how can the LLM be adapted so it learns to fix the *specific* systematic errors produced by the visual model (rather than assuming clean phonetic input), and what information should the LLM be conditioned on at inference to resolve homophone/viseme ambiguities?",
      "answer": "Adapt the LLM by building an error-aware instruction dataset from the VSR model\u2019s own imperfect outputs: save multiple VSR checkpoints from different training stages, run them to decode the *training* videos, and use the resulting hypotheses (with varying error rates) to form instruction\u2013response pairs where the input contains (i) a possibly wrong predicted Pinyin sequence and (ii) multiple candidate character hypotheses (an N-best list), while the response is the correct character transcription. Then fine-tune the LLM efficiently with LoRA using an autoregressive language-modeling objective so it learns the VSR model\u2019s recurring error patterns. At inference, condition the LLM on the predicted Pinyin sequence together with the character decoder\u2019s N-best candidate transcriptions to rescore/refine and choose the most probable final character sequence, using the explicit phonetic cue plus linguistic context to correct homophone-induced errors.",
      "source_document": "papers/2512.20032v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a Mandarin lip-reading system that predicts both Chinese characters and (toneless) Pinyin from the same video encoder, how can you formulate the dual-decoder learning objective (including how sequence alignment is handled), and what do ablation results typically show about (i) the benefit of adding the Pinyin auxiliary decoder and (ii) the difference between using an LLM refiner zero-shot versus fine-tuning it on model-generated VSR errors?",
      "answer": "A shared visual frontend+encoder feeds two decoders (character and Pinyin). Each decoder is trained with a hybrid sequence objective that combines Connectionist Temporal Classification (CTC) to handle unknown frame-to-token alignment and a cross-entropy (CE) term: L_char = \u03bb_ctc L_char_ctc + (1\u2212\u03bb_ctc) L_char_ce, and analogously L_py = \u03bb_ctc L_py_ctc + (1\u2212\u03bb_ctc) L_py_ce. The overall multi-task loss is a weighted sum of the two decoder losses: L_total = \u03b1 L_char + (1\u2212\u03b1) L_py (with \u03b1=0.5 in the experiments). Ablations show (i) adding the Pinyin decoder improves CER versus a character-only decoder, indicating the phonetic auxiliary task strengthens visual\u2013semantic representations, and (ii) an LLM refiner used zero-shot can underperform the no-LLM setting, while fine-tuning the LLM on instruction data built from the VSR model\u2019s own checkpoint-generated errors yields a clear CER reduction\u2014highlighting that error-aware LLM adaptation is important.",
      "source_document": "papers/2512.20032v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a Mandarin visual speech recognition system for both multi-speaker generalization and real-world domain shift, what evaluation protocol is used (datasets and metric), and what qualitative performance trend should you expect when you (i) increase the amount/diversity of CNVSRC training data (S1\u2192S2\u2192S3), and (ii) replace a character-only decoder with the dual-decoder (character+toneless Pinyin) plus LLM refinement\u2014especially on the more challenging self-collected test set?",
      "answer": "Evaluation uses two testbeds: (1) the CNVSRC multi-speaker evaluation set (sentence-level silent lip-reading with 43 speakers from both studio and Internet videos), and (2) a harder self-collected Mandarin lip-reading dataset with more speakers and higher variability in environment, pose, illumination, and speaking style to induce stronger domain shift. Performance is measured by character error rate (CER), computed from substitutions, deletions, and insertions normalized by the number of ground-truth characters.\n\nAs training data are expanded from S1 to S2 to S3 by progressively adding CNVSRC subsets (starting from CN-CVS+CNVSRC.Dev, then adding CN-CVS2-P1, then CN-CVS3), CER decreases\u2014showing the expected benefit of more/diversified training. Replacing a character-only decoder with the dual-decoder that also predicts toneless Pinyin, and then refining with an LLM conditioned on the predicted Pinyin plus an N-best list of character hypotheses, further reduces CER. The improvement is reported to be more pronounced on the self-collected dataset, indicating that the phonetic constraint from Pinyin and the language-model refinement improve robustness under complex, domain-shifted visual conditions.",
      "source_document": "papers/2512.20032v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a reconstruction-based latent-space lip-sync editor that is initially trained with a mouth-region mask, what self-supervised fine-tuning procedure can remove the need for an explicit mouth mask at inference while still keeping edits localized to the lips, and how are the training pairs and model initialization constructed for this refinement?",
      "answer": "After the reconstruction editor converges, lip-pose vectors are sampled and the editor is used to synthesize mouth-altered variants of the original frames. These synthesized variants are then treated as pseudo ground truth to form symmetric pseudo-pairs in both directions (source \u2192 changed) and (changed \u2192 source). A dedicated LipsChange network is initialized from the reconstruction model\u2019s weights and fine-tuned on these symmetric pseudo-pairs, which teaches it to localize modifications to the lips while preserving surrounding regions, enabling mask-free inference without external segmentation.",
      "source_document": "papers/2512.20033v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an audio-driven lip-sync pipeline that predicts a low-dimensional mouth-pose latent from speech (to drive a separate visual editor), how can the audio-to-pose transformer be trained with optimal-transport conditional flow matching\u2014specifically, how are the noisy/interpolated latent z_t and target velocity u constructed from a Gaussian sample and the ground-truth lip latent, what regression loss is optimized, and what conditioning signals are concatenated into the transformer input?",
      "answer": "The audio-to-lips module uses optimal-transport conditional flow matching in the lip-latent space. For each training example, it samples noise \u03f5 ~ N(0, I) and a time t ~ U(0,1), forms an interpolated latent z_t = (1\u2212t)\u00b7\u03f5 + t\u00b7z_lips, and defines the target velocity u = z_lips \u2212 \u03f5. A transformer v_\u03b8 is trained to regress the velocity field by minimizing an L2 loss L_FM = E_{t,\u03f5,a} ||v_\u03b8(z_t, t, c) \u2212 u||_2^2, where the conditioning c concatenates (i) frame-aligned wav2vec 2.0 audio features a, (ii) an emotion embedding e(a) from a pre-trained audio emotion encoder, and (iii) K randomly sampled source lip latents z_lips^K (reference lip latents).",
      "source_document": "papers/2512.20033v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a one-step VAE-latent lip-sync editor using reconstruction objectives (no GAN/diffusion), what set of loss terms are combined to (i) match the target latent, (ii) preserve the non-mouth regions, and (iii) sharpen/retain identity in pixel space\u2014and how do the lower-face mask and the lips mask gate where each term is applied?",
      "answer": "The editor is trained with a weighted sum of latent-space regression plus masked pixel/perceptual losses.\n\n\u2022 Latent space (match the target latent): use an unmasked L1/MAE over the latent residual \u0394z = z\u0302_target \u2212 z_target, and a second MAE term masked by the downsampled lower-face latent mask m:\n  \u2013 L_lat,L1 = MAE(\u0394z)\n  \u2013 L_lat,L1m = MAE_m(\u0394z)\n\n\u2022 Pixel space (preserve non-mouth regions and localize edits): apply an MAE on the reconstructed RGB frame, but only over the lower-face pixel mask M:\n  \u2013 L_pix,L1M = MAE_M(x\u0302_src \u2212 x_src)\n\n\u2022 Pixel space lips-focused term (extra pressure on accurate mouth details): apply an MAE over a lips-only mask M_lips from face parsing, but only if a valid lips mask is detected and its area exceeds a threshold \u03c4_lips:\n  \u2013 L_pix,L1lips = 1{|\u03a9_lips| \u2265 \u03c4_lips} \u00b7 MAE_{M_lips}(x\u0302_src \u2212 x_src)\n\n\u2022 Perceptual/identity sharpening in pixel space: add (a) a VGG-19 feature loss and (b) a VGGFace2-pretrained feature loss:\n  \u2013 L_VGG = \u03a3_l MAE(\u03d5_l(x\u0302_src) \u2212 \u03d5_l(x_src))\n  \u2013 L_faceVGG = \u03a3_l MAE(\u03c8_l(x\u0302_src) \u2212 \u03c8_l(x_src))\n\nThese terms are summed with small weights on the latent losses and larger weights on the masked pixel, lips, and perceptual/face-perceptual losses; the lower-face mask restricts where reconstruction is enforced, while the lips mask further concentrates supervision on the mouth when it is reliable/large enough.",
      "source_document": "papers/2512.20033v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When designing a low-dimensional lips-pose control vector for a two-stage lip-sync system (image editor driven by an audio-to-pose model), how do the ablations compare a frozen expression-encoder-only variant versus adding a residual predicted from a mouth crop in terms of (i) reconstruction fidelity and (ii) identity/appearance leakage in the cross-audio setting\u2014and what final lips-vector composition is selected to balance quality with disentanglement, including how leakage is tested?",
      "answer": "Two variants are contrasted: (V1) using only a frozen expression encoder followed by a small projection head, and (V2) augmenting V1 with an additional residual predicted from a mouth crop (obtained via face parsing) by a lightweight CNN/ResNet.\n\nAblation findings:\n- V1 already performs strongly with low identity/appearance leakage, and its reconstruction gains largely saturate once the lips vector reaches about 8 dimensions.\n- Adding the mouth-crop residual (V2) progressively improves reconstruction quality, but it hurts disentanglement: identity leakage increases, reflected as worse identity preservation in the cross-audio setting as the residual grows (especially when the residual reaches larger dimensionalities).\n\nLeakage is explicitly probed by substituting lips-pose vectors across different identities; increased leakage manifests when swapping poses also transfers identity/appearance cues.\n\nFinal choice:\n- A 12D lips vector composed of an 8D main component from the expression-encoder pathway plus a 4D mouth-crop residual is selected as the best quality\u2013disentanglement trade-off (high reconstruction quality while keeping identity leakage relatively low), which also simplifies Stage 2 (audio cannot predict appearance) and improves pseudo-pair generation for mask-free self-refinement.",
      "source_document": "papers/2512.20033v1.pdf",
      "mode": "textual",
      "content_refs": [
        "lines 995-1020",
        "lines 1002-1006",
        "lines 1007-1019",
        "lines 944-947"
      ]
    },
    {
      "question": "In an audio-to-lip-pose transformer that conditions on a set of reference lip-pose vectors (e.g., sampled from reference frames) to help preserve identity, what trade-off appears as you increase the number of reference vectors, and what rationale supports selecting a moderate number of references rather than using many?",
      "answer": "Increasing the number of reference lip\u2011pose vectors from 1 to 4 improves identity preservation while having negligible effect on lip\u2013audio synchronization. Beyond 4, the gains become marginal and synchronization can occasionally degrade. The proposed explanation is that supplying too many references injects excessive identity information, reducing how much the audio-to-lips network relies on audio and making it more sensitive when the reference vectors come from a different clip at inference. As a result, a moderate setting of 4 reference frames/vectors is used.",
      "source_document": "papers/2512.20033v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a hybrid probabilistic+deterministic cross-view geolocalization pipeline for disaster imagery, how can you quantify how \u201ceasy\u201d an individual ground photo is to geolocalize from the probabilistic model\u2019s predicted distribution, how is this quantity estimated in practice, and how is the probabilistic output then used (via a radius threshold) to guide the subsequent retrieval-based geolocalization stage?",
      "answer": "The pipeline defines a per-image \u201clocalizability score\u201d from the probabilistic geolocalization output as the negative entropy of the predicted location distribution on the sphere: \n\nLocalizability(c) = \u222b_{S^2} p(x|c) log2 p(x|c) dx,\n\nso higher scores correspond to a more spatially concentrated (less ambiguous) predicted distribution. In practice, the integral is approximated with Monte-Carlo sampling using 10,000 samples.\n\nAfter this probabilistic step, the method applies a deterministic cross-view retrieval stage, but only within a radius threshold r: the probabilistic distribution/predicted location is used to narrow the area of interest (AOI) for retrieval, balancing the probabilistic model\u2019s uncertainty-aware, coarser localization with the retrieval model\u2019s finer-grained accuracy (at additional compute). The threshold r controls how much the retrieval stage relies on/considers the probabilistic predictions.",
      "source_document": "papers/2512.20056v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a cross-view image retrieval geolocalization pipeline, how can an anchor-based reranking step refine the initial cosine-similarity ranking, and how is the reranking score computed from (i) query\u2013candidate similarity and (ii) anchor\u2013candidate consistency\u2014including what the balance parameter controls?",
      "answer": "After the initial retrieval ranks satellite (RSI) candidates by cosine similarity between the query embedding and each candidate embedding, reranking selects the top-k retrieved items as \u201canchors\u201d (often including the query and top results) and rescoring each candidate to enforce local neighborhood consistency. The base similarity is the cosine score S(q,x)= (q\u00b7x)/(||q||\u00b7||x||). For each candidate x, the reranking score combines its similarity to the query with its average similarity to the anchors: R(x)= \u03b1\u00b7S(q,x) + (1\u2212\u03b1)\u00b7(1/k)\u00b7\u2211_{j=1..k} S(a_j,x). The parameter \u03b1\u2208[0,1] balances how much weight is given to direct query\u2013candidate similarity versus anchor\u2013candidate consistency; increasing anchor influence mitigates mismatches in noisy, high-variation scenes by favoring candidates consistent with the top-ranked neighborhood.",
      "source_document": "papers/2512.20056v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a generative geolocalization model that predicts latitude/longitude as samples on the Earth\u2019s sphere (S^2) using flow matching, how can the forward noising process be formulated so that all intermediate noisy locations stay on the spherical manifold, and what loss is minimized to learn the time-dependent velocity field on the sphere (i.e., the Riemannian flow-matching objective)?",
      "answer": "To keep the diffusion/flow trajectory on the sphere S^2, the noisy sample at time t is generated via Riemannian exponential/log maps (i.e., moving along a geodesic on the manifold) rather than Euclidean interpolation:\n\n\u2022 Forward/noising step on S^2:\n  x_t = exp_{x_{t\u22121}}( \u03b2(t) \u00b7 log_{x_{t\u22121}}(\u03b5) ),\n  where log_{x_{t\u22121}} maps a point on S^2 to the tangent space at x_{t\u22121} and exp_{x_{t\u22121}} maps the tangent vector back to S^2. This construction ensures both the noisy location x_t and the noise \u03b5 remain on S^2.\n\n\u2022 The corresponding target velocity field is expressed on the tangent space (pointing along the geodesic):\n  v(x_t) = d x_t / d t = (d\u03b2(t)/dt) \u00b7 log_{x_t}(x_0).\n\n\u2022 Riemannian flow-matching training objective:\n  L_RFM = E_{x0,c,\u03b5,t} [ || \u03c6(x_t | c) \u2212 v(x_t) ||^2_{x_t} ],\n  i.e., minimize the squared error between the network-predicted velocity field \u03c6(x_t|c) and the analytic target v(x_t), using the norm induced by the Riemannian metric on the tangent space at x_t (the only change vs Euclidean flow matching).",
      "source_document": "papers/2512.20056v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a deterministic cross-view geolocalization (image-retrieval) module that embeds ground (VGI) images and overhead (RSI) images into a shared feature space with a Siamese ViT backbone, how should the two branches be parameterized across the two modalities, and how is the contrastive InfoNCE training objective defined in terms of (i) how positives and negatives are sampled and (ii) the probability of selecting the positive sample that the loss minimizes?",
      "answer": "The two branches use the same ViT-DINOv2 encoder architecture in a Siamese setup, but with separate (independent) weights for each modality (VGI vs RSI) so each branch can handle modality-specific image characteristics (e.g., different image sizes), while still producing embeddings in a common cross-view space.\n\nTraining uses a contrastive InfoNCE objective: for a given context vector c, a positive sample x_pos is drawn from the conditional distribution p(x|c), while the (N\u22121) negatives {x_j} are drawn from the marginal p(x) without conditioning. Using a similarity/scoring function f(x,c) (proportional to P(x|c)/P(x)), the probability of selecting the positive among a batch is\nP(C=pos|X,c) = f(x_pos,c) / ( f(x_pos,c) + \\sum_{j=1}^{N-1} f(x_j,c) ).\nThe InfoNCE loss is L_InfoNCE = \u2212E[ log P(C=pos|X,c) ], i.e., it minimizes the negative log-probability of choosing the positive sample.",
      "source_document": "papers/2512.20056v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a teacher\u2013student self-supervised pretraining setup (DINOv2) used to initialize a ViT encoder for cross-view geolocalization, what training loss is optimized between the teacher and student outputs, and what representation property does this objective enforce when the two networks see different augmented views of the same image?",
      "answer": "The encoder is pretrained with a DINO cross-view consistency objective that minimizes the cross-entropy between the teacher\u2019s and student\u2019s output distributions over features: \\(\\mathcal{L}_{\\text{DINO}}=-\\sum_{i=1}^{N} P^{(t)}(x_i)\\,\\log P^{(s)}(x_i)\\), where \\(P^{(t)}(x_i)\\) and \\(P^{(s)}(x_i)\\) are the teacher and student predicted distributions for the i-th feature and \\(N\\) is the number of feature dimensions. This trains the student to match (align to) the teacher across varying augmentations, yielding stable, semantically consistent, domain-invariant representations for different augmented views of the same image.",
      "source_document": "papers/2512.20056v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For a progressive image codec optimized for classification, how can an adaptive decoding controller be trained to decide the earliest progressive reconstruction level to stop decoding while maintaining a target prediction reliability, and what specific confidence/logit-derived features and training data generation strategies are used (including both codec-based and codec-agnostic augmentations)?",
      "answer": "The controller trains a lightweight \u201csufficiency\u201d predictor g that estimates whether the downstream classifier will be correct at a given progressive cutoff. Training data are generated by taking a set of images (e.g., 1k ImageNet training images) and compressing/decoding them at each progressive level \u2113\u2208{1,\u2026,L} to obtain reconstructions x\u0302(\u2113). For each reconstruction, the downstream classifier produces logits z=f(x\u0302(\u2113)) and a predicted label \u0177=argmaxk zk; a binary target is assigned as s=1[\u0177=y] (correct vs. incorrect). The input to g is a 12\u2011D \u201cconfidence profile\u201d feature vector extracted from the classifier outputs, combining (i) softmax-based statistics such as top\u20111 probability (max confidence), entropy, and top\u20111/top\u20112 probability ratio; (ii) logit-based statistics such as mean, max, standard deviation, and the top\u20111/top\u20112 logit margin; and (iii) an energy-based uncertainty signal. A logistic-regression model is then trained to output g(\u03d5)=Pr(s=1|\u03d5).\n\nAt inference time, decoding starts from the lowest level (\u2113=1) and proceeds progressively; at each level the controller computes a predicted correctness probability p(\u2113)=g(\u03d5(\u2113)) and stops decoding as soon as p(\u2113)\u2265\u03c4 for a user-chosen confidence threshold \u03c4, otherwise it continues to \u2113+1.\n\nTo avoid needing codec-generated multi-quality samples for training g, codec-agnostic alternatives are also considered: (1) noise-based augmentation by adding Gaussian noise with \u03c3\u2208{0.05,0.1,0.15,0.2,0.3} to original images, and (2) blur-based augmentation by downsampling by scales s\u2208{1.2,1.5,2.0,3.0} followed by bilinear upsampling.",
      "source_document": "papers/2512.20070v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a progressive learned image codec that uses trit-plane coding, what constraints does progressive transmission impose on latent-symbol prioritization, what are the two decoder-feasible prioritization scores used in prior progressive codecs (including how they trade off expected distortion reduction vs. coding cost), and what does a controlled comparison against task-driven \u201cpseudo-optimal\u201d orderings (channel-wise or patch-wise based on downstream confidence gains) reveal about how much intra-plane ordering actually affects downstream classification performance?",
      "answer": "Progressive transmission requires choosing a symbol (or coefficient) ordering that can be computed from information available at the decoder during incremental decoding\u2014i.e., it must be based on decoder-accessible parameters such as the hyperprior-derived mean/scale (M, \u03a3 from \\hat Z), rather than encoder-only values like the unquantized latent Y.\n\nTwo decoder-feasible, commonly used prioritization scores are:\n1) Expected-variance (expected distortion) based sorting: for a coefficient c after planes 1..i are decoded, define the current expected distortion as the conditional variance D_i^c = Var_{p_i}(\\hat y_c). After sending the next trit, the expected distortion becomes D_{i+1}^c = \\sum_{d\\in\\{0,1,2\\}} p_i(d)\\, D_{i+1}^{c,d}. The priority score is the (negative) rate\u2013distortion trade-off\n\u03bb_{c,i} = \u2212(D_{i+1}^c \u2212 D_i^c) / H(p_i),\nso symbols with larger variance/MSE reduction per expected bit (entropy H(p_i)) are transmitted earlier (but this is computationally expensive because it needs conditional distributions).\n2) Sigma-based sorting: an efficient surrogate that orders by the hyperprior scale, using\n\u03bb_{c,i} = \\hat \u03c3_c / H(p_i),\nmotivated by larger scales implying wider distributions and more potential distortion reduction.\n\nWhen these are compared in a controlled study against task-driven \u201cpseudo-optimal\u201d orderings\u2014optimal-channel (group by channel and rank by improvements in downstream confidence such as cross-entropy) and optimal-patch (rank spatial patches by confidence gain), plus random ordering\u2014no single ordering consistently dominates across the bitrate range. The results indicate that the dominant factor for downstream performance is largely how many trits/planes are transmitted (i.e., improving coefficient precision), and intra-plane ordering has limited, non-uniform impact; consequently, existing decoder-feasible prioritization methods already capture much of the practical benefit for machine-oriented progressive coding.",
      "source_document": "papers/2512.20070v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a hyperprior-based learned image codec that uses progressive trit-plane coding, how can the decoder-determined hyperprior scale parameter be used to allocate (per latent coefficient) how many trit-planes will be transmitted, and how is each quantized latent coefficient converted into ternary digits for plane-wise entropy coding and progressive reconstruction?",
      "answer": "Plane-length allocation uses the hyperprior-predicted scale per coefficient (\u03c3\u0302c) to estimate an effective coefficient range tailc = 2\u03ba\u03c3\u0302c (\u03ba is a constant from the inverse CDF of a standard Gaussian). The required ternary digit/plane length is then Lc = \u2308log3(tailc)\u2309 with a minimum constraint Lc \u2265 1, and the bitstream supports up to Lmax = maxc Lc across locations.\n\nFor ternary decomposition, each mean-removed, uniformly quantized coefficient \u0177c is shifted to a non-negative integer index sc = round(\u0177c) + \u230a3^{Lc}/2\u230b and expressed in base-3 as sc = \u2211_{\u2113=1}^{Lc} d_{c,\u2113} \u00b7 3^{Lc\u2212\u2113} with trits d_{c,\u2113} \u2208 {0,1,2}. Entropy coding proceeds plane-by-plane from early to later planes using probability models derived from the Gaussian scale (precomputed PMFs over 3^i symbols for digit-length i); each plane is coded with conditional PMFs that marginalize over lower planes and refines the interval after each decoded plane. The decoder mirrors this process to progressively reconstruct \u0177c from the decoded trits, producing coarse-to-fine reconstructions as more planes are decoded.",
      "source_document": "papers/2512.20070v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a progressive learned image codec specifically for downstream image classification, what components make up the end-to-end optimization objective (rate term vs. task-driven distortion terms), including how the classification loss is obtained and why an additional pixel-domain reconstruction term is included; and, after training, what qualitative rate\u2013accuracy behavior should you expect relative to (i) human-oriented progressive learned codecs and (ii) machine-oriented non-progressive codecs?",
      "answer": "The codec is trained with a combined rate\u2013task objective that adds a bitrate (bpp) term to a weighted \u201cdistortion\u201d term composed primarily of a task loss and secondarily a small MSE reconstruction loss for stability: L = L_bpp + \u03bb_distortion \u00b7 (L_task + \u03bb_MSE \u00b7 L_MSE). For ImageNet classification, L_task is the cross-entropy computed by passing the reconstructed image through a pre-trained (frozen) ResNet-50 classifier.\n\nIn rate\u2013accuracy evaluation on ImageNet-1K, the resulting progressive codec is expected to be more bitrate-efficient than human-oriented progressive learned codecs and to achieve the best performance at the high-rate endpoint when the full bitstream is decoded; at very low bpp it is comparable but can be slightly worse (with accuracy in that regime being practically too low). Compared with machine-oriented non-progressive codecs, it provides finer-grained scalability and more flexible bit allocation (because it can stop at many intermediate cutoffs), but its rate\u2013accuracy curve can sit below non-progressive baselines since those are optimized separately for specific bitrate points.",
      "source_document": "papers/2512.20070v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "If you introduce a progressive learned image codec optimized for downstream image classification but there are no existing machine-oriented progressive codecs to compare against, how can you set up a fair rate\u2013accuracy evaluation on a standard benchmark: which two baseline groups should you compare to, what specific codecs fall into each group, and what common preprocessing/evaluation pipeline (dataset split, resizing/cropping, metric, classifier) should be applied to all codecs to make the comparison meaningful?",
      "answer": "A fair comparison can be made by evaluating on a standard classification benchmark (ImageNet-1K validation) using a consistent compression/evaluation pipeline, and by comparing against two complementary baseline groups:\n\n1) Human-oriented progressive learned codecs (to assess progressive/FGS behavior): DPICT and Efficient-PIC.\n\n2) Machine-oriented non-progressive learned codecs (to assess task-optimized RD without progressive scalability): TransTIC, AdaptICMH, SA-ICM, and MPA.\n\nCommon pipeline used across codecs: images are resized to 256\u00d7256 for compression, then after decompression they are center-cropped to 224\u00d7224 and normalized before classification; performance is reported as top-1 accuracy using a pre-trained ResNet-50 (timm implementation). This allows rate\u2013accuracy curves for a progressive codec to be meaningfully compared to progressive human-oriented codecs and to bitrate-point\u2013optimized machine-oriented non-progressive codecs.",
      "source_document": "papers/2512.20070v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an item-region\u2013based fashion style classifier that combines a domain-specific backbone with a frozen CLIP vision encoder and fuses item features via item region pooling (IRP) and gated feature fusion (GFF), which architectural component removals cause the largest accuracy drop on FashionStyle14, and how does replacing adaptive average pooling (AAP) with standard global average pooling (GAP)\u2014or reducing AAP\u2019s spatial resolution\u2014affect accuracy?",
      "answer": "On FashionStyle14 (using the Swin-Base + IRSN setup for ablation), removing IRP or removing the CLIP vision encoder each caused the largest degradation: \u22121.4% accuracy (82.0% \u2192 80.6%). Replacing AAP with GAP reduced accuracy by \u22121.1% (82.0% \u2192 80.9%). Further, decreasing AAP\u2019s spatial resolution from 5\u00d73 to 5\u00d71 reduced accuracy by \u22120.6% (82.0% \u2192 81.4%), and reducing it to 1\u00d71 (equivalent to GAP) reduced it by \u22121.1% (82.0% \u2192 80.9%), indicating that preserving spatial information in pooled features is important for style recognition.",
      "source_document": "papers/2512.20088v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a fashion style classifier that fuses a domain-specific backbone with a CLIP-based general feature extractor, what training objective and regularization are used to reduce overconfident competition between visually similar style classes, and how are the CLIP vision-encoder parameters treated during training?",
      "answer": "The model is trained with cross-entropy classification loss, and label smoothing is applied to prevent excessive competition/overconfidence between visually similar style classes. The CLIP vision encoder used as the general feature extractor is kept frozen during training to preserve its general knowledge learned from large-scale image\u2013text pretraining.",
      "source_document": "papers/2512.20088v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an item-region\u2013aware fashion style classifier that relies on a zero-shot text-prompted segmenter, how are the item regions specified (including the exact text prompts used to retrieve them), and what mechanism in the item-feature fusion ensures that regions that are absent or missed by the segmenter do not contribute to the final style prediction?",
      "answer": "The model defines four item regions\u2014head, top, bottom, and shoes\u2014by running a pre-trained CLIPSeg segmenter on the input image with region-specific text prompts. The prompts are empirically chosen as: \u201chead\u201d, \u201ccasual top cloth\u201d, \u201cpants, skirt cloth\u201d, and \u201cshoes\u201d, producing binary masks for each region.\n\nTo prevent missing/absent regions from affecting classification, each region is processed by an item encoder that outputs both an item feature map and an importance (gating) map. The gating map uses a sigmoid (not softmax) so multiple items can simultaneously receive high weights; if an item is absent or undetected, the importance map for that item is set to zero everywhere. During gated feature fusion, each item feature map is multiplied element-wise by its importance map before pooling/concatenation, so zero-gated regions contribute nothing to the final fused feature used by the MLP classifier.",
      "source_document": "papers/2512.20088v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking fashion style classifiers on FashionStyle14 and ShowniqV3, how does an item-region\u2013based design (IRSN) compare against adding a generic dual-attention module (criss-cross + spatial attention) to the same ConvNeXt-Tiny backbone, in terms of absolute accuracy and improvement over the plain backbone\u2014and what does that comparison indicate about the value of explicit item-region modeling for style recognition?",
      "answer": "Using ConvNeXt-Tiny as the backbone, the plain baseline achieves 74.8% on FashionStyle14 and 61.2% on ShowniqV3. Adding the dual-attention module increases accuracy only slightly to 75.4% and 61.6% (i.e., +0.6% and +0.4% over the baseline). In contrast, ConvNeXt-Tiny + IRSN reaches 79.4% and 65.3% (i.e., +4.6% and +4.1% over the baseline). This gap suggests that explicitly extracting and fusing item-region features (in addition to global features) provides a substantially larger benefit for fashion style classification than attaching a generic attention block to the backbone.",
      "source_document": "papers/2512.20088v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a fashion style classifier that uses an external item segmenter, how does item region pooling (IRP) turn the segmenter\u2019s per-item masks into item-specific feature maps aligned with the backbone\u2019s feature map, and which item regions are pooled for downstream item-wise encoding and fusion?",
      "answer": "IRP starts from binary segmentation masks m_i over the input image for each item region (the model uses four: head, top, bottom, and shoes). Because the backbone/domain-specific feature extractor produces a lower-resolution feature map d_x of size c\u00d7h\u00d7w, each mask is resized/downsampled to the same h\u00d7w spatial resolution and then applied as a spatial gate by element-wise multiplication with the backbone feature map (with broadcasting across channels): f_i(x) = m_i\u2193(h\u00d7w) \u2297 d_x. The resulting per-item feature maps f_i are then fed to separate item encoders for further item-specific processing before gated feature fusion combines them with global features for style prediction.",
      "source_document": "papers/2512.20088v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For a video-based human activity recognition pipeline trained and tested on both a relatively clean benchmark (UCF101) and a noisier, motion-rich one (HMDB51), what training objective and data augmentations are used for the recurrent classifiers, which preprocessing step(s) are identified as critical for stable convergence, and why does a ConvLSTM paired with ReLU and an adaptive optimizer (Adam/RMSprop) generalize more robustly than a BiLSTM trained under the same activation/optimizer grid?",
      "answer": "Both recurrent classifiers are trained with a sparse categorical cross-entropy objective. The training setup applies standard video-frame augmentations (random cropping, horizontal flipping, and brightness jitter). Videos are converted into fixed-length frame sequences and normalized; the analysis notes that frame normalization is essential for stable convergence (omitting it increases training instability and validation loss), and that removing temporal down-sampling hurts ConvLSTM accuracy by several percentage points because longer sequences introduce more noise.\n\nConvLSTM is more robust because it jointly learns spatial and temporal features: convolutional operations preserve the spatial structure of frames while recurrent gating models temporal evolution, letting it capture motion transitions and fine-grained visual cues that are important on HMDB51. ReLU further improves training by reducing gradient saturation and enabling more stable gradient flow, while Adam/RMSprop improve learning stability via adaptive learning-rate updates. In contrast, BiLSTM lacks a dedicated spatial feature extractor and (in this setup) is trained from scratch without pre-trained spatial features; together with vanishing-gradient issues from Sigmoid/Tanh choices and the small/noisy HMDB51 subset, this leads to poor generalization on HMDB51 even though BiLSTM performs strongly on UCF101.",
      "source_document": "papers/2512.20104v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a diffusion framework that generates LiDAR point clouds from coarse 3D scene layouts, how are the layouts converted into pixel-aligned control signals for the generator, and what training objective is used for (i) the unconditional LiDAR generator and (ii) the conditional, layout-controlled generator (include how the noise level enters the score network and how the loss is aggregated across noise scales)?",
      "answer": "The layouts are built from semantic primitives (e.g., cuboids for cars, planes for roads) and then projected into a range-image\u2013like representation using scene raycasting from the LiDAR origin: each ray corresponds to a range-image pixel, intersects the layout\u2019s mesh triangles, and provides (a) a depth value from the intersection point and (b) a semantic label from the intersected triangle ID. The conditional input is the concatenation of a semantic range image and a depth range image, giving pixel-level aligned control.\n\nTraining uses noise-conditioned score matching with a denoising score network that explicitly depends on the current noise scale \u03c3i. For unconditional generation, the network is S\u03b8(x\u0303,\u03c3i) and is trained by a multi-scale loss that sums expectations over data and Gaussian-perturbed samples x\u0303~N(x,\u03c3i^2I), weighting each noise level (with \u03c3i-based weighting) and penalizing the difference between the predicted score and the analytic score term (x\u0303\u2212x)/\u03c3i^2. For conditional generation, the objective has the same multi-scale, noise-conditioned structure but the score network also takes the condition xc: S\u03b8(x\u0303,xc,\u03c3i), again summed/averaged across noise levels with \u03c3i-based weighting.",
      "source_document": "papers/2512.20105v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In layout-conditioned diffusion for LiDAR range-image generation, what happens to generation quality when (a) the layout-to-range control signal is projected with a coarse BEV projection instead of a raycasting-based, pixel-aligned projection, and when the conditional signal ablates (b) semantic segmentation cues or (c) depth cues? Explain what each outcome implies about the role of accurate projection, semantic conditioning, and depth conditioning in controllable LiDAR synthesis.",
      "answer": "Using a coarse BEV projection in place of raycasting causes a pronounced degradation across evaluation metrics, indicating that spatially accurate, pixel-aligned layout projection is necessary for effective control. Removing semantic segmentation cues further degrades performance, showing semantic conditioning is key for preserving object-level structure (e.g., vehicles) and structural accuracy. Omitting depth information yields a more moderate decline, implying depth primarily contributes to geometric fidelity rather than being the sole driver of correct scene structure. Together these ablations support that raycast-based alignment, semantics, and depth provide complementary benefits for high-quality LiDAR point cloud synthesis.",
      "source_document": "papers/2512.20105v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When converting a single RGB driving image into a conditioning signal for LiDAR point-cloud diffusion via an intermediate 3D layout, what is the end-to-end pipeline that turns the image into a layout (name the perception outputs and the clustering step), and what qualitative failure mode does this layout-based conditioning avoid compared with directly conditioning the generator on image features?",
      "answer": "Pipeline: run a semantic segmentation model (SAM) and a monocular depth estimator (DepthAnything) on the input image to obtain per-pixel semantic labels and depth; use these to lift the image into a pseudo point cloud; cluster the pseudo point cloud with DBSCAN by semantic category to produce object/region bounding boxes, which are used to build the scene layout; then generate the LiDAR point cloud conditioned on this layout. Compared to directly conditioning on image features (as in prior image-conditioned LiDAR diffusion), the layout-based conditioning avoids synthesizing spurious/non-existent objects\u2014e.g., extra vehicles appearing in the generated point cloud even when the input depicts an empty road\u2014and reduces unintended artifacts by enforcing object-level geometric/semantic structure through the layout.",
      "source_document": "papers/2512.20105v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When adding layout conditioning to a pretrained, unconditional diffusion model that generates LiDAR range images, what architectural mechanism is used to inject the layout signal into the generative backbone (include the role of \u201czero-convolution\u201d connections and which parts of the backbone receive them), and what fine-tuning schedule/initialization strategy enables efficient training without retraining the unconditional model from scratch?",
      "answer": "Layout conditioning is added via a ControlNet attached to the pretrained diffusion backbone: the ControlNet has four residual blocks (channels {128, 128, 256, 256}), and the output of each ControlNet block is passed through a zero-convolution layer and then connected to the corresponding decoder/refinement block of the pretrained Stable-Diffusion-style generator (decoder blocks with channels {256, 256, 128, 128}). Training is made efficient by freezing the unconditional generative model and fine-tuning only the ControlNet (with an observed schedule of first freezing ControlNet and training only the zero-convolution layers, then jointly training ControlNet + zero-conv layers). To speed convergence further, ControlNet parameters are initialized by copying the pretrained diffusion model weights, allowing visually compelling conditional generation in about 5,000 steps instead of training from scratch.",
      "source_document": "papers/2512.20105v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a LiDAR range-image diffusion generator that claims to support *multiple* conditioning modalities (semantic maps, text prompts, RGB images, and single-frame point clouds), what quantitative metrics should be reported for generated point-cloud quality, and how can common diffusion baselines be adapted so the conditioning interface is comparable across all four modalities (including what to do for a text-only baseline)?",
      "answer": "Report Frechet Range Distance (FRD), Minimum Matching Distance (MMD), Jensen\u2013Shannon Divergence (JSD), and Frechet Point-cloud Distance (FPD).\n\nTo make diffusion baselines comparable across the four condition types, encode each condition into the same unconditional point-cloud diffusion backbone by concatenating the condition representation with the model\u2019s latent (denoising latent) input. A latent-diffusion baseline can be run by concatenating each of the four conditions with the latent representation; a LiDAR-diffusion baseline that originally supports fewer modalities can be extended to the missing modality (the fourth condition) via the same concatenation operation. For text-conditioned generation specifically, compare against a dedicated text-to-LiDAR method (Text2LiDAR) as the text-only state-of-the-art baseline.",
      "source_document": "papers/2512.20105v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a hybrid sparse-view novel view synthesis system that uses (1) a deterministic per-token regression head and (2) a masked-autoregressive diffusion head, how does the model decide which target patches to render deterministically versus sample stochastically at inference time, and how is the diffusion unmasking budget adjusted when only a subset of tokens are deemed uncertain?",
      "answer": "The model predicts a pixel-wise confidence map for the target, defines each patch\u2019s confidence as the minimum confidence over its pixels, and then splits target tokens into a deterministic set xD and an uncertain set xS using a threshold \u03c4: patches with confidence > \u03c4 are rendered in a single deterministic forward pass, while the remaining patches are generated by iterative diffusion sampling. Because only xS needs stochastic unmasking, the number of unmasking steps is scaled to the fraction of uncertain tokens using TS = \u2308(|xS|/|x|)\u00b7Tmax\u2309 (with Tmax a fixed maximum step budget), and sampling uses a cosine unmasking schedule.",
      "source_document": "papers/2512.20107v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-modal attention model for bridge-deck delamination classification that also outputs calibrated uncertainty, what composite training objective can be used to (i) optimize standard classification accuracy, (ii) discourage overconfident wrong predictions via a predicted-variance term, and (iii) encourage diversity in the learned attention weights? Give the full loss and briefly explain what each term does.",
      "answer": "A composite objective is\n\nL = L_CE + \u03bb1 L_unc + \u03bb2 L_att,\n\nwhere:\n\u2022 L_CE is the standard cross-entropy loss for the 3-class delamination prediction.\n\u2022 L_unc penalizes overconfident incorrect predictions using the model\u2019s predicted variance \u03c3_i^2, defined as\n  L_unc = -(1/N) \\sum_{i=1}^N \ud835\udfd9[\u0177_i \u2260 y_i] \u00b7 log(\u03c3_i^2).\n  This adds extra penalty when the prediction is wrong and the model reports low variance (i.e., high confidence).\n\u2022 L_att is an attention-entropy regularizer that encourages attention diversity:\n  L_att = -(1/N) \\sum_{i=1}^N \\sum_{t=1}^T \u03b1_{i,t} log \u03b1_{i,t},\n  where \u03b1_{i,t} are attention weights (summed over attention positions/steps t).",
      "source_document": "papers/2512.20113v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When tuning the number of heads in a multi-head cross-modal attention module for fusing GPR time-series features with IRT image features, you observe that validation accuracy and macro-F1 barely change, but ROC-AUC improves substantially as you go from 1 head to 4\u20138 heads. What does this metric pattern imply about what multi-head attention is improving (vs. not improving), and what head-count range is sufficient before returns diminish?",
      "answer": "The pattern (accuracy/F1 essentially invariant while AUC rises from single-head to multi-head) implies that adding heads mainly improves probabilistic ranking and confidence calibration rather than changing the final decision boundaries/argmax classification behavior. In the reported results, AUC increases strongly from h=1 to h=4 with only marginal additional gain at h=8, indicating that about 4\u20138 heads are sufficient and larger head counts provide minimal benefit.",
      "source_document": "papers/2512.20113v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a safety-critical multi-modal classifier that uses both Monte Carlo dropout and an auxiliary variance head, how can you (1) decompose total predictive uncertainty into aleatoric vs. epistemic components at inference time, (2) post-hoc calibrate the predicted class probabilities, and (3) use the calibrated uncertainties for selective prediction to improve the reliability of bridge-deck delamination decisions? Describe the specific computations and the observed effect on calibration and selective accuracy compared with the uncalibrated baseline.",
      "answer": "(1) Run T stochastic forward passes with dropout enabled to approximate the predictive posterior: p(y|x,D) \u2248 (1/T)\u2211_{t=1}^T p(y|x, \u03b8\u0302_t). Total predictive uncertainty is decomposed as Var[y|x] = E_\u03b8[Var_y(y|x,\u03b8)] (aleatoric) + Var_\u03b8(E_y[y|x,\u03b8]) (epistemic). In practice, epistemic uncertainty is estimated from the variance of the T sampled predictive probabilities, while aleatoric uncertainty is predicted directly by a parallel variance head (\u03c3_i^2) for each sample.\n\n(2) After training, apply temperature scaling on a held-out calibration set to rescale logits so that predicted confidence better matches empirical accuracy (reducing Expected Calibration Error substantially).\n\n(3) Use the resulting calibrated confidence/uncertainty scores to implement selective prediction: reject (defer to human review) the most uncertain samples and report decisions only on the retained set (coverage). This yields markedly improved reliability\u2014ECE drops by roughly ~70% from the baseline, and selective accuracy increases (e.g., at 80% coverage accuracy rises into the low-90% range) because the rejected cases concentrate the hard/ambiguous examples.",
      "source_document": "papers/2512.20113v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-modal attention model that fuses GPR time-series with IRT images for 3-class delamination severity, what characteristic metric pattern signals \u201cmajority-class collapse\u201d under extreme class imbalance, what mechanism explains why attention heads drift toward the dominant (intact) class in that regime, and what concrete training or modeling changes are recommended once minority classes fall below ~2% (including when a simpler baseline may be preferable)?",
      "answer": "When the class distribution becomes extremely skewed (e.g., the intact class is ~90% and the rarest class is <2%), the attention-based model can show a large mismatch between overall accuracy and class-balanced metrics: accuracy can remain \u201creasonable\u201d while macro-F1 drops sharply, indicating the model is largely predicting the dominant class and failing on minorities. The document highlights this on the most imbalanced bridge: the attention model attains high apparent accuracy but much lower macro-F1, whereas a Simple CNN has a much smaller accuracy\u2013F1 gap.\n\nThe proposed explanation is gradient dominance: because the vast majority of training samples (and thus gradients) come from Class 1, temporal attention heads learn to model variations within normal GPR responses instead of diagnostic minority patterns, and spatial attention similarly focuses on sound-deck thermal patterns; with very few Class 3 examples, attention weights get negligible updates that would encourage deep-delamination detectors.\n\nRecommended actions once minority classes are below ~2% include using specialized imbalance handling\u2014focal-loss variants, class reweighting, and synthetic oversampling (the discussion also mentions SMOTE and two-stage training as candidates)\u2014or even preferring simpler baselines with simpler decision boundaries (e.g., SVM or a plain CNN), which were observed to be more robust on the extremely imbalanced dataset. The deployment guidance is that attention-based fusion is preferred when imbalance is below roughly 8:1, but extreme imbalance calls for these interventions or model simplification.",
      "source_document": "papers/2512.20113v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When validating a multi-modal GPR+IRT delamination classifier that uses (i) temporal attention on 1D radar A-scans, (ii) spatial+channel attention on thermal image patches, and (iii) cross-modal (multi-head) attention fusion, how can you ablate these components to isolate what each attention stage contributes\u2014and what improvement pattern would demonstrate that cross-modal attention is adding complementary information beyond within-modality attention alone?",
      "answer": "A clean ablation is to start from a no-attention baseline CNN and then add attention modules incrementally: (1) add only temporal attention in the GPR branch, (2) add only spatial/channel attention in the IRT branch, (3) add both within-modality attention modules but no cross-modal fusion attention, and (4) add the full cross-modal attention fusion. The observed pattern is that temporal attention gives a modest gain (+1.7% validation accuracy), spatial attention gives a modest gain (+1.2%), and combining both within-modality attentions yields a larger gain (+4.4% vs the baseline). Crucially, adding cross-modal attention on top of both within-modality attentions gives an additional +2.8% to reach 87.6% validation accuracy (overall +7.2% vs the baseline), showing that modeling inter-modality relationships provides extra performance beyond attention within each modality separately.",
      "source_document": "papers/2512.20113v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a bank-grounded, query-based audio\u2013visual segmentation model like DDAVS, how is waveform-level audio augmentation used to set up the contrastive (InfoNCE-style) training signal\u2014specifically, what perturbations are applied, what form the positive and negative pairs, and how is this contrastive term combined with the pixel-level segmentation losses in the overall objective?",
      "answer": "The model forms two audio views by applying random waveform-level perturbations to the raw audio (additive noise, reverberation, and pitch shift, following a WavAugment-style operator) to get an augmented waveform A\u2032a=g(Aa). Both the clean waveform and the augmented waveform are encoded by the audio encoder and passed through the same Audio Query Module to produce a refined query set Q={qi} and an augmented counterpart Q\u2032={q\u2032i}.\n\nFor contrastive learning, each query is projected and \u21132-normalized (zi=\u03c6(qi)/||\u03c6(qi)||2 and z\u2032i=\u03c6(q\u2032i)/||\u03c6(q\u2032i)||2), and similarities are computed as si,j=zi\u1d40z\u2032j. The InfoNCE loss is\nLcon=\u2212(1/n)\u2211i log( exp(si,i/\u03c4) / \u2211j exp(si,j/\u03c4) ).\nThe positive pair is the matched query slot between clean and augmented audio (zi,z\u2032i), while negatives are mismatched slots (zi,z\u2032j) for j\u2260i, which encourages large inter-query margins and robustness under acoustic perturbations.\n\nThis contrastive term is added to the segmentation objective as a weighted sum:\nLtotal = \u03bbce Lce + \u03bbdice Ldice + \u03bbiou Liou + \u03bbcon Lcon,\nwhere cross-entropy gives pixel-wise supervision and Dice/IoU encourage region completeness and accurate boundaries, while Lcon makes the audio queries more discriminative and noise-resilient.",
      "source_document": "papers/2512.20117v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a delayed bidirectional audio\u2013visual alignment design for pixel-level audio\u2013visual segmentation, how is \u201cbidirectional\u201d fusion implemented inside each alignment block (i.e., what are the two cross-attention directions and what each one is intended to do), what does it mean for fusion to be \u201cdelayed\u201d in terms of where audio is injected into the visual backbone, and what ablation result supports injecting audio only in late layers rather than early layers?",
      "answer": "Bidirectional fusion is realized with two consecutive cross-attention operations per alignment block: (1) **audio-guided filtering**, where the audio semantic queries attend to visual tokens to pull out sound-relevant visual evidence; and then (2) **visual-guided enhancement**, where the updated audio representations are used as keys/values so the visual stream attends back to them, injecting discriminative acoustic cues into the visual features to emphasize sound-producing regions.\n\nFusion is \u201cdelayed\u201d by applying these cross-modal alignment blocks only in the later part of the visual transformer (specifically, injected between the 3rd and 4th layers/blocks of a 4-block backbone) rather than from the earliest layers, motivated by the idea that later layers contain richer, instance-level/global representations and early fusion is more sensitive to low-level noise.\n\nThe ablation on audio-injection positions shows that late injection (using the last two blocks, i.e., blocks 3 and 4) performs best, while injecting in earlier blocks (e.g., block 1 or 2) is consistently worse, supporting the delayed, late-layer fusion choice.",
      "source_document": "papers/2512.20117v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When designing the audio\u2013visual fusion point inside an AVS alignment/segmentation decoder, what fusion placement works best in practice\u2014injecting audio only at the decoder head (either before or after LayerNorm) versus fusing earlier at an encoder\u2013decoder stage\u2014and how does the best choice affect multi-source and semantic-source performance compared to head-only fusion?",
      "answer": "Fusing audio at an encoder\u2013decoder stage (i.e., performing audio\u2013visual fusion inside the alignment decoder rather than only at the final head) works best. In the evaluated setup, encoder\u2013decoder fusion achieves the highest J&F on both AVS-MS3 and AVSS, outperforming head-only fusion whether the head fusion is done pre-LN or post-LN. Concretely, encoder\u2013decoder fusion reaches 75.08 J&F on AVS-MS3 and 52.62 J&F on AVSS, versus 73.03/52.16 for head (pre-LN) and 72.33/50.60 for head (post-LN), showing that earlier fusion yields a clear gain (about +2 points J&F on AVS-MS3 and up to +2 points on AVSS over head fusion).",
      "source_document": "papers/2512.20117v1.pdf",
      "mode": "textual",
      "content_refs": [
        "Table 6 (fusion position ablation)"
      ]
    },
    {
      "question": "In an audio\u2013visual segmentation model that \u201cgrounds\u201d learnable audio queries in a class-wise prototype memory bank to handle multi-source mixtures, how can the prototype bank be constructed from training audio, how is it kept/used at train and test time, and how does the model use this bank to refine the initial query embeddings (e.g., via cross-attention)?",
      "answer": "A practical construction is to build class-specific prototype sets from clean, single-sounding audio clips: for each semantic class, collect clips where that class is the only audible source, extract log-mel spectrograms, and run them through the same audio backbone used by the AVS model to obtain d-dimensional embeddings. Then cluster each class\u2019s embedding set with K-means (K-means++ initialization) to capture multiple acoustic modes, and select a few embeddings nearest to each cluster mode as that class\u2019s representative prototypes. Concatenate the prototypes from all classes to form a global memory bank M \u2208 R^{N_M\u00d7d}. Once built, the bank is kept fixed during both training and inference, serving as stable, class-aware anchors.\n\nTo use the bank for query refinement, the model first produces a small set of initial audio queries Q_a (e.g., from a Q-Former over the audio tokens), then applies cross-attention from Q_a (as queries) to the memory bank M (as keys/values): compute attention weights A = Softmax((Q_a W_Q)(M W_K)^T / sqrt(d)), produce an attended prototype summary \\tilde{Q} = A (M W_V), and form bank-grounded queries Q = LN(Q_a + \u03b3\\tilde{Q}). This anchors each query to its most relevant prototype region and stabilizes semantics under mixtures before downstream audio\u2013visual alignment/decoding.",
      "source_document": "papers/2512.20117v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a query-based audio\u2013visual segmentation model that represents the soundtrack with a small set of learnable audio queries, what empirical behavior is observed as you vary the number of audio queries n (e.g., from 1 upward), what default n is chosen, and what practical interpretation is given for why performance degrades when n becomes too large?",
      "answer": "When the number of audio queries is increased from 1 up to 5, performance (J&F) improves rapidly on both AVS-MS3 and AVSS, suggesting that a small set of diverse queries helps capture different sounding patterns. Once n exceeds 5, performance starts to drop, indicating that using too many queries is unnecessary in practice; accordingly, the default setting chooses n = 5.",
      "source_document": "papers/2512.20117v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a Hessian-guided dynamic pruning scheme for Vision Transformers that prunes both patch tokens and attention heads, how is the per-component importance score derived from a second-order loss approximation, how is it computed efficiently without forming the Hessian, and what criterion ensures a chosen pruned set respects a user-specified loss-increase budget during inference?",
      "answer": "The importance of removing a component z (either a token activation or a head output) is derived by a second-order Taylor expansion of the loss around the converged model and substituting \u0394z = \u2212z. With \u2207zL \u2248 0 at convergence, the loss increase from removing z is approximated by \u0394Lz(x) \u2248 (1/2) z(x)\u1d40Hz(x) z(x), so the (scaled) importance score is defined as Sz(x) = z(x)\u1d40Hz(x) z(x) for both tokens and heads (heads are scored via vectorizing the head output).\n\nSz is computed efficiently using a Hessian\u2013vector product (HVP) rather than explicitly forming Hz: using Pearlmutter\u2019s trick, Hz(x)z(x) = d/d\u03f5 [\u2207zL(z(x)+\u03f5z(x))] |_{\u03f5=0}, so Sz(x)=\u27e8Hz(x)z(x), z(x)\u27e9 can be obtained with two backprop passes; scores can be stabilized by averaging over a small calibration batch.\n\nFor a pruned set Z, the total loss increase is additively approximated as \u0394L(x;Z) \u2248 (1/2) \u03a3_{z\u2208Z} Sz(x). Enforcing \u03a3_{z\u2208Z} Sz(x) \u2264 2\u03b5 guarantees \u0394L \u2264 \u03b5 under the quadratic model, providing an explicit loss-budget pruning criterion at inference (implemented via thresholding/top-k selection on Sz).",
      "source_document": "papers/2512.20120v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a Vision Transformer\u2019s multi-head self-attention layer, how does the dominant FLOPs cost scale when you prune tokens versus when you prune attention heads (e.g., n\u2192\u03b1n tokens kept and H\u2192\u03b2H heads kept), and what practical implication does this scaling have for designing asymmetric token/head pruning ratios to maximize efficiency without overly harming accuracy?",
      "answer": "The dominant self-attention cost comes from the quadratic query\u2013key interaction and scales approximately as FLOPs\u2113 \u2248 O(H\u2113 \u00b7 n\u2113^2 \u00b7 dk). If token pruning keeps an \u03b1 fraction of tokens (n\u2113 \u2192 \u03b1 n\u2113), the attention cost becomes O(H\u2113 \u00b7 (\u03b1 n\u2113)^2 \u00b7 dk), i.e., it drops quadratically with \u03b1 (\u221d \u03b1^2). If head pruning keeps a \u03b2 fraction of heads (H\u2113 \u2192 \u03b2 H\u2113), the cost becomes O(\u03b2 H\u2113 \u00b7 n\u2113^2 \u00b7 dk), i.e., it drops only linearly with \u03b2. Therefore, most computational/latency savings are driven by token pruning (because it reduces the n\u2113^2 term), while head pruning mainly provides finer-grained redundancy removal with comparatively smaller FLOPs impact. This motivates asymmetric pruning schedules that prioritize pruning tokens for efficiency, while using head pruning as a secondary lever to improve the accuracy\u2013efficiency trade-off at a given budget rather than expecting large compute reductions from heads alone.",
      "source_document": "papers/2512.20120v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a Vision Transformer that supports *dynamic* pruning of both patch tokens and attention heads, how can the pruning decision be made differentiable for post-pruning fine-tuning (i.e., what gating function is used, how is it annealed toward a hard mask, and where is it applied in the forward pass), and what optimizer/schedule and data-augmentation recipe is used when training and then fine-tuning the pruned model (including which augmentations are turned off at evaluation)?",
      "answer": "Differentiable fine-tuning is done by replacing the hard keep/drop mask with a *soft gate* per component (token or head) using a sigmoid over the normalized sensitivity score: \n- Hard gating for inference keeps components by thresholding or top\u2011k: Mz(x)=I[Sz(x)\u2265\u03c4].\n- Soft gating for fine-tuning uses Gz(x)=\u03c3(\u03b3(\u015cz(x)\u2212\u03c4))\u2208(0,1), where \u015cz is the layerwise normalized score (using that layer\u2019s mean and standard deviation) and \u03b3 is a temperature that is *annealed upward* so the sigmoid saturates and Gz approaches a hard 0/1 mask. The forward pass applies Gz multiplicatively to token activations and to attention-head outputs.\n\nTraining/fine-tuning uses a standard ViT/DeiT recipe:\n- Optimizer AdamW (\u03b21=0.9, \u03b22=0.999) with weight decay 0.05.\n- Cosine learning-rate decay with a 5\u2011epoch linear warm\u2011up; peak LR 5\u00d710\u22124 for ViT\u2011B/16 and 3\u00d710\u22124 for DeiT\u2011B/16 (scaled with global batch size).\n- Global batch size 1024 (with gradient accumulation if needed) and mixed precision (AMP).\n- Augmentations: RandAugment, random resized crop to 224, horizontal flip; label smoothing \u03b5=0.1; Mixup=0.8 and CutMix=1.0 during training, and Mixup/CutMix are disabled for evaluation.\n- Regularization: DropPath 0.1 for ViT\u2011B/16 and 0.1\u20130.2 for DeiT\u2011B/16.\n- Training runs 200 epochs with early exit (patience 15), then fine-tuning after pruning runs 100 epochs with the same early-exit rule.",
      "source_document": "papers/2512.20120v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In joint token+attention-head pruning for Vision Transformers, what depth-dependent pruning behavior best preserves accuracy (early vs. middle vs. late layers), and what curvature/sensitivity-based reasoning explains why token pruning is generally \u201ccheap\u201d while head pruning becomes increasingly risky in later layers?",
      "answer": "A depth-aware policy is to prune tokens aggressively in early layers while keeping heads largely intact, continue favoring token pruning in middle layers but be cautious with head pruning, and avoid heavy head pruning in late layers where it can strongly hurt accuracy.\n\nThe reasoning is based on second-order (curvature-weighted) sensitivity S^\u2113_z(x)=z(x)^\u22a4H^\u2113_z(x)z(x): many early-layer patch tokens\u2014especially background patches\u2014align with flatter curvature directions, giving small sensitivity scores and making them inexpensive to remove with little loss increase. Attention heads, however, tend to correspond to specialized functional subspaces (e.g., locality, long-range context, semantics) that align with sharper curvature directions, so removing heads has higher loss impact. Layerwise, early layers have high token redundancy (safe to drop many tokens), middle layers rely more on heads for feature aggregation (head pruning starts to harm), and late layers compress information toward the class token so pruning heads disproportionately damages semantic integration; thus token pruning mainly governs efficiency while head pruning mainly affects representational diversity and accuracy.",
      "source_document": "papers/2512.20120v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "If you apply the same structured pruning level (e.g., pruning 50% of tokens and 50% of attention heads) to a standard ViT-B/16 and a distillation-trained DeiT-B/16, what layerwise representation changes would you expect to observe (using metrics like CKA similarity, CLS-token cosine similarity, and residual input/output norm ratios), and what do these trends imply about each model\u2019s robustness and how fine-tuning restores performance?",
      "answer": "Under identical 50% token + 50% head pruning, representation perturbations are largest in shallow-to-mid layers: CKA similarity to the dense model decreases with depth for both backbones, indicating pruning alters intermediate hidden states most strongly there. DeiT-B/16 shows lower CKA overall (stronger representational shift) than ViT-B/16, consistent with distillation-driven supervision making its dense features more tightly constrained and thus more sensitive to pruning.\n\nCLS-token cosine similarity remains very high for ViT and diverges smoothly while staying tightly coupled to the dense trajectory (>0.992), whereas DeiT\u2019s CLS cosine drops earlier and more sharply, suggesting more aggressive reorganization of the global semantic embedding under pruning.\n\nResidual norm ratios also differ: ViT exhibits a smooth transformation-to-identity trade-off across depth, and fine-tuning restores balance in later layers; DeiT shows sharper fluctuations, with pruned layers oscillating between more identity-like and more transformation-heavy behavior. Fine-tuning compensates in both models, but ViT tends to converge back to a nearby solution space while DeiT adapts via a more reorganized representational geometry.",
      "source_document": "papers/2512.20120v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For dual\u2011view mmWave radar 2D human pose estimation under specular reflections, what architectural choices enable spatio\u2011temporal modeling in both the feature extractor and the decoder, and how does the training objective enforce both per\u2011frame keypoint accuracy and temporal smoothness (including how multi\u2011frame predictions are used at inference)?",
      "answer": "The approach uses a Cross\u2011View Fusion Mamba encoder that first extracts features in separate horizontal/vertical branches, then fuses the two views by concatenation (after adding view\u2011specific positional embeddings) and converts the fused tensor to a 1D token sequence with a zigzag scan over range \u2192 angle \u2192 view (h\u2192v) \u2192 frame. The sequence is processed bidirectionally with forward/backward state\u2011space (Mamba) branches to capture long\u2011range spatio\u2011temporal dependencies with linear complexity.\n\nFor decoding, it uses a Multi\u2011Pose Spatio\u2011Temporal Cross\u2011Attention (STCA) decoder operating on a fixed set of J\u00d7T learnable keypoint queries (one per joint per frame). Each decoder layer applies (1) spatial self\u2011attention within each frame to model inter\u2011joint structure, (2) temporal self\u2011attention across frames for the same joint to encourage motion consistency, and then (3) cross\u2011attention from the refined queries to the encoder features so the decoder can use context from all frames to recover missing/weakly reflected joints. The decoder outputs poses for all T frames.\n\nTraining optimizes a sum of an OKS\u2011based keypoint loss and a velocity loss, where velocity is defined as the difference between consecutive predicted joint positions and is matched to the ground\u2011truth joint velocity, encouraging temporal smoothness. Although the model predicts a T\u2011frame pose sequence, at inference it retains only the central frame prediction from each sliding window.",
      "source_document": "papers/2512.20128v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "If you want to make spatio-temporal learning feasible for mmWave radar\u2013based 2D pose estimation without exploding sequence/token cost, how can the raw FMCW radar cube be preprocessed into a compact representation? Describe the key signal-processing steps used to form the 3D angle\u2013doppler\u2013range heatmap (including any clutter removal, sub-sampling, and zero-padding choices), and summarize the ablation evidence that this 3D FFT representation is preferable to both a 2D density map projection and a conventional 4D FFT heatmap in terms of pose accuracy and preprocessing efficiency.",
      "answer": "A compact input is obtained by replacing expensive 4D heatmap construction with a 3D FFT angle\u2013doppler\u2013range heatmap.\n\n\u2022 Start from complex FMCW radar cubes X \u2208 C^{12\u00d7128\u00d7256} (virtual-antenna pairs \u00d7 chirps \u00d7 ADC samples). For each frame: (1) remove static clutter by subtracting the mean across chirps; (2) uniformly sub-sample the chirp dimension down to 8 chirps/frame to reduce compute while preserving Doppler resolution; (3) apply three 1D FFTs to produce a 3D heatmap: FFT over the ADC-sample dimension (range), then FFT over the chirp dimension (doppler), and finally FFT over the virtual-antenna dimension after zero-padding it from 12 to 64 to improve angular resolution. The resulting heatmap tensor is organized as Y \u2208 C^{H\u00d7D\u00d7W} = C^{64\u00d78\u00d7256} (angle \u00d7 doppler \u00d7 range).\n\n\u2022 This 3D FFT heatmap omits elevation padding and one FFT dimension used by the 4D FFT approach, which mitigates token-count growth and reduces preprocessing cost; it is reported to cut preprocessing memory by 11\u00d7 and latency by 8.6\u00d7 versus the conventional 4D approach.\n\n\u2022 In ablations on TransHuPR, input representations show the 3D FFT heatmap yields the best pose accuracy among the tested options: the 2D density-map projection is lowest (AP 58.5), 4D FFT improves markedly (AP 72.0), and 3D FFT is higher still (AP 74.5), while also being substantially cheaper to generate than 4D FFT.",
      "source_document": "papers/2512.20128v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When designing a dual\u2011view mmWave radar encoder for 2D human pose estimation using a 1D sequence model (e.g., Mamba/SSMs), how can the horizontal and vertical radar heatmap features be fused and tokenized so that the encoder captures both cross\u2011view information and bidirectional spatio\u2011temporal context? Describe (i) how per\u2011view positional information is injected, (ii) how the two views are merged into a single encoder input, (iii) the scanning/tokenization order used to serialize the multi\u2011dimensional feature volume, and (iv) how bidirectional context is obtained in the sequence model.",
      "answer": "A practical fusion/tokenization scheme is:\n(i) Process each radar view in its own CNN branch to produce feature maps, then add separate learnable positional embeddings to each view (Ph for horizontal and Pv for vertical) to encode the spatial axes (angle and range).\n(ii) Concatenate the two position\u2011encoded view features to form one encoder input tensor F = [Fh; Fv], effectively adding a view dimension (horizontal followed by vertical).\n(iii) Serialize F into a 1D token sequence using a zigzag scan ordered by range \u2192 angle \u2192 view (h \u2192 v) \u2192 frame.\n(iv) Feed this sequence to Vision Mamba layers implemented with two independent SSM branches run in forward and backward directions, giving bidirectional context over the scanned spatio\u2011temporal, cross\u2011view token stream while retaining linear complexity.",
      "source_document": "papers/2512.20128v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When choosing between a single mmWave radar and a dual-radar (horizontal+vertical) setup for 2D human pose estimation, what performance pattern emerges across horizontal-only, vertical-only, and dual-radar configurations, and what sensing/geometry limitation does the dual-radar setup mitigate to explain its gains?",
      "answer": "The vertical-only radar configuration performs competitively and clearly better than horizontal-only, while combining both radars (horizontal+vertical) yields the best overall pose accuracy. The improvement from dual-radar is explained as complementary viewpoints that compensate for the limited elevation resolution of mmWave radar sensors, providing additional information that a single view (especially horizontal-only) cannot capture reliably.",
      "source_document": "papers/2512.20128v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "If you\u2019re deciding between a Transformer encoder and a Mamba/SSM encoder for spatio-temporal feature extraction from multi-frame mmWave radar heatmaps, what empirical trade-off emerges in terms of (i) achievable sequence length under GPU memory limits, (ii) encoder memory/compute scaling, and (iii) resulting pose AP\u2014especially when comparing a short-sequence setting where both can run versus longer sequences where one becomes impractical?",
      "answer": "The comparison shows that a Transformer encoder is constrained by much higher memory demand, to the point that training becomes impractical for longer input sequences (it can be run only with very short windows, e.g., 3 frames, to avoid out-of-memory). A Mamba-based encoder, with linear-time/linear-memory sequence modeling, has a far smaller memory footprint at the same short sequence length while achieving comparable (slightly better) AP. Crucially, because it remains memory-feasible as T increases, Mamba can be trained with longer windows (e.g., 7\u20139 frames) and attains substantially higher AP at a computational cost that is in the same range as the Transformer\u2019s short-window setting, demonstrating superior scalability under fixed hardware constraints.",
      "source_document": "papers/2512.20128v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When you filter training/validation labels by an upper bound on occlusion, standard F1 can incorrectly treat correctly detected but unlabeled objects as false positives. How is the \u201cneutralized\u201d (neutral) F1 computed to remove this bias, and what 3D box IoU threshold is used to decide true positives vs. false positives?",
      "answer": "True positives/false positives are determined by the 3D box intersection-over-union (IoU) between the predicted and ground-truth 3D bounding boxes with a threshold of 0.5 (IoU > 0.5 is a TP; otherwise it is an FP; a ground-truth box with no prediction is an FN). To neutralize the bias introduced by dropping highly occluded labels, precision is recalculated using all labels (so predictions of fruits that were present but omitted due to occlusion filtering are not counted as false positives). The neutralized F1 is then computed using this recalculated precision together with the original recall.",
      "source_document": "papers/2512.20148v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In end-to-end 3D fruit pose estimation, apples are rotationally symmetric around their stem axis, which makes full 6-DoF box rotation ambiguous. How is the apple\u2019s orientation parameterized to remove this redundant degree of freedom, and what orientation-specific loss term is used during training (and what other loss components is it combined with)?",
      "answer": "Orientation is not regressed as a full 6-DoF rotation. Instead, the model constrains the oriented 3D bounding box so that its X-axis aligns with the vector from the fruit center to the calyx, reducing orientation prediction to a unit \u201cstem direction\u201d vector. Training uses a stem-direction loss that encourages the predicted unit vector v\u0302 to point toward the annotated calyx, and this term is combined with standard 3D box localization and size losses plus classification losses.",
      "source_document": "papers/2512.20148v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When estimating pose from orchard images, it\u2019s often useful to condition training/evaluation on how occluded each fruit is. How can a per-fruit occlusion rate be computed for a specific camera view using depth renders from (i) an isolated 3D representation of that fruit and (ii) the full reconstructed scene, and what artifacts can cause even fully visible fruits to get a non-zero occlusion estimate?",
      "answer": "Compute the occlusion rate by rendering two depth images for the same camera pose: (1) a depth render of the fruit alone (from its segmented 3D point cloud), which defines the fruit\u2019s \u201ctotal visible\u201d pixel support sT (all pixels where the fruit depth exists), and (2) a depth render of the full 3D reconstructed scene (3DGS), which includes any occluders. For each pixel where the fruit-only depth exists, compare the fruit-only depth to the full-scene depth; if the full-scene depth is closer than the fruit-only depth by more than a small tolerance (15 mm in the implementation), that pixel is counted as occluded. Let sO be the number of such pixels; the occlusion rate is o = (sO / sT) \u00d7 100%. Fully visible fruits can still receive a non-zero estimated occlusion because the per-fruit point cloud does not perfectly cover the fruit surface (small holes), and because 3DGS depth can contain errors when thin/small obstacles appear in front of the fruit, both of which can falsely trigger the occlusion test (leading to a baseline occlusion estimate around 13% for fully visible fruit).",
      "source_document": "papers/2512.20148v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building an RGB-D training set for per-fruit 3D pose estimation from orchard photos where multiple trees appear in the same frame, how can you construct leakage-free train/val/test splits and generate per-tree RGB and depth inputs that still respect occlusions from neighboring trees?",
      "answer": "Use the trees as the unit of splitting so that every fruit instance belongs to exactly one split (train/val/test), avoiding the same apple appearing across splits. To make each original photo compatible with these tree-based splits and meaningfully provide depth, create per-tree views by (1) rendering a depth image at the photo\u2019s camera intrinsics/extrinsics using the full 3D Gaussian Splatting scene, and (2) rendering a second depth image using only the cropped 3DGS for the target tree. Because the cropped depth lacks occlusions caused by other trees, compare the two depths pixelwise: wherever the full-scene depth is smaller than the cropped-tree depth, zero out that pixel in the cropped depth (indicating the target tree is occluded and depth should be invalid). Finally, use this masked cropped depth to mask the RGB image (set invalid-depth pixels to black), yielding an RGB-D pair that contains only the target tree while preserving occlusions from neighboring trees.",
      "source_document": "papers/2512.20148v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When preparing high-resolution RGB-D orchard images for a geometry-aware 3D pose detector, it\u2019s common to split each image into overlapping square patches to fit GPU memory. What adjustment must be made to the camera intrinsics for each patch so that projecting the 3D annotations into the patch remains geometrically consistent, and how are the labels incorporated into the patched dataset?",
      "answer": "For each cropped/patch image, the camera intrinsics must be updated by shifting the principal point (image center) to account for the patch\u2019s offset within the original frame, so that 3D-to-2D projection is still correct in the patch coordinate system. The dataset is then formed by pairing each patched RGB and depth image with the same 3D fruit annotations transformed/projection-mapped into that patch\u2019s image coordinates (i.e., labels are transformed to each patch before storing the patched RGB-D sample).",
      "source_document": "papers/2512.20148v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a low-shot diffusion counter that must specialize to the target category at test time, how are the provided exemplar boxes turned into timestep-adaptive conditioning signals for the denoising UNet, and where in the UNet is this conditioning applied to support accurate localization of small objects in dense scenes?",
      "answer": "Exemplars are converted into per-timestep, per-layer prototypes built from the UNet\u2019s current feature maps. At diffusion step t and UNet layer k, an appearance prototype is extracted for each exemplar box by RoIAlign on the layer feature map z_t^(k) and a linear projection (W_k\u00b7RoIAlign(z_t^(k), b_i)). In parallel, a shape prototype is computed from the exemplar box width/height via a two-layer MLP. All appearance and shape prototypes are concatenated and refined with a self-attention operation to form layer-specific prototypes p_t^(k). Conditioning is applied by injecting a single transformer layer after selected UNet layers, using the UNet features z_t^(k) as queries and the prototypes p_t^(k) as keys/values (multi-head attention with three heads). These exemplar-conditioning layers are inserted only after the first and last UNet layers, where feature-map resolution remains high enough to help detect very small objects.",
      "source_document": "papers/2512.20153v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In multi-teacher distillation of a vision backbone from a text-aligned teacher (e.g., SigLIP2) and a dense/self-supervised teacher (e.g., DINOv3), how can a relational distillation term be modified so it improves image\u2013text alignment without degrading kNN-style clustering quality, and what asymmetric rule/decision boundary is used to decide when to penalize \u201cshrinking\u201d vs. \u201cexpanding\u201d student pairwise distances?",
      "answer": "Use an asymmetric relational knowledge distillation (ARKD) loss that matches the teacher\u2019s pairwise geometry within each batch but only applies one-sided penalties depending on whether a teacher pair is \u201cclose\u201d or \u201cfar.\u201d Concretely, compute teacher and student pairwise Euclidean distances between summary embeddings (DT_ij and DS_ij), normalize distances using the teacher\u2019s mean pairwise distance, then use the intra-batch median of normalized teacher distances m as the decision boundary. Define one-sided errors shrink_ij = max(DS\u0302_ij \u2212 DT\u0302_ij, 0) and expand_ij = max(DT\u0302_ij \u2212 DS\u0302_ij, 0). Weight them with w_shrink,ij = 1{DT\u0302_ij < m} and w_expand,ij = 1 \u2212 w_shrink,ij, so expansion is penalized only for teacher-close (intra-cluster) pairs and shrinkage is penalized only for teacher-far (inter-cluster) pairs, using a smooth-L1 penalty. This avoids the symmetric RKD behavior that can over-push/pull pairs and harm kNN, while preserving alignment gains.",
      "source_document": "papers/2512.20157v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When distilling a vision student from multiple teachers using mixed native-resolution images that are packed into fixed-length token sequences (so each GPU processes roughly the same token budget), what is the loss normalization and global aggregation scheme that keeps gradients unbiased across resolutions and across packed sequences with different numbers of images, and what failure mode is it designed to prevent?",
      "answer": "Use per-image, token-normalized losses and then average over images (not tokens or sequences) in the global batch. Concretely, for each image q and teacher t: (i) a global/summary loss L_CLS^(t)(q)=1\u2212cos(z_q^(t,s), z\u0302_q^(t,s)); (ii) a patch loss normalized by that image\u2019s patch-token count N_q: L_patch^(t)(q)=(1/N_q)\u2211_{\u2113=1..N_q}||z_{q,\u2113}^(t,p)\u2212z\u0302_{q,\u2113}^(t,p)||\u00b2; and (iii) for DINOv3, a register loss normalized by the number of registers K: L_reg^(t)(q)=(1/K)\u2211_{k=1..K}||z_{q,k}^(t,reg)\u2212z\u0302_{q,k}^(t,reg)||\u00b2. Combine them per image as L^(t)(q)=L_CLS^(t)(q)+L_patch^(t)(q)+L_reg^(t)(q), and aggregate by averaging across all images in the global batch regardless of how they were packed: L_global^(t)=(1/B_global)\u2211_{all images q in all ranks/sequences} L^(t)(q), then sum over teachers. This is designed to stop high-resolution images (with many more tokens) from dominating the gradient and destabilizing optimization / causing low-resolution feature forgetting when training with native-resolution, packed multi-resolution batches.",
      "source_document": "papers/2512.20157v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In label-free multi-teacher vision distillation where different teachers\u2019 embedding spaces have very different means/variances, how can a distribution-balancing transform be applied so that MSE-style feature matching does not implicitly overweight high-variance teachers/channels\u2014and under what condition should this normalization be skipped for a subset of token types (e.g., teacher \u201cregister\u201d tokens), including the specific failure mode it causes during training?",
      "answer": "A way to balance heterogeneous teacher feature scales is to apply PHI-S (PCA\u2013Hadamard Isotropic Standardization) to each teacher target during training: learn an invertible linear mapping (PCA whitening + Hadamard rotation built from second\u2011order moment estimates) that rotates/scales the teacher features so per-channel variance is distributed more uniformly; the mapping is then inverted at inference so the student still outputs features in the teacher\u2019s original space. The transform is learned per teacher and per feature type (e.g., summary/global and patch tokens) using a few million samples (3M) from the training data.\n\nThis normalization should be skipped for token types whose distributions are multi-modal and therefore cannot be well fit by a single mean/covariance estimate\u2014specifically, a DINOv3 register token where PHI-S statistics become \u201cbetween-mode\u201d rather than representative. Forcing PHI-S on such a multi-mode register leads to incorrect centering/scaling, producing high-norm gradients, dramatically slowed learning, and training instability; therefore registers are supervised in their original space (no PHI-S applied to registers during distillation).",
      "source_document": "papers/2512.20157v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In multi-teacher distillation of a ViT that is trained only up to moderate resolutions (e.g., 256\u2013768 px), what positional-encoding choice allows the student to generalize to much higher, unseen test resolutions, and how are the 2D RoPE coordinates normalized (including the aspect-ratio scaling) to prevent the feature-map degradation seen with axial RoPE?",
      "answer": "Use a normalized RoPE variant (Golden RoPE) rather than standard axial RoPE. Instead of feeding absolute integer patch indices, build RoPE coordinates that are normalized by the image aspect ratio so the grid is mapped roughly into a unit square / [\u22121, 1] range: for an image of width W and height H, x-coordinates are scaled from \u2212\u221a(W/H) to \u221a(W/H) and y-coordinates from \u2212\u221a(H/W) to \u221a(H/W). This keeps RoPE\u2019s frequency scaling consistent across image sizes, avoids axial RoPE\u2019s tendency to spread attention along entire rows/columns, and preserves coherent, scale-invariant feature maps at extreme unseen resolutions (e.g., 2048\u00d72048), where axial RoPE otherwise produces degraded global structure and grid-like artifacts.",
      "source_document": "papers/2512.20157v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating an agglomerative vision backbone that has multiple teacher-specific heads (e.g., a DINOv3 head and a SigLIP2 head), how can you build a per-input, entropy-weighted head ensemble for downstream tasks like zero-shot image\u2013text classification, kNN classification, and text\u2013image retrieval\u2014specifically, how are (1) each head\u2019s confidence distribution and entropy computed from its task scores, (2) the resulting head weights defined, and (3) the final fused score formed for metric computation?",
      "answer": "For each task and head t, first compute a task-specific score vector s_t(x) for input x (e.g., cosine similarities to class prompts for image\u2013text classification, class posteriors from kNN votes, or similarity scores to a retrieval gallery). Convert scores to a confidence distribution with a temperature \u03c4>0: q_t(x)=softmax(s_t(x)/\u03c4). Compute the entropy of that distribution: H_t(x)=\u2212\u2211_i q_{t,i}(x) log q_{t,i}(x). Define per-input weights that favor low-entropy (more confident) heads via an exponential sharpening with \u03b3>0 and normalize across heads: \u03b1_t(x) \u221d exp(\u2212\u03b3 H_t(x)) with \u2211_t \u03b1_t(x)=1. Finally fuse head scores as a weighted sum s_ens(x)=\u2211_t \u03b1_t(x) s_t(x), and compute the task metric from the fused scores (Top-1 for classification/kNN; Recall@1 for retrieval using fused similarities).",
      "source_document": "papers/2512.20157v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a two-stage natural-language document image retrieval pipeline where a contrastive VLM (e.g., SigLIP) first recalls the top-100 candidate pages, what is the re-ranking training recipe used to get fine-grained image\u2013text matching (i.e., what interaction module is tuned/added, how are hard negatives chosen, and which loss terms are combined during training)?",
      "answer": "The re-ranking stage refines the initial top-100 list by introducing fine-grained image\u2013text interaction via cross-attention: it either fine-tunes the existing cross-attention modules of models such as BLIP-ITM and Pix2Struct, or adds an extra cross-attention module after the encoders. Training uses a hard-negative mining strategy based on the recall stage outputs, selecting challenging negative document images (specifically targeting the top-10 hard negatives from the recall results). Optimization combines pointwise and pairwise losses; empirically, removing the pairwise loss slightly reduces performance, while removing the pointwise loss slows convergence and hurts performance more, and the best configuration uses a single added cross-attention layer (more layers make convergence harder).",
      "source_document": "papers/2512.20174v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a natural-language\u2013to\u2013document-image retrieval setup where you want to reuse OCR-free generative VDU backbones as the image encoder, how is the recall-stage fine-tuning designed to align text and image embeddings\u2014specifically: what representation is extracted from the VDU visual encoder, which encoder(s) are kept frozen vs adapted, what extra projection/alignment layers are added (including their structure/output dimension), and which contrastive objectives are used for CLIP/BLIP compared to SigLIP?",
      "answer": "The recall-stage fine-tuning aligns a text encoder with a (generative) VDU visual encoder using contrastive learning.\n\n- Visual representation: for generative VDU models, the method takes the final output of the visual encoder and applies mean pooling to obtain a single visual embedding.\n- Frozen vs tuned components: the VDU visual encoder is kept frozen; the text side is adapted by applying LoRA to the CLIP/BLIP text encoders plus an extra alignment/projection layer so the text embeddings align with the frozen VDU visual features. (For SigLIP, both the text and visual encoders are fine-tuned with LoRA.)\n- Added alignment/projection: mean-pooled visual features are passed through linear layers consisting of two layers with a residual connection, mapping the original feature dimension to a 512-dim embedding.\n- Objectives: CLIP and BLIP are optimized with an InfoNCE contrastive loss; SigLIP is fine-tuned with Sigmoid loss (with LoRA applied to both encoders).",
      "source_document": "papers/2512.20174v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a scalable natural-language document image retrieval system, what efficiency trade-offs are highlighted between (1) an OCR-based text retrieval pipeline, (2) LVLM-based retrievers that use large/multi-vector embeddings (e.g., DSE/ColPali), and (3) a two-stage OCR-free approach that uses a strong contrastive VLM for single-vector recall (e.g., SigLIP) followed by re-ranking\u2014specifically in terms of what dominates offline encoding time, how embedding representation impacts storage footprint, and what practical optimization is suggested to reduce re-ranking latency?",
      "answer": "The document contrasts three deployment regimes:\n\n- **Offline encoding bottleneck:** OCR-based text retrieval is dominated by the OCR step, making offline encoding much slower than directly embedding images with a contrastive VLM. LVLM-based retrievers also have relatively long offline encoding because of model size/compute, whereas a strong contrastive VLM used for recall can encode images much faster.\n\n- **Storage footprint from embedding design:** Using **single-vector dense embeddings** (as in SigLIP recall, and similarly in BGE/DSE) keeps per-document storage small, while **multi-vector embeddings** (as in ColPali) greatly increase storage requirements.\n\n- **Reducing re-ranking latency:** In the re-ranking stage, generating per-candidate visual embeddings is a significant cost, and the suggested practical optimization is to **parallelize visual encoding over the top retrieved candidates (e.g., top-100)** to reduce total time consumption.\n\n(These points come from the time/storage analysis and discussion of single-vector vs multi-vector representations and re-ranking computation.)",
      "source_document": "papers/2512.20174v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a latent-space generative image codec that uses VQ-indices, how can a code-prediction module be used to improve semantic consistency without becoming part of the inference pipeline, and what does the ablation comparison show about (i) removing code-prediction entirely, (ii) inserting the predictor into the network for reconstruction, versus (iii) using code prediction only as an auxiliary training loss?",
      "answer": "A code-prediction module can be trained to predict the discrete VQ indices from the (compressed/decoded) latent and used only to define an auxiliary supervision loss (cross-entropy on the target indices plus an L2 term between original and reconstructed latents). This encourages the transmitted latent to retain enough semantic information to correctly predict codebook indices, improving semantic consistency, while the predictor itself is not used at inference.\n\nThe ablation shows that the best approach is using code prediction purely as supervision: removing code-prediction supervision degrades compression performance, and inserting the predictor into the reconstruction network (i.e., using predicted codes as part of the forward path) causes a large performance drop, whereas using it only as an auxiliary loss achieves the best BD-Rate on the evaluated FID\u2013bpp curve (normalized as the reference setting).",
      "source_document": "papers/2512.20194v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In ultra-low-bitrate learned image compression, what is the rationale for replacing a standard factorized (continuous) hyperprior with a vector-quantized categorical hyper module, and how does this categorical hyper module integrate into the entropy model at inference time (i.e., how are hyper-latents represented/encoded, what kind of information they tend to carry, and how they condition probability estimation for the main latents alongside spatial context)?",
      "answer": "A factorized continuous hyperprior tends to spend many bits on side information because its hyper-latent z often encodes low-level details (e.g., color/texture) at ultra-low bitrate, leading to high bit cost. A categorical hyper module instead vector-quantizes the hyper-latent using a learned hyper codebook Ch so that z is biased toward capturing essential high-level semantic/structural information that can be conveyed with significantly fewer bits.\n\nMechanism/inference integration:\n- The main latent is transformed to a code y (via latent-space analysis transform), scalar-quantized to \u0177, and entropy-coded.\n- For the hyperprior, a hyper analysis transform produces z = ha(y), then z is vector-quantized by nearest-neighbor lookup in Ch to get a discrete hyper-latent \u1e91 = VQ(z, Ch). A hyper synthesis transform produces prior features prior_z = hs(\u1e91).\n- During inference, the discrete indices of \u1e91 are compressed using fixed-length coding, with each hyper code index coded using log2(Mh) bits (Mh = number of entries in the hyper codebook).\n- The entropy model for \u0177 is conditioned on both (i) the categorical hyper module output (prior_z) and (ii) a quadtree-partition-based spatial context module that predicts probabilities using the hyper prior and previously decoded parts of \u0177.",
      "source_document": "papers/2512.20194v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In evaluating ultra-low-bitrate *generative* image compression with distributional metrics like FID/KID, what protocol can be used to compute these metrics on natural-image benchmarks with varying resolutions, how does this differ from computing FID/KID on a fixed-resolution face dataset, and why might FID/KID be omitted on a small set like Kodak?",
      "answer": "A practical evaluation protocol is to compute FID/KID directly on full images only when the dataset is fixed-resolution and sufficiently large (e.g., on the 512\u00d7512 CelebAHQ face set, FID and KID are computed over all 30k images).\n\nFor natural-image datasets with varying/high resolutions (as commonly done in generative compression), FID/KID can instead be computed on many 256\u00d7256 patches extracted from each image: split each H\u00d7W image into \u230aH/256\u230b\u00b7\u230aW/256\u230b non-overlapping patches, then shift the extraction origin by 128 pixels in both dimensions to extract an additional overlapping grid of patches ((\u230aH/256\u230b\u22121)\u00b7(\u230aW/256\u230b\u22121) patches). FID/KID are computed over the resulting large pool of patches (e.g., tens of thousands for CLIC2020 and thousands for DIV2K).\n\nFID/KID may be omitted on very small datasets like Kodak because patch extraction from only 24 images produces too few patches (only a few hundred), making FID/KID estimates unreliable/unstable compared to larger patch pools.",
      "source_document": "papers/2512.20194v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an ultra-low-bitrate generative codec that compresses VQ-VAE latents with a transform-coding module, how can a single model support rate-variable compression (instead of training separate models per bitrate), and why is this easier with transform coding than with direct VQ indices-map coding? Describe the key mechanism used in the transform pipeline and how training is set up to cover multiple rates.",
      "answer": "Rate-variable compression is enabled by using transform coding (mapping the latent to a unified Gaussian-like code distribution) rather than directly entropy-coding the discrete VQ index map, because the VQ codebook/index distribution is essentially fixed and does not naturally adapt to different target rates. In the transform-coding pipeline, variable rate is achieved by varying the parameters of the (Gaussian) latent code distribution (e.g., means and scales), and GLC follows the DCVC-style design by inserting learned scalers in the transform coding module (implemented as learned scalers qenc and qdec) to modulate the code for different rates. During optimization, the model is trained with different rate\u2013distortion trade-off weights \u03bb (within a batch) so that a single set of weights can operate at multiple bitrates.",
      "source_document": "papers/2512.20194v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For high-resolution image compression with a VQ-VAE\u2013style latent auto-encoder, why can global self-attention in the latent space be suboptimal, and how does switching to a latent \u201cpatch attention\u201d design change the attention computation? Also, what does the ablation trend on a high-resolution benchmark indicate about the relative performance of global attention versus different patch granularities (e.g., large vs medium vs small patches)?",
      "answer": "Global attention in the latent space becomes less effective on high-resolution images because correlations between far-apart regions/objects are relatively weak, so spending capacity on fully global interactions is inefficient. The patch-attention alternative first partitions the latent feature map into patches and then applies attention within patches (instead of over the entire latent grid), focusing modeling on more local correlations that matter for high-resolution content.\n\nThe reported ablation on a high-resolution test set shows that patch attention consistently improves compression performance over global attention, with a medium patch granularity performing best among the tested options; using patches that are too large underperforms the best setting, and making patches very small also slightly degrades relative to the best medium size.",
      "source_document": "papers/2512.20194v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a unified underwater image enhancement model designed for *coupled* degradations, how is the training objective constructed to explicitly balance color fidelity, sharpness, and contrast while still enforcing perceptual realism? Describe (1) how the multi-objective AquaBalanceLoss is computed (what three indices it aggregates and how each index is measured), (2) how this loss is combined with the other supervision terms used during training, (3) what data augmentations are applied during training, and (4) which evaluation metrics are reported on standard underwater benchmarks and why the CCF metric can be misleading when comparing against traditional (non-deep) enhancement methods.",
      "answer": "(1) AquaBalanceLoss is defined as a squared difference between an aggregated attribute score computed on the output versus the input, with a bias term: AquaBalanceLoss = |AbL_out \u2212 AbL_in + \u03bb_imp|^2. The aggregated score AbL is a weighted sum of three indices: a color index L_coi, a sharpness index L_si, and a contrast index L_cti (AbL = c1\u00b7L_coi + c2\u00b7L_si + c3\u00b7L_cti). L_coi measures underwater color imbalance using opponent color spaces RG and YB (RG = R\u2212G; YB = (R+G)/2 \u2212 B) and combines robust statistics (alpha-trimmed mean and alpha-trimmed variance) of these components. L_si measures sharpness as a weighted sum of channel edge strengths computed from Sobel edge maps (via an edge-map-energy/EME-style measure). L_cti measures contrast by computing block-wise contrast using max/min values within blocks (top=max\u2212min, bot=max+min) with a weighting factor and an exponent parameter controlling the entropy scale.\n\n(2) The overall training loss is a weighted sum of four terms: a perceptual loss based on VGG16 features (Loss_vgg16), a KL-divergence loss (Loss_KL) that aligns the learned feature distribution with a reference distribution to promote perceptually natural/structurally coherent outputs, a reconstruction loss (Loss_Re) to keep outputs close to ground truth, and the AquaBalanceLoss term to explicitly balance color/clarity/contrast.\n\n(3) Training-time data augmentation uses horizontal flipping, random rotations by 90\u00b0, 180\u00b0, and 270\u00b0, and random cropping of inputs into 256\u00d7256 patches.\n\n(4) Reported benchmark metrics include PSNR, SSIM, UIQM, UCIQE, CCF, and PCQI (and also model efficiency indicators such as parameter count, FLOPs, and runtime). A key caveat is that CCF\u2014being a statistical measure of colorfulness/contrast/clarity\u2014can give high scores to traditional non-deep methods even when their results look visually distorted, because it is not sensitive to over-enhancement or unnatural color distributions; it should therefore be treated as an auxiliary metric and interpreted alongside subjective assessment and other no-reference measures.",
      "source_document": "papers/2512.20213v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a unified underwater image enhancement pipeline that uses style-modulation to handle *coupled* degradations, how can a \u201cprobabilistic bootstrap\u201d module be implemented so that it (i) aligns the global statistics of mined content features with training/reference statistics while preserving scene structure, and (ii) avoids the detail loss and boundary artifacts that AdaIN-style normalization can introduce? Describe the full mechanism end-to-end: how the module computes and perturbs per-channel mean/standard-deviation (including how Gaussian distributions are constructed and what is extracted to form AdaIN\u2019s normalization parameters), what role the subsequent post-processing module plays (grayscale correction + border enhancement mask), and what ablation evidence shows about the separate contributions of PB vs. post-processing when evaluating on coupled-degradation test sets (e.g., which failure modes appear when either is removed, and which no-reference metrics improve most in the complete system).",
      "answer": "A workable probabilistic-bootstrap (PB) design starts from the coupled-degradation feature map F1 produced by the joint feature extraction (JFE) encoder\u2013decoder and then uses AdaIN-style feature normalization, but with stochastic, learned perturbations of the normalization statistics so the adjusted features match training/reference statistics while keeping the scene layout.\n\n1) **How PB forms AdaIN\u2019s normalization parameters via a probabilistic \u201cbootstrap\u201d distribution (PG submodule):**\n- From F1 (B\u00d72C\u00d7H\u00d7W), compute **per-channel mean and standard deviation** over spatial dimensions:\n  - \u03bcf = (1/(H\u00b7W)) \u03a3_i \u03a3_j f_ij\n  - \u03c3f = sqrt((1/(H\u00b7W)) \u03a3_i \u03a3_j (f_ij \u2212 \u03bcf)^2)\n- Feed these statistics through **1\u00d71 convolutions** to produce parameters used to define distributions:\n  - From \u03bc: produce a and m (both B\u00d7N\u00d71\u00d71)\n  - From \u03c3: produce b and n (both B\u00d7N\u00d71\u00d71)\n- Construct **n-dimensional Gaussian distributions** for mean and std:\n  - N_mean = N(a, m^2)\n  - N_std = N(b, n^2)\n- **Extract** the distribution\u2019s mean and std to obtain the *final* AdaIN conditioning statistics:\n  - \u03bc_optimal = ExtractMean(N_mean)\n  - \u03c3_optimal = ExtractStd(N_std)\nThese \u03bc_optimal and \u03c3_optimal are then used as the unique inputs to AdaIN-style modulation so the PB module can \u201cunify\u201d global degradation adjustment (color/contrast/structure) while preserving semantic/scene structure. An ESE-ResBlock is integrated to keep the network focused on key features and improve adaptability across different underwater degradation conditions.\n\n2) **Why a dedicated post-processing module (FPP) is needed and what it does:**\nBecause AdaIN-like mean/variance modulation can be overapplied, PB can cause **excessive smoothing**, **detail/color-fidelity loss**, and **boundary distortions** after passing through earlier stages. The feature post-processing (FPP) module is introduced to correct these artifacts using:\n- **Grayscale correction:** rescales each channel to align its mean with a target grayscale mean to restore a more natural brightness/color balance:\n  - I\u2032_c(x,y) = I_c(x,y) \u00b7 (\u03bc_g / \u03bc_c)\n- **Border Enhancement Mask (BEM):** applies a Gaussian filter to extract low-frequency content Flow = G(F3, \u03c9), builds an enhancement mask FBEM = F3 \u2212 Flow + \u03bb, and then nonlinearly fuses FBEM with F3 to **enhance structure/edges** and mitigate boundary shifts and artifacts.\n\n3) **Ablation evidence about PB vs. FPP on coupled degradations (and which metrics benefit):**\n- **JFE-only** is inadequate for unified coupled-degradation handling: it fails to jointly resolve degradations such as **color cast + low illumination + backscattering** when they occur together.\n- Removing **PB** yields *incomplete enhancement* because PB is the component responsible for **global consistency adjustment** and for leveraging the latent information mined by JFE; adding PB improves global consistency and shows strong synergy with JFE.\n- Removing **FPP** harms **detail/texture preservation**; FPP is the module responsible for refining texture and improving edge sharpness, and PB+FPP together produce more consistently **artifact-free** results with better local details.\n- Quantitatively, the complete system achieves the best overall performance across the reported no-reference metrics for coupled degradations\u2014specifically it attains the highest values in **UIQM**, **UCIQE**, and **CCF** in the referenced ablation comparison\u2014showing that PB primarily drives global consistency/overall enhancement while FPP primarily drives detail and artifact/boundary correction, and both are needed for the strongest coupled-degradation performance.",
      "source_document": "papers/2512.20213v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a unified underwater image enhancement network aimed at handling *nonlinearly coupled* degradations, what architectural design choices in the joint feature extraction (JFE) stage make it better suited to model coupled color-cast/low-illumination/backscatter than a \u201cstandard\u201d CNN encoder\u2013decoder, and what ablation evidence supports those choices? In your answer, describe (1) how the JFE encoder\u2013decoder is organized (multi-stage FE modules, where pooling happens, and how skip/concat fusion is done), (2) how the embedded ESE-ResBlock performs channel-wise recalibration (the GAP\u21921\u00d71 conv\u2192ELU\u21921\u00d71 conv\u2192sigmoid gating form and the residual connection), (3) why using a higher-nonlinearity activation (ELU) matters for strongly coupled degradations, and (4) what qualitative failure modes are observed when (i) ELU is replaced with LeakyReLU and when (ii) the ESE-ResBlock/FE modules are replaced by a standard SE-ResNet block plus conventional conv+pooling layers.",
      "answer": "(1) JFE is built as an encoder\u2013decoder feature-mining unit with three consecutive feature-extraction (FE) modules in the encoder. Each FE module applies stacked convolutions/activations while keeping spatial resolution through the conv layers, and then reduces spatial size only via MaxPooling; repeating this yields progressively downsampled feature maps (e.g., after pooling, the spatial size is reduced by 2\u00d7 each stage). The decoder then progressively upsamples the deepest features and fuses them with the corresponding encoder features by concatenation at each scale (i.e., an Up() followed by Concat[encoder feature, upsampled feature]), enabling multi-scale integration of degradation cues.\n\n(2) Between the encoder and decoder, JFE inserts an ESE-ResBlock that increases nonlinear modeling capacity while keeping spatial size. It applies two 3\u00d73 convolutions with ELU in between, then an efficient squeeze\u2013excitation style channel recalibration: global average pooling (GAP) is followed by a 1\u00d71 conv, ELU, another 1\u00d71 conv, and a sigmoid to produce per-channel gates; the input feature tensor is reweighted by these gates (element-wise multiplication). A residual connection adds the original encoder feature map back to the recalibrated output, improving gradient flow and stabilizing training.\n\n(3) ELU is kept because stronger nonlinearity is needed when degradations interact (rather than add linearly): the network must represent nonlinear mixtures of color distortion, scattering/backscatter, illumination drop, and texture loss. With ELU, the JFE stage has higher expressive power for joint modeling of these coupled distortions, which improves both correction strength and generalization.\n\n(4) Ablations show clear coupled-degradation failure modes when these design choices are removed:\n- Replacing ELU with LeakyReLU leads to diminished color-correction completeness, weaker backscatter suppression, and lower local contrast; the variant also shows poorer brightness and local color contrast, indicating reduced ability to jointly correct coupled color/visibility degradations.\n- Replacing the ESE-ResBlock with a standard SE-ResNet block and substituting the FE module with conventional convolution+pooling causes incomplete enhancement under coupled conditions (e.g., failing to sufficiently increase brightness after color correction in a color-cast+low-illumination setting), and insufficient detail enhancement/brightness improvement with overall inferior color rendering\u2014consistent with weaker coupled-feature mining and less effective adaptive recalibration.",
      "source_document": "papers/2512.20213v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "If you want to justify an underwater image enhancement (UIE) network as a *preprocessing* step for downstream perception, what evaluation protocol can you use to show that the enhanced images actually improve high-level vision performance? Describe (1) which representative downstream tasks and backbone models are used to probe different kinds of visual information (geometry, boundaries, semantics, saliency, detection), (2) which underwater detection datasets are used for the object-detection test, (3) which detection metrics are reported, and (4) what overall trend is observed when comparing performance on original vs. UIE-enhanced inputs and why this trend is expected from the kinds of degradations being corrected.",
      "answer": "A concrete protocol is to run fixed, off-the-shelf underwater vision models on both the original degraded images and the UIE-enhanced outputs, and compare their task metrics.\n\n1) Tasks/models used: depth estimation with MiDaS, edge detection with RCF, semantic segmentation with SUIM-Net, saliency detection with U2Net, and object detection with YOLOv5.\n\n2) Detection datasets: RUOD and URPC2020 are used for the quantitative object-detection evaluation.\n\n3) Detection metrics reported: precision (P), recall (R), mAP@0.5 (mAP50), and mAP averaged over IoU thresholds (mAP50\u201395).\n\n4) Observed trend and rationale: detection performance improves consistently after applying UIE (P/R/mAP increase on both datasets). This is consistent with the claim that correcting coupled underwater degradations (e.g., color casts, low contrast/illumination, scattering-induced haze and detail loss) yields more color-consistent, higher-contrast, sharper inputs, which provide more reliable features for downstream models.",
      "source_document": "papers/2512.20213v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training an underwater image enhancement network with a multi-objective attribute loss intended to balance brightness/contrast/detail, what training-time evidence can you use to decide an early-stopping point, and how can loss-term dominance cause a no-reference metric to keep improving even as perceptual quality degrades? Describe the characteristic qualitative changes across epochs, the metric trends that indicate overfitting, and the resulting rationale for selecting a specific stopping epoch.",
      "answer": "A practical early-stopping signal is the combination of (i) qualitative drift toward over-bright outputs with lost fine detail and eventual visible inference artifacts, and (ii) a quantitative pattern where full-reference/structure-oriented metrics rise early and then fall as training continues. In the described training behavior, image quality improves during early training, with brightness increasing, but prolonged training progressively smooths/erases details; after extended epochs, inference errors appear, indicating overfitting. Quantitatively, metrics such as PSNR, SSIM, UCIQE (and other complementary quality indices) increase up to an intermediate epoch and then decrease afterward, while UIQM can continue increasing monotonically because the attribute-based AquaBalanceLoss increasingly dominates later training, biasing optimization toward what UIQM rewards (its components related to colorfulness/contrast/sharpness) rather than overall perceptual fidelity. Based on the loss curve stabilizing and the best tradeoff between qualitative appearance and broad metric performance/generalization, training is stopped at the intermediate epoch (300 epochs) rather than continuing to later epochs where overfitting becomes apparent.",
      "source_document": "papers/2512.20213v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a camera-centric 3D detector that uses LiDAR only as a source of geometric priors (i.e., without a dedicated 3D sparse-conv LiDAR backbone), how can a depth-aware embedding module fuse LiDAR-derived depth with intermediate image features using quaternion feature adaptation, and what supervision signal is used to train the initial depth-aware feature?",
      "answer": "A depth-aware embedding (DAE) first spatially aligns the previous-stage depth feature with the current image feature map (padding/interpolation), then reduces both streams to a compact aligned channel dimension using 1\u00d71 convolutions (g1 for the image feature and g2 for the depth feature). The two reduced features are concatenated and passed through a quaternion mapping layer: internally the image feature is placed on the real axis and the LiDAR-derived depth feature on the first imaginary axis (Qh = F\u0302_img\u00b7r + \u0108_depth\u00b7i + 0\u00b7j + 0\u00b7k). The quaternion layer applies a Hamilton product with learnable quaternion weights (initialized orthogonally from a suprasphere) followed by an element-wise activation on the real/imaginary components, and the fused representation is then mapped back to the original feature dimensionality to produce the depth-aware feature. The initial depth-aware feature (M_depth) is supervised by a depth map derived from the LiDAR point cloud.",
      "source_document": "papers/2512.20217v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When adapting a vision-based BEV transformer 3D detector to use LiDAR only as an auxiliary geometric cue (i.e., without a dedicated 3D sparse-conv LiDAR backbone), what alternative strategies can be used to inject LiDAR-derived geometry embeddings into the camera-based network, which strategy yields the best overall detection improvement, and why do simple \u201csummation\u201d injections provide only limited gains compared to a progressive fusion strategy?",
      "answer": "Four injection strategies are contrasted: (1) summing the LiDAR geometry embedding into the network at the input (\u201cinput summation\u201d), (2) summing it into deeper layers (\u201cdeep summation\u201d), (3) feeding it through a separate geometry-integration path (\u201cseparate input\u201d), and (4) progressively injecting geometry information layer-by-layer (\u201cprogressive input/progressive response\u201d). The progressive input strategy achieves the best overall detection performance because it provides an effective integration structure that incrementally refines features, allowing the network to progressively incorporate and preserve useful geometric cues from LiDAR across layers. In contrast, simple summation\u2014whether at the input or deeper layers\u2014lacks such an integration mechanism, so directly adding point-cloud-derived features yields only minimal improvements since the injected information is not effectively utilized by the subsequent feature extraction/BEV encoding stages.",
      "source_document": "papers/2512.20217v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating whether a diffusion model\u2019s sampling procedure is reducing or amplifying a known spurious correlation (bias) without retraining, what end-to-end protocol can be used to (i) define/estimate the model\u2019s bias level from generated samples, (ii) choose an appropriate \u201cbias oracle\u201d for synthetic color-bias data versus face-gender bias data, and (iii) identify which sampler hyperparameters should be systematically varied to study conditioning strength, computational cost, and stochasticity effects?",
      "answer": "A training-free evaluation protocol is:\n\n(i) Define bias as the probability \u03c1 that a generated sample is bias-aligned (target attribute agrees with the spuriously correlated bias attribute). Estimate \u03c1model via a na\u00efve Monte-Carlo procedure: for each target condition y (or for multiple classes), generate N i.i.d. samples x_i^y ~ p\u03b8(x|y), apply an oracle fb(x)\u2208{0,1} that flags whether x is bias-aligned, and compute the sample mean \u03c1\u0302model^y = (1/N)\u2211_i fb(x_i^y) (and average over K classes as \u03c1\u0302model = (1/KN)\u2211_k\u2211_i f_{b_k}(x_i^{y_k})). Confidence intervals can be obtained by treating the estimator as binomial and using Clopper\u2013Pearson intervals.\n\n(ii) Use a bias oracle appropriate to the bias type: for Biased MNIST (color\u2013digit correlation), infer the bias attribute by computing the average color of non-white pixels and selecting the closest predefined class color. For BFFHQ and Stable Diffusion (gender bias in faces), infer gender by prompting a multimodal VLM (LLaVA-v1.5-7B) with the image and a yes/no question (\u201cIs the person a male?\u201d), keeping only images where the model outputs a clean yes/no.\n\n(iii) Systematically vary sampler hyperparameters grouped into: conditioning strength (classifier-free guidance scale w), computational cost (number of sampling/integration steps nsteps and, in continuous samplers, the integration scheme/second-order correction that changes network evaluations per step), and stochasticity (in Karras/EDM-style sampling: Schurn controlling the variance of injected \u2018fresh noise\u2019 and Stmin/Stmax defining the time window where noise is added; in DDIM for Stable Diffusion: \u03b7 controlling sampling noise, with \u03b7=0 deterministic and \u03b7=1 matching ancestral-noise variance).",
      "source_document": "papers/2512.20233v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When pretraining a YOLOv10x-style document layout detector before fine-tuning on a large multilingual Indic layout dataset, how does pretraining on a curated bundle of diverse human-annotated document datasets compare to pretraining on a much larger bundle dominated by automatically annotated/synthetic sources, and what dataset curation choices explain the difference?",
      "answer": "Pretraining on the curated bundle (UED-mini) is more effective: it yields better downstream performance after fine-tuning than pretraining on the larger \u201call datasets\u201d bundle (UED), and English-dataset pretraining in general does not consistently improve results. The main explanation is that UED is dominated by synthetic/automatically annotated data from PubLayNet and DocBank (the vast majority of its images), which limits layout/domain diversity and hurts transfer. UED-mini is explicitly curated for higher-quality manual annotations and broader domain/logical-layout coverage: it excludes PubLayNet and DocBank due to automated annotations, excludes D4LA due to grayscale images with limited inter-domain variability, and resolves inconsistencies between DocLayNet and M6Doc by merging/remapping semantically similar labels (e.g., table-caption and image-caption \u2192 caption; DocLayNet text \u2192 M6Doc paragraph) and dropping less significant labels, producing a unified 25-label taxonomy.",
      "source_document": "papers/2512.20236v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In multilingual document layout parsing, what does cross-lingual evaluation across Indic scripts show about (a) how much performance drops when a detector trained on one language is applied zero-shot to an unseen language/script, and (b) whether training one model on all scripts is better or worse than training separate monolingual models\u2014and what evidence indicates that script effects become visible only after controlling for training-set size?",
      "answer": "Cross-lingual testing shows that a detector trained on a single language generalizes poorly to unseen scripts: zero-shot evaluation on unseen languages typically drops by about 20\u201325 mAP points, implying heavy reliance on script-specific visual cues. Training a single model on the full multilingual IndicDLP (all scripts) performs better than training separate monolingual models, because the much larger pooled training set outweighs the lack of script homogeneity. However, when training-set size is controlled using a smaller multilingual subset sampled across languages with similar domain proportions, monolingual \u201csame-script\u201d training has higher performance than this size-matched multilingual baseline\u2014indicating that script influences layout prediction once data volume is adequately accounted for. Script sharing gives only slight transfer gains, and distribution shift can still make cross-lingual shared-script transfer underperform compared to language-specific training.",
      "source_document": "papers/2512.20236v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking different detector/transformer-based layout parsers on a large, multilingual document-layout dataset, what architectural or training differences can explain (1) why a YOLOv10-style detector and a DocLayout-YOLO variant achieve very similar accuracy, (2) why a robustness-oriented DINO variant can outperform vanilla DINO on degraded scans, and (3) why a very large vision\u2013language model can still lag behind specialized detectors?",
      "answer": "(1) A YOLOv10x detector and DocLayout-YOLO perform similarly because DocLayout-YOLO is essentially a YOLOv10 variant with document-specific design optimizations, so their core detection paradigm is closely aligned.\n\n(2) A robustness-focused variant of DINO (RoDLA) can outperform standard DINO because its robustness enhancements make it better suited to naturally degraded document images present in the dataset.\n\n(3) A large vision\u2013language model (e.g., Florence-2) can still underperform specialized object detectors because its pretraining is oriented toward broad natural-image diversity rather than document-layout structure, so scale/capacity does not compensate for the mismatch in inductive bias and pretraining emphasis for layout parsing.",
      "source_document": "papers/2512.20236v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating document-layout detectors across diverse document domains, which kinds of domains tend to be the hardest versus easiest, and what specific layout characteristics drive those domain-wise mAP differences?",
      "answer": "Hardest domains are those whose layouts differ strongly from the rest or are visually dense/heterogeneous: Forms and Acts & Rules perform poorly because their layouts are substantially different from other domains; Newspapers and Brochures are difficult because they contain a wider range of unique regions; and dense multi-column layouts in Newspapers and Magazines further increase detection difficulty. Easiest domains are those with simpler, more uniform and predictable layouts\u2014such as Novels and Research Papers\u2014leading to higher mAP due to better detection accuracy.",
      "source_document": "papers/2512.20236v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When you need a document-layout detector that *transfers well across datasets* (i.e., you train on one dataset and evaluate zero-shot on a different dataset), what choice of training dataset yields the strongest cross-dataset generalization on shared \u201cphysical\u201d region types like **paragraph, table, and figure**, and what evidence shows it suffers a smaller performance drop than detectors trained on DocLayNet, M6Doc, or D4LA?",
      "answer": "Training on **IndicDLP** yields the strongest cross-dataset generalization. Evidence is that a YOLOv10x model trained on IndicDLP retains relatively high mAP when evaluated on other datasets for the shared labels (paragraph/table/figure), e.g., it achieves around **76/73/69 mAP on M6Doc** and still non-trivial performance on **D4LA (\u224859/31/28)** and **DocLayNet (\u224849/67/43)**. In contrast, models trained on **DocLayNet, M6Doc, or D4LA** show much larger drops when evaluated on the other datasets (e.g., very low table/figure mAP when transferring to D4LA or DocLayNet). The underlying explanation given is that IndicDLP\u2019s broad **layout diversity across many languages and domains** produces more robust, distribution-tolerant visual features, so cross-dataset performance degrades less even for basic region categories.",
      "source_document": "papers/2512.20236v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a degradation-adaptive Mixture-of-Experts design for unified hyperspectral image restoration, how can continuous degradation descriptors be used to route inputs to a sparse set of experts, and what specific mechanism can be added to the gating computation to encourage balanced expert utilization?",
      "answer": "Use the continuous degradation descriptors (degradation prompts, DP) jointly with the input features to compute expert gating scores via a projection function and softmax, then sparsify the gate with a top\u2011k operator so only the k highest-scoring experts are activated. The activated experts\u2019 outputs are combined as a weighted sum using their gating scores, and then fused with shared (degradation-agnostic) features. To promote balanced expert usage, add Gaussian noise (\u03b5) to the pre-softmax gating logits (i.e., to the projected joint representation) before applying softmax/top\u2011k, which encourages load balancing across experts.",
      "source_document": "papers/2512.20251v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a unified hyperspectral restoration model that uses Spatial\u2013Spectral Adaptive Modules (SSAMs) as experts, how does an SSAM combine spatial and spectral feature extraction, what constraint is imposed on the fusion weights, and what concrete architectures are used for the spatial and spectral branches?",
      "answer": "Each SSAM processes an input feature tensor with two parallel branches\u2014a spatial feature extractor Es(\u00b7) and a spectral feature extractor Ec(\u00b7)\u2014and fuses their outputs using learnable, expert-specific coefficients: F_expert^(i) = \u03bb_s^(i)\u00b7Es(F) + \u03bb_c^(i)\u00b7Ec(F). The fusion weights are constrained to sum to 1 (\u03bb_s^(i) + \u03bb_c^(i) = 1), enabling each expert to emphasize spatial vs. spectral cues depending on degradation patterns. In the instantiation used here, Es(\u00b7) is implemented as a Transformer-based spatial feature extractor, while Ec(\u00b7) is implemented as a 1D convolution-based spectral feature extractor.",
      "source_document": "papers/2512.20251v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When designing a unified hyperspectral image restoration system that does not use explicit degradation labels, how can a compact \u201cdegradation prompt\u201d be constructed from spatial\u2013spectral degradation metrics: what criteria are used to prune an initial pool of candidate metrics (including how redundancy and discriminability are checked), what procedure is used to measure which metrics best separate degradation types, and which six metrics end up forming the final prompt?",
      "answer": "A degradation prompt can be built by starting from a broad pool of candidate spatial\u2013spectral statistics (covering entropy-, gradient-, frequency-, and correlation-based metrics) and pruning them using three criteria: (1) interpretability (each metric should correspond to a clear physical degradation phenomenon), (2) coverage (the selected set should jointly represent both spatial and spectral degradation characteristics), and (3) discriminability with low redundancy (remove highly correlated metrics via Pearson-correlation filtering, and keep metrics that are important for distinguishing degradation types).\n\nTo quantify discriminability/importance, the approach computes all candidate metrics on a labeled set of degraded HSI patches and trains a Random Forest classifier to predict the degradation type; the classifier\u2019s feature-importance scores are then used to rank metrics.\n\nThe final degradation prompt is formed from six selected metrics: high-frequency energy ratio (HFER), spatial texture uniformity (STU), spectral curvature mean (SCM), spectral curvature standard deviation (SCSD), gradient standard deviation (GSD), and spatial correlation coefficient (SCC).",
      "source_document": "papers/2512.20251v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a Mixture-of-Experts unified hyperspectral image restoration model, how does using a continuous degradation-metric prompt as the routing signal compare to (i) frequency-based routing and (ii) routing by discrete degradation type in terms of restoration quality, and what does this performance gap imply about the usefulness of continuous degradation descriptors for expert selection?",
      "answer": "Using degradation-prompt (DP) routing yields the best average restoration quality, achieving about 51.43 dB PSNR and 0.989 SSIM. Replacing DP routing with frequency-based routing drops performance to about 47.72 dB / 0.983, and routing by discrete degradation type drops it further to about 46.27 dB / 0.982. Thus DP routing improves over frequency-based routing by roughly +3.71 dB PSNR and +0.006 SSIM, and over degradation-type routing by roughly +5.16 dB and +0.007 SSIM. This gap indicates that continuous degradation descriptors provide more informative, fine-grained cues for selecting experts than either generic frequency statistics or coarse discrete degradation labels, leading to better routing and overall restoration.",
      "source_document": "papers/2512.20251v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When setting up experiments for a unified hyperspectral image restoration model intended to work without explicit degradation labels, what is an appropriate training/evaluation protocol to test both multi-task performance and generalization to unseen degradations, and what concrete data preparation (patching) and optimization objective are used to make the comparison fair across natural-scene vs remote-sensing domains?",
      "answer": "A suitable protocol uses (1) unified training/evaluation, where one model is jointly trained on five restoration tasks\u2014Gaussian denoising, Gaussian deblurring, super-resolution, image inpainting, and spectral band completion\u2014and evaluated on those tasks; and (2) zero-shot generalization, where the same trained model is directly evaluated on unseen degradation types (motion deblurring and Poisson denoising) without any fine-tuning.\n\nFor data preparation, images are randomly cropped and split into 128\u00d7128\u00d7100 patches (with 80% of each image used for patch-based fine-tuning and the remaining 20% reserved for testing). Because of the large domain gap between natural-scene and remote-sensing hyperspectral data, separate models are trained per domain. Optimization uses AdamW and an L1 reconstruction loss as the objective (batch size 4; trained 3,000 epochs for natural-scene HSIs and 1,500 epochs for remote-sensing HSIs).",
      "source_document": "papers/2512.20251v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "How does a heatmap-driven bidirectional feature\u2013semantic interaction module simultaneously (i) update per-class embeddings from pixel features and (ii) inject the refined class semantics back into the feature map, and what auxiliary supervision terms can be added to make the intermediate heatmaps spatially meaningful and the embedding space more class-separable?",
      "answer": "The interaction is driven by class heatmaps that link abstract class embeddings to image-space locations. In the feature\u2192class-embedding (F2CE) direction, the class heatmap for each class is used to select Top\u2011K high-response pixels; their pixel features are aggregated (with normalized heatmap weights) to update that class\u2019s embedding so the embedding adapts to the current image content. In the class-embedding\u2192feature (CE2F) direction, the refined class embeddings generate class-specific affine modulation parameters (\u03b3n, \u03b2n) that modulate the feature map, and the per-pixel Softmax-normalized heatmap Hsoft provides mixing weights to fuse the class-modulated features; this fused result is combined with the original features (via a learnable \u03b1) to form the enhanced feature map.\n\nTwo auxiliary supervision terms are used: (1) hierarchical heatmap deep supervision LHM, which treats each decoder-layer heatmap Hl as a low-resolution segmentation prediction (upsampled) and supervises it with cross-entropy plus Dice loss; and (2) a Fisher Discriminant loss LFD on class embeddings that minimizes the ratio of within-class scatter Sw to between-class scatter Sb (with \u03b5 for stability), encouraging intra-class compactness and inter-class separation, and is applied across decoder layers with a weighted sum. The total objective is Ltotal = Lmain + \u03bb1 LHM + \u03bb2 LFD, with Lmain = CE(P,Y) + Dice(P,Y).",
      "source_document": "papers/2512.20255v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a high-resolution remote-sensing semantic segmentation model on LoveDA and the ISPRS Vaihingen/Potsdam datasets, what evaluation protocol and preprocessing steps are used to ensure fair, reproducible comparison across methods\u2014especially regarding (i) how the datasets are split, (ii) how large aerial tiles are converted into model inputs for training/inference, and (iii) how test-set performance is obtained on LoveDA given that its test labels are not publicly available?",
      "answer": "A standard, reproducible protocol is used by following the dataset splits defined in the GeoSeg remote-sensing segmentation framework. For the ISPRS Vaihingen and Potsdam benchmarks, the original large aerial images are cropped into non-overlapping 1024\u00d71024 patches before being fed to the model for both training and inference. For LoveDA, because the ground-truth labels of the test set are not publicly available, test performance is obtained by submitting the model\u2019s predictions to the official online evaluation server (while training/validation follow the standard split).",
      "source_document": "papers/2512.20255v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an iterative remote-sensing segmentation decoder that stacks multiple heatmap-driven bidirectional feature\u2013class-embedding interaction blocks, what practical criteria determine how many interaction blocks to cascade, and what performance trend is observed when increasing the number of blocks from one to three (including the likely reason accuracy can drop when the stack becomes too deep)?",
      "answer": "Cascading more interaction blocks deepens the semantic\u2013feature co-refinement, but it also increases computation and can introduce redundant/overlapping refinement. Empirically, using only one block yields relatively weak performance because a single interaction is insufficient for adequate semantic alignment; increasing to two blocks gives a clear improvement; pushing to three blocks causes a slight degradation, attributed to information redundancy from overly deep interactions. Therefore, two blocks are selected as the default because they provide the best balance between accuracy and efficiency.",
      "source_document": "papers/2512.20255v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a heatmap-guided feature-to-class-embedding update (region-guided pooling) that uses a Top-K selection of high-response pixels per class, how does the Top-K threshold trade off between under-informative updates and noisy updates, and what Top-K setting yields the best segmentation performance (and why) on a standard HR remote-sensing benchmark?",
      "answer": "The Top-K threshold controls how many of the highest-response pixels in each class heatmap are used to pool features for updating that class embedding. If K is too small, the embedding update is based on too little evidence and misses useful contextual/appearance variation (insufficient information). If K is too large, many low-quality/irrelevant pixels are included, which injects noise into the class embedding update. Empirically, keeping the top 2% of pixels per class (Top-K = 0.02) gives the best result on LoveDA (55.49% mIoU), because this small high-confidence set captures the most representative visual information while filtering out irrelevant regions.",
      "source_document": "papers/2512.20255v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In scribble-supervised camouflaged object detection, how can you counteract the spatial label-imbalance/bias of scribble annotations by (a) predicting a per-pixel scribble-placement probability and (b) using that probability to reweight the segmentation supervision, and how are these terms combined with the main segmentation loss across multi-level decoder outputs?",
      "answer": "One approach is to add an auxiliary scribble-probability prediction head and a debiasing loss that uses this predicted probability to modulate the segmentation loss.\n\n(a) Scribble-probability prediction: at each decoder level k (k=1,2,3), predict p^scrib_{i,k}\u2208[0,1], the probability that pixel i would be labeled by a scribble. Train this predictor with a weighted binary cross-entropy over all N pixels (y_i=1 for scribble pixels, 0 otherwise):\nL^k_Scrib = -(1/N) * \u03a3_i [ y_i log(p^scrib_{i,k}) + w_n (1-y_i) log(1-p^scrib_{i,k}) ],\nwhere w_n balances the large non-scribble (unlabeled) class.\n\n(b) Debiasing/reweighting the segmentation supervision: define a debias loss (derived from focal loss) on the segmentation prediction p^seg_{i,k}\u2208[0,1] that uses a joint modulation factor depending on both annotation likelihood and prediction confidence:\nL^k_Debias = -(1/N_scrib) * \u03a3_i y_i * (1 - p^scrib_{i,k} p^seg_{i,k})^\u03b3 * log(p^seg_{i,k}),\nwhere the sum is taken over scribble-labeled pixels (via y_i), N_scrib is the number of scribble pixels, and \u03b3 controls modulation strength. Compared to standard focal loss\u2019 (1-p^seg)^\u03b3 term, (1 - p^scrib p^seg)^\u03b3 down-weights regions more likely to be annotated (high p^scrib) and emphasizes pixels with low scribble probability, encouraging learning beyond the biased scribble locations.\n\nMain segmentation loss and total objective: at each level k, the camouflaged object detection loss is\nL^k_COD = L_BCE(P^seg_k, Y_mix) + L_IoU(P^seg_k, Y_mix),\nwhere Y_mix is a mixture of the generated pseudo masks and dilated scribble annotations. The overall training objective sums multi-level supervision:\nL_total = \u03a3_{k=1..3} ( L^k_COD + \u03b1 L^k_Scrib + \u03b2 L^k_Debias ).",
      "source_document": "papers/2512.20260v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a scribble-supervised COD pipeline that uses SAM for pseudo masks, how can you convert sparse foreground/background scribbles into an effective set of point prompts by prioritizing structurally informative regions while keeping the prompts spatially diverse? Describe the full sampling procedure, including (i) how local entropy is computed from a pixel\u2019s neighborhood, (ii) how candidate points are thresholded, and (iii) how spatial filtering and farthest-point sampling are used to enforce coverage and avoid point clustering.",
      "answer": "A robust way to turn scribbles into SAM-compatible point prompts is an adaptive entropy-driven point sampling procedure:\n\n1) **Compute local entropy on scribble pixels**: For each pixel (x,y) that lies on a foreground or background scribble set S, estimate the neighborhood intensity histogram {h_i} and normalize it to probabilities p_i = h_i / (\\sum_j h_j) (with p_i>0). Define local entropy\nH(x,y) = \u2212 \\sum_i p_i log p_i.\nHigh entropy indicates ambiguous/structurally informative regions that are important for segmentation.\n\n2) **Entropy-threshold candidate selection**: Form a candidate set by keeping scribble pixels whose entropy is above a normalized threshold relative to the maximum entropy value:\nC = { p \u2208 S | H(p) \u2265 \u03c4 \u00b7 H_max },\nwhere \u03c4 is a normalized entropy threshold.\n\n3) **Enforce spatial diversity**:\n- Apply a **minimum-distance constraint** during selection to prevent over-clustering in high-entropy zones: enforce ||p_i \u2212 p_j|| > d_min for all previously selected points.\n- Then apply **Farthest Point Sampling (FPS)** to pick N prompts with good coverage: iteratively select the next point p_k that maximizes its minimum distance to the already selected set,\np_k = arg max_i min_{j<k} ||p_i \u2212 p_j||,  k=2,\u2026,N.\n\nThis yields prompts that are both informative (entropy-driven) and spatially balanced (distance constraint + FPS), improving the reliability of SAM-generated pseudo masks for WSCOD.",
      "source_document": "papers/2512.20260v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a scribble-supervised camouflaged object detection pipeline that uses SAM to propose multiple candidate masks per image, how can a multi-agent debate mechanism with multimodal chain-of-thought reasoning be organized to filter noisy candidates and keep informative pseudo labels? Describe (i) what information is included in the prompts (and what the few-shot exemplars teach the agents to focus on), (ii) how the affirmative and negative debaters interact across rounds, and (iii) how the judge uses the debate history to make the final retain/discard decision.",
      "answer": "A multi-agent debate can be used to replace brittle rule/threshold filtering by having multimodal LLM agents explicitly reason about whether an image\u2013mask pair corresponds to a true camouflaged object. (i) A meta prompt introduces the COD task and constraints, and includes few-shot demonstration \u201cdebates\u201d that provide a consistent template (image captions + masked-object descriptions + chain-of-thought). These exemplars guide agents to evaluate camouflage-specific cues such as how distinguishable the object is from the background, boundary clarity/mask completeness, and potential false positives/false negatives. (ii) Two debaters participate: an affirmative debater arguing the mask should be kept and a negative debater arguing it should be discarded; they take turns in a fixed order over multiple iterations, each conditioning on the accumulated debate history to encourage divergent reasoning and thorough checking of pseudo-label correctness. (iii) A judge agent observes the full debate history, reviews and summarizes the key points from both sides, and then outputs a final decision to retain or discard the pseudo mask for subsequent COD training.",
      "source_document": "papers/2512.20260v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a scribble-supervised camouflaged object detection model that explicitly separates low- and high-frequency cues, how can you (i) extract low-frequency global semantics and high-frequency edge/texture details, and (ii) progressively fuse them across multiple spatial scales using window-based cross-attention? Describe the concrete feature construction steps (including the Laplacian pyramid residuals and hierarchical HF encoders), how low-frequency features are resized to match high-frequency scales, and how the per-window cross-attention (Q,K,V and Softmax) is applied and reintegrated during progressive fusion.",
      "answer": "A frequency-aware design can separate global context (low frequency) from boundary/detail cues (high frequency) and then align and merge them progressively.\n\n(i) Frequency-aware extraction:\n\u2022 Low-frequency semantic extractor (LFSE): Use a ViT encoder, leveraging the tendency of multi-head self-attention to behave like a low-pass filter. The RGB image is split into non-overlapping 16\u00d716 patches, each patch is flattened and linearly projected to D-dimensional tokens, and after n transformer layers the low-frequency representation LFR \u2208 R^{H/16 \u00d7 W/16 \u00d7 D} captures global semantics.\n\u2022 High-frequency detail enhancer (HFDE): Use a Laplacian pyramid. Starting from I0, build downsampled images Ik+1 = f\u2193^{k+1}(Ik) and residual (high-frequency) components HF^k = I0 \u2212 f\u2191^{k+1}(Ik+1) for pyramid levels k \u2208 {0,1,2,3}, where f\u2193 and f\u2191 are down/upsampling with scale factor 2^k. Concatenate all residual components along channels to form HF_compose \u2208 R^{H\u00d7W\u00d712}, then project to a C-dimensional high-frequency embedding HFE via a 2\u00d72 stride-2 convolution followed by layer normalization. To capture multi-scale detail, stack six hierarchical encoders in a pyramid-like network at progressively reduced resolutions, each encoder using a residual block y = ReLU(ConvLN(ReLU(ConvLN(x))) + x). This produces multi-scale/multi-level high-frequency representations {HFR_i^j | i,j \u2208 {1,2,3}, i \u2264 j}, with shape HFR_i^j \u2208 R^{H/2(i+1) \u00d7 W/2(i+1) \u00d7 2^i\u00b7C}.\n\n(ii) Progressive fusion with window-based cross-attention:\n\u2022 First reshape/resize the low-frequency LFR into a set {LFR^i | i=1,2,3} so that each LFR^i matches the spatial dimensions of the corresponding high-frequency representations {HFR_i^j}.\n\u2022 Progressive fusion proceeds from coarse to fine levels: for i = 3 down to 1, initialize L \u2190 LFR^i, then for j = 3 down to i, fuse H \u2190 HFR_i^j into L by a fusion function F(H,L), and set the fused feature at level i as F^i \u2190 L.\n\u2022 The fusion function F(H,L) is window-based cross-attention that emphasizes local alignment: at fusion level i, divide both feature maps into non-overlapping windows of size w\u00d7w. For each window, flatten to tokens and linearly project to obtain query/key/value vectors (dimension d). Attention weights are computed within the window as A = Softmax(Q_h K_l^T / \u221ad), and semantic cues are aggregated as \\hat{W}_hf^k = A V_l (i.e., low-frequency values guide high-frequency content). Finally, restore all windows back to 2D feature maps, and use a skip connection so the locally aligned, frequency-fused features can be fed hierarchically to the CNN-based decoder for multi-scale camouflaged object segmentation.",
      "source_document": "papers/2512.20260v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a scribble-supervised camouflaged object detection pipeline that generates pseudo masks from a scribble-annotated training set and then trains a detector, which benchmark datasets and segmentation metrics can be used to assess generalization, and what kind of quantitative comparison result would demonstrate that the method reaches state-of-the-art among scribble-supervised approaches while also being competitive with fully supervised COD models?",
      "answer": "A standard evaluation protocol is to use the scribble-annotated S-COD dataset to generate pseudo masks and train the model, then test generalization on the widely used COD benchmarks CAMO, COD10K, and NC4K. Performance can be reported with four common COD metrics: mean absolute error (MAE), structure measure (Sm), mean E-measure (Em), and weighted F-measure (F_w\u03b2). A result indicating state-of-the-art would be consistently better scores than prior scribble-supervised baselines (e.g., CRNet, WS-SAM, SAM-COD) across these datasets/metrics, and being competitive with (or better than) most fully supervised methods\u2014only slightly behind the strongest fully supervised approaches (notably CamoDiffusion and MCRNet) on some metrics.",
      "source_document": "papers/2512.20260v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a heterogeneous ensemble for medical image recognition where you want uncertainty-aware SHAP explanations, what concrete steps can you use to (1) train the individual models to encourage explanatory/decision diversity, (2) infer Bayesian reliability weights for each model via a tempered Dirichlet\u2013multinomial update, and (3) convert each model\u2019s pixel-wise SHAP values into Dempster\u2013Shafer basic probability assignments that can be fused into belief, plausibility, and epistemic-uncertainty maps (including how the sensitivity parameter and the uncertainty definition are used)?",
      "answer": "(1) Train a deliberately heterogeneous ensemble of architectures (e.g., a lightweight custom CNN for local texture, a ResNet-18 that captures more global structure but may speckle, and a ViT that can introduce blocky artifacts). Use the same basic supervised objective\u2014Adam with standard cross-entropy loss\u2014but train models for different numbers of epochs (e.g., 8/12/16) so they do not converge to the same local minimum, increasing diversity in their evidential signals. Inputs are resized (e.g., 128\u00d7128) and normalized (ImageNet mean/std); SHAP uses a small background set sampled from training images.\n\n(2) Estimate model reliability on a held-out validation subset by treating each model\u2019s correct-prediction count vector c as multinomial evidence under a Dirichlet prior \u03b10 (uniform/non-informative). Apply tempering with a temperature T to control the entropy of the posterior, using an update of the form \u03b1_post = \u03b10 + c/T, then sample model weights w ~ Dir(\u03b1_post). Low T makes the best model dominate; higher T keeps weaker models\u2019 weights nonzero so minority opinions can still influence fusion.\n\n(3) For a target class hypothesis H, map each model\u2019s SHAP value \u03d5_{j,i} at pixel i into a bounded evidential signal via a hyperbolic tangent with sensitivity \u03bb: u = tanh(\u03bb\u00b7\u03d5_{j,i}). Construct a Dempster\u2013Shafer mass function per pixel and model by assigning (a) evidence for H: m_{j,i}({H}) = w_j\u00b7max(0, u), (b) evidence against H: m_{j,i}({\u00acH}) = w_j\u00b7max(0, \u2212u), and (c) remaining mass to ignorance: m_{j,i}(\u03a9) = 1 \u2212 (m_{j,i}({H}) + m_{j,i}({\u00acH})). \u03bb controls the signal-to-noise ratio: too small pushes mass toward \u03a9 (high ignorance everywhere), too large turns small background noise into high belief (bloated/binary-looking maps).\n\nFuse the per-model mass functions pixel-wise with Dempster\u2019s rule; extract epistemic decision metrics as Bel(H) = m_fused({H}) (lower bound), Pl(H) = 1 \u2212 m_fused({\u00acH}) (upper bound), and define epistemic uncertainty/ignorance as U(H) = Pl(H) \u2212 Bel(H) (the \u201cepistemic gap\u201d), where high values indicate total ignorance or potential OOD/acquisition issues.",
      "source_document": "papers/2512.20288v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When converting a two-speaker text dialogue into interactive facial motion, what are the main alternative strategies for fusing the two semantic token streams in the Motion Mapper, and why does a joint self-attention fusion better support turn-taking and speaker\u2013listener coordination than simple concatenation or per-stream (dual) attention?",
      "answer": "The Motion Mapper is analyzed with three fusion strategies for the two semantic streams: (1) concatenating the two streams\u2019 motion features, (2) computing attention for each stream separately over the video hidden states and then combining the two attention outputs (\u201cdual attention\u201d), and (3) a joint self-attention mechanism.\n\nConcatenation and dual attention can separate the two speakers and produce plausible role switches, but they are limited in modeling inter-stream correlations\u2014i.e., how one participant\u2019s behavior depends on the other\u2019s\u2014so they don\u2019t capture dyadic dependencies as effectively. The joint self-attention design (inspired by an MMDiT-style block) first modulates each stream with its own normalization/linear layers, then performs joint attention to capture mutual dependencies while preserving stream-specific semantics, followed by further modulation and linear projections to produce interactive motion features. Empirically, this joint-attention fusion yields the most consistent overall performance and better coordinated motion (strong lip-sync/image realism for talking-head generation and robust, coordinated behavior for listening-head generation), indicating improved modeling of cross-stream interdependencies needed for natural turn-taking and speaker\u2013listener coordination.",
      "source_document": "papers/2512.20296v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a text-driven audio-visual dialogue generator that must make the synthesized voice match a reference face, how can a face-conditioned \u201cspeaker mapper\u201d be trained and then used at inference to transfer vocal characteristics from visual identity\u2014what are its inputs/target, and what loss objective enforces this alignment?",
      "answer": "Train the speaker mapper to regress an audio-derived speaker embedding (extracted from the target utterance during training) from visual cues computed on the reference image. Concretely, it takes a face embedding c_face and additional reference-image features c_ref (from a ReferenceNet/ResNet-style visual pathway), fuses them, and predicts \n\ne_spk^audio = f_\u03b8(c_face, c_ref).\n\nIt is optimized with an L2 regression loss between the predicted embedding and the ground-truth audio speaker embedding:\n\nL_speaker = E[ || e_spk^audio \u2212 f_\u03b8(c_face, c_ref) ||_2 ].\n\nAt inference, the predicted (face-driven) speaker embedding replaces the audio-driven one and is fed as the speaker-conditioning signal to the speech generation pipeline (the acoustic denoiser), yielding speech whose vocal characteristics are consistent with the visual persona in the reference image.",
      "source_document": "papers/2512.20296v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a text-driven system that jointly generates interactive face video (via a latent diffusion model) and conversational speech (via a text-to-semantic module plus a flow-matching acoustic model), how can classifier-free guidance be applied during training across the video and speech components\u2014i.e., which conditioning inputs are randomly dropped for each component, with what purpose, and what modification is used in the text-to-semantic module to avoid the \u201cspeed-drift\u201d problem seen with vanilla CFG?",
      "answer": "Classifier-free guidance is implemented by randomly dropping conditioning signals during training so the models learn both conditional and \u201cunconditional\u201d behaviors (later enabling stronger guidance at inference to improve quality).\n\n\u2022 Video diffusion pipeline: CFG is applied by dropping (i) the reference image, (ii) the semantic tokens, and (iii) the motion frames with probability 0.05 during training.\n\n\u2022 Speech generation:\n  \u2013 Text-to-semantic module: text inputs are dropped with probability 0.1, and a CFG-filter variant is used (instead of vanilla CFG) specifically to mitigate the speed-drift issue.\n  \u2013 Acoustic denoiser: semantic tokens together with speaker embeddings are omitted with probability 0.3 to support CFG-style guidance and improve speech quality.",
      "source_document": "papers/2512.20296v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a two-party (dual-stream) audio-visual dialogue generator on a mix of conversational data and single-speaker segments, how can the single-speaker segments be converted into the required dual-channel semantic-token representation, and how are those semantic tokens extracted and discretized in the first place?",
      "answer": "Single-speaker segments are made compatible with the dual-stream setup by creating a second (inactive) token stream filled with silence: for single-role samples, indices corresponding to silence are appended to form dual-channel semantic tokens. The semantic tokens themselves are obtained by extracting continuous speech features from the 35th layer of XLS-R and then discretizing them into 10,000 clusters using K-means; the resulting cluster indices are used as the semantic-token sequence.",
      "source_document": "papers/2512.20296v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a text-conditioned dyadic talking/listening head system that uses discrete \u201csemantic tokens\u201d to drive both the video diffusion model and the speech generator, how can adding prosody-aware information to those tokens be operationalized (what speech representation is clustered, and how), and which specific behaviors/metrics degrade when you replace them with tokens that largely omit prosody?",
      "answer": "Prosody-aware conditioning is implemented by extracting continuous speech representations from a self-supervised model that retains prosodic cues (continuous features from the 35th layer of XLS-R), then discretizing them with K-means into a 10k-codebook to form prosody-aware semantic tokens (denoted S_prosody). When these are replaced with HuBERT-based semantic tokens (noted as rich in linguistic content but limited in prosodic cues) or when prosody information is omitted, generation quality drops: for talking-head generation the lip-sync accuracy decreases noticeably (lower SyncNet alignment), and for listening-head generation the model shows degraded motion realism and coordination\u2014specifically worse Frechet Distance (FD) and residual Pearson correlation coefficient (RPCC), indicating less realistic listener motion and weaker speaker\u2013listener coordination.",
      "source_document": "papers/2512.20296v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a diffusion-transformer video virtual try-on pipeline that uses keyframes to inject fine-grained garment and background details, how can an instruction-guided keyframe sampling scheme be designed to (i) prioritize frames that capture view/action changes and garment visibility and (ii) avoid selecting redundant frames, and what frame-level scores does it rely on?",
      "answer": "One approach is to (1) use a vision-language model to parse a predefined view\u2013action instruction into target views and actions, (2) generate standardized multi-anchor pose \u201canchor frames\u201d for those targets (via human parsing/pose processing), and then (3) score every candidate frame against those anchors and the visible garment area. Concretely, for each frame f in the input video, compute a motion-difference score Sm(f) with respect to the anchor frames (lower Sm indicates larger motion/view change) and a garment-area ratio score Sr(f) measuring how much of the frame is occupied by the garment. Combine them into a single ranking score\n\nSf(f) = 1 \u2212 Sm(f) + \u03bb \u00b7 Sr(f),\n\nwhere \u03bb balances motion/view diversity vs garment visibility. After sorting frames by Sf, avoid redundancy by using a dual-selection rule that enforces both (a) a minimum score difference and (b) a minimum temporal interval between an already-selected keyframe and any new candidate, yielding a temporally spread, non-redundant keyframe set.",
      "source_document": "papers/2512.20340v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When fine-tuning a pretrained flow-matching video Diffusion Transformer for video virtual try-on with keyframe-driven latent guidance, what is the flow-matching training target (how are $x_t$ and the ground-truth velocity $v_t$ constructed from $x_0,x_1,t$), what loss is minimized, and which modules/parameters are updated during training versus kept frozen (including how LoRA is used)?",
      "answer": "Flow matching constructs the noisy training input by linearly interpolating between a random noise sample and the target-video latent: with target latent $x_1$, noise $x_0\\sim\\mathcal N(0,I)$, and timestep $t\\in[0,1]$ (sampled from a logit-normal distribution), $x_t = t\\,x_1 + (1-t)\\,x_0$. The ground-truth velocity field is the derivative of this path, which is constant: $v_t = \\frac{d x_t}{dt} = x_1 - x_0$. The model predicts a velocity $u(x_t, L, c_{txt}, t;\\theta)$ (conditioned on text embeddings and the fused guidance tokens/latents $L$), and training minimizes an L2 regression objective to match the true velocity:\n\\[\n\\mathcal L = \\mathbb E_{c_{txt},t,x_1,x_0}\\left[\\tfrac12\\,\\|u(x_t, L, c_{txt}, t;\\theta)-v_t\\|_2^2\\right].\n\\]\nDuring training, only a subset of parameters are fine-tuned: LoRA parameters added to the DiT blocks (applied to self-attention, cross-attention, and also extended to FFN linear layers), the single-image try-on model, and the mask guider and pose guider are updated (initialized from pretrained weights), while the remaining modules are kept frozen.",
      "source_document": "papers/2512.20340v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a DiT-based video virtual try-on pipeline that extracts (i) keyframe-enriched garment latents and (ii) a background latent from an inpainted garment-agnostic video plus a single \u201cmost complete\u201d keyframe, what is the exact sequence of operations used to fuse pose latents, mask latents, garment latents, noise latents, and background latents into the final DiT guidance tokens\u2014and where in the DiT block are the garment latents injected to preserve fine garment details?",
      "answer": "The background condition is first refined by blending the coarse background latent from the agnostic video with the best (highest background-completeness) keyframe background latent: \\(\\bar L_{bg}=\\alpha\\,L_{bg}+(1-\\alpha)\\,L^{\\max}_{key}\\) (with default \\(\\alpha=0.3\\)). After obtaining \\(\\bar L_g\\) (garment latents enriched using keyframes) and \\(\\bar L_{bg}\\), the guidance tokens for the DiT are built in three steps: (1) concatenate the pose latent \\(L_p\\) with the resized agnostic mask latent \\(L_m\\), then patchify to form input tokens \\(T_{inp}\\); (2) fuse these tokens with the garment latent \\(\\bar L_g\\) through a projection layer \\(R\\) to produce \\(L\\), then concatenate \\(L\\) with patchified noise \\(\\epsilon\\) to obtain \\(\\bar L\\); (3) inject \\(\\bar L_{bg}\\) into \\(\\bar L\\) via an \u201caddto\u201d operation to yield the final guidance tokens.\n\nInside the DiT, LoRA is applied to finetune attention modules, and the garment latents \\(\\bar L_g\\) are injected into the cross-attention component by substituting for the original text tokens, which is used to mitigate garment detail loss.",
      "source_document": "papers/2512.20340v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a keyframe-driven video virtual try-on system that uses an inpainted garment-agnostic video for background guidance, how can you define a single \u201cbackground completeness/integrity\u201d score for each frame that jointly measures (i) how much background area is preserved and (ii) how sharp/clear that background is? Give the explicit formula for the score, including how the background mask and the clarity term are computed, and explain how the highest-scoring frame is then used to form the final background latent via weighted fusion with the agnostic-video background latent.",
      "answer": "Define the background integrity score as the product of a background-area ratio and a clarity term:\n\n\u2022 Background integrity score:\nSbg(f) = Background Ratio(f) \u00d7 Clarity(f).\n\n\u2022 Background Ratio(f):\nBackground Ratio(f) = area(segment-background(f)) / area(f),\nwhere segment-background(f) is computed by first extracting the human region with HumanParsing and then taking the complement:\nsegment-background(f) = f \u2299 (1 \u2212 segment-human(f)).\n\n\u2022 Clarity(f): compute a Sobel edge map E over the background image and threshold edges at T (default 50). Clarity is the edge density times the normalized average edge strength:\nClarity(f) = ( |{E(x,y) > T}| / |{background pixels}| ) \u00d7 ( (1/255) \u00b7 ( \u03a3_{E(x,y)>T} E(x,y) / |{E(x,y) > T}| ) ).\n\nUsage in background fusion: select the keyframe with the maximum background completeness/integrity score, encode its cropped background region with the VAE encoder to obtain Lbg_key (denoted Lmax_key for the selected frame), and fuse it with the global background latent Lbg extracted from the agnostic video via a weighted sum:\n\\bar{L}bg = \u03b1\u00b7Lbg + (1\u2212\u03b1)\u00b7Lmax_key (with \u03b1 default 0.3).",
      "source_document": "papers/2512.20340v1.pdf",
      "mode": "textual",
      "content_refs": [
        "lines 1599-1636",
        "lines 1605-1616",
        "lines 525-547"
      ]
    },
    {
      "question": "In a video virtual try-on DiT that injects keyframe-derived garment latents via cross-attention, how can a *keyframe-aware LoRA* adaptation be formulated so that the query projection is dynamically modulated by the keyframe content? Give the explicit parameterization of the DiT query projection (including the additional keyframe-conditioned term), specify what keyframe summary is used, and explain what part of attention this term is intended to influence.",
      "answer": "A keyframe-aware LoRA can be defined by first applying standard LoRA to the DiT query projection and then adding a keyframe-conditioned low-rank update to obtain a keyframe-specific query matrix:\n\n- Standard LoRA query for DiT:\n  \\[Q_{\\text{DiT}} = W_{Q,D} + A_{Q,D}B_{Q,D}^{\\top}.\\]\n\n- Keyframe-aware query (dynamic modulation):\n  \\[Q_{\\text{key}} = Q_{\\text{DiT}} + A_{\\text{key}}B_{\\text{key}}^{\\top}\\cdot L_{\\text{avg-key}}^{\\top},\\]\n  where \\(L_{\\text{avg-key}}\\) is an averaged/summarized keyframe latent (the \u201cavg-key\u201d keyframe representation).\n\nIn the same LoRA scheme, key and value projections are parameterized as:\n\\[K_{\\text{DiT}} = W_{K,D} + A_{K,D}B_{K,D}^{\\top},\\quad V_{\\text{DiT}} = W_{V,D} + A_{V,D}B_{V,D}^{\\top}.\\]\n\nHere, \\(W_{\\*}\\) are the frozen pretrained projection weights, and the low-rank factors \\(A_{\\*},B_{\\*}\\) are trainable with rank \\(r \\ll \\min(d,k)\\). The additional \\(A_{\\text{key}}B_{\\text{key}}^{\\top}L_{\\text{avg-key}}^{\\top}\\) term is designed to modulate the attention\u2019s query projection in a way that is conditioned on keyframe content (i.e., it influences how attention is queried based on keyframe-derived information).",
      "source_document": "papers/2512.20340v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an inference-time refinement loop for text-to-image generation that verifies outputs with binary visual questions, how is the verification score computed and used to (1) decide whether a newly generated image replaces the current best, (2) break ties between candidates, and (3) trigger early stopping of the iterative process?",
      "answer": "The loop assigns each generated image a DVQ verification score defined as the fraction of visual questions answered YES: score_DVQ(I)= (1/|Q|) * sum_{q in Q} 1[ VLM(I,q)=YES]. After the first iteration, each new image is compared against the incumbent best using this score: if the new image has a higher DVQ score it becomes the new best; if the scores are equal, a multimodal side-by-side judge is invoked to pick which image better satisfies the original prompt and has fewer visual artifacts. The loop runs up to a configured maximum number of iterations, but stops early when score_DVQ(I)=1.0, i.e., all questions are answered YES.",
      "source_document": "papers/2512.20362v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an inference-time prompt-refinement pipeline for text-to-image generation that aims to improve text rendering and compositional correctness, how should the prompt editor treat (a) text enclosed in quotation marks versus (b) unquoted \u201ctitled/named/called X\u201d phrases, and what explicit editing constraints are used to avoid accidentally changing required visible text or hallucinating new text?",
      "answer": "The prompt editor uses quotation marks as a hard signal for **visible, literal text that must be rendered**. Any span in quotes is preserved *exactly* as written (including case, punctuation, and the quotation marks themselves) and is not allowed to be rephrased, translated, removed, or have quotes added/removed. By contrast, unquoted phrases like \u201ctitled X\u201d, \u201cwith the title X\u201d, \u201cnamed X\u201d, or \u201ccalled X\u201d are treated as **metadata/theme** (the artwork\u2019s name), not text that should appear in the image; these can be rewritten into descriptive style/theme wording without implying visible typography. The editor is additionally constrained to never introduce quotation marks that were not present and never delete existing quotation marks, preventing both text hallucination and corruption/loss of required rendered text.",
      "source_document": "papers/2512.20362v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a text-to-image evaluation that targets text rendering fidelity (i.e., whether the image contains the intended strings), how can you compute a length-normalized character error between the expected phrase and the OCR-read phrase, and what two aggregate metrics can you report from it to summarize both \u201csoft\u201d accuracy and strict exact correctness?",
      "answer": "Compute Normalized Edit Distance (NED) as Levenshtein(x,y) divided by max(|x|,|y|), where x is the expected text phrase and y is the OCR-extracted phrase (normalizing by length to compare phrases fairly). The evaluation extracts the expected phrases from the prompt using an LLM and performs OCR on the generated image using a vision-language model. From NED it reports (1) Text Accuracy = 1 \u2212 NED as an intuitive soft score, and (2) Exact Match Rate, defined as the percentage of phrases with NED = 0, as a strict correctness metric.",
      "source_document": "papers/2512.20362v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a scene-graph\u2013style evaluation for text-to-image generation where a prompt is decomposed into binary visual questions with prerequisite dependencies (e.g., checking an object exists before checking its color), how is the compositional consistency score computed so that dependent questions only contribute when their parent conditions hold, and what exact question set is averaged over?",
      "answer": "The prompt is converted into binary visual questions arranged in a dependency graph G. A question is evaluated only if its parent conditions are satisfied (e.g., an attribute is checked only after the corresponding object is confirmed). The score averages YES answers only over the subset of questions whose parents hold: \n\nscore_DSG(I) = (\\sum_{q \\in Q_valid} \ud835\udfd9[answer_q = YES]) / |Q_valid|,\n\nwhere Q_valid is the set of questions with satisfied parent conditions, yielding a hierarchy-aware estimate of prompt\u2013image faithfulness.",
      "source_document": "papers/2512.20362v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When turning a text-to-image prompt into a constraint set for iterative, question-guided refinement, what two generic \u201cdiagnostic\u201d visual questions can be added to the usual object/attribute/relation questions, how must they be answered, and under what condition should the text-related diagnostic be included versus always-on?",
      "answer": "In addition to the base question set derived from the prompt (covering objects, attributes, spatial relations, and high-level scene properties), you can append two generic diagnostic constraints: (1) a text-visibility/legibility check (e.g., \u201cIs the text in the attached image fully clear and easily readable by a human?\u201d) and (2) a general artifact/naturalness check (e.g., \u201cDoes the image look natural and free from visual glitches, artifacts, or distortions?\u201d). Both diagnostics are answered strictly with a discrete YES/NO. The text-visibility question is included only when the prompt contains explicit textual content intended to appear in the image (such as spans in quotation marks or other instructions to render text), whereas the artifact check is always included.",
      "source_document": "papers/2512.20362v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a CLIP-based automated BBPS scoring model that augments a global image encoder with a stool-focused branch, how are localized stool cues extracted using anchor regions and text prompts, how are these cues fused with the global visual embedding, and what loss and parameter-freezing strategy is used during training?",
      "answer": "Localized stool cues are extracted by sampling multiple anchor regions (with varied shapes/aspect ratios) from the input image, cropping/resizing each anchor, and encoding it with a dedicated CLIP visual encoder plus an adapter (separate from the global branch). Stool-related text prompts (e.g., \u201cyellow stool\u201d, \u201cresidual feces\u201d) are encoded with the CLIP text encoder; for each anchor, its visual embedding is compared to the prompt embeddings via a similarity operation, and the resulting similarity scores are aggregated into a single fecal feature vector representing stool likelihood/cues across anchors.\n\nTo fuse global and local information, the fecal feature is first linearly projected into the same embedding space as the global visual feature, then concatenated with the global feature and passed through a lightweight gating network with a sigmoid to produce fusion weights. The final fused representation is an element-wise weighted sum: z_all = \u03b1 \u2299 z_v + (1\u2212\u03b1) \u2299 z_f, which is fed to a fully connected classifier.\n\nTraining uses standard multi-class cross-entropy between the classifier logits and BBPS labels. Gradients update the trainable components (e.g., adapters, gating network, classifier), while the CLIP visual encoder in the fecal branch is kept frozen to conserve memory (and validation/testing is performed with parameters frozen).",
      "source_document": "papers/2512.20374v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When adapting a pretrained CLIP ViT-B/16 to 4-way BBPS classification with the backbone largely frozen, what is the observed effect of (1) inserting parameter-efficient adapter modules after the visual encoder and (2) additionally adding the region-aware fecal-feature branch on overall accuracy and per-class consistency\u2014and which of these two additions provides the larger incremental benefit?",
      "answer": "Adding adapters on top of the frozen CLIP backbone improves transfer to the endoscopy domain, giving a modest boost in mean BBPS accuracy over fine-tuning only the classification head. Adding the region-aware fecal-feature branch on top of the adapter model produces a larger additional gain in average accuracy and also makes predictions more consistent across classes (in particular, BBPS 1\u20133 all exceed 90% accuracy in the full model), indicating that localized stool cues complement the global representation more than adapters alone.",
      "source_document": "papers/2512.20374v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an anchor-based local-cue branch for BBPS scoring, what practical trade-offs determine how many anchor regions to sample and at what scales/aspect ratios, and what \u201crange-aware\u201d strategy is used to adjust the anchor set so that the final default configuration balances spatial coverage, noise/redundancy, and efficiency?",
      "answer": "Anchor sampling is treated as a coverage\u2013specificity trade-off: too few anchors leave parts of the image insufficiently covered and miss localized stool cues (hurting accuracy), while adding more anchors generally improves fine-grained perception but can become redundant and inefficient; very small anchors are also prone to noisy cues, and overly large anchors can lose specificity. To capture diverse local patterns, anchors are generated across multiple scales and aspect ratios (e.g., grid-based scales and non-square ratios such as 2:1 and 1:2). Rather than uniformly scaling the anchor count, a range-aware strategy is used: reduce sparse, large-scale anchors and expand anchor density in regions/scales that performed well in preliminary trials. This leads to choosing a default anchor configuration that balances accuracy with computational efficiency (set to 180 anchors).",
      "source_document": "papers/2512.20374v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When curating a colonoscopy image dataset for automated BBPS (0\u20133) scoring, what practical steps can be used to (i) avoid near-duplicate/low-quality frames and (ii) reduce inter-observer subjectivity in the labels, and what quantitative/qualitative indicators can be used to argue that the resulting dataset is more diverse and clinically ambiguous than a small-subject benchmark like NERTHU?",
      "answer": "A workable curation pipeline is to start from colonoscopy videos and (i) sample frames at a fixed temporal interval (0.5 s in this dataset) and semi-automatically filter out blurred frames to prevent overly similar/low-quality images, then (ii) have multiple experienced endoscopists independently assign integer BBPS labels (0/1/2/3) and keep only images with unanimous agreement to minimize subjectivity and improve label consistency (yielding 2,240 labeled images from 517 subjects).\n\nTo support the claim of higher diversity and more realistic difficulty versus NERTHU (10 subjects), the document reports both dataset-level separation statistics (higher inter-class distance, 20.20 vs 18.61, alongside higher intra-class distance) and feature-space visualization behavior: NERTHU forms sharply separated clusters, while the larger-subject dataset shows more continuous distributions with noticeable overlap\u2014especially between BBPS 1 and 2\u2014consistent with the fuzzy class boundaries seen in real clinical scoring.",
      "source_document": "papers/2512.20374v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When fine-tuning a pretrained CLIP ViT backbone for 4-way BBPS scoring using newly added modules (e.g., adapters/fusion head) on a relatively small colonoscopy dataset, what training recipe helps reduce overfitting while still adapting to domain shift\u2014specifically (i) what image augmentation policy is used, and (ii) how is optimization handled differently for the pretrained visual encoder versus newly introduced layers?",
      "answer": "A practical recipe is to keep augmentation and optimization conservative while letting only lightweight components adapt strongly: (i) apply the standard CLIP-style augmentations\u2014random horizontal flips, random rotations, color jittering, and Gaussian blur; (ii) optimize with AdamW using a much smaller learning rate for the pretrained CLIP visual encoder than for the newly introduced layers (adapters/fusion/classifier), with weight decay, and use mixed-precision training with model selection based on best validation checkpoint.",
      "source_document": "papers/2512.20374v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In cross-modal contrastive pretraining for exocentric video paired with ambient sensors, how can the contrastive loss be modified so that negatives from the same spatial context are emphasized (hard negatives) while pairs that are similar in both spatial context and temporal dynamics are down-weighted to avoid false negatives, and what is the resulting bidirectional objective?",
      "answer": "Use a spatial\u2013temporal adaptive weight on each negative pair (i,j) in a bidirectional InfoNCE loss.\n\n1) Compute a combined spatial similarity by taking the maximum of the (ReLU\u2019d) cosine similarities of the video spatial features and sensor spatial features:\nsspatial_ij = max(ReLU(sv-spatial_ij), ReLU(ss-spatial_ij)).\n\n2) Up-weight spatially similar negatives (hard negatives) with\nW^spatial_ij = 1.0 + (\\lambda_hard \u2212 1.0) \u00b7 sspatial_ij,  with \\lambda_hard > 1,\nso weights interpolate from 1 (easy negatives, low spatial similarity) to \\lambda_hard (hard negatives, high spatial similarity).\n\n3) Mitigate false negatives by reducing the weight when temporal similarity is also high:\nFirst define combined temporal similarity (temporal features are aligned in a shared space) as\nstemporal_ij = (sv-temporal_ij + ss-temporal_ij)/2,\nthen\nW^temporal_ij = 1.0 \u2212 sspatial_ij \u00b7 ReLU(stemporal_ij).\n\n4) Final negative weight:\nW_ij = W^spatial_ij \u00b7 W^temporal_ij, with W_ii = 0.\n\n5) Plug weights into a bidirectional InfoNCE. For video\u2192sensor,\nLv2s = \u2212(1/N) \u03a3_i log( exp(s_ii) / (exp(s_ii) + \u03a3_{j\u2260i} W_ij \u00b7 exp(s_ij)) ),\nwhere s_ij = (z_video^(i) \u00b7 z_sensor^(j)) / \\tau_contrast.\nCompute Ls2v symmetrically (sensor\u2192video) and use\nLcontrast = 1/2 (Lv2s + Ls2v).",
      "source_document": "papers/2512.20409v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a staged framework for aligning exocentric video with ambient (object-mounted) sensors, how can spatial grounding be learned without manual object labels by (i) extracting sensor \u201cspatial\u201d features from multichannel time-series, (ii) selecting which video clips are trusted to supervise the video spatial encoder, and (iii) refining noisy/ambiguous sensor cluster assignments using video predictions? Specify the key feature construction choices and the losses used for clustering and refinement, including how confident vs. ambiguous samples are determined and how the two sensor losses are combined.",
      "answer": "Spatial grounding is learned by first discovering sensor-defined spatial sources via online clustering and then using those pseudo-labels to train a video spatial encoder, followed by video-guided refinement of uncertain sensor assignments:\n\n(i) Sensor spatial feature extraction: Sensor windows are encoded with a base encoder (1D CNN + GRU) to capture activation patterns. To make clusters reflect \u201cwhere/which object\u201d activated, an additional per-channel activation-intensity cue is computed as the 99th percentile of each channel\u2019s values (robust to outliers), projected, and fused with the encoder output via element-wise addition. Online deep clustering is then performed with learnable centroids and a memory bank.\n\n(ii) Trusting video samples for supervision: Because sensor activations and unsupervised clustering can be noisy, only high-quality (confident) clustered samples are used to supervise the video spatial encoder. Confidence is determined per cluster by Euclidean distance to the assigned centroid: samples below that cluster\u2019s 75th-percentile distance are marked confident; others are ambiguous. During joint learning, sensor clustering runs on all samples, while the video spatial classifier is trained only on confident samples using cross-entropy to predict the sensor pseudo-labels.\n\n(iii) Refining ambiguous sensor assignments with video: After joint learning, the video spatial encoder is frozen and its predictions are used to refine the sensor clusters on ambiguous samples. A refinement loss applies cross-entropy between the sensor cluster-probability for the video-derived hard pseudo-label (argmax of video cluster logits) and the sensor prediction on ambiguous samples.\n\nLosses: The clustering loss is a class-balanced cross-entropy over sensor pseudo-labels,\nL_cluster = -(1/N) * sum_i w_{y_i} log(p_{i,y_i}), with inverse-frequency weight w_{y_i} = 1/|C_{y_i}|^{0.5} to handle cluster imbalance. The refinement loss is cross-entropy on ambiguous samples using video-derived hard pseudo-labels. The total sensor objective combines both: L_sensor = \u03b1 L_cluster + \u03b2 L_refine, where \u03b1 and \u03b2 balance clustering vs. refinement.",
      "source_document": "papers/2512.20409v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a decomposed spatio-temporal alignment setup for exocentric video and ambient sensors, what encoder inputs and backbone choices can be used to separately represent (i) video spatial context, (ii) video temporal motion, and (iii) multichannel sensor dynamics, and how are the spatial and temporal embeddings combined into the representations that are contrasted across modalities?",
      "answer": "A workable decomposition is:\n\n- Video spatial context: compute a clip-level spatial signal by temporally averaging the video frames, then pass that averaged frame through a 2D convolutional spatial encoder (with a linear head when training it to predict pseudo-labels).\n\n- Video temporal motion: extract motion-focused inputs (frame differences and optical flow) and encode them with a 3D-convolution temporal encoder to capture fine-grained dynamics.\n\n- Sensor dynamics (and spatial cues for sensors): encode each multichannel sensor window with a 1D CNN + GRU\u2013based encoder (the default variant adds attention), and augment it with a per-channel activation-intensity statistic computed as the 99th percentile of signal values per channel; project this statistic and fuse it with the encoder output via element-wise addition.\n\n- Combination for alignment: form spatially-conditioned representations by concatenating the frozen spatial embedding and the trainable temporal embedding for each modality, i.e., z_video = [v_spatial, v_temporal] and z_sensor = [s_spatial, s_temporal], with spatial and temporal parts having the same dimensionality d so the concatenation yields a 2d-dimensional vector that is used in cross-modal contrastive learning.",
      "source_document": "papers/2512.20409v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking an unsupervised exocentric-video + ambient-sensor alignment method on datasets with different label granularity (window-level mid-level actions vs. sequence-level high-level activities), how should the data be windowed and split to avoid leakage, how is the pretrained sensor encoder evaluated downstream (freezing, classifier/pooling choices), and which metrics are used to handle class imbalance?",
      "answer": "Use fixed-length sliding windows of 2 seconds with 1 second overlap. For Opportunity++ (which provides per-window mid-level labels), window first to obtain ~5K samples, split into 80% pretraining and 20% downstream evaluation, then split the downstream portion into train/val/test as 80%/10%/10%; z-score normalize sensor channels. Downstream evaluation freezes the sensor encoder and trains a linear classifier on the frozen window features for 14-class mid-level action classification.\n\nFor HWU-USP (only high-level labels over long sequences), split raw files first to prevent leakage (70% pretraining / 30% downstream evaluation while preserving label distribution), then apply the same windowing, and split the downstream windows into train/test (80%/20%); z-score normalize sensor channels. Downstream evaluation freezes the sensor encoder, extracts window features, temporally pools them across the sequence (as in Temporal Segment Networks-style pooling), and feeds the pooled feature into a classifier for 5-class high-level action classification.\n\nReport weighted F1-score and mean Average Precision (mAP) to account for class imbalance.",
      "source_document": "papers/2512.20409v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-agent surveillance anomaly system that uses a persistent State-of-Thoughts (SoT) reasoning graph, what are the thematic reasoning layers explored during the initial non-criminally-biased dialogue, what exploration-control operations can the supervisor invoke to steer the graph, and how does the final anomaly classification stage inject criminal/anomaly bias before outputting the decision?",
      "answer": "The SoT dialogue is organized as a reasoning graph whose nodes are question\u2013answer pairs along a logical path and whose edges are the operations that change the course of exploration. The initial stage performs a structured, non-criminally-biased exploration across themed reasoning layers: (1) scenario understanding (e.g., location, time, scene objects), (2) entity extraction (e.g., people grouping, demographics, clothing), (3) social context (e.g., proxemics, gestures, social roles), and (4) event understanding (e.g., actions, spatiotemporal information, causality, abnormality cues). The supervisor steers exploration by selecting operations: proceed (have the detective generate three follow-up questions and choose the most relevant based on the witness\u2019s answers and contribution to the main goal), refine (rewrite the previous question using newly obtained context), split (branch a node into multiple reasoning paths), and stop (mark a node as fully explored). After this exploration, an anomaly/criminal classification layer injects criminal bias by prompting the witness with a predefined set of optimized, anomaly-focused questions; the supervisor then produces the final classification by combining evidence from the unbiased exploration and the targeted criminally-biased inquiry.",
      "source_document": "papers/2512.20417v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-task network that uses task-specific sigmoid-gated BatchNorm scalers as soft feature gates, how can you compute a per-task \u201ccapacity\u201d measure from the learned gates and then decompose that capacity into (i) a task\u2019s shared component with other tasks and (ii) its task-specific/independent component? Provide the definitions in terms of the task\u2013filter importance vectors and the projection used for the decomposition.",
      "answer": "First form a task\u2013filter importance matrix I\u2208R^{T\u00d7F} from the learned \u03c3BN scalers, where each task\u2019s importance vector is I_t\u2208R^F with entries I_{t,i}=\u03c3(\u03b3_{t,i}) (bounded in [0,1]).\n\nTotal capacity for task t is defined as the normalized sum of its filter importances:\nC_t = (1/F) * \u03a3_{i=1..F} \u03c3(\u03b3_{t,i}).\n\nTo split capacity into shared vs independent components, project I_t onto the subspace spanned by the other tasks\u2019 importance vectors. Let A be the matrix obtained by stacking all task vectors except I_t. The projection is\n\\hat I_t = P_A I_t = A (A^T A)^{-1} A^T I_t,\nwith the orthogonal (independent) residual\nI_t^\u22a5 = I_t \u2212 \\hat I_t.\n\nDefine the independent and shared capacities as scaled versions of C_t using the squared L2-norm proportions:\nC_t^{indep} = (||I_t^\u22a5||_2^2 / ||I_t||_2^2) * C_t,\nC_t^{shared} = (||\\hat I_t||_2^2 / ||I_t||_2^2) * C_t.",
      "source_document": "papers/2512.20420v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When adapting a CNN with ImageNet-pretrained BatchNorm layers to a multi-task setup that replaces BN\u2019s affine transform with sigmoid-gated scaling (\u03c3BN/TS\u03c3BN), how can you initialize the new \u03c3BN scaling parameters and handle the original BN bias term to avoid a disruptive \u201cconversion shock\u201d during fine-tuning, and what property of the pretrained BN scales makes this initialization practical?",
      "answer": "To avoid conversion shock when converting pretrained BN to \u03c3BN (which removes the additive \u03b2 term and uses only a bounded multiplicative gate \u03c3(\u03b3)), the approach keeps the pretrained BN biases: it copies the pretrained \u03b2 values but freezes them during training, since an affine BN layer cannot be converted cleanly to a purely multiplicative form unless \u03b2=0. For the scales, because in ImageNet-pretrained ResNet-50 most BN scale parameters lie in the interval (0,1), they can be represented as sigmoid outputs; therefore the \u03c3BN raw gate parameters are initialized by applying the inverse sigmoid (logit) to the pretrained BN scales so that \u03c3(\u03b3_init) matches the pretrained scaling distribution.",
      "source_document": "papers/2512.20420v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In multi-task dense prediction, loss terms often have very different scales, which can cause one task to dominate optimization. How can you evaluate whether an architectural method is robust to loss-scale mismatch using controlled loss-weight perturbations, and what qualitative outcome indicates that sigmoid-gated task-specific normalization improves this robustness compared with hard sharing or vanilla task-specific BatchNorm?",
      "answer": "A simple robustness check is to perturb loss weights for one task at a time while keeping the others fixed, then measure how much the overall multi-task performance changes across these perturbations. Concretely, scale each task\u2019s loss by a set of multiplicative factors {0.5, 1.5, 2.0} while keeping the default weight of 1.0 for the remaining tasks, run training/evaluation under each perturbation, and examine the distribution/variance of the relative multi-task performance (e.g., average % gain/drop across tasks relative to a baseline). Improved robustness is indicated by the lowest variance (smallest sensitivity) of performance under these loss-scale perturbations; sigmoid-gated task-specific normalization (TS\u03c3BN) exhibits the lowest variance, signaling reduced task dominance and more stable optimization than hard parameter sharing or vanilla task-specific BN.",
      "source_document": "papers/2512.20420v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When extending task-specific sigmoid-gated normalization from CNNs to a pretrained Vision Transformer used for multi-task dense prediction, how should you treat (i) the patch-embedding normalization layer versus (ii) the remaining LayerNorms in the backbone, and what differential learning-rate multipliers are applied to the task-specific sigmoid BN and (task-specific) LayerNorm parameters during fine-tuning (including any relevant regularization settings like dropout)?",
      "answer": "Use a sigmoid LayerNorm (\u03c3LN) in the ViT patch embedding, but convert the remaining backbone LayerNorms to task-specific LayerNorm (TSLN) rather than \u03c3LN because pretrained LN scale parameters can exceed the sigmoid\u2019s effective co-domain. During fine-tuning, apply differential learning rates with a 100\u00d7 multiplier for TS\u03c3BN parameters and a 10\u00d7 multiplier for LN-related parameters (TSLN/TS\u03c3LN). In this setup, dropout and DropPath are disabled while training the ViT with Adam using a small base learning rate and polynomial decay.",
      "source_document": "papers/2512.20420v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-task CNN that uses task-specific sigmoid-gated BatchNorm scalers as per-channel \u201cimportance\u201d scores, how can you operationally define when a convolutional filter is *specialized* for a particular task (as opposed to being generic/shared), and how can you validate that this specialization measure is meaningful using a targeted pruning experiment? Give the definition in terms of normalized per-task importances and the threshold used, and describe the pruning protocol and the expected pattern of performance drops across tasks if the definition is correct.",
      "answer": "Define a task\u2013filter importance for task t and filter i as \u03c3(\u03b3_{t,i}). Convert this to a *normalized* importance share for each task by dividing by the sum over tasks for that filter. A filter i is deemed specialized for task t\u2032 if\n\n\u03c3(\u03b3_{t\u2032,i}) / (\u2211_{t} \u03c3(\u03b3_{t,i})) > \u03c4,\n\nwith \u03c4 set to 0.5, meaning the filter contributes predominantly to one task rather than being shared.\n\nTo validate the measure, prune (remove) the top 200 most important filters *per task* according to these importances/specialization assignments, then reevaluate all tasks. If the specialization definition is meaningful, pruning filters specialized for a given task should hurt that same task substantially more than other tasks\u2014i.e., the largest performance drops occur on the diagonal when you prune-task-by-task (self-impact > cross-impact).",
      "source_document": "papers/2512.20420v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a soft-voting ensemble for dermoscopic skin-lesion classification after lesion segmentation, what are the key architectural steps used to combine MobileNetV2, VGG19, and InceptionV3 into a single classifier (including how feature outputs are merged), what optimizer/loss are used for training, and how are the final ensemble predictions produced at test time?",
      "answer": "The ensemble is formed by loading three pre-trained CNN backbones (MobileNetV2, VGG19, InceptionV3) and freezing their weights during fine-tuning to reduce overfitting. Each model\u2019s output is passed through global average pooling, the three pooled feature vectors are concatenated into one vector, and a final dense layer with softmax is added for classification. The combined model is compiled with the Adam optimizer (learning rate 0.001) using categorical cross-entropy loss. At inference, each backbone produces class-probability predictions and the ensemble output is obtained by (soft-voting) averaging/weighted-averaging the models\u2019 predicted probabilities to produce the final prediction.",
      "source_document": "papers/2512.20431v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a dermoscopic skin-lesion classification pipeline that uses segmentation before CNN classification, how can you structure preprocessing to (i) mitigate class imbalance without resampling the full dataset, (ii) expand training signal primarily for underrepresented classes, and (iii) suppress common acquisition artifacts/noise\u2014what specific rebalancing, augmentation, normalization, and filtering operations are used?",
      "answer": "Preprocessing is organized as: (i) rebalancing via class weighting, assigning weights inversely proportional to each class frequency so errors on minority classes are penalized more than majority classes; (ii) normalization by scaling pixel values into the [0,1] range (divide by 255); (iii) augmentation applied specifically to minority classes using affine transformations\u2014rotation, zooming, cropping, flipping, and translation\u2014to increase their effective sample count; and (iv) a filtering stage to improve image quality and prepare for segmentation/classification using a sequence of Gaussian blur, median filtering, Sobel edge detection, and histogram equalization to reduce noise and enhance salient structure/contrast.",
      "source_document": "papers/2512.20431v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a dermoscopic skin-lesion pipeline that segments lesions before classification, how can a transfer-learning \u201cdual-encoder\u201d segmentation module be constructed to improve mask quality\u2014what two backbone encoders are run in parallel, how are their encoder outputs fused, and what is the intended benefit of this fusion for downstream lesion classification?",
      "answer": "Construct the segmentation stage as a hybrid dual-encoder network with two encoder instances applied in parallel to each input image, using VGG-16 and VGG-19 as the backbone encoders. The outputs/features from both encoders are combined (fused together) so the segmentation model captures both fine-grained and global features, which improves segmentation accuracy. More accurate lesion masks then help the classifier focus on clinically relevant lesion regions while reducing background artifacts/noise that would otherwise degrade classification.",
      "source_document": "papers/2512.20431v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a dermoscopic skin-lesion classifier intended for clinical use, why is recall treated as the most critical metric, and\u2014when comparing MobileNetV2, VGG19, and InceptionV3 against a soft-voting ensemble built on segmented lesions\u2014which single backbone is described as the strongest overall, which is fastest and slowest at inference, and what is the main trade-off introduced by soft-voting ensembling in terms of recall/accuracy versus inference time? Also, how does the ensemble\u2019s benchmark performance compare in broad terms to other recent methods on HAM10000, ISIC 2016, and ISIC 2019?",
      "answer": "Recall is prioritized because missing a potentially serious/cancerous lesion is highly risky, so the evaluation emphasizes not overlooking positives (while precision is still needed to limit false positives). Among the individual pretrained CNNs, InceptionV3 is described as the best overall performer, MobileNetV2 has the shortest (fastest) inference time, and VGG19 has the longest (slowest) inference time. The soft-voting ensemble (averaging predictions from the three models) generally improves performance\u2014especially recall and overall accuracy\u2014relative to any single backbone, but it increases processing/inference time compared to running one CNN because it must execute and combine all three models; despite this overhead, it is characterized as offering a good balance of high accuracy at a reasonable inference time. Across HAM10000, ISIC 2016, and ISIC 2019, the ensemble is reported to outperform the set of recent comparison methods included for those datasets.",
      "source_document": "papers/2512.20431v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-class dermoscopic skin-lesion classifier evaluated with per-class ROC/AUC, which lesion categories emerge as the hardest to separate (i.e., show comparatively lower AUC) on HAM10000 and on ISIC 2019, and how should these lower AUC values be interpreted in terms of class discriminability (contrast with a class noted as reliably differentiated)?",
      "answer": "The ROC/AUC analysis highlights that on HAM10000, melanoma and dermatofibroma have lower AUC values, suggesting these classes are less distinguishable from the other lesion types. On ISIC 2019, the vascular and basal cell carcinoma (BCC) classes show lower AUCs, again indicating reduced discrimination efficiency for those categories. In contrast, a higher AUC (noted for Nevus) indicates that class can be reliably differentiated from the others.",
      "source_document": "papers/2512.20431v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a decomposition-based anomaly detector for textured industrial surfaces (smooth low-rank background + quasi-periodic texture + sparse defects), how can an SSD-style model be modified to explicitly reconstruct normal texture using a learned texture-basis dictionary so textures are less likely to be absorbed into the sparse anomaly term, and what estimators/updates does this imply for the background, texture-reconstruction coefficients, and anomaly map?",
      "answer": "Introduce a learned texture basis (dictionary) Bt (learned from defect-free images under a quasi-periodicity assumption) and add a sparse regularizer on its reconstruction coefficients, so textures are explained by Bt rather than leaking into the sparse anomaly component. The resulting anomaly-detection objective is\n\nmin_{\\theta,\\theta_t} \\|e\\|_2^2 + \\lambda\\,\\theta^T R\\theta + \\gamma\\|\\theta_t\\|_1 + \\eta\\|C_a\\|_1\\quad s.t.\\quad Y = C_{bg}+C_{tex}+C_a+e = B\\theta + B_t\\theta_t + C_a + e.\n\nWith this formulation, the smooth background is estimated by ridge regression\n\\hat\\theta = (B^T B + \\lambda R)^{-1} B^T\\,(Y - B_t\\hat\\theta_t - C_a),\ntextures are reconstructed as \\hat C_{tex}=B_t\\hat\\theta_t, and anomalies are obtained from the residual via elementwise soft-thresholding\n\\hat C_a = S_{\\eta/2}(Y - B\\hat\\theta - B_t\\hat\\theta_t).\nThe coefficient update for \\theta_t is also derived from the residual and uses a soft-threshold/shrinkage form to encourage a sparse combination of texture bases. By giving the model an explicit, sparse texture-reconstruction pathway (Bt\\theta_t), normal quasi-periodic texture is fit as texture rather than being misidentified as sparse defects.",
      "source_document": "papers/2512.20432v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When learning a texture dictionary from a small set of defect-free images for decomposition-based anomaly detection on textured surfaces, how can dominant texture expansion directions be identified via a rotation-based 1D line-sampling scheme, and how are those directions then used to decompose the 2D texture component into directional sub-textures for texture-basis learning?",
      "answer": "A rotation-based Linear Sampling in Equal Rotation Angle (LSERA) procedure samples the estimated texture (or the defect-free image after background removal) along a line that is rotated around a fixed point to produce a set of 1D signals \\{S(k\\theta)\\}. For each rotation angle k\\theta, a direction score is computed as F(k\\theta)=std(S(k\\theta))\u2212std(S(k\\theta+\\pi/2)). If F(k\\theta) exceeds a threshold \\(\\phi_S>0\\), then \\((k\\theta+\\pi/2)\\) is declared a texture expansion direction and added to the direction set \\(D\\) (otherwise it is excluded). The rationale is that when the sampling direction aligns with the texture extension direction, the 1D sample mixes background and texture and has high standard deviation, while the orthogonal sample has lower standard deviation; by the 2D quasi-periodicity property, quasi-periodicity detected in the reduced 1D signal implies quasi-periodicity along the orthogonal (expansion) direction.\n\nOnce \\(D\\) is determined, the 2D texture component is modeled as a sum of quasi-periodic directional sub-textures along these directions:\n\\(C_{tex}=\\sum_{d\\in D} C^{(d)}_{tex}\\).\nTexture-basis functions are then learned separately per direction \\(d\\) (after dimensional reduction to 1D quasi-periodic modes), and the full overcomplete texture basis/dictionary is formed by collecting the directional bases \\(B_t=\\{B_t^{(d)}\\mid d\\in D\\}\\) (constructed using clustering of extracted texture modes).",
      "source_document": "papers/2512.20432v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For ischemic stroke lesion segmentation from paired DWI and ADC diffusion MRI, what architectural changes to a TransUNet help (1) preserve modality-specific information and (2) inject limited inter-slice context without moving to full 3D processing, and what loss and data augmentations are used during training to achieve better Dice overlap than convolutional U-Net baselines?",
      "answer": "A TransUNet can be adapted for multimodal diffusion MRI by replacing early fusion (channel-wise concatenation of DWI and ADC into a single encoder) with a dual-encoder design: DWI and ADC are passed through two independent but architecturally identical encoders with separate parameters, and their feature maps are fused at the bottleneck (late fusion) before the transformer and decoder. To add limited 3D context while keeping slice-wise efficiency, the dual-encoder input is extended to stack three consecutive axial slices per modality (adjacent-slice aggregation), which improves boundary delineation and helps with small/fragmented lesions compared with single-slice inputs. Training supervises voxel-wise predictions using Binary Cross-Entropy with Logits loss, and generalization is encouraged via consistent geometric augmentation applied to both images and masks (random horizontal/vertical flips, random rotations up to 270\u00b0, and resizing inputs to 128\u00d7128). Under this unified setup, the hybrid CNN\u2013transformer TransUNet baseline outperforms CNN U-Net variants, and the dual-encoder plus multi-slice contextual input further improves Dice performance over early-fusion single-encoder TransUNet.",
      "source_document": "papers/2512.20436v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a slice-wise multimodal network on paired DWI and ADC volumes for ischemic stroke lesion segmentation, how can you convert each 3D case into aligned 2D training samples that still include limited inter-slice context, and what preprocessing steps remove background/low-signal content before saving the samples for efficient training?",
      "answer": "Each case is converted to 2D samples using a unified preprocessing pipeline that preserves cross-modality alignment while adding lightweight 3D context:\n\n- Normalize DWI and ADC independently with min\u2013max scaling to map voxel intensities to [0, 1].\n- Crop away background using a bounding box computed from the non-zero region of the DWI volume; apply the same bounding box to the ADC volume and the lesion mask so all modalities remain spatially aligned.\n- Use slice-wise training but stack three consecutive axial slices per modality to inject limited inter-slice context (a pseudo-3D input). After resizing to 128\u00d7128, each sample becomes a 128\u00d7128\u00d73\u00d72 tensor (3 slices, 2 modalities).\n- Exclude the first and last slices of each volume to ensure valid three-slice stacks, and discard slices with negligible signal content based on an intensity threshold.\n- Save the resulting inputs and corresponding 2D masks in NumPy format for efficient training.",
      "source_document": "papers/2512.20436v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking common 2D segmentation backbones for ischemic stroke lesion delineation on paired DWI+ADC diffusion MRI under the same preprocessing and augmentation pipeline, which type of model emerges as the strongest baseline (U-Net CNN variants, Swin-UNet, or a hybrid CNN\u2013transformer like TransUNet), and what architectural capability is used to explain why it outperforms the others on this task?",
      "answer": "A hybrid CNN\u2013transformer model (TransUNet) is the strongest baseline among the compared U-Net CNN variants and Swin-UNet. Its advantage is attributed to combining convolutional feature extraction (capturing local texture details) with transformer self-attention (modeling long-range/global contextual dependencies), which better fits diffusion MRI lesion segmentation.",
      "source_document": "papers/2512.20436v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When adding physics-inferred joint-torque features to a silhouette-based gait recognition model, what type of fusion mechanism best aligns the force embedding with the spatial appearance features, and why do naive feature-level operators (e.g., add/concat/element-wise multiply) generally fail to deliver consistent gains?",
      "answer": "An adaptive, spatially aware fusion mechanism\u2014a spatial transform module (BMM) that projects/aligns the force embedding with the silhouette (and kinematic) feature maps and then uses the dynamics to reweight local appearance features\u2014works best. In contrast, naive operators like simple addition, concatenation, or element-wise multiplication tend to give minimal or even negative improvements because they mix modalities without learning how to spatially align and gate them, leading to feature interference rather than complementary integration (learnable gating/attention helps somewhat but is still weaker than the spatial transform).",
      "source_document": "papers/2512.20451v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "If you want to extract stylized glyph foregrounds as transparent RGBA layers from design images using a detection+segmentation pipeline, what specific changes can make a generic instance segmentation model suitable for glyph alpha-mask prediction, and what loss components are used to fine-tune it so that it learns both accurate foreground/background classification and good mask overlap?",
      "answer": "A practical approach is to detect character-level bounding boxes (e.g., with YOLO) and use those boxes as prompts to an instance segmentation model (SAM) to segment each glyph, then merge the predicted mask into the image as an alpha channel to output RGBA. To adapt SAM (which normally outputs binary masks and generalizes poorly to glyphs), remove its final binarization step and clip the mask logits/outputs so the predicted alpha mask is continuous in [0,1]. Then fine-tune the model in a supervised way on synthetic data with ground-truth alpha channels using a weighted sum of: (1) an L2/MSE pixel loss between predicted and ground-truth alpha (pixel-wise difference), (2) a focal-style classification loss defined via p_t that penalizes incorrect foreground/background classification, and (3) an IoU-based loss (1 \u2212 mIoU) to maximize overlap between predicted and ground-truth foreground regions. The overall objective is L = L_mse + \u03bb_focal\u00b7L_focal + \u03bb_iou\u00b7L_iou.",
      "source_document": "papers/2512.20479v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an MLLM-based layout planner for stylized text synthesis in design images, what is the two-stage layout planning strategy (what does each stage predict), and during RL-based optimization with GRPO what three rule-based reward terms are used and what does each one encourage/penalize in the predicted boxes?",
      "answer": "The layout planner is trained to do layout hierarchically: (1) coarse planning predicts bounding boxes for whole text lines; (2) fine-grained planning refines within each line by predicting the position of each individual glyph.\n\nFor GRPO-based RL optimization it uses three rule-based rewards on the predicted boxes: (i) an IoU reward R_iou that encourages accurate layout prediction by increasing mean IoU between predicted and ground-truth boxes; (ii) an overlap penalty R_ol that penalizes overlaps among predicted boxes to encourage better spatial distribution; and (iii) a balance/size-variance penalty R_bl that penalizes size variance among predicted boxes to promote balanced, consistent glyph sizes. The total reward is a weighted sum R = R_iou + \u03bb_ol R_ol + \u03bb_bl R_bl.",
      "source_document": "papers/2512.20479v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a diffusion-transformer stylized text system that is pre-trained with a dedicated glyph style encoder for style-preserving editing, how can you turn background images + text descriptions into conditioning signals that live in the same style embedding space without fine-tuning the whole MLLM, and what exact alignment objective and trainable module are used to do this?",
      "answer": "Use a frozen multi-modal LLM to encode the design background image together with a textual description (caption, target text, and its box), then pass the MLLM\u2019s variable-length hidden states through a Perceiver Resampler that uses learnable query tokens to produce a fixed-size condition embedding. Keep both the MLLM and the original glyph style encoder frozen, and train only the Perceiver Resampler parameters by minimizing an L2 feature-alignment loss between the resampled MLLM condition features and the style encoder\u2019s embedding of the style reference glyphs: \n\nL_align = E_{R_s,B,D} || P(M(B,D)) \u2212 S(R_s) ||^2_2,\n\nwhere R_s is the style reference, B the background, D the description, M the frozen MLLM, P the Perceiver Resampler, and S the frozen style encoder.",
      "source_document": "papers/2512.20479v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When selecting an object detector for UAV-based multispectral (RGB\u2013LWIR) landmine detection, how do a transformer-based detector (RF-DETR), a two-stage detector (Faster R-CNN), and single-stage detectors (YOLOv11 and RetinaNet with focal loss) differ in the practical trade-offs between accuracy, training efficiency, altitude robustness, and sensitivity to RGB\u2013LWIR fusion\u2014and what general fusion regime emerges as the most reliable across architectures?",
      "answer": "The evaluated detectors show a clear accuracy\u2013efficiency\u2013robustness trade space. RF-DETR delivers the highest overall detection accuracy among the compared architectures, but it is computationally expensive to train (taking on the order of many hours), making it less attractive when rapid iteration or limited compute matters. YOLOv11 achieves competitive accuracy while training dramatically faster (tens of minutes versus many hours for RF-DETR), making it the most favorable option when training speed and operational efficiency are priorities.\n\nFaster R-CNN\u2019s two-stage design (region proposals followed by refinement) is the most robust to increasing sensor altitude, exhibiting the smallest performance degradation from low to high altitude among the compared models; this makes it comparatively better suited for higher-altitude/greater-coverage flights where pixel-level detail is reduced. RetinaNet, despite using focal loss to address foreground/background class imbalance, underperforms the other architectures in overall accuracy; focal loss alone does not overcome the multispectral and small-object challenges in this setting. However, RetinaNet shows comparatively greater tolerance to higher LWIR contributions than the other models.\n\nAcross architectures, the most reliable RGB\u2013LWIR fusion strategy is consistently RGB-dominant fusion with only a modest LWIR contribution: low-to-moderate LWIR blending (roughly in the 10\u201330% range, and generally not exceeding about 40%) maintains near-peak performance, while heavy LWIR weighting and especially pure thermal input causes large performance drops. This indicates that RGB supplies critical edge/context information for feature extraction, with LWIR acting mainly as a complementary cue rather than a standalone modality in this dataset.",
      "source_document": "papers/2512.20487v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In UAS RGB\u2013LWIR landmine detection, what is the practical benefit of training a single detector on an aggregated multi-temporal dataset (spanning different seasons/times of day) instead of training season-specific detectors, and what best-performing configuration and performance (mAP@0.5 and recall) demonstrates this\u2014along with the main reasons the aggregated training generalizes better despite environmental variability?",
      "answer": "Training on an aggregated multi-temporal dataset provides better generalization and higher accuracy than season-specific training: the best configuration combined YOLOv11 with January/May combined training and achieved 96.5% mAP@0.5 with 84.5% recall. This multi-temporal model outperformed May-only training (92.4% mAP@0.5) and January-only training (84.6% mAP@0.5), giving a 9.6% advantage over January-alone and showing that training-set choice (up to ~14.8% swing) matters more than the ~3.7% max difference among YOLO variants. The aggregated approach generalizes better because broader temporal coverage lets the model learn more transferable feature representations across changing thermal/illumination backgrounds, while January-only models underperform due to weaker winter thermal contrast, substantially different backgrounds (e.g., dormant vegetation/frozen soil), and the smaller January dataset contributing to underfitting.",
      "source_document": "papers/2512.20487v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a UAS landmine detector that fuses RGB and LWIR by alpha-blending the thermal image on top of RGB, what altitude-dependent fusion policy emerges for choosing the LWIR weight as the drone flies higher, and what practical imaging/feature-extraction effect explains why the optimal LWIR contribution shifts with altitude?",
      "answer": "An altitude-dependent policy is to use more LWIR at low altitude but become increasingly RGB-dominant as altitude increases: at 5\u201310 m elevation the best performance occurs with moderate thermal blending (roughly 20\u201330% LWIR), while at 15\u201320 m the optimum shifts to a small thermal contribution (about 10% LWIR) with performance staying close to pure RGB. The reason is that RGB provides the high-frequency edge and contextual detail needed for reliable feature extraction; as altitude increases and pixel-level resolution drops, making LWIR too dominant increasingly hurts because thermal imagery is lower-detail and more prone to thermal-noise/contrast issues, so heavy LWIR fusion degrades faster at higher elevations (pure thermal collapses across altitudes compared to RGB-dominant fusion).",
      "source_document": "papers/2512.20487v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multispectral UAS landmine detection setting with strong class imbalance and very small targets (anti-personnel mines), why can a single-stage RetinaNet-style detector that uses focal loss still lag behind a two-stage Faster R-CNN pipeline\u2014and what result pattern across mine types supports the conclusion that target physical size/pixel detail is a stronger bottleneck than the specific detector family?",
      "answer": "Focal loss helps reweight hard/rare examples, but it does not recover spatial detail that is lost when mines occupy only a few pixels; small-target localization and refinement become the dominant limitation. In the reported architecture comparison, Faster R-CNN\u2019s two-stage region proposal + refinement achieved the best performance on the hard AP category (e.g., AP metal and AP plastic), while RetinaNet remained far behind despite using focal loss. Moreover, the performance gap between easy large mines and hard small mines was larger than the gap between architectures: AT plastic detection was around ~90% while AP plastic was in the ~20\u201340% range (with an ~44% best-to-worst mine-type spread), indicating mine size/visibility constrains performance more than model sophistication; the text also links improved altitude robustness to Faster R-CNN\u2019s explicit region proposal network when pixel-level detail is limited.",
      "source_document": "papers/2512.20487v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When choosing among recent YOLO variants for multispectral UAS landmine detection, how do YOLOv8, YOLOv10, and YOLOv11 differ in the specific architectural/training-design changes they introduce (e.g., head design, NMS handling, attention/feature blocks), and which variant ends up being both the most accurate and the most temporally consistent across different time-of-day/season training splits\u2014suggesting those design changes actually help under environmental variability?",
      "answer": "YOLOv8 modernizes the YOLOv5 lineage by (i) replacing the older C3 module with the more efficient C2f module and (ii) moving to an anchor-free split-head design (instead of YOLOv5\u2019s anchor-based approach), aiming to improve efficiency and general-purpose detection capability.\n\nYOLOv10 targets efficiency and deployment simplicity by removing the need for Non-Maximum Suppression (NMS) at inference via \u201cconsistent dual assignments\u201d during training, and by using lightweight classification heads, spatial-channel decoupled downsampling, and a rank-guided block design to reduce redundant computation.\n\nYOLOv11 introduces feature-extraction/representation upgrades meant to better capture fine detail: it adds the C3k2 block (CSP with kernel size 2) and C2PSA (a convolutional block with parallel spatial attention), uses SPPF for efficient multiscale feature aggregation, and incorporates refined training methodologies to improve both speed and accuracy.\n\nAcross the evaluated temporal training splits (month \u00d7 time-of-day), YOLOv11 is the most accurate overall and also the most consistent: it achieves the highest overall detection performance among the YOLO variants and shows the narrowest performance range across the single-period training datasets (while YOLOv10 is described as the most variable). This indicates the added feature/attention blocks and multiscale aggregation in YOLOv11 help maintain performance under changing illumination/thermal conditions better than the earlier variants\u2019 design changes.",
      "source_document": "papers/2512.20487v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In multimodal knowledge distillation for egocentric action recognition, how can a teacher be formed as an ensemble of modality-specific models without letting weak modalities dominate, and what is the full student training objective (including the roles of \u03b3, \u03c4, and \u03bb)?",
      "answer": "Train one modality-specific teacher model f^m per modality using standard cross-entropy on its modality data. Form the ensemble teacher by aggregating the teachers\u2019 logits, but weight each teacher based on how well it performs: estimate each teacher\u2019s cross-entropy e_m on a held-out set and set ensemble weights with a softmax over negative cross-entropies, w_m \u221d exp(\u2212e_m/\u03b3), where \u03b3 is a temperature that controls how uniform vs peaked the weights are (\u03b3\u2192\u221e gives equal weights, i.e., an arithmetic mean). The teacher prediction is then the sigmoid/softmax of the weighted logit sum, \n\u0177_i^t = \u03c3(\u2211_m w_m f^m(x_i^m)).\n\nDistill to an RGB-only student by minimizing KL divergence between teacher and student class-probability vectors, using a distillation temperature \u03c4 to soften probabilities (and rescaling the KL term by \u03c4^2). Combine distillation with the standard supervised action cross-entropy via\nL = \u03bb\u00b7L_KL + (1\u2212\u03bb)\u00b7L_CE,\nwhere \u03bb balances distillation vs ground-truth supervision (\u03bb=1 is pure distillation; \u03bb=0 is standard supervised RGB training).",
      "source_document": "papers/2512.20501v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a two-branch action recognition model that fuses an object-detections \u201clayout\u201d transformer with an RGB \u201cappearance\u201d transformer using cross-attention, what is the exact cross-attention directionality between the branches (i.e., what serves as queries vs. keys/values on each side), and how does the CentralNet-style modification in CACNF change the training objective and inference rule compared to plain cross-attention fusion?",
      "answer": "Cross-attention fusion (CAF) uses the hidden states from the layout branch (\u0124) and appearance branch (\u00c2) and applies cross-attention separately on each branch: for the layout branch, the multi-head attention queries are the layout hidden states \u0124 while the keys and values come from the appearance hidden states \u00c2; for the appearance branch, the queries are \u00c2 while the keys and values come from \u0124. After cross-attention, each branch is further processed with standard transformer self-attention and feed-forward modules, and the final class-token hidden states from both branches are concatenated for classification.\n\nCACNF (Cross-Attention CentralNet Fusion) keeps CAF\u2019s cross-attention module but adds CentralNet-style unimodal heads: before feeding \u0124 and \u00c2 through CAF, it takes each branch\u2019s class hidden state (\u0125_class and \u00e2_class) and attaches a separate linear classifier to each branch in addition to the CAF classifier. Training then minimizes three cross-entropy losses against the action label: (i) the STLT (layout) classifier loss, (ii) the 3D-ResNet/appearance-transformer classifier loss, and (iii) the CAF (cross-attended fused) classifier loss. At inference time, it averages the logits from the three classifiers and predicts the action with the maximum probability.",
      "source_document": "papers/2512.20501v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In medical text grounding to a 3D human atlas where each target organ is a voxel point cloud and a document can mention multiple organs, how can the training objective be designed to (i) enable within-organ localization without sub-organ labels and (ii) avoid collapsing multi-organ supervision to an averaged \u201cmid-way\u201d coordinate? Give the concrete loss construction using point sampling and soft-min weighting, and explain the roles of the two temperature terms.",
      "answer": "Use a regression model that predicts a 3D point \\(\\hat y\\) (obtained by linearly projecting the BERT [CLS] embedding) and train it with the Soft Organ Distance (SOD) loss, which uses soft-min weighting at two levels so the prediction is pulled toward the nearest relevant region/organ rather than an average.\n\nFor each training sample and for each of its target organs, randomly sample \\(N\\) voxel points \\(\\{y_i\\}_{i=1}^N\\) from that organ\u2019s 3D point cloud. Compute pointwise loss contributions as a distance multiplied by a soft-min weight over sampled points:\n\\[\nL_p(y)=\\|\\hat y-y\\|_2\\;\\frac{\\exp(-\\|\\hat y-y\\|_2/\\gamma_p)}{\\sum_{i=1}^N \\exp(-\\|\\hat y-y_i\\|_2/\\gamma_p)}\n\\]\nwhere \\(\\gamma_p\\) is a temperature controlling how strongly the loss focuses on the closest points (smaller \\(\\gamma_p\\) makes the weighting peak more on nearest points). The loss for one organ is the sum over its sampled points:\n\\[\nL_o=\\sum_{i=1}^N L_p(y_i).\n\\]\nIf the document has \\(M\\) target organs, compute a soft-min over the organ losses to weight organs and sum them:\n\\[\nL_t=\\sum_{i=1}^M L_o^{(i)}\\;\\frac{\\exp(-L_o^{(i)}/\\gamma_o)}{\\sum_{j=1}^M \\exp(-L_o^{(j)}/\\gamma_o)}\n\\]\nwhere \\(\\gamma_o\\) controls how much training emphasizes the closest matching organ (smaller \\(\\gamma_o\\) more strongly selects the nearest organ). This setup preserves within-organ reasoning by encouraging proximity to specific organ subregions (via point-level soft-min) and handles multi-organ labels by pulling toward the nearest ground-truth organ instead of an averaged location (via organ-level soft-min).",
      "source_document": "papers/2512.20501v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In large-scale fact linking from surface-form OpenIE triplets (s; r; o) to canonical knowledge-graph facts (e1; p; e2), how can you design a scalable two-stage transformer-based linker that (1) pre-ranks each slot against millions of KG entries and (2) re-ranks full candidate facts to resolve ambiguity? Specify (i) how the OIE and KG entries are serialized and embedded, (ii) the exact training loss and negative-sampling strategy used for the slot pre-ranker (including how it compensates for in-batch negatives covering only a small KG subset), (iii) how the fact re-ranker constructs \u201chard\u201d negative facts and any masking used during training, and (iv) what empirical trends are observed when adding sentence context and the re-ranking stage and when increasing the KG size under transductive vs. inductive vs. polysemous evaluations.",
      "answer": "A practical scalable design is a two-step retrieval-style pipeline:\n\n(i) Serialization/embeddings.\n\u2022 OIE slots are serialized into one text sequence with special tokens marking slot boundaries, e.g., \u201c<SUBJ> \u2026 <REL> \u2026 <OBJ> \u2026\u201d. A shared RoBERTa encoder produces token embeddings; the slot representations are formed by pooling the embeddings of the special slot tokens and linearly projecting them, giving one embedding per slot (3 embeddings per OIE).\n\u2022 Each KG entry (entity or predicate) is serialized as its label followed by its description (if available), separated by a \u201c<DESC>\u201d token, encoded with the same RoBERTa; the KG entry embedding is obtained by pooling the <CLS> embedding and linearly projecting it into the same space.\n\u2022 Slot\u2194entry similarity for pre-ranking is computed by dot product between the (norm-scaled) embeddings (equivalent to cosine similarity under normalization).\n\n(ii) Slot pre-ranker training objective and negatives.\n\u2022 The slot pre-ranker is trained contrastively with a temperature-scaled InfoNCE loss over a positive OIE-slot\u2194KG-entry pair contrasted against negatives:\n  L = \u2212log \\frac{\\exp(\\hat{o}^T\\hat{k}/\\tau)}{\\exp(\\hat{o}^T\\hat{k}/\\tau)+\\sum_{n=1}^{N\u22121}\\exp(\\hat{o}^T\\hat{k}^{\u2212}_n/\\tau)}\n  where N is batch size, \\hat{o} is the slot embedding, \\hat{k} is the aligned KG entry embedding, \\hat{k}^{\u2212}_n are negative KG entry embeddings, and \\tau is the temperature.\n\u2022 Negatives are taken as (a) N\u22121 in-batch negative KG entries for each positive pair.\n\u2022 Because in-batch negatives only cover KG entries that appear paired with some training OIE (a small KG subset), the method additionally samples extra negative entities and predicates uniformly at random from the whole KG for each OIE slot during training, so the model learns to reject arbitrary KG entries it will face at inference (when each slot is compared against the full KG).\n\n(iii) Fact re-ranker hard negatives and masking.\n\u2022 The re-ranker scores a concatenated input consisting of the full OIE and a candidate KG fact separated by a \u201c<FACT>\u201d token, using a single RoBERTa transformer with self-attention over the combined sequence; it outputs a scalar similarity via a linear layer and sigmoid.\n\u2022 Training uses positive matching OIE\u2194KG-fact pairs and negatives formed by corrupting the gold KG fact: replace one or more fact slots (subject/predicate/object) with incorrect ones sampled from the top-k most similar KG entries (\u201chard negatives\u201d), where \u201cmost similar\u201d is defined by nearest neighbors in the embedding space produced by the slot pre-ranker.\n\u2022 Additionally, with 50% probability the descriptions of KG fact entries are masked (replaced with a <mask> token) during training.\n\n(iv) Empirical trends (context, re-ranking, KG size, and split type).\n\u2022 Performance drops markedly from transductive (entities/predicates seen during training) to inductive (entities unseen during training) and to polysemous cases (surface forms mapping to multiple KG concepts), showing linking is harder under generalization and ambiguity.\n\u2022 Appending the original source sentence as extra context to the OIE improves performance especially on inductive and polysemous evaluations, helping generalization and disambiguation.\n\u2022 Adding the re-ranking stage yields a significant gain, particularly for correctly linking complete facts (all three slots correct), because whole-fact interaction helps resolve slot-level ambiguity.\n\u2022 Increasing KG size makes linking consistently harder across splits, since each slot must be matched against a much larger candidate set (e.g., millions of entities and thousands of predicates instead of a smaller restricted KG).",
      "source_document": "papers/2512.20501v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a monocular dynamic-4D reconstruction system that uses SIREN-style periodic activations for high-frequency motion, how can a keypoint-based supervisory loss be constructed so it (1) anchors absolute keypoint locations and (2) enforces physically/structurally plausible keypoint relations in a way that matches SIREN\u2019s functional space\u2014and how is this loss integrated with the standard photometric/reconstruction objective during end-to-end training?",
      "answer": "Use a two-term keypoint loss composed of: (i) a positional accuracy term that L2-penalizes absolute keypoint errors, L_pos = \\sum_{i=1}^M ||\\hat{k}_i - k_i||_2^2; and (ii) a geometric consistency term defined over edges (i,j) in a keypoint graph E, but evaluated in the same periodic space as the SIREN network by applying the sine nonlinearity to relative keypoint vectors: L_geo = \\sum_{(i,j)\\in E} || sin(\\omega_0(\\hat{k}_i-\\hat{k}_j)) - sin(\\omega_0(k_i-k_j)) ||_2^2. The complete SirenPose loss is L_SirenPose = L_pos + \\lambda_geo L_geo. For training the 4D reconstruction model, this acts as a physically motivated regularizer added to the main reconstruction loss (e.g., photometric loss) as L_total = L_recon + \\lambda_sp L_SirenPose, so optimization receives gradients from both pixel-level fidelity and top-down geometric/temporal structure constraints.",
      "source_document": "papers/2512.20531v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a monocular dynamic 4D reconstruction model that relies on keypoint-based geometric regularization, how can you supply the keypoint supervision on real-world video benchmarks that do not provide ground-truth keypoint annotations, and what failure mode does this introduce into the reconstruction/pose optimization?",
      "answer": "Use pseudo-label keypoints produced by an external, pre-trained keypoint detector (here, an X-Pose model) as the supervisory signal for the geometric/keypoint losses. This introduces a dependency on the detector\u2019s accuracy: positional noise, mislocalization, or missing/incorrect detections can propagate into training and cause the optimization to enforce an incorrect geometric structure, potentially degrading reconstruction/pose estimates (even though the method shows robustness in practice).",
      "source_document": "papers/2512.20531v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a monocular dynamic 4D reconstruction setup that adds a SirenPose-style regularizer on top of a baseline (e.g., MoSCA), how does ablating (i) the geometric keypoint priors versus (ii) the combined high\u2013low frequency supervision affect reconstruction quality on a real-video benchmark in terms of MSE-Score and EPE-Score, and what does the outcome indicate about whether these two components are redundant or complementary?",
      "answer": "On DAVIS (using MoSCA as the baseline), removing geometric keypoint priors degrades both metrics (about \u22124.4 MSE-Score and \u22122.1 EPE-Score relative to having both components). Ablating the high\u2013low frequency supervision also degrades both (about \u22124.0 MSE and \u22123.5 EPE). When both components are removed simultaneously, performance drops much more (about \u22127.5 MSE and \u22125.4 EPE compared to using both), indicating the frequency-based supervision and the geometric priors contribute complementary (non-redundant) gains: each helps, and removing both compounds the harm.",
      "source_document": "papers/2512.20531v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a dynamic 4D reconstruction pipeline that uses a SIREN branch to model high-frequency motion/details, what weight initialization scheme is used for (i) the first coordinate-input layer and (ii) subsequent sine-activation layers to keep pre-activation variance stable, and how does the frequency scaling parameter \u03c90 interact with this initialization to enable high-frequency fitting without unstable gradients?",
      "answer": "The SIREN branch maintains stable gradients by explicitly controlling the variance of the sine pre-activations z\u2113=\u03c90(W\u2113h\u2113\u22121+b\u2113) across depth.\n\n(i) First layer (coordinate input): weights are sampled from a uniform distribution W0 ~ U(\u22121/n0, 1/n0), where n0 is the input dimension. This maps raw coordinates into a suitable initial range.\n\n(ii) Subsequent layers (\u2113>0): weights are initialized inversely to fan-in to prevent variance compounding, using W\u2113 ~ U(\u2212sqrt(6/n\u2113\u22121), sqrt(6/n\u2113\u22121)), so the weight variance is 2/n\u2113\u22121 and counteracts fan-in scaling, keeping the pre-activation variance approximately invariant across layers.\n\n\u03c90 is then applied as a multiplicative frequency scale inside each sine activation (\u03c3(z)=sin(\u03c90 z)); setting \u03c90 (here 30) pushes these stabilized pre-activations into the high-frequency regime of the sine function for fine detail/rapid motion modeling, while the variance-preserving initialization prevents gradients from becoming chaotic or collapsing.",
      "source_document": "papers/2512.20531v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In monocular dynamic-scene reconstruction where you also evaluate camera pose estimation on datasets like Sintel, Bonn, and DAVIS, what pose-alignment procedure is applied before scoring, which standardized error metrics are then reported, and what qualitative difference in the frame-wise error trajectories indicates improved temporal stability versus baselines such as Monst3R or Robust-CVD?",
      "answer": "Before evaluation, the predicted and reference camera poses are aligned using the Horn quaternion closure method. Pose accuracy is then quantified with three standardized metrics: Absolute Translation Error (ATE), Relative Pose Error in translation (RPE-Trans / RPEtrans), and Relative Pose Error in rotation (RPE-Rot / RPErot). Improved temporal stability is reflected in per-frame error curves that remain smooth and consistently low with fewer abrupt spikes (mitigating drift or sudden jumps), whereas baselines show larger fluctuations and sudden error spikes, consistent with temporal jitter/discontinuities.",
      "source_document": "papers/2512.20531v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-granularity text-guided image fusion system that modulates multi-scale visual features with corresponding textual features, how is hierarchical (multi-grained) supervision implemented to reduce text\u2013vision misalignment during training\u2014specifically, what intermediate reconstructions are produced at the detail and structure levels, what reference signals are they compared against, and what loss terms are used for those levels and for the final fused output?",
      "answer": "Hierarchical supervision is applied by decoding intermediate text-guided visual features back into image space and supervising them with level-appropriate reconstruction objectives, then also supervising the final fused image.\n\n\u2022 Total objective: L_total = L_feat + L_base.\n\n\u2022 Feature reconstruction loss (L_feat) provides multi-grained supervision on intermediate text-guided features:\n  \u2013 Detail level: the detail-level text-guided visual features are decoded into an intermediate fusion image \\hat{I}_{f1} and supervised with a gradient loss L_feat_grad = L_grad(\\hat{I}_{f1}, max(I_{ve1}, I_{ve2})), where L_grad compares Sobel-gradient magnitudes using an element-wise L1 norm.\n  \u2013 Structure level: the structure-level text-guided visual features are decoded into an intermediate reconstruction \\hat{I}_{f2} and supervised with an SSIM-based loss against both enriched inputs: L_feat_SSIM = 2 \u2212 SSIM(\\hat{I}_{f2}, I_{ve1}) \u2212 SSIM(\\hat{I}_{f2}, I_{ve2}).\n  \u2013 These are combined as L_feat = \\alpha1 L_feat_grad + \\alpha2 L_feat_SSIM.\n\n\u2022 Base loss (L_base) supervises the final fused image I_out to preserve intensity and texture:\n  \u2013 Pixel loss: L_base_pixel = (1/HW) || I_out \u2212 max(I_{ve1}, I_{ve2}) ||_1.\n  \u2013 Gradient loss: L_base_grad = L_grad(I_out, max(I_{ve1}, I_{ve2})).\n  \u2013 Combined as L_base = \\beta1 L_base_pixel + \\beta2 L_base_grad.\n\nThis multi-grained decoding-and-reconstruction scheme is used to progressively bridge the semantic gap between textual and visual modalities and make textual modulation more consistent across granularities.",
      "source_document": "papers/2512.20556v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a text-guided image fusion pipeline that uses training-time data augmentation to make the auxiliary text more informative, how can a saliency-driven visual enrichment module generate multiple enriched variants from an input image pair\u2014what signal is used to locate \u201cimportant\u201d regions, what is the center\u2013periphery cropping strategy used to extract patches, and what is the intended benefit compared with standard random cropping?",
      "answer": "The enrichment module first runs a semantic saliency detector on the input pair to produce saliency responses/masks that indicate semantically important regions. It then applies a semantic-aware center\u2013periphery partitioning crop: the region with the highest saliency response is selected as a central crop, and four additional peripheral crops are extended outward around that central region, producing multiple enriched image-pair variants {I^n_ve1, I^n_ve2}. The goal is to increase both diversity and (especially) the effective density of informative/semantic content in training samples; unlike standard random cropping, it avoids omitting key objects or producing semantically incoherent regions, which would weaken learning and reduce the effectiveness of multi-grained textual guidance.",
      "source_document": "papers/2512.20556v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-granularity text-guided image fusion network that injects language priors into visual features at multiple semantic levels, how does the text-guided visual modulation at each level actually operate\u2014what are the inputs to the module, which modality provides the queries vs. keys/values in the cross-attention, and how are the modulated features combined to form the final fusion feature map and reconstructed output image?",
      "answer": "At each granularity level l, the network first has paired visual features from the two inputs (F^l_{v1}, F^l_{v2}) and a same-level textual feature F^l_t (extracted by a text encoder from multi-grained descriptions). A text-guided visual modulation (TGVM) module is applied separately to each image\u2019s visual feature using cross-attention in which the textual feature serves as the query and the visual feature serves as the key and value, producing text-guided visual features F^l_{tv1}=TV(F^l_{v1},F^l_t) and F^l_{tv2}=TV(F^l_{v2},F^l_t). At the last stage (level L), the two text-guided visual features are fused by concatenating them along the channel dimension (the fusion operation M) to obtain a fusion feature map F_f, which is then passed through a decoder D to reconstruct the final fused image I_out: I_out = D(M(F^L_{tv1}, F^L_{tv2})).",
      "source_document": "papers/2512.20556v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-granularity text-guided image fusion system, how are the auxiliary text features for each granularity obtained from an input image pair\u2014what model is used to generate the multi-grained descriptions, what model encodes those descriptions into feature vectors, and what are the three granularity levels the descriptions are meant to cover?",
      "answer": "The pipeline first uses a multi-modal large language model (GPT-4o) to generate three complementary textual descriptions for the input image pair at different granularities\u2014(1) detail-level (fine-grained details), (2) structure-level (structural/layout cues), and (3) semantic-level (high-level semantics). These multi-grained descriptions are then fed into a textual encoder (BLIP-2) to extract the corresponding textual feature embeddings for each level, which are later used to guide/modulate visual features at the matching semantic level.",
      "source_document": "papers/2512.20556v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For a fair evaluation of a multi-exposure (MEF) and multi-focus (MFF) image fusion model, what dataset protocol is used to train/test and to check cross-dataset generalization in each task, and what do the six reported fusion metrics (EN, SD, SF, AG, VIF, Qabf) each intend to measure about the fused output?",
      "answer": "Dataset protocol:\n- MEF: use SICE as the main dataset, with a subset of image pairs for training and the remainder for testing; additionally validate on MEFB.\n- MFF: use RealMFF as the main dataset, split into training and testing sets; additionally evaluate on Lytro to assess generalization.\n\nMetrics:\n- EN (Entropy): amount of information preserved in the fusion image.\n- SD (Standard deviation): global contrast.\n- SF (Spatial frequency): textural/detail richness based on frequency variations.\n- AG (Average gradient): overall image sharpness.\n- VIF (Visual information fidelity): alignment of the fusion image with human visual perception.\n- Qabf: effectiveness of transferring edge information from the source images to the fusion image.",
      "source_document": "papers/2512.20556v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a video VLM augmented with pretrained 3D/4D reconstruction priors, what is the design of a text-guided geometry selection module that avoids overwhelming the language model with long/noisy 3D token sequences, and how are the selected geometry features fused with the original VLM inputs?",
      "answer": "The module uses two stacked Q-Formers to produce a fixed-size set (N) of geometry tokens conditioned on the question: (1) a \u201csemantic condenser\u201d Q-Former where N learnable queries attend to the question tokens to distill question semantics into language-conditioned queries (Q_lang); (2) a \u201crelevant-geometry selector\u201d Q-Former where Q_lang attends to the 3D tokens extracted from a 3D foundation model (e.g., \u03c03 encoder outputs) to retrieve only question-relevant geometric information, yielding compact geometry tokens (Q_geo). Because N is fixed, the VLM receives a bounded, task-aligned geometric summary rather than a long, variable-length 3D token stream. The selected geometry tokens are then late-fused by concatenating them with the original VLM vision tokens and question tokens as [T_vis ; Q_geo ; T_text], and the combined token stream is passed to the LLM head.",
      "source_document": "papers/2512.20557v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a large vision\u2013language model that wants to prune visual tokens at inference time without relying on unstable deep attention maps, how can a text-guided selector compute a query-conditioned relevance score in the LLM embedding space and combine it with intrinsic visual saliency and a diversity-preserving step to choose the final fixed-budget token set?",
      "answer": "A stable attention-free approach is to (1) compute an intrinsic, query-agnostic saliency score from the vision encoder, and (2) compute an extrinsic, query-conditioned relevance score by explicit cross-modal similarity in the LLM space, then fuse the two and enforce diversity under a budget.\n\n\u2022 Intrinsic saliency (Sintrinsic): derive a per-patch importance signal from the vision tower\u2019s own attention (e.g., final-layer attention averaged over heads) and min\u2013max normalize it so it reflects general visual prominence independent of the query.\n\n\u2022 Extrinsic query relevance (Sextrinsic) in LLM space: project visual patch features into the LLM feature space with the multimodal projector and L2-normalize them. For the text query, apply a norm-based gate that upweights semantically informative tokens (higher embedding magnitude) and downweights low-information tokens (e.g., stopwords), then L2-normalize. Compute a patch\u2013token similarity matrix via dot product between projected visual tokens and gated/normalized text embeddings. Aggregate each patch\u2019s similarity to all text tokens with a temperature-controlled softmax weighting over text tokens to obtain a single relevance score per patch. To make relevance sparse and robust under high compression, apply a lightweight multi-stage \u201ccontrastive sharpening\u201d pipeline (sharpening via low-temperature normalization, emphasizing strong responses with a power-law transform, re-normalization, and sparsity enforcement via top\u2011p gating), and finally min\u2013max normalize to get Sextrinsic. If there is no text query, set Sextrinsic to zero so the method degrades gracefully to saliency-only pruning.\n\n\u2022 Fusion (Sfused): combine Sintrinsic and Sextrinsic with a weighted geometric mean implemented in the log domain (followed by min\u2013max normalization). This favors tokens that are simultaneously visually salient and semantically aligned with the query.\n\n\u2022 Diversity-preserving fixed-budget selection: split the kept-token budget into two parts\u2014(i) \u201cimportant\u201d tokens selected as the top-ranked patches by Sfused, and (ii) \u201cdiverse background\u201d tokens chosen from the remaining candidates using iterative residual pruning on L2-normalized features, greedily removing near-duplicate/background tokens (based on high pairwise similarity) until the budget is met. The final token set is the union of important and diverse background tokens, preserving both query-relevant evidence and enough global context for reasoning.",
      "source_document": "papers/2512.20561v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an inference-time visual token pruning method for large VLMs, what architectural placement of the pruning operation can keep the transformer\u2019s attention computation dense (remaining compatible with FlashAttention) while also reducing FLOPs, KV-cache, and end-to-end latency compared with approaches that sparsify/recompute attention maps inside transformer layers (e.g., FastV/SparseVLM)? Explain the key mechanism difference that leads to the runtime gains.",
      "answer": "Place token selection as a single-shot step at the vision-encoder/LLM interface (i.e., prune the visual token sequence before it enters the LLM transformer blocks), without modifying or sparsifying any internal transformer layers. By operating entirely outside the transformer stack, the model preserves standard dense attention throughout the LLM and avoids repeatedly sparsifying and recomputing attention maps within layers (as in FastV/SparseVLM). This introduces negligible overhead, keeps full compatibility with FlashAttention, and therefore yields the lowest FLOPs, KV-cache usage, GPU memory, and CUDA latency under the same retained-token budgets.",
      "source_document": "papers/2512.20561v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an iterative transformer-based point tracker that refines trajectories over multiple iterations and predicts both visibility and a per-point confidence score, how can the training objective be constructed to (i) prioritize accurate localization of visible points over occluded ones and (ii) emphasize later refinement iterations\u2014while still supervising visibility and confidence? Describe the specific loss terms, weighting scheme, and how the confidence target is defined.",
      "answer": "Use a three-term objective summed over refinement iterations: L = L_track + L_vis + L_conf.\n\n\u2022 Trajectory loss (L_track): supervise predicted coordinates at every iteration t with a Huber loss on point locations using a 6-pixel threshold. Weight each iteration with a temporal discount factor \u03b3^{T\u2212t} (\u03b3 = 0.8) so later iterations matter more, and apply asymmetric occlusion weighting so occluded points contribute 5\u00d7 less than visible points:\nL_track = \u03a3_{t=1..T} \u03b3^{T\u2212t} ( (1_occ/5) + 1_vis ) \u00b7 Huber(P^(t), P\u0302 ),\nwhere 1_occ and 1_vis indicate whether the point is occluded/visible.\n\n\u2022 Visibility loss (L_vis): supervise visibility logits V^(t) at each iteration with binary cross entropy against ground-truth visibility V\u0302, using the same temporal discounting:\nL_vis = \u03a3_{t=1..T} \u03b3^{T\u2212t} CE(\u03c3(V^(t)), V\u0302 ).\n\n\u2022 Confidence loss (L_conf): supervise confidence logits C^(t) with binary cross entropy, where the target label is 1 if the current predicted point is within 12 pixels of the ground-truth location and 0 otherwise, again with the same temporal discounting:\nL_conf = \u03a3_{t=1..T} \u03b3^{T\u2212t} CE(\u03c3(C^(t)), 1[||P^(t) \u2212 P\u0302||_2 < 12]).",
      "source_document": "papers/2512.20606v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a point-tracking system that uses both a pre-trained video Diffusion Transformer (providing query/key features) and a ResNet backbone, how can the two backbones be fused so that the DiT\u2019s pre-trained correspondence/matching behavior is preserved? Describe (i) how the local multi-scale 4D matching cost is computed from each backbone, (ii) how the two costs are fused and converted into cost embeddings for the tracking head, and (iii) why cost-level fusion (especially concatenation) is preferred over feature-level concatenation or simple averaging.",
      "answer": "A way to preserve the DiT\u2019s internal matching behavior is to avoid concatenating DiT+ResNet feature maps and instead fuse them after each backbone has produced its own correlation/matching-cost volume.\n\n(i) Local multi-scale 4D matching cost from each backbone:\n\u2022 Build an S-scale feature pyramid by interpolating backbone features to resolutions (H/(r\u00b72^{s\u22121}), W/(r\u00b72^{s\u22121})) for scale s, where r is the model stride.\n\u2022 For a query point p=(x,y) in query frame i, sample a (2\u0394+1)\u00d7(2\u0394+1) local neighborhood of query features around p to form local query features q_i^s.\n\u2022 For each target frame j, sample the same-sized local neighborhood of target features around the current estimated point P_j=(x_j,y_j) to form local key/target features k_j^s.\n\u2022 Compute a local 4D matching cost volume with a scaled dot product followed by a softmax:\n  C_{i,j}^{s,DiT} = Softmax( q_i^s (k_j^s)^T / \u221ad_head ) \u2208 R^{(2\u0394+1)^4} for DiT (using query/key features), and analogously\n  C_{i,j}^{s,ResNet} = Softmax( \u03c6_i^s (\u03c6_j^s)^T / \u221ad_ResNet ) \u2208 R^{(2\u0394+1)^4} for ResNet.\n\n(ii) Cost fusion and cost embeddings:\n\u2022 For each scale, flatten each cost volume and fuse at the cost level by concatenation:\n  C_{i,j}^{s,fused} = [Flatten(C_{i,j}^{s,DiT}), Flatten(C_{i,j}^{s,ResNet})] \u2208 R^{2(2\u0394+1)^4}.\n\u2022 Concatenate fused costs across scales and project them with an MLP to produce per-frame cost embeddings E_j that are fed to the iterative transformer tracking head.\n\n(iii) Why cost-level fusion (especially concatenation) is preferred:\n\u2022 Direct feature-level concatenation disrupts the DiT feature/cost distribution learned during pre-training and degrades matching.\n\u2022 Cost-level fusion preserves the DiT\u2019s softmax-based matching mechanism by letting each backbone compute its own cost volume first.\n\u2022 Among cost-level fusions, concatenation can outperform averaging because the tracking head can learn adaptive weighting between DiT costs (robust global matching under challenging conditions) and ResNet costs (fine-grained local precision), whereas averaging imposes fixed uniform weights.",
      "source_document": "papers/2512.20606v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When repurposing a pre-trained video diffusion transformer (with a VAE+denoising transformer) as the feature backbone for point tracking on long video sequences, what strategy can be used to (i) handle sequences longer than the model\u2019s effective context while keeping query\u2013key feature statistics consistent across chunks, and (ii) avoid temporal compression artifacts from the VAE during feature extraction? Describe the concrete steps taken before extracting the query/key features used for matching.",
      "answer": "Use chunked processing with a shared anchor frame and per-frame VAE encoding. Concretely: split the input video into N temporal chunks, and prepend the global first frame X1 to every chunk as a shared anchor so the query\u2013key feature distributions remain consistent across chunks. For feature extraction, encode each RGB frame independently through the VAE (rather than jointly across time) to avoid VAE temporal compression, then run the (LoRA-adapted) video DiT and extract the query\u2013key features Ql,m and Kl,m from the chosen layer l/head m (at the final denoising timestep) for subsequent matching-cost computation.",
      "source_document": "papers/2512.20606v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a point-tracking model that repurposes a video Diffusion Transformer as a correspondence backbone, how can a *hierarchical local 4D matching cost volume* be constructed directly from the transformer\u2019s query/key features? Describe (i) how multi-scale query/key maps are formed, (ii) how local query and key neighborhoods are sampled around the query point and current trajectory estimate, and (iii) how the final local cost tensor is computed (including normalization) and what its dimensionality represents.",
      "answer": "A hierarchical local 4D matching cost can be built by mirroring the DiT\u2019s internal attention matching between query/key features, but restricted to local neighborhoods and repeated across a multi-scale pyramid:\n\n(i) Multi-scale Q/K maps: start from the chosen DiT layer/head query and key feature maps Q_i and K_i for each frame i, then create an S-level pyramid by interpolating Q_i and K_i to progressively lower spatial resolutions. For each scale s, obtain interpolated maps Q^s_i and K^s_i at resolution H/(r\u00b72^{s\u22121}) \u00d7 W/(r\u00b72^{s\u22121}), where r is the model stride.\n\n(ii) Local neighborhood sampling: for a query point p=(x,y) in query frame i, bilinearly sample a (2\u0394+1)\u00d7(2\u0394+1) grid of local query vectors q^s_i centered at p (in the scaled coordinates). For every target frame j, bilinearly sample an analogous (2\u0394+1)\u00d7(2\u0394+1) local key grid k^s_j centered at the current estimated point location P_j=(x_j,y_j).\n\n(iii) Local 4D cost computation: compute scaled dot-product similarities between every spatial offset in the local query grid and every spatial offset in the local key grid, then apply a softmax normalization (as in attention) over the resulting similarity scores:\nC^{s,DiT}_{i,j} = Softmax( q^s_i (k^s_j)^T / sqrt(d_head) ).\nThe resulting cost tensor has shape (2\u0394+1)^4: two spatial dimensions for the query neighborhood (x/y offsets around p) and two for the key neighborhood (x/y offsets around P_j), capturing a local 4D correspondence distribution between the two local patches at scale s.",
      "source_document": "papers/2512.20606v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a modern point-tracking pipeline that refines trajectories iteratively with a transformer, how can the refinement module be set up to update (i) 2D point coordinates, (ii) visibility, and (iii) a confidence score at every iteration using precomputed per-frame cost embeddings? Describe the full refinement loop: how trajectories are initialized, how per-frame motion is encoded, what fields are concatenated into each per-frame token, how the transformer outputs are applied as residual updates, and why correlation/cost features are re-sampled after each update.",
      "answer": "A practical design is an iterative, residual refinement loop driven by per-frame tokens.\n\n\u2022 Initialization: for each query point, initialize a trajectory by copying (broadcasting) the query-frame point location to all frames, and initialize visibility and confidence to zero for all frames.\n\n\u2022 Motion encoding: at iteration t, form per-frame displacement embeddings with a Fourier encoding \u03b7(\u00b7) of frame-to-frame coordinate differences, e.g., \u03b7i\u2192i+1 = \u03b7(Pi+1 \u2212 Pi). For each frame i, build a token Gi by concatenating the displacement embeddings from the adjacent steps (\u03b7i\u22121\u2192i and \u03b7i\u2192i+1), the current visibility Vi, the current confidence Ci, and the per-frame cost embedding Ei (from the cost-volume / matching module) for that query point.\n\n\u2022 Refinement head: process the sequence of tokens with a tracking head \u03a8 implemented as a transformer using factorized temporal attention and query-point attention, and predict residual updates (\u0394P(t), \u0394V(t), \u0394C(t)) = \u03a8(G).\n\n\u2022 Residual updates: update all estimates additively each iteration:\n  P(t+1) = P(t) + \u0394P(t),\n  V(t+1) = V(t) + \u0394V(t),\n  C(t+1) = C(t) + \u0394C(t).\n\n\u2022 Re-sampling: after each update, re-sample the correlation/matching (cost) features at the newly refined locations so that the next iteration\u2019s cost embeddings reflect the current coordinate estimates. After T iterations, output the final per-frame trajectory, visibility, and confidence {Pi, Vi, Ci} over frames.",
      "source_document": "papers/2512.20606v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a long-horizon, stochastic image-to-video \u201cworld simulator,\u201d why is it helpful to split avatar control into a high-level planner and a low-level action-caption grounder, and what failure mode shows up when the grounding module is removed?",
      "answer": "Because open-domain avatar actions are expressed as natural-language commands, a single planner\u2019s abstract subgoal descriptions are usually too underspecified and not formatted to reliably constrain a particular I2V generator. The architecture therefore uses a high-level reasoning module (System 2) to maintain the belief state, choose the next subgoal, and predict the next state, and a separate grounding module (System 1) to translate that abstract plan and predicted outcome into detailed, I2V-model-specific captions that improve execution fidelity. When the grounding module is removed, task success and human-preference performance drop: directly sending System 2\u2019s abstract commands to the I2V model produces imprecise generations that are less reliable for completing subgoals over multiple steps.",
      "source_document": "papers/2512.20615v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using reinforcement learning with verifiable rewards to improve a multimodal LLM\u2019s spatial intelligence, how can the reward be made \u201chierarchy-aware\u201d so that it helps both low-level perceptual skills (e.g., depth/orientation/counting) and high-level reasoning skills (e.g., navigation planning/causal reasoning)\u2014what reward terms are changed for each group and what behavior is each change intended to induce?",
      "answer": "A hierarchy-aware reward differentiates between \u201cintuitive perception\u201d and \u201ccomplex reasoning\u201d tasks. For intuitive perception tasks (e.g., depth estimation, object counting, orientation), the training removes rewards for producing explicit \u201cthinking process\u201d tokens and adds a response-length penalty, discouraging over-deliberation and pushing the model to rely on direct visual\u2013text alignment. For complex reasoning tasks (e.g., navigation planning, causal reasoning), the reward keeps and even amplifies the reward for explicit reasoning steps, encouraging the model to spend more tokens on intermediate computation during training.",
      "source_document": "papers/2512.20617v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an embodied 3D navigation evaluation for multimodal LLMs where the ground truth is a continuous 6-DoF camera trajectory (translations + rotations), what is a practical way to discretize that trajectory into a small, executable action space, and how can you score a model\u2019s predicted action at each step so the metric jointly reflects both distance accuracy and direction correctness?",
      "answer": "Discretize the continuous Camera-to-World trajectory by decomposing each frame-to-frame transform (\u0394R, \u0394t) into a fixed set of motion primitives (camera controls) and quantizing their magnitudes using a speed threshold. The action space uses six navigation primitives: Truck (translate left/right along X), Dolly (forward/back along Z), Pedestal (up/down along Y), Pan (yaw left/right), Tilt (pitch up/down), and Roll (roll CW/CCW). For evaluation, compute a step-wise score combining (1) a relative distance term sd = max(0, 1 \u2212 ||\u0394p_pred \u2212 \u0394p_gt|| / ||\u0394p_gt||) and (2) a directional term given by cosine similarity s\u03b8 = (\u0394p_pred \u00b7 \u0394p_gt)/(||\u0394p_pred|| ||\u0394p_gt||); then define step accuracy as s_step = sd \u00b7 max(0, s\u03b8), ensuring the per-step score lies in [0,1] and penalizes wrong-direction moves even if the step length is close.",
      "source_document": "papers/2512.20617v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In ability-specific supervised fine-tuning aimed at improving an MLLM\u2019s metric distance/depth perception, how can the training set be constructed and augmented (data sources + augmentation strategy), and what transfer behavior should you expect within other low-level perception skills versus higher-level spatial tasks?",
      "answer": "A practical recipe is to generate distance-relevant QA supervision from RGB-D / 3D-annotated indoor datasets (e.g., SUNRGBD, Hypersim, Matterport3D), then augment the resulting examples with visual prompting variants and multi-scale image transformations to increase viewpoint/scale diversity and encourage distance reasoning beyond a fixed template. This targeted distance SFT tends to show little benefit\u2014and can even cause interference\u2014on other L1 perception sub-abilities (negative/limited intra-level transfer), while producing clearer gains on higher-level abilities that depend on metric understanding (e.g., improved spatial understanding and better action/goal execution in agentic manipulation), indicating stronger cross-level transfer from L1 distance perception to L2\u2013L4 skills.",
      "source_document": "papers/2512.20617v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating an MLLM on camera orientation estimation (pitch/roll/vFOV) where the ground-truth labels come with per-sample uncertainty, how can you score predictions in a way that is tolerant to noisy ground truth\u2014what similarity function is used, what inputs does it take, and how does increasing the ground-truth uncertainty change the penalty for a given prediction error?",
      "answer": "Use a probabilistic, uncertainty-aware similarity score per predicted parameter based on a Gaussian kernel:\n\nS(ypred, ygt, \u03c3gt) = exp(\u2212(ypred \u2212 ygt)^2 / (2\u03c3gt^2)),\n\nwhere ypred is the model\u2019s predicted value, ygt is the ground-truth value, and \u03c3gt is the ground-truth uncertainty (treated as a standard deviation). The score equals 1 for a perfect match and decays toward 0 as the squared error grows. Larger \u03c3gt makes the decay slower (i.e., the metric becomes more lenient for the same absolute error). The final task score is the average of the individual scores for pitch, roll, and vFOV.",
      "source_document": "papers/2512.20617v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "Suppose you have a multi-task benchmark that measures spatial intelligence as a hierarchy of abilities (low-level perception \u2192 mental mapping \u2192 mental simulation \u2192 agentic competence). How can you aggregate all sub-task scores into a single overall score in a way that reflects this dependency structure instead of flat averaging, and what two sources of information are used to decide which nodes should receive higher weights?",
      "answer": "Use a hierarchical, bottom-up weighted aggregation over the capability tree: each internal node\u2019s score is computed as the weighted sum of its immediate children\u2019s scores, and this is applied recursively until the root score (overall spatial intelligence) is obtained. The weights are chosen to reflect (1) a cognitive-hierarchy principle that prioritizes foundational perceptual abilities because they are prerequisites for higher-level skills, and (2) empirical dependency information from Pearson-correlation analysis across abilities, where atomic skills that show strong, widespread correlations with many other skills are treated as more fundamental and thus assigned higher weights within their subtrees.",
      "source_document": "papers/2512.20617v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a long-video QA multi-agent setup where a master LLM chooses between (1) requesting temporal grounding, (2) issuing a targeted visual query on a localized clip, or (3) answering, what reinforcement-learning reward signals and trajectory return are used to train the master policy for efficient, well-structured tool use, and which sub-agents remain fixed during this RL fine-tuning?",
      "answer": "The master is trained with two simple rule-based rewards: (1) a per-step structural validity reward r_fmt^t \u2208 {0,1} that gives 1 only if the emitted action string is well-formed\u2014i.e., contains exactly one top-level action tag with proper closure and no extra text; otherwise 0; and (2) a terminal answer-correctness reward r_ans \u2208 [0,1] given at termination by exact-match accuracy on the multiple-choice answer (if no valid <answer> is produced, r_ans = 0). The trajectory return is defined as R(\u03c4) = \u03b1 * \u03a3_{t=0..T} r_fmt^t + r_ans, where \u03b1 > 0 weights the per-step structural shaping and r_ans supplies the final task reward. RL fine-tuning (with GRPO) updates only the master policy, while the grounding agent and vision agent are kept frozen.",
      "source_document": "papers/2512.20618v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a tool-augmented long-video QA system that separates temporal localization from visual perception, what intermediate representations are exchanged between the master agent, the temporal grounding module, and the vision module during multi-turn reasoning, and how does restricting the master agent to text-only inputs shape the overall architecture and inference procedure?",
      "answer": "The master agent iteratively builds a text context and never receives raw images. First, it can call the grounding module using the question and full subtitles; the grounding module returns a symbolic temporal identifier like a <clip_X> tag (optionally a short run of consecutive clip tags for a wider window), which marks the localized segment on the episode timeline and can be re-queried to refine/validate the location. Given a grounded <clip_X> and an on-demand prompt describing what visual evidence is needed, the vision module reads frames inside that localized segment and returns textual observations (e.g., objects/entities, attributes, actions, OCR/on-screen text, and brief scene cues). The system appends the returned clip tag (and the corresponding localized subtitle snippet) and the vision observations to the master\u2019s running text context, and the master repeats this loop until it judges evidence sufficient and outputs the final answer.",
      "source_document": "papers/2512.20618v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-agent long-video QA pipeline where a master LLM can iteratively (re)ground relevant clips and issue visual queries over localized frames, what trade-offs are observed when increasing (a) the master\u2019s maximum allowed decision steps and (b) the temporal evidence window size, and what default settings are chosen to balance answer/localization accuracy against extra tool calls and latency?",
      "answer": "Increasing the master\u2019s step budget improves temporal localization and final QA accuracy at first, but then saturates: allowing more steps helps the agent refine grounding and re-query vision when uncertain, yet beyond a modest budget the answer accuracy stops improving (diminishing returns). Expanding the evidence window to include adjacent clips similarly boosts both localization and answer accuracy by providing extra cross-shot context for disambiguation, but larger windows require more visual queries and incur higher latency while yielding smaller incremental gains. To balance accuracy and efficiency, the system uses a modest default step limit of K=5 and a default evidence window of 1 clip (single localized clip, no adjacent clips).",
      "source_document": "papers/2512.20618v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When converting a clip-based video QA dataset into an episode-level (hour-scale) long-video benchmark for evaluating temporal reasoning, how are the original clips, subtitles, and questions merged into a single episode timeline, and how are temporal/spatial grounding annotations (e.g., TVQA+ timestamps and bounding boxes) handled to remain valid after aggregation?",
      "answer": "The benchmark is created by aggregating all clips that belong to the same TV episode into one continuous episode-level sequence. For each episode, the visual streams from its clips are merged, the subtitles are merged, and all questions associated with those clips are collected together; the original clip timestamps are re-indexed so that each question\u2019s referenced moment is expressed in the unified episode timeline. For TVQA+, the spatio-temporal grounding is kept consistent by preserving the provided annotations (precise timestamps and the frame-level bounding boxes) at their corresponding frames after this re-indexing/aggregation.",
      "source_document": "papers/2512.20618v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a long-form video QA system that decomposes reasoning into (1) temporal grounding over subtitles and (2) targeted visual inspection of localized frames, what concrete benefits does each component provide toward higher answer accuracy, and how can visual inspection help when the initially grounded segment is uncertain?",
      "answer": "Temporal grounding helps by identifying the question-relevant clip, which filters distractors, narrows the context the master must reason over, and guides the master agent\u2019s attention to the right moment. Targeted visual inspection then complements subtitles with missing fine-grained evidence (e.g., object/attribute cues and OCR/on-screen text). When the grounded segment is uncertain, the vision stage can validate or refine the grounding through repeated calls\u2014using visual evidence from the localized frames to confirm the setting/details or to trigger re-grounding if needed.",
      "source_document": "papers/2512.20618v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a two-stage text-to-video diffusion pipeline that first generates compact semantic features and then refines to VAE latents, how can the model be scaled to minute-long videos without quadratic attention cost, and what attention pattern is used in the semantic stage versus the VAE-latent refinement stage to preserve long-term consistency?",
      "answer": "Scale to long videos by doing global, bidirectional full-attention modeling only in the highly compressed semantic token space (for global planning and long-range consistency across scenes/characters), and then mapping/refining into the much larger VAE-latent token space using shifted-window (Swin) attention so compute does not grow quadratically with the number of frames. In the described implementation, the semantic space has a high compression ratio (about 1/16 the number of tokens of the VAE space), so full attention is affordable there while windowed attention keeps the VAE-stage cost manageable.",
      "source_document": "papers/2512.20619v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a two-stage text-to-video system that uses an off-the-shelf video semantic encoder to guide a VAE-latent diffusion generator, how can the semantic feature space be made easier for diffusion-based sampling, and how are the resulting semantic embeddings incorporated into the latent denoiser during training/inference to reduce the train\u2013test gap?",
      "answer": "Make the semantic space diffusion-friendly by passing the high-dimensional semantic features through a learnable MLP that (1) compresses the feature dimension and (2) parameterizes the compressed space as a Gaussian by outputting a mean and variance; a KL-divergence regularizer is added so the learned compressed semantic space matches a Gaussian prior, and a sampled embedding z_sem is used as the semantic condition. The latent denoiser is conditioned via in-context conditioning by concatenating the noised VAE latents z_t with the compressed semantic embeddings as the model input (z_input = [z_t, z_sem]). At inference, z_sem is produced by the semantic generator and (similar to training) noise is added to z_sem to reduce the training\u2013inference gap.",
      "source_document": "papers/2512.20619v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a text-to-video generator that relies on an off-the-shelf video-understanding tokenizer to provide a \u201csemantic space\u201d for global planning, what properties should that semantic encoder/tokenizer have to be an effective conditioning signal (in terms of its pretraining data and the structure of its outputs), and\u2014using the Qwen2.5-VL vision tower as an example\u2014how does the tokenizer temporally/spatially compress an input video and what is the resulting semantic feature tensor shape in terms of (Fs,H,W) and the embedding dimension d?",
      "answer": "The semantic encoder should (1) be pre-trained on large-scale video data so it captures temporal semantics such as object motion and camera movement (image-only tokenizers like SigLip2/DINOv3 are insufficient for temporal information), (2) produce representations that are compact in both spatial and temporal dimensions to enable high-level global planning given video redundancy, and (3) be trained on diverse video lengths/resolutions so it can support variable-length/aspect-ratio content.\n\nFor Qwen2.5-VL\u2019s vision tower: it samples video frames at a low fps (default 2.0), compresses each 14\u00d714 image patch into a single token, then further compresses along each dimension by a factor of 2. This maps an input video V \u2208 R^{3\u00d7F\u00d7H\u00d7W} into a semantic representation z\u2032_sem \u2208 R^{d\u00d7(Fs/2)\u00d7(H/28)\u00d7(W/28)}, where d is the embedding dimension and Fs is the number of sampled input frames.",
      "source_document": "papers/2512.20619v2.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a VLM-based gaze-following benchmark framed as VQA, where multiple gaze annotations or paraphrases can be valid for the same scene, how can a Best-of-N test-time decoding strategy be used, and what is the selection criterion for choosing the final answer from the N candidates?",
      "answer": "Use multi-sample decoding by having the model generate N candidate responses for each query, then select the candidate that achieves the highest task-specific score. This helps because gaze annotations (and model outputs) are multi-modal\u2014different yet valid gaze points/descriptions can exist for the same observer and scene\u2014so sampling multiple candidates better captures this diversity and allows picking the most consistent prediction.",
      "source_document": "papers/2512.20735v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When formulating gaze point localization as a VQA task for a vision-language model, how can you handle cases where the gaze target lies outside the image so the evaluation remains comparable to traditional gaze-following benchmarks, and what metrics are used to score the predictions?",
      "answer": "Treat gaze point localization as text output that is parsed into coordinates and score in-frame predictions by the L2 distance between predicted and ground-truth (x, y). To stay comparable with traditional benchmarks\u2019 in-/out-of-frame protocol, prompt the model to output the sentinel coordinates \u201c(-1, -1)\u201d when the gaze target is outside the frame, and compute a classification accuracy (ACC) over this in/out decision in addition to the L2 distance.",
      "source_document": "papers/2512.20735v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a VLM-based gaze-following benchmark that treats gaze direction as a language output, how can you derive the ground-truth direction labels from gaze annotations, and what two complementary metrics can you use to evaluate predicted directions (including how the angular metric is computed)?",
      "answer": "Compute a gaze direction vector by connecting the observer location to the annotated gaze target location, then discretize that vector into one of eight coarse directions (left, right, up, down, plus the four diagonals). Evaluate predictions with (1) classification accuracy over the 8-way direction labels, and (2) angular error: map each discrete direction to a canonical angle (e.g., up/top\u21920\u00b0, right\u219290\u00b0, etc.) and report the average angular deviation between the predicted direction\u2019s angle and the ground-truth direction\u2019s angle.",
      "source_document": "papers/2512.20735v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a unified gaze-following VQA benchmark with multiple sub-tasks (e.g., describing the gazed-at object, predicting gaze direction, localizing the gaze point, and detecting ambiguous queries), what happens if you fine-tune a vision\u2013language model only on the natural-language \u201cobject description\u201d subset instead of jointly fine-tuning on the full multi-task set, and what does this imply about the relationship between gaze description, direction, and localization supervision?",
      "answer": "Fine-tuning on only the object-description queries helps on that description-style output but leads to clearly worse performance on the geometric gaze tasks (direction estimation and point localization) than a model jointly trained on all task types. Joint multi-task training performs best overall, indicating the tasks are mutually reinforcing: learning to describe gaze targets, infer directions, and localize points provides complementary supervision that improves robustness and generalization across sub-tasks.",
      "source_document": "papers/2512.20735v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When constructing gaze-following VQA data from images that only have person boxes and gaze points, how can you make the automatically generated *observer* referring expressions reliable enough for training\u2014specifically, how does a self-consistency validation step filter \u201cunique\u201d versus \u201cgeneral/ambiguous\u201d descriptions, and how are the two types of general descriptions used to create negative samples (i.e., what should the model answer instead of hallucinating)?",
      "answer": "Generate multiple candidate observer descriptions (both intended-to-be unique and general/ambiguous) with a VLM, then validate each candidate by feeding it back to the VLM with the same image and asking: \u201cPlease tell me the number of people in the image that match this description.\u201d Keep a \u201cunique\u201d description only if the model says exactly one person matches; otherwise discard it. Apply the same check for general descriptions to filter inconsistent ones.\n\nThe retained general descriptions are used as negative samples to discourage hallucination: they include (1) existing general descriptions that refer to multiple real people in the image and (2) non-existing general descriptions that describe a person not present. In both cases, when the question\u2019s observer description is not uniquely identifiable, the model is expected to respond that it cannot identify the observer / that the description is not unique, rather than producing a random gaze answer.",
      "source_document": "papers/2512.20735v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When doing alternating evolutionary NAS that optimizes the detector backbone and the neck/head in separate cycles under hardware budgets, what mechanisms can be used to (1) avoid losing search progress when switching modules and (2) reduce the cost of fitness evaluation, and how does each mechanism operate?",
      "answer": "Two mechanisms are used:\n\n1) Population passthrough to preserve progress across module alternation: each module (backbone b or head/neck h) keeps a memory buffer of its best previously found architectures. When the search returns to that module, the next initial population is formed by inheriting a fraction \u03c1 of candidates from that buffer (elite passthrough/exploitation) and filling the remaining (1\u2212\u03c1) with newly sampled feasible architectures from the supernet (diversity augmentation/exploration). This prevents full population reinitialization at each switch and stabilizes convergence in modular NAS.\n\n2) A lightweight accuracy (mAP50) predictor as a surrogate fitness: because computing true mAP50 for every candidate subnet is expensive, an accuracy predictor estimates a candidate\u2019s expected mAP50 (after OFA progressive shrinking) and is used to guide the evolutionary search instead of repeatedly running full mAP50 evaluations, reducing search overhead while keeping good correlation with true performance.",
      "source_document": "papers/2512.20746v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building an Once-for-All, weight-sharing object-detection supernet that will later be specialized by NAS for TinyML deployment, how can the search space be parameterized across backbone/neck/head in terms of (i) depth, (ii) width, and (iii) expansion ratio, and which parts of the detector should keep depth fixed versus variable to control the combinatorial complexity while still allowing meaningful architectural adaptation?",
      "answer": "A practical OFA detection supernet can jointly parameterize backbone, neck, and head while exposing three main architectural degrees of freedom: (1) depth (how many blocks are active), (2) width (channel scaling via stage-wise width choices), and (3) expansion ratio (the intermediate-channel expansion inside residual bottleneck blocks). In this design, depth is made variable in the backbone by allowing each backbone stage to choose how many residual blocks are active, so the network can scale representational capacity at different semantic levels. In contrast, the neck and head (e.g., FPN/PAN and a YOLO-style detection head) keep the number of residual blocks fixed (no depth variation) and participate in the search only through width and expansion-ratio choices. This keeps the joint backbone\u2013neck\u2013head search space manageable while still letting NAS adapt capacity and compute across all components.",
      "source_document": "papers/2512.20746v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When targeting a CNN-accelerator microcontroller for real-time object detection, what concrete network-level constraints should be enforced during NAS to guarantee that discovered subnets are actually mappable to the accelerator, and how do these constraints help explain why two different Pareto-optimal deployments can emerge (one prioritizing efficiency and one prioritizing accuracy) even within the same search framework?",
      "answer": "To ensure a searched detector can be deployed on the MAX78002 CNN accelerator, the NAS must enforce device-mappability constraints rather than only generic FLOPs/parameter limits. The constraints described include: (1) strict limits on convolution configuration (only certain kernel sizes, padding, and stride settings are supported); (2) only specific pooling and activation functions are supported; (3) per-layer input and output channel dimensions must not exceed 2048; (4) the number of accelerator-mapped layers is capped (up to 128 CNN \u201cprimal\u201d layers); and (5) activation memory is tightly limited (about 80 KiB). In addition, the design/search must account for the accelerator\u2019s streaming mode, which reuses intermediate activations across layers to improve memory utilization and can enable higher input resolutions early in the network.\n\nUnder these constraints, the same hardware-aware search can yield multiple Pareto-efficient subnets with different trade-offs because different backbone styles satisfy the constraints differently: one discovered subnet (TrashDet\u2013ResNet) is tuned for aggressive efficiency (very low energy per inference and low latency, yielding high FPS), while a second subnet (TrashDet\u2013MBNet) uses a different backbone style to prioritize accuracy (substantially higher mAP50) while still remaining far more efficient than the baseline. Both are feasible because they respect the operator/channel/layer-count/memory constraints but allocate capacity differently within that feasible set.",
      "source_document": "papers/2512.20746v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a modular, alternating NAS setup for an object detector where the architecture is decomposed into a backbone configuration b and a neck/head configuration h, how should per-module resource budgets be enforced so that the final combined detector remains within an overall hardware budget, and why does imposing these separate budgets make the search more tractable in practice?",
      "answer": "Enforce module-specific constraints during the alternating optimization: when searching the backbone, require Cost(b) \u2264 \u03c4_b with the head fixed; when searching the neck/head, require Cost(h) \u2264 \u03c4_h with the backbone fixed. Choose the module budgets so they collectively respect the device limit by satisfying \u03c4_b + \u03c4_h \u2264 \u03c4 (the overall budget for the full detector). This decomposition reduces the effective dimensionality of the discrete, combinatorial search (depth/width/kernel/expansion choices no longer have to be jointly optimized in one step), making heuristic search like evolutionary optimization more tractable while still guaranteeing that the resulting combined architecture is hardware-feasible under the global constraint.",
      "source_document": "papers/2512.20746v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a benchmark like TACO where detectors show different precision\u2013recall trade-offs, how can one interpret the case where a TinyML-targeted detector achieves higher mAP@0.5 but lower average recall than a larger baseline, and which compared baseline detector exemplifies the \u201chighest recall but only moderate mAP@0.5\u201d regime?",
      "answer": "A higher mAP@0.5 paired with lower average recall indicates the detector is producing more precise, higher-quality detections (higher precision / better PR curve area at IoU=0.5) rather than maximizing the number of objects found; i.e., it emphasizes prediction quality over raw recall. In the comparison, Deformable DETR exemplifies the \u201chighest recall but only moderate mAP50\u201d regime (best AR but moderate mAP50), whereas the TinyML-targeted TrashDet-l attains the highest mAP50 despite lower AR, suggesting its architectures are better aligned with TACO\u2019s cluttered, long-tailed characteristics and yield a better accuracy\u2013efficiency trade-off for resource-constrained deployment.",
      "source_document": "papers/2512.20746v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a camera-based semantic scene completion (SSC) setup that extracts per-frame voxel grids from a global scene reconstruction, how are the three voxel masks typically defined to support training/evaluation: (1) the invalid mask, (2) the surface mask, and (3) the occluded mask\u2014and what does each mask represent geometrically or visibility-wise?",
      "answer": "\u2022 Invalid mask: marks voxels whose centers lie outside the truncated camera viewing frustum (outside the field of view); these voxels are treated as invalid and are excluded from evaluation.\n\u2022 Surface mask (view-independent): among voxels inside the frustum, first define occupancy as whether the voxel is non-empty; a voxel is marked as surface if it is occupied and has at least one 6-neighbor that is empty, i.e., it lies on a geometric boundary.\n\u2022 Occluded mask (view-dependent): for each image pixel, traverse voxels along its camera ray from near to far within the frustum; find the first occupied voxel on that ray (the visible surface). Any further voxel on that same ray that is also occupied is marked as occluded (the first hit is not), capturing occupied regions hidden behind the first visible surface; voxels outside all rays or invalid are set to non-occluded.",
      "source_document": "papers/2512.20770v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In camera-based semantic scene completion where the 3D voxel prior is initialized by back-projecting a per-frame *metric* depth map, how is the depth-estimation component adapted for an aerial benchmark setting, and what altitude-dependent pattern shows up when evaluating the resulting depth model (e.g., normalized vs absolute error behavior as altitude increases)?",
      "answer": "The benchmark replaces CGFormer\u2019s original depth module (MobileStereNetV2) with a monocular metric depth estimator trained for the aerial domain: an affine-invariant Depth Anything V2 ViT-Small Metric model is fine-tuned on the dataset\u2019s training split using metric-depth supervision (referred to as DAv2-OccuFly). When evaluated across altitudes, the fine-tuned model substantially outperforms the off-the-shelf DAv2-metric baseline at every altitude and on all depth metrics. As altitude increases, normalized/scale-invariant errors (e.g., AbsRel, SILog) remain relatively stable, while absolute errors (e.g., RMSE, MAE) grow\u2014indicating that higher viewpoint height increases metric depth error even when relative scaling is robust.",
      "source_document": "papers/2512.20770v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building camera-only semantic scene completion (SSC) ground truth from a reconstructed semantic point cloud, how can class-aware densification/voxelization be done differently for (i) instance objects, (ii) ground surface classes, and (iii) all remaining classes\u2014and how are conflicting labels between these groups resolved when forming the final scene-level voxel grid?",
      "answer": "A practical class-aware pipeline first partitions the semantic label set into three disjoint groups: instance classes (object-like categories to be densified per object), ground classes (categories that should form continuous surfaces), and other classes (everything else). It then processes each group differently:\n\n\u2022 Instance classes: separate the instance-class points into individual object point clouds using Euclidean clustering (DBSCAN with class-wise settings). For each object, construct a voxelized visual hull by (1) placing many virtual pinhole cameras around the object, (2) projecting the object\u2019s 3D points into each view to obtain 2D point sets, (3) turning each 2D set into a binary silhouette via an \u03b1-shape boundary, and (4) performing multi-view silhouette carving by back-projecting silhouettes into generalized cones and intersecting them; voxels whose centers lie inside this carved hull (within a tight, slightly dilated 3D bounding box) are marked occupied and assigned the object\u2019s semantic label.\n\n\u2022 Ground classes: densify the ground-class point subset with Poisson surface reconstruction to obtain a watertight triangle mesh that fills holes and enforces surface continuity, then voxelize the mesh; per-voxel ground semantics are assigned by majority voting over contributing mesh samples.\n\n\u2022 Other classes: directly voxelize the corresponding points by standard binning, assigning each voxel a semantic label via majority voting over points that fall into the voxel.\n\nFinally, all three voxelized outputs are merged into one scene-level semantic voxel grid using a fixed precedence order to resolve overlaps: instance voxels override other voxels, which override ground voxels; voxels not covered by any group are set to empty.",
      "source_document": "papers/2512.20770v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In reciprocal cross-domain evaluation for *metric* monocular depth (e.g., between a terrestrial driving dataset and an aerial UAV dataset), how do you set up the two \u201cswap the fine-tuning domain\u201d tests, and what contrasting behavior typically shows up between (a) metric-sensitive absolute-error measures (like RMSE/MAE) and (b) scale-invariant/normalized measures (like AbsRel/SILog) when comparing an aerial-fine-tuned model against a driving-fine-tuned baseline?",
      "answer": "The reciprocal cross-domain test is set up by (1) fine-tuning the metric depth model on a terrestrial driving dataset (Virtual KITTI 2) and evaluating it zero-shot on the aerial OccuFly test set, and (2) fine-tuning the same model on OccuFly and evaluating it zero-shot on the Virtual KITTI 2 test set. In this swap, the OccuFly-fine-tuned model generalizes better in metric-sensitive absolute-depth accuracy (it yields lower absolute errors such as RMSE/MAE, i.e., a more accurate absolute scale), whereas the driving-fine-tuned baseline tends to look better on scale-invariant/normalized metrics (e.g., AbsRel and SILog), reflecting relatively better depth relationships but poorer transfer of absolute metric calibration across domains.",
      "source_document": "papers/2512.20770v1.pdf",
      "mode": "textual",
      "content_refs": [
        "lines 1728-1738",
        "lines 1660-1690 (Table 10)",
        "lines 1808-1816"
      ]
    },
    {
      "question": "When transferring a state-of-the-art *vision-based* semantic scene completion model that builds its 3D geometric prior by back-projecting depth maps from terrestrial benchmarks to an aerial benchmark with a different voxel-grid shape, what practical changes are needed to make the method run end-to-end, and what evaluation metrics are used for (i) geometry and (ii) semantics?",
      "answer": "To run the SSC model end-to-end on the aerial setup, the depth component used to produce metric depth maps for back-projection needs to be swapped to a metric monocular depth model trained on the aerial data (the benchmark replaces CGFormer\u2019s original MobileStereNetV2 depth module with a Depth-Anything-V2 variant fine-tuned on OccuFly). In addition, the implementation must be adapted to the aerial voxel-grid specification, changing the grid size to 192\u00d7128\u00d7128 voxels (instead of the 256\u00d7256\u00d732 grid used in common terrestrial SSC benchmarks). For evaluation, geometry is measured with voxel-level Intersection-over-Union (IoU), while semantics is measured with mean IoU (mIoU) computed over non-empty semantic classes.",
      "source_document": "papers/2512.20770v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a prompt-optional breast-ultrasound segmentation model trained on mixed datasets where text metadata may be missing, how can nullable prompts be implemented so the network can (a) train on samples with and without prompts in one forward pass and (b) inject text conditioning at both a global (image-level) and local (feature-level) pathway? Describe the role of learnable null embeddings, presence masks, prompt dropout during training, what happens at inference, and the specific form of conditioning used in the global and local branches.",
      "answer": "Nullable prompts are implemented by giving each pathway (global and local) its own learnable \u201cnull\u201d text embedding and a binary mask that indicates whether a real prompt is present. Let the text encoder be T(\u00b7), and let G and L be the global and local prompts (or \u2205 when missing). Introduce learnable null embeddings z_g\u2205 and z_l\u2205 plus presence masks m_g=1[G\u2260\u2205], m_l=1[L\u2260\u2205]. During training, apply prompt dropout with rate p by sampling d_g,d_l~Bernoulli(1\u2212p) and setting \u03b1_g=m_g d_g and \u03b1_l=m_l d_l, then pass to the network\nz_g = \u03b1_g T(G) + (1\u2212\u03b1_g) z_g\u2205,\nz_l = \u03b1_l T(L) + (1\u2212\u03b1_l) z_l\u2205,\nwhich yields a single forward pass that covers all regimes (both prompts present, only one present, or neither). At inference, dropout is disabled (d_g=d_l=1), so each branch uses T(\u00b7) when its prompt is available and otherwise uses its learned null embedding.\nText is injected differently at the two levels: in the global pathway, token features A from a frozen CLIP ViT are modulated by a light conditional blend using two MLP heads \u03b3(\u00b7) and \u03b2(\u00b7): \u00c3 = \u03b3(z_g) \u2299 A + \u03b2(z_g), before spatializing and projecting. In the local pathway, Text-Conditioned Modulation injects z_l into deep ResNet stages (k\u2208{4,5}) via channel-wise scale and shift: \u00ea_k = \u03b3_k(z_l) \u2299 e_k + \u03b2_k(z_l). When prompts are absent, the same transforms operate using the learned nulls (not zero-filling), providing stable text-free conditioning.",
      "source_document": "papers/2512.20783v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a prompt-optional, dual-path breast-ultrasound segmenter that fuses a global CLIP-based context pathway with a local high-resolution ResNet pathway, what do component ablations typically show about (1) the contribution of the global pathway, (2) the contribution of the local pathway, and (3) the necessity of text guidance? Summarize the approximate impact on overlap (IoU/Dice) and miss rate (FNR) when each is removed, and explain the resulting interpretation about where most of the segmentation accuracy comes from.",
      "answer": "Ablations show a clear hierarchy:\n\n- Removing the global (CLIP-derived) pathway causes only a small degradation (about \u22121.8 IoU and \u22121.5 Dice relative to the full model), with FNR increasing modestly (to ~0.09). This indicates global, image-level context helps but is not the dominant source of performance.\n\n- Removing the local high-resolution pathway produces a large drop (on the order of \u221220 IoU and \u221216 Dice), and FNR worsens (to ~0.10). This shows fine-scale local features are critical for accurate boundaries.\n\n- Training/evaluating with \u201czero text guidance\u201d (both paths forced to use null text) collapses performance (IoU drops to ~0.36, Dice to ~0.44) and miss rate becomes very high (FNR ~0.46), implying semantic prompt guidance is essential rather than merely a mild regularizer.\n\nThe full configuration achieves the best tradeoff (IoU ~0.86, Dice ~0.91, FNR ~0.07), supporting the interpretation that local detail drives boundary quality, global context provides complementary gains, and text conditioning is crucial for robust lesion delineation under mixed prompt availability.",
      "source_document": "papers/2512.20783v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a dual-path breast-ultrasound segmenter that combines a frozen CLIP-ViT global context stream with a ResNet-based local detail stream, how are the two streams brought to a common spatial representation, fused at the bottleneck, and decoded back to a full-resolution mask? Describe (i) how the global token features are converted into a spatial feature map and channel-aligned to the local bottleneck, (ii) the exact fusion operation used at the bottleneck, and (iii) the main modules used in each decoder upsampling stage and the final prediction resolution.",
      "answer": "(i) The global stream uses a frozen CLIP ViT-B/16 prompt encoder to produce intermediate token features; after modulation, tokens are reshaped into a grid and upsampled to a spatial map. A Global Feature Projector then applies a 1\u00d71 projection with GroupNorm to output a 2,048-channel global feature map Fg that is aligned to the local bottleneck resolution.\n\n(ii) The local stream produces a bottleneck feature Fl via an ASPP module (rates 1, 6, 12, 18). The bottleneck fusion concatenates the two bottleneck maps and refines them as: Fuse = ASPP(Conv1\u00d71([Fg \u2225 Fl])).\n\n(iii) The decoder consists of four UpFusion stages: each stage upsamples, fuses with the corresponding encoder skip connection, applies squeeze\u2013excitation, and refines with a depthwise\u2013pointwise residual block. A final 2\u00d7 upsampler followed by a 1\u00d71 prediction head produces the logit map, which is bilinearly upsampled to a 352\u00d7352 output mask.",
      "source_document": "papers/2512.20783v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a prompt-optional breast-ultrasound segmenter on a unified pool that mixes datasets with metadata and datasets with no text prompts, how can you set up a fair comparison against (i) image-only CNN/CNN\u2013Transformer baselines and (ii) prompt-dependent models, and what characteristic performance tradeoff should you expect to see in overlap scores (IoU/Dice) versus error rates (FNR/FPR) under this mixed prompt-availability setting?",
      "answer": "Use identical preprocessing and supervision for all methods (grayscale normalization, resizing to a fixed resolution, and the same 5-fold image-level cross-validation splits). Retrain every baseline on the unified pool. For prompt-dependent models, supply text/vision prompts only for samples whose source metadata actually contains them and do not synthesize missing prompts; this exposes their inability to benefit from prompt-missing samples. A nullable-prompt model instead trains on the entire pool by feeding learned null embeddings (selected via presence masks, with prompt dropout during training) whenever text is absent, and can run with or without text at inference.\n\nUnder this setup, the nullable-prompt model achieves the best overlap (highest mean IoU/Dice, improving by roughly +9 IoU and +7 Dice over the strongest baseline) and the lowest miss rate (FNR \u2248 0.07), but at the cost of a much higher false-positive rate (FPR \u2248 0.18 versus ~0.01\u20130.02 for image-only baselines), indicating a shift toward higher sensitivity (fewer missed lesions) over background specificity.",
      "source_document": "papers/2512.20783v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a prompt-optional breast-ultrasound segmenter that uses a ResNet-50 as the local/high-resolution branch, how can the local branch be designed to (a) expand long-range context at the deepest features and (b) inject text guidance only where it is most semantically meaningful? Describe which ResNet stages receive text-conditioned modulation, the exact modulation form, what extra context module(s) are inserted around the deepest blocks, and the dilation rates used in the ASPP bottleneck.",
      "answer": "The local branch uses a ResNet-50 encoder producing stage features {e1,\u2026,e5}. To add long-range context near the deepest representation, two self-attention layers are placed around the deepest blocks, and an ASPP module is used to form the local bottleneck with dilation rates (1, 6, 12, 18). Text is injected only at deep stages via Text-Conditioned Modulation (TCM): for k \u2208 {4,5}, the feature ek is modulated by a channel-wise scale-and-shift conditioned on the local text embedding zl, i.e., \n\\tilde e_k = \u03b3_k(z_l) \u2299 e_k + \u03b2_k(z_l). Standard skip connections from e1\u2192e4 then feed the decoder.",
      "source_document": "papers/2512.20783v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an end-to-end RAW-to-segmentation co-design setup where optics, a learnable CFA, sensor noise/quantization, and a lightweight decoder are optimized jointly, what composite training loss can be used to keep optimization stable while improving mIoU/IoU on thin structures, and how are the individual terms weighted?",
      "answer": "A three-term objective is used that mixes (1) an OHEM (online hard example mining) pixelwise cross-entropy loss over the selected hard-pixel set, (2) a Lov\u00e1sz loss (to better optimize IoU), and (3) a smoothness regularizer over neighboring pixels. The total loss is\n\nL = 0.6\u00b7L_OHEM + 0.4\u00b7L_Lov\u00e1sz + \u03bb_smooth\u00b7L_smooth,\n\nwhere L_OHEM is the mean cross-entropy over hard pixels, L_Lov\u00e1sz = Lovasz(\u0177, y), and L_smooth penalizes differences between neighboring predictions (weighted by w_pq). The training notes also indicate that adaptive weighting of the OHEM and smoothness terms improves convergence on thin structures.",
      "source_document": "papers/2512.20815v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an end-to-end RAW-to-segmentation co-design setup that includes differentiable lens blur (PSF-based optics) and a learnable exposure module, what training schedule and stabilization tricks can be used to keep optimization from becoming unstable or degenerate when you start updating the optics parameters?",
      "answer": "Use a two-phase training schedule: first run a warm-up stage where the segmentation network is trained while the optics parameters are frozen; then switch to joint optimization by unfreezing the optics while throttling/attenuating gradients through the lens renderer (with an annealed throttle factor). To prevent degenerate scaling during this process, normalize lens outputs to have mean 0.5 and keep exposure bounded via a constrained gain range of [0.25, 4.0]. Mixed precision (AMP) is also used for efficiency and stability.",
      "source_document": "papers/2512.20815v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a differentiable RAW-to-semantic-segmentation co-design pipeline, how can you model realistic sensor degradation from both noise and limited bit-depth, and what differentiability trick lets you train end-to-end even though quantization is discrete?",
      "answer": "Model sensor noise with a Poisson\u2013Gaussian (shot+read) process by adding signal-dependent shot noise and signal-independent read noise to the mosaiced RAW signal: \u03b7_shot \u223c N(0, \u03c3_s^2\u00b7R) and \u03b7_read \u223c N(0, \u03c3_r^2), yielding R\u2032 = R + \u03b7_shot + \u03b7_read. Then simulate limited bit-depth by quantizing R\u2032 to b bits (after appropriate normalization/scaling). Because quantization is non-differentiable, include it during training using a straight-through estimator so the forward pass uses discrete quantization while the backward pass still propagates gradients through the quantization operator.",
      "source_document": "papers/2512.20815v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When pretraining a channel-adaptive ViT on heterogeneous multi-channel microscopy, which self-supervised objective families were compared (contrastive vs reconstructive vs self-distillation), and what did the results indicate about (i) the better-performing channel-handling architecture between bag-of-channels (BoC) and multi-channel attention (MCA), (ii) the best-performing SSL algorithm among those tested, and (iii) the typical relative gains from model scaling and from adding weak supervision?",
      "answer": "Three representative SSL families were evaluated: SimCLR-style contrastive learning, MAE-style masked/reconstructive learning, and DINO-style self-distillation. Under SSL pretraining, the bag-of-channels (BoC) strategy was found to be more effective and more scalable than multi-channel attention (MCA), reaching up to ~19% relative improvement while being cheaper computationally. Among the SSL algorithms tested, DINO was the top performer, giving roughly ~15% relative improvement over MAE and ~7% over SimCLR. Increasing model size (e.g., ViT-small to ViT-large) yielded a further ~10% relative improvement, and adding weak supervision (even with noisy labels) improved performance by about 1\u201319% relative in the MCA-SSL setting.",
      "source_document": "papers/2512.20833v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In image-based profiling, how can you evaluate whether learned microscopy embeddings disentangle technical batch effects from true biological signal, and in the specific landmark-compound setup used here what are (i) the batch vs biological variables, (ii) the profiling/aggregation level and batch-correction method applied, (iii) the three feature representations compared, and (iv) the key outcome in raw vs post-correction performance (including how much the best feature set improves over the baselines after correction)?",
      "answer": "Disentanglement is evaluated with a batch-correction benchmarking framework that scores feature sets on (a) how much biological signal they preserve and (b) how much batch signal remains, both before and after applying standard batch-correction algorithms; a good embedding needs little correction initially and yields high biological signal after correction.\n\nIn the landmark-compound experiment: (i) the task is classification of 302 landmark compounds generated by three different laboratories using the same microscope type; the batch variable is Laboratory ID and the biological variable is Compound identity. (ii) Profiles are computed with a population-averaged, well-level aggregation approach, and Seurat CCA is used as the batch-correction method (applied to all feature sets). (iii) Compared feature representations are: CellProfiler baseline (hand-crafted morphology features), IDRCell features extracted from a ViT-Small DINO-BoC model trained on IDRCell100K (~100K images), and CHAMMI-75 features extracted from a ViT-Small DINO-BoC model trained on the CHAMMI-75 Large dataset. (iv) CHAMMI-75 features perform best in the uncorrected (raw) state and remain best after Seurat CCA; after correction they provide the highest biological signal, improving by 1.7% over CellProfiler features and by 11.8% over IDRCell features, while IDRCell features are consistently worse than CellProfiler in both raw and corrected states.",
      "source_document": "papers/2512.20833v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a high-throughput chemical\u2013genetic interaction microscopy screen with replicate plates, how can you convert frozen backbone embeddings into a hit-calling / compound-ranking benchmark\u2014specifically, what is the pipeline to (i) aggregate single-cell embeddings, (ii) define an effect-size score relative to controls using distances, (iii) reduce batch/replicate effects when combining replicates, and (iv) evaluate ranking quality?",
      "answer": "Use the model to compute single-cell embeddings, then aggregate them by taking the mean per image. Build two Euclidean distance matrices: one comparing treatment images vs control images and another comparing control vs control images. Define the compound\u2019s effect size as the Wasserstein distance between the treatment\u2013control distance matrix and the control\u2013control distance matrix. To mitigate batch effects when combining the two experimental replicates, aggregate replicates using PCA whitening normalization. Rank compounds for each cell line by this effect score, and evaluate ranking quality with Recall@50, Recall@100, and AUROC.",
      "source_document": "papers/2512.20833v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When pretraining a representation model on a heterogeneous multi-study microscopy collection where random crops often capture mostly empty background and where magnification/resolution varies widely, what data-loading strategy can you use to ensure crops contain relevant cellular content at controlled scales, and what are the concrete sampling steps in that pipeline?",
      "answer": "Use a content-aware, hierarchical cropping data loader driven by precomputed single-cell locations and per-study scale annotations. First, run a segmentation pipeline (Cellpose) to obtain single-cell centroid coordinates (targeting the nucleus as the most consistently available structure) so crops can be centered on cells rather than arbitrary image regions. Second, manually define study-level crop-size annotations corresponding to available views (subcellular, cellular, multi-cellular) to handle scale variability across studies. During training, the loader samples: (1) a multi-channel image, (2) a single cell from the coordinates table, (3) an available scale size for that study, (4) crops a region around the selected cell at that scale and resizes it to the model\u2019s standard input size, and (5) applies any additional transformations/augmentations.",
      "source_document": "papers/2512.20833v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a multi-channel-attention (variable-channel) Vision Transformer with a DINO-style self-distillation objective, how can the model handle minibatches where each sample has a different number of channels (i.e., different token sequence lengths) so that attention does not mix in nonexistent/padded channels? Describe the padding/masking strategy used.",
      "answer": "Handle mixed sequence lengths by using masked attention: pad samples with fewer channels using zero (0) tokens to match the maximum sequence length in the batch, and add an attention bias in the softmax so the transformer ignores the padded tokens (i.e., padded channels do not contribute to attention).",
      "source_document": "papers/2512.20833v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "How does an input-adaptive visual preprocessing front-end reduce FastVLM inference cost without changing model weights: what lightweight signals are used to estimate image complexity, how do they control (i) resolution selection and (ii) content-aware cropping, and what efficiency/fidelity metrics are reported to validate the trade-off against the static preprocessing baseline?",
      "answer": "The approach adds a modular preprocessing stage before the unchanged FastVLM pipeline (FastViTHD vision encoder \u2192 projection \u2192 LLM), so no retraining or architectural changes are needed. It first performs content-aware image analysis by converting the image to grayscale and computing low-cost proxies of visual complexity\u2014edge density and/or entropy (also informed by coarse text-detection responses). The resulting complexity score is used to adapt computation: (i) adaptive resolution selection downscales visually simple/low-information images to a lower input resolution to cut the number of patch/tokens (and thus vision-encoding + prefilling cost), while keeping visually complex images at higher resolution to preserve fine-grained document details; (ii) content-aware cropping removes spatial redundancy by filtering low-information pixels (after grayscale/edge processing), computing a bounding box around content-dense regions (e.g., text/tables), cropping to that box, and optionally resizing to the chosen resolution. Validation is done with paired, inference-only comparisons against the static baseline using efficiency-oriented metrics\u2014end-to-end per-image inference time, mean full generation time, and relative visual token reduction\u2014and a fidelity check via per-image visual quality scoring (SSIM when applicable, with a fallback metric) plus qualitative inspection of VQA answer stability.",
      "source_document": "papers/2512.20839v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an end-to-end FastVLM inference pipeline (vision encoding + LLM generation), why can a large reduction in vision-side compute from adaptive visual preprocessing (e.g., fewer visual tokens) translate into only a small improvement in mean full generation time, and which stage is identified as the dominant latency bottleneck?",
      "answer": "Because the preprocessing-driven savings mainly reduce the vision-encoding portion of the pipeline, but end-to-end generation time also includes the language model\u2019s decoding steps; the document notes that vision encoding is only part of the overall process and that the later language-model decoding stage dominates, so reductions in vision computation only partially translate to total generation-time gains and do not scale linearly.",
      "source_document": "papers/2512.20839v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using adaptive resolution selection and content-aware cropping to cut visual tokens before a VLM\u2019s vision encoder, how can you verify that the efficiency gains are not coming from unacceptable visual degradation or degraded VQA outputs, and what qualitative/quantitative evidence indicates that large token reductions preserve fidelity?",
      "answer": "Verification is done by analyzing post-preprocessing visual quality alongside output stability: (1) compute per-image visual quality using structural similarity (SSIM), with a fallback quality metric when SSIM is not applicable, and examine the relationship between token count and quality; and (2) qualitatively inspect generated VQA answers under baseline vs adaptive pipelines for semantic consistency. The reported evidence is that most samples maintain high quality (typically >0.90, with many around 0.90\u20130.97) even when tokens are reduced substantially (often to <500), the token\u2013quality scatter shows no strong negative correlation (suggesting redundancy removal rather than semantic loss), and generated answers remain semantically consistent with only minor phrasing differences rather than semantic errors.",
      "source_document": "papers/2512.20839v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When matching pipeline detections from B-/C-/D-scan GPR images to recover a consistent 3D pipeline instance, what 3D similarity metric can be used instead of plain 3D-IoU, how is it computed (including what the center-distance penalty terms represent), and what failure case of 3D-IoU does this metric mitigate?",
      "answer": "A 3D-DIoU (3D Distance-IoU) metric can be used for cross-view 3D matching. It is computed by extending DIoU into 3D space:\n\n3D-DIoU = 3D-IoU \u2212 d\u00b2/c\u00b2,\n\nwhere 3D-IoU is the intersection-volume over union-volume of the two 3D boxes, d is the Euclidean distance between the centers of the two boxes, and c is the diagonal length of the smallest enclosing cuboid that covers both boxes. The added center-distance penalty mitigates the key limitation of using 3D-IoU alone: when two boxes do not overlap, IoU becomes 0 and provides no useful gradient/feedback for adjusting box positions; the distance penalty still provides a meaningful signal (and improves convergence/localization) even in the no-overlap case.",
      "source_document": "papers/2512.20866v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When adapting a YOLO-style detector to recognize small, noisy underground pipeline signatures in multi-view 2D GPR images, which specific YOLOv11 components are modified by (i) DySample, (ii) CGLU, and (iii) OutlookAttention, and what failure mode in GPR feature extraction each modification is intended to fix (in terms of edge/detail preservation, position-aware noise suppression, and cross-dimensional coherence of pipeline signatures)?",
      "answer": "(i) DySample replaces the standard nn.Upsample (nearest-neighbor interpolation) in the neck. Nearest-neighbor upsampling lacks dynamic adjustment and tends to lose fine-grained details; DySample uses dynamic offset generation plus Pixel Shuffle (with a grid_sample resampling on a dynamic sampling grid) to align features at sub-pixel level, helping reconstruct and preserve small-scale pipeline edge details such as B-scan hyperbolic edges.\n\n(ii) CGLU replaces the SE channel-attention in the C2PSA block. SE collapses each channel to a global scalar and ignores local spatial variation, which is problematic under heterogeneous GPR noise. CGLU puts a 3\u00d73 depthwise convolution in the gating branch of a gated linear unit so the gate becomes position-dependent, generating spatially varying attention that suppresses background/noise in low-reflectivity regions while amplifying local pipeline edges and preserving local discontinuities such as phase jumps.\n\n(iii) OutlookAttention is integrated into the C3K2 module. While C3K2 provides multi-scale kernels, it lacks explicit cross-dimensional correlation needed for joint spatial\u2013spectral modeling of GPR signatures; OutlookAttention aggregates a local window by weighting peripheral tokens relative to a center token via softmax, improving coherence between the hyperbola vertex (central geometric primitive) and its diffraction tails (peripheral context) and reducing false positives from edges.",
      "source_document": "papers/2512.20866v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When preparing stepped\u2011frequency continuous\u2011wave (SFCW) 3D GPR measurements for a YOLO-style pipeline detector, what signal/image preprocessing stages are applied before labeling, and how can you *quantitatively* verify that each stage is actually improving feature quality (including the definition of the metric used and what ablation outcome indicates the most critical preprocessing contribution to preserving hyperbolic/edge signatures)?",
      "answer": "The preprocessing pipeline converts the SFCW frequency\u2011domain measurements into a cleaner time\u2011domain representation and then suppresses nuisance components before image generation/labeling: (1) convert the frequency\u2011domain signal to a time\u2011domain signal using inverse selective discrete Fourier transform (ISDFT); (2) remove antenna direct\u2011coupling, surface reflections and system noise via a background\u2011removal filter; (3) apply a gradual low\u2011pass filter to suppress high\u2011frequency electronic noise, eliminate periodic noise and smooth random noise; and (4) apply exponential gain adjustment to compensate attenuation and enhance deep target visibility.\n\nTo verify each step improves usable features, an ablation study evaluates the *image information entropy* (IE) of the resulting GPR images, defined as IE = \u2212\u2211_{i=0}^{L\u22121} p(i)\u00b7log2 p(i), where L is the number of gray levels and p(i) is the fraction of pixels with gray value i. Higher IE is taken to mean richer/cleaner information and better preservation of discriminative pipeline cues (edge gradients, hyperbolic signatures). In the ablation, removing any individual preprocessing step degrades IE, and the strongest degradation occurs when only ISDFT is kept (i.e., omitting gain adjustment, background removal, and low\u2011pass filtering), implying that the combined denoising + gain stages are the most critical contributors to preserving discriminative pipeline signatures and improving SNR beyond the baseline transform alone.",
      "source_document": "papers/2512.20866v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a deep detector on 3D GPR-derived B/C/D-scan images, under what conditions can adding FDTD forward-simulated GPR data as a form of data augmentation *reduce* real-world performance, and what specific real-vs-sim discrepancies cause the model to learn biased features?",
      "answer": "Using FDTD-simulated GPR data for augmentation can hurt deployment performance when there is a large domain gap between simulation and field measurements, because the detector can overfit to idealized simulated signatures instead of the statistics of real noise and heterogeneity. The key discrepancies are that forward models cannot fully reproduce the complex noise patterns seen in field GPR (including system/measurement noise and clutter), and simulations typically rely on idealized assumptions about material homogeneity and simple geometries, whereas real underground environments are unpredictable and heterogeneous. As a result, simulated augmentation can bias the model toward \u201cclean/ideal\u201d features and degrade generalization on real data; prioritizing large volumes of authentic annotated field measurements avoids this bias.",
      "source_document": "papers/2512.20866v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-class GPR pipeline detector where some orientation classes are much rarer than others, what evidence can you use to argue that the model still detects the rare classes reliably, and what are two practical reasons you might *avoid* applying explicit class-imbalance countermeasures (e.g., reweighting or synthetic oversampling) in this setting?",
      "answer": "Reliability of rare classes can be supported by the per-class precision\u2013recall (P\u2013R) curves: even for rare categories (e.g., deeply inclined pipeline classes), the average precision stays above 0.90, meaning that when the model predicts these classes the predictions are highly reliable despite fewer training examples. Explicit imbalance countermeasures may be avoided because (1) they can cause overfitting on the rare classes, and (2) they can introduce unrealistic synthetic data; additionally, the achieved rare-class performance is already sufficient for the intended practical goal of accurate pipeline identification/typing with high recall (avoiding false negatives).",
      "source_document": "papers/2512.20866v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For a NeRV-style neural representation aimed at 6K 360\u00b0 video playback where a user views only a viewport, how can the decoder be made viewpoint-conditional while avoiding full panorama reconstruction (include when/where viewport extraction is performed and how the affine conditioning is computed/applied), what distortion loss is used to train the model, and what decoding-time memory and speed change is obtained versus an HNeRV baseline on JVET Class S2 sequences with the same decoder size?",
      "answer": "Viewport extraction is moved ahead of image-space decoding: the encoder produces an embedding y_t from the full equirectangular frame, and the user\u2019s viewpoint parameters (longitude \u03b8, latitude \u03c6, with a fixed FoV) drive a perspective projection that crops the corresponding viewport region y^vp_{t,\u03b8,\u03c6} directly in embedding space (with a channel expansion layer applied before the projection to reduce bilinear-interpolation artifacts). The viewport decoder is made viewpoint-conditional with a spatial-temporal affine transform (STAT) that extends the temporal-aware affine transform by also taking \u03b8 and \u03c6: embeddings of time t and viewpoint (\u03b8, \u03c6) are mapped to affine parameters \u03b2_{t,\u03b8,\u03c6} and \u03b3_{t,\u03b8,\u03c6}, and features are modulated as STAT(f_{t,\u03b8,\u03c6}) = \u03b3_{t,\u03b8,\u03c6} f_{t,\u03b8,\u03c6} + \u03b2_{t,\u03b8,\u03c6}. Training uses a distortion loss composed of (i) an L1 loss in the Fourier domain between FFT(viewport) and FFT(reconstructed viewport), plus (ii) a weighted pixel-domain L1 term and (iii) a weighted MS-SSIM term: L_d = L1(FFT(x^vp), FFT(x\u0302^vp)) + \u03bb \u03b1 L1(x^vp, x\u0302^vp) + \u03bb(1\u2212\u03b1)(1 \u2212 MS-SSIM(x^vp, x\u0302^vp)), with \u03bb=60 and \u03b1=0.7. On JVET Class S2 6K 360\u00b0 videos, this viewport-decoding design reduces decoding GPU memory by about 7\u00d7 and increases decoding speed by about 2.5\u00d7 compared to HNeRV at the same decoder size, while also improving PSNR/MS-SSIM.",
      "source_document": "papers/2512.20871v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a cross-modality ship re-identification setup that keeps a ViT-based vision foundation model fully frozen and adapts it by injecting learned feature deviations into intermediate layers, how are (i) the domain representation and per-layer deviations computed, (ii) where exactly are the deviations injected inside each Transformer block (relative to LayerNorm and to the Attention/MLP submodules), and (iii) what training objective and dataset-specific augmentation choices are used to learn the injector?",
      "answer": "(i) A lightweight Offset Encoder (OE) processes the raw input image to extract a compact domain representation f_d that encodes modality/identity attributes. The OE is instantiated as a small ViT (two layers) with RoPE and includes a learnable domain token; f_d is taken from the final normalized domain/[CLS]-like token. For each VFM block \u2113, separate linear modulators generate layer- and module-specific deviations: \u0394x^\u2113_attn = W^\u2113_attn f_d + b^\u2113_attn and \u0394x^\u2113_mlp = W^\u2113_mlp f_d + b^\u2113_mlp, with weights/biases initialized to zero for a stable start.\n\n(ii) The deviations are injected via additive fusion *after* Layer Normalization (post-norm injection) and into both core submodules of each Transformer block: Attention uses Attn(Norm(x_{\u2113\u22121}) + \u0394x^\u2113_attn) and the MLP uses MLP(Norm(x\u2032_\u2113) + \u0394x^\u2113_mlp), where x\u2032_\u2113 is the residual output after the Attention update.\n\n(iii) The injector is trained with the standard Re-ID combination of Triplet Loss and ID (cross-entropy) Loss using a BNNeck strategy, i.e., L_total = L_triplet + L_ID. For data processing, HOSS-ReID uses the original augmentation recipe (random horizontal flip, random crop, random erasing) with images resized to 128\u00d7256, while CMShipReID uses no data augmentation and resizes images to 224\u00d7224.",
      "source_document": "papers/2512.20892v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a transformer-based cross-modality ship re-identification pipeline where the dataset provides per-instance ship size and aspect-ratio metadata, how can this metadata be incorporated into a frozen ViT backbone as an additional token, and what measurable retrieval improvement does adding this \u201cship size token\u201d provide over the same frozen-backbone baseline on HOSS-ReID (mAP and Rank-1)?",
      "answer": "Encode the ship size and aspect ratio into a vector using a linear layer, then concatenate this resulting token immediately after the ViT\u2019s [CLS] token in the input token sequence. On HOSS-ReID, adding this ship size token improves the frozen ViT-S baseline from 34.4% mAP / 52.3% Rank-1 to 40.7% mAP / 56.2% Rank-1 (i.e., +6.3 mAP and +3.9 R1).",
      "source_document": "papers/2512.20892v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using feature-space domain injection to adapt a fully frozen ViT backbone for cross-modal ship Re-ID, what modulator design choices (projection architecture and parameter initialization) give the most stable and accurate adaptation, and why do these choices outperform (i) random initialization and (ii) a higher-capacity MLP modulator in a data-scarce setting?",
      "answer": "The most effective modulator is a simple per-layer linear projection (affine map) from the global domain representation f_d to the layer/module-specific deviation \u0394x, with the modulator weights initialized to zero so that \u0394x starts at 0.\n\n\u2022 Why zero initialization: with random initialization, the modulator injects nonzero deviations at the very start of training, effectively injecting \u201cnoise\u201d that disrupts the frozen backbone\u2019s pre-trained feature manifold; early training must first \u201cunlearn\u201d this disturbance. Zero initialization makes the injector an identity at step 0 (\u0394x = 0), so the model begins exactly from the pre-trained state and can gradually learn useful deviations without an initial shock, yielding much more stable and better adaptation.\n\n\u2022 Why linear beats MLP here: although an MLP has higher representational capacity, it increases optimization difficulty and overfitting risk under limited cross-modal Re-ID data. Since the Offset Encoder already extracts high-level semantic/domain attributes, the modulator mainly needs to project those attributes into each layer\u2019s channel space; a linear map is sufficient and provides a smoother optimization landscape and better generalization than an MLP in this setting.",
      "source_document": "papers/2512.20892v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a feature-injection approach to cross-modal ship re-identification that trains only lightweight injector modules while keeping the vision backbone frozen, how are the metric-learning and classification losses applied (i.e., which feature representation each loss uses and how BNNeck is involved), and what similarity function is used at test time to rank query\u2013gallery matches?",
      "answer": "Training uses a two-term objective: (1) Triplet loss is computed directly on the global representation f_g produced by the frozen backbone, and (2) the ID loss is a cross-entropy classification loss computed with a BNNeck setup (i.e., applying batch-normalization necking for the features used for the classifier). The total loss is L_total = L_triplet + L_ID. At inference, retrieval is performed by computing Euclidean distances between the global representations f_g of query and gallery images and ranking by this distance matrix.",
      "source_document": "papers/2512.20892v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a longitudinal CT + clinical-feature malignancy predictor that uses dual graphs for multimodal fusion, how are the intra-modal and inter-modal graphs defined (i.e., which feature types become nodes in each graph and how are edges formed), and what attention-block ordering is used in the hierarchical fusion module to progressively fuse modalities while reducing cross-modal alignment noise?",
      "answer": "The dual-graph setup uses fully connected edges in both graphs, but with different node types:\n\n\u2022 Intra-modal graph: nodes are the local features (Li) and global features (Gi) extracted from the same modality (CT), and the fully connected edges model relationships between these within-modality feature types to capture a hierarchical representation of nodule morphology.\n\n\u2022 Inter-modal graph: nodes are multimodal features including fused CT features from different time points (Ft0 and Ft1) and the clinical feature vector (Ftext), with fully connected edges to model cross-modal associations and spatiotemporal evolution.\n\nFor fusion, the hierarchical cross-modal graph fusion module uses a three-stage \u201cself-attention \u2192 cross-attention \u2192 self-attention\u201d ordering (implemented as SAB-CAB-SAB): an initial dual-path SAB refines each modality independently, a middle CAB applies bidirectional multi-head cross-attention to exchange information and align modalities, and a final SAB globally recalibrates the fused joint features to reduce alignment noise and improve semantic consistency/spatial coherence.",
      "source_document": "papers/2512.20898v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a 3D CT encoder that explicitly separates local texture cues from global contextual cues for longitudinal nodule malignancy prediction, how is the Global\u2013Local Feature Encoder designed to (1) extract local vs. global features with low compute, (2) fuse them progressively across stages (including any reuse of earlier fused features), and (3) use Adaptive Graph Channel Attention\u2014where what components form the adaptive adjacency matrix\u2014to reduce redundancy while enhancing representations?",
      "answer": "The Global\u2013Local Feature Encoder (GLFE) uses a three-branch design to separately capture (i) local features, (ii) global features, and (iii) their fused representation.\n\n1) Local vs. global feature extraction with low compute:\n- Local branch: the raw 3D nodule RoI is fed directly into a local feature branch that applies depthwise convolution to preserve channel-wise information while reducing cost, followed by layer normalization and then pointwise convolution to mix/reorganize channels and strengthen inter-channel correlations.\n- Global branch: the raw 3D input is first partitioned into non-overlapping 3D patches and processed with Swin-Transformer-style windowed attention, specifically Windows Multi-head Self-Attention (W-MSA) and Shifted Windows Multi-head Self-Attention (SW-MSA), to model broader context while keeping computation efficient.\n\n2) Progressive fusion across stages:\n- At each stage, an Adaptive Feature Fusion (AFF) block merges the local and global features to produce fused features.\n- After the encoder has progressed beyond early stages, fused features from the previous stage are also forwarded/connected into the current stage\u2019s AFF block, enabling hierarchical, progressive fusion rather than a single late concatenation.\n\n3) Adaptive Graph Channel Attention (AGCA) placement and adjacency components:\n- AGCA modules are inserted after both the local and the global feature extraction stages to enhance features and cut redundancy using adaptive graph convolution in the channel-attention pathway.\n- The adaptive adjacency matrix used by AGCA is composed of three parts: A0 (an identity matrix), A1 (a diagonal matrix), and A2 (a learnable adjacency matrix), with an additional learnable weight matrix used to model relationships among feature vertices.",
      "source_document": "papers/2512.20898v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a longitudinal, multimodal CT+clinical model for pulmonary nodule malignancy prediction under limited labeled data, what concrete training/evaluation protocol can be used to improve generalization and to measure cross-dataset robustness\u2014specifically, how are (1) the feature encoder pre-trained vs. the full model trained (including the loss used), (2) performance estimated on the primary dataset, and (3) an external dataset held out for validation?",
      "answer": "A workable protocol is:\n1) Two-stage optimization: first pre-train the Global\u2013Local Feature Encoder (GLFE) using a standard cross-entropy classification loss (with Adam), then train the entire multimodal/graph model end-to-end under the same loss/optimizer setting.\n2) Because the primary dataset is limited, estimate performance using 5-fold cross-validation to improve reliability/generalizability of the reported results.\n3) For cross-dataset robustness, use an external dataset purely as a test set (no training on it). Concretely, a held-out CLST split is formed by selecting 36 cases with two time points (72 total samples) and using this set exclusively for testing.",
      "source_document": "papers/2512.20898v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In longitudinal CT-based pulmonary nodule malignancy prediction, if the available clinical metadata is very limited (e.g., only nodule diameter), how should that diameter feature be incorporated so it helps without becoming the dominant cue, and what broader set of cues does the model rely on to make the malignancy decision?",
      "answer": "Diameter is treated as a supplementary clinical input rather than the primary determinant of malignancy. The prediction is meant to be driven mainly by richer cues that clinical diagnosis typically considers\u2014such as voxel-level texture characteristics, dynamic growth patterns across follow-up scans, and the complex interface between the nodule and surrounding lung tissue\u2014so the model learns a more holistic representation instead of over-weighting size alone.",
      "source_document": "papers/2512.20898v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a longitudinal, multimodal pulmonary-nodule malignancy predictor against prior work, what comparison protocol and set of evaluation metrics can be used to simultaneously assess (i) predictive quality and (ii) computational efficiency, and how can an external dataset be incorporated so the reported results reflect true out-of-domain generalization rather than training-set tuning?",
      "answer": "A fair benchmark can be run by training all competing methods under the same training environment and hyperparameter settings, then reporting both efficiency and predictive metrics. Efficiency is assessed via parameter count, while predictive quality is assessed with accuracy, precision, F1 score, AUC, and recall. For generalization, performance should be reported not only on the primary longitudinal dataset (NLST-cmst) but also on an external dataset that is held out from training entirely; here, the CLST cohort is used purely for testing (36 cases with two time points, 72 datapoints) so improvements on CLST reflect out-of-domain generalization rather than model selection on the training data. The method is compared against eight baselines: Scans, DeepCAD, NAS-Lung, ICHPro, CSF-Net, MMFusion, HGCN, and SAG-VIT, and it is characterized as outperforming them while remaining lightweight (fewer parameters).",
      "source_document": "papers/2512.20898v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "If you want a vision-language model to stay accurate on heavily compressed images from multiple codecs and bitrate levels without fine-tuning the whole model, what conditioning mechanism can be added inside a ViT-based vision encoder to represent the codec/bitrate, and what training loss is used to align features from compressed inputs with those from the uncompressed domain?",
      "answer": "Use a lightweight adaptor that turns the codec identity and compression level into a condition embedding: one-hot encode the (codec, bitrate-level) choice and map it through an embedding layer to a codec condition embedding Cemb with the same dimension as the vision encoder. Inject this condition throughout the ViT vision encoder by adding Cemb to the rotary positional embedding (RoPE), yielding a unified conditional vision encoder (CVE). Train CVE on compressed images via feature distillation from the original vision encoder (VE) run on the corresponding uncompressed image, minimizing an L2/MSE loss between their output features: Ld = ||CVE(X\u0302, Cemb) \u2212 VE(X)||^2.",
      "source_document": "papers/2512.20901v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a vision-language model\u2019s drop in task performance after lossy image compression, how can the total performance gap be decomposed into an (irreducible) information gap and a (reducible) generalization gap? In your answer, give the mathematical decomposition (including how the optimal parameter set for the compressed domain is defined) and explain which part must be addressed by improving the codec versus which part can be mitigated by adapting/fine-tuning the VLM.",
      "answer": "Let the original image be X, the compressed image be X\u0302, the VLM parameters be \u03b8, and the benchmark score be L(\u00b7,\u00b7). The total performance gap is decomposed as:\n\nL(X,\u03b8) \u2212 L(X\u0302,\u03b8) = [L(X,\u03b8) \u2212 L(X\u0302,\u03b8*)] + [L(X\u0302,\u03b8*) \u2212 L(X\u0302,\u03b8)],\n\nwhere \u03b8* is defined as the best achievable model parameters on the compressed inputs:\n\nL(X\u0302,\u03b8*) = max_\u03b8 L(X\u0302,\u03b8).\n\nThe first term, L(X,\u03b8) \u2212 L(X\u0302,\u03b8*), is the information gap: it comes from irreversible information loss during compression and cannot be eliminated by fine-tuning the VLM (it must be addressed by improving the compression algorithm/codec). The second term, L(X\u0302,\u03b8*) \u2212 L(X\u0302,\u03b8), is the generalization gap: it is due to the VLM failing to generalize to compressed/distorted images and can be reduced by fine-tuning/adapting the VLM on compressed images (e.g., via an adaptor).",
      "source_document": "papers/2512.20901v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "Suppose you train a lightweight VLM adaptor using compressed images from only a small set of codecs. How can you test whether this adaptor generalizes to *unseen* codecs at inference time without retraining, and what qualitative outcome would suggest the adaptor\u2019s conditioning is less compatible when the unseen codec belongs to a different codec family (e.g., GAN-based vs diffusion-based generative compression)?",
      "answer": "Generalization can be tested by freezing the already-trained adaptor and directly evaluating it on images produced by unseen codecs, while feeding the adaptor a *surrogate* codec-condition label (i.e., \u201cpretend\u201d the unseen codec is one of the trained codecs that is most similar). In the document\u2019s setup, HM, MLICpp, and DiffEIC are evaluated using an adaptor trained on JPEG, ELIC, and MS-ILLM by conditioning HM\u2192JPEG, MLICpp\u2192ELIC, and DiffEIC\u2192MS-ILLM. The expected outcome for good generalization is consistent rate\u2013accuracy improvements on the unseen codecs. A sign of conditioning mismatch is that gains are uneven across tasks\u2014e.g., DiffEIC shows improvement on a coarse benchmark (POPE) but a slight degradation on a fine-grained benchmark (SEEDBench), attributed to the larger semantic/structural gap between the trained GAN-based codec (MS-ILLM) used as the surrogate condition and the unseen diffusion-based codec (DiffEIC).",
      "source_document": "papers/2512.20901v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When trying to improve a VLM\u2019s robustness to heavily compressed images, you can intervene on the *codec side* (to preserve task-relevant information) or on the *model side* (to better handle compression artifacts). Consider a setup with (i) a base learned codec, (ii) an \u201cimage coding for machines\u201d (ICM) variant of that codec trained using the VLM\u2019s vision encoder, and (iii) a lightweight VLM adaptor that is applied at inference/training time without changing the codec. How can BD-Metric results on a benchmark like POPE be used to tell which intervention primarily reduces the **information gap** versus the **generalization gap**, and what qualitative ordering of BD-Metric improvements indicates that combining the ICM codec with the adaptor addresses both gaps and performs best?",
      "answer": "BD-Metric comparisons can be interpreted as follows: the ICM-style codec variant (the codec trained with the VLM vision encoder) mainly targets the **information gap** by changing the compressed signal so that more task-relevant information survives compression; this typically yields only a modest gain over the base codec. In contrast, applying the VLM adaptor mainly targets the **generalization gap** by adapting the VLM\u2019s vision encoder to distorted/compressed inputs; this yields a larger gain over the corresponding non-adapted model for the same codec. If the BD-Metric ordering is\n\n(base codec) < (ICM codec) < (base codec + adaptor) < (ICM codec + adaptor),\n\nthen the last condition (ICM codec + adaptor) indicates that the two interventions are complementary\u2014codec-side changes reduce information loss while the adaptor reduces the VLM\u2019s failure to generalize\u2014so addressing both gaps jointly gives the best POPE robustness.",
      "source_document": "papers/2512.20901v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "If you expect that simply scaling up a vision-language model (e.g., from 1B to 8B parameters) will make it inherently more robust to low-bitrate compression artifacts, how can you test this hypothesis using rate\u2013metric evaluation, and what specific observed behaviors indicate that the usual scaling-law intuition fails on compressed images (including how the bitrate\u2013performance monotonicity can break for a generative codec like StableCodec in smaller models)?",
      "answer": "Test it by running scaling experiments across multiple model sizes and plotting rate\u2013metric curves (or equivalently, measuring the performance drop relative to the uncompressed baseline) for several codec types and multiple downstream benchmarks. The failure of the scaling-law intuition is evidenced by two observations: (1) increasing model size does not consistently reduce the compression-induced degradation (the drop between compressed and uncompressed performance is not reliably smaller for larger models), and (2) for StableCodec, smaller models can show non\u2011monotonic rate\u2013distortion behavior where the highest bitrate does not yield the best POPE score; this non\u2011monotonicity diminishes as model size increases, implying generalization capacity modulates the rate\u2013distortion curve rather than guaranteeing robustness.",
      "source_document": "papers/2512.20901v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a panorama-to-3D visual grounding pipeline that uses an autoregressive VLM to output 2D boxes, how can the 2D bounding-box coordinates be encoded for supervision, and what extra training signals can be added to (1) make the coordinate prediction aware of numerical proximity and (2) teach geometric reasoning from the rendered depth/range map without additional human annotation?",
      "answer": "Coordinates are supervised by normalizing each 2D box coordinate to a fixed integer range and predicting it as a short sequence of digit tokens with token-level cross-entropy under teacher forcing: each coordinate is mapped to [000,999], discretized into N=3 digits, and generated as digits {0,\u2026,9}. To make the digit prediction numerically distance-aware (so near-miss digits are penalized less than far-off ones), an auxiliary Earth Mover\u2019s Distance (1\u2011Wasserstein) loss is applied over the predicted per-digit distribution: for each digit position i, the loss sums the predicted probability mass times |Di\u2212d| (distance from the ground-truth digit Di), scaled by a place-value weight Wi (hundreds/tens/ones), and the total coordinate loss is the sum over digits of CE + \u03bb\u00b7EMD. To add geometry-aware supervision without new labels, auxiliary \u201cGeometric QA\u201d samples are generated on-the-fly during training by randomly sampling two pixels in a panorama, reading their ground-truth depths from the rendered range map, and creating a text prompt asking which point is closer/farther; these QA pairs are interleaved with standard 3D visual grounding batches at a fixed ratio during fine-tuning.",
      "source_document": "papers/2512.20907v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-view 3D visual grounding system that predicts a 2D box on each panoramic rendering, how can the method lift these per-view 2D predictions into 3D and fuse them into a single final 3D bounding box, including (i) how the 2D box is converted to a 3D point set, (ii) how the \u201cbest\u201d view is selected using cross-view consistency, and (iii) how visibility filtering is applied before fitting the final box?",
      "answer": "(i) For each panorama view s, take the predicted 2D box b_s and run a segmentation model (e.g., SAM) on the RGB panorama I_s to obtain a mask M_s. Unproject the masked pixels using the view\u2019s rendered range/depth map D_s and known camera extrinsics to map them to world coordinates, forming a per-view 3D point set P_s.\n\n(ii) Select the best view by checking cross-view geometric consistency: for each candidate source view s, project its 3D points P_s into every other view t\u2260s, build the tight 2D box \\hat b_{s\u2192t} around the projected points, and compute a consistency score score(s)=\u2211_{t\u2208S\\{s}} IoU(\\hat b_{s\u2192t}, b_t). Choose s* = argmax_s score(s).\n\n(iii) Fuse points across views by merging all per-view point sets P=\u22c3_s P_s and denoising them, then project the denoised points into the selected best view s* and keep only those whose projections fall inside b_{s*}, yielding a visibility-filtered set P_vis. Fit an axis-aligned bounding box (AABB) to P_vis to obtain the final 3D bounding box.",
      "source_document": "papers/2512.20907v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using panoramic renderings as intermediate inputs for 3D visual grounding, how can you choose a compact set of camera locations in a structure-aware way\u2014i.e., what scoring terms should be used, how are they combined into a single score, and what greedy selection rule ensures broad scene coverage while avoiding degenerate viewpoints?",
      "answer": "A compact set of panorama centers can be selected by laying a dense candidate grid on the estimated floor plane (regular 2D grid over the floor footprint, with camera height fixed to the average raw capture-camera height) and scoring each candidate location p with three geometry-aware factors:\n\n\u2022 Ray coverage A(p)\u2208[0,1]: the fraction of other grid points visible from p within a fixed radius (r_max=3 m), encouraging viewpoints that \u201csee\u201d as much of the scene as possible.\n\u2022 Distance-to-surface D_surf(p): Euclidean distance from p to the nearest scene geometry (walls/furniture); larger values are preferred to avoid oversized foregrounds, panoramic distortion, and occlusions caused by being too close to obstacles.\n\u2022 Distance-to-trajectory D_traj(p): Euclidean distance to the nearest raw RGB camera center (when available); smaller values encourage consistency with the original capture trajectory, which improves reconstruction fidelity and panorama quality.\n\nThese are combined as\n\nS(p) = A(p) \u00b7 D_surf(p) / (D_traj(p) + \u03b5), with \u03b5=10^\u22123,\n\nfavoring wide coverage, clearance from obstacles, and proximity to the capture path.\n\nSelection is greedy: repeatedly pick the highest-scoring remaining point; after selecting a camera, mark all grid points visible from it as \u201ccovered\u201d and set their contribution to ray coverage A(p) to zero for all remaining candidates. Continue until at least 90% of the scene\u2019s grid points are covered.",
      "source_document": "papers/2512.20907v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When augmenting a pretrained vision-language model to ground referring expressions on panoramic renderings that include extra semantic and range cues, how can the additional modalities be injected into the VLM\u2019s ViT vision tokens without initially disturbing the pretrained representation, and which transformer layer ranges are appropriate for injecting (i) geometric/range features versus (ii) semantic features to best support spatial scaffolding and high-level contextual reasoning?",
      "answer": "Use a lightweight per-modality feature adapter that maps each modality\u2019s per-patch features into the ViT token space and adds them residually to the ViT tokens. Concretely, each modality is passed through a 2-layer MLP followed by a 1\u00d71 convolution whose weights and bias are initialized to zero (Zero-Convolution), so at initialization the adapter output is exactly zero and the VLM behaves like its pretrained form; fine-tuning then learns task-relevant signals via the adapters. The injected update is patch-wise: X_{l,n} \u2190 X_{l,n} + ZeroConv(MLP(f_{m,n})). Geometric (range-derived) features are injected into mid-level ViT layers (approximately l \u2208 [L/3, 2L/3]) to provide a spatial scaffold, while semantic features are injected into later layers (approximately l \u2208 [2L/3, L]) to provide high-level contextual cues.",
      "source_document": "papers/2512.20907v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For a 3D visual grounding pipeline that feeds equirectangular panoramas into a 2D vision-language model, what yaw-based augmentation strategy can be used during training and inference to improve robustness, and what is a reasonable number of in-place yaw rotations to use at test time (including the rationale for that choice)?",
      "answer": "Use yaw invariance by treating camera yaw changes as horizontal circular shifts (wrap-around) of the equirectangular panorama: apply random in-place yaw rotations as data augmentation during training, and at inference perform test-time augmentation by rendering/creating multiple yaw-rotated panoramas via horizontal circular shifts and running independent predictions on each. Using four 90\u00b0 in-place rotations is reported as the best trade-off: accuracy improves quickly with a small number of rotations and continues to increase until about 4\u20138 rotations, after which gains saturate, so 4 rotations provide near-optimal accuracy improvement for the computational cost. This can be viewed as a self-consistency scheme where agreement across yaw-perturbed inputs serves as a proxy for confidence.",
      "source_document": "papers/2512.20907v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a mixture-of-experts cross-modal fusion module that activates only a subset of experts per input, what mechanisms can be used to (1) avoid gating collapse, (2) encourage experts to learn complementary fusion strategies, and (3) later drive the activated experts toward a consistent fused representation\u2014and how are these objectives combined into a single training loss over time?",
      "answer": "A unified approach is to couple top\u2011k noisy gating with three auxiliary objectives. First, avoid gating collapse with a workload balancing loss defined on the expert weights, using the coefficient of variation: Lwb = (\u03c3(Wexp)/mean(Wexp))^2, encouraging all experts to receive nontrivial weight during training. Second, encourage complementary expert behavior with an expert diversity loss that reduces similarity between different experts\u2019 outputs, implemented as the average pairwise cosine similarity between expert outputs (lower similarity \u21d2 higher diversity): Ldiv = (1/(N(N\u22121))) * \u03a3_{i\u2260j} cos(F\u0302i, F\u0302j), where F\u0302i is the i-th expert output. Third, enforce consistency via a consensus feature computed as the weighted average of expert outputs, Fconsensus = \u03a3_i Wi_exp\u00b7F\u0302i, and a consensus loss that penalizes deviations from this aggregate: Lcons = \u03a3_i Wi_exp \u00b7 ||F\u0302i \u2212 Fconsensus||_2^2. These are combined with a time-decayed weighting so training emphasizes diversity early and consensus later: Lmccm = Lwb + \u03bb(t)\u00b7Ldiv + (1\u2212\u03bb(t))\u00b7Lcons with \u03bb(t)=cos((t/T)\u00b7\u03c0/2), where t is the current epoch and T is the total number of epochs.",
      "source_document": "papers/2512.20921v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "How can a self-supervised bi-level contrastive regularizer be constructed to explicitly preserve high-frequency detail in a general (task-agnostic) image-fusion network, and what are treated as positive vs. negative targets at (1) the fused-feature level and (2) the fused-image (pixel) level; additionally, how is this regularizer combined with other training losses in the final objective?",
      "answer": "A bi-level self-supervised contrastive regularizer can be built by explicitly decomposing both fused representations and source-modality representations into low- vs. high-frequency components using a Haar wavelet lifting scheme. At the feature level, the modality-enhanced feature for each modality is channel-split into two correlated subsets (coarse/low-frequency Fc1 and fine Fc2); a Prediction block uses Fc1 to predict Fc2, yielding a high-frequency residual F^h, and an Update block refines Fc1 using that residual to produce an updated low-frequency component F^l. The same decomposition is applied to the fused feature to obtain fused F^h_mf and F^l_mf.\n\nFeature-level contrastive targets: form concatenated source-modality high-frequency features F^h_mc = Cat(F^h_m1, F^h_m2) and low-frequency features F^l_mc = Cat(F^l_m1, F^l_m2). The feature-level contrastive loss pulls fused high-frequency F^h_mf toward F^h_mc (positive) while pushing it away from F^l_mc (negative), and symmetrically pulls fused low-frequency F^l_mf toward F^l_mc (positive) while pushing it away from F^h_mc (negative), implemented with L2 distances.\n\nPixel-level contrastive targets: similarly decompose the fused image and source images into Ih and Il, concatenate Ih_mc = Cat(Ih_m1, Ih_m2) and Il_mc = Cat(Il_m1, Il_m2), then define a pixel-level contrastive loss that pulls fused Ih_mf toward Ih_mc and pushes away from Il_mc (and pulls fused Il_mf toward Il_mc while pushing away from Ih_mc), again via L2 distances.\n\nFinal objective: the total training loss combines the feature-level contrastive loss Lfcl and pixel-level contrastive loss Lpcl with the MCCM-related loss Lmccm plus SSIM loss and an intensity loss: L_total = \u03bb1 Lfcl + \u03bb2 Lpcl + \u03bb3 Lmccm + \u03bb4 L_ssim + \u03bb5 L_int (with \u03bb1\u2026\u03bb5 set to 0.8, 0.4, 1, 1, 1).",
      "source_document": "papers/2512.20921v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a general-purpose image-fusion network that needs to preserve local detail while also modeling long-range context efficiently, how can a modality-agnostic feature enhancement block be built using (1) a patchwise gated local branch and (2) a global state-space (Mamba/SSM) branch that mixes information via both spatial\u2013channel scanning and frequency-domain (amplitude/phase) scanning; and how are the local and global features combined into the enhanced per-modality representation passed to the fusion module?",
      "answer": "A modality-agnostic feature enhancement (MAFE) block can start by extracting shallow per-modality features with a 3\u00d73 convolution followed by layer normalization: F_s^k = LN(Conv_{3\u00d73}(I_m^k)).\n\n1) Local branch (detail preservation):\n\u2022 Tokenize/split the shallow feature map into smaller patches (patch tokens) F_s^{k,j}.\n\u2022 Apply a 3\u00d73 depth-wise convolution to each patch, then use a learned gating unit to adaptively modulate fine-grained details: a 1\u00d71 conv followed by a gate produces an attention map that is multiplied element-wise with the depth-wise patch features to produce the local feature F_L.\n\n2) Global branch (long-range context) with two complementary SSM scans:\n\u2022 Spatial\u2013channel SSM: process F_s^k through parallel sub-branches (one direct SiLU; one 1\u00d71 conv + 3\u00d73 depth-wise conv, SiLU), then apply a spatial\u2013channel scanning operator SC-Scan(\u00b7) (with LN) and modulate with the SiLU-activated features to get a spatial global feature F_spa.\n\u2022 Frequency\u2013rotational SSM: transform F_s^k to the Fourier domain (DFT), separate amplitude A(F_s^k) and phase P(F_s^k), apply 3\u00d73 depth-wise conv + SiLU to each, then apply a frequency-rotational scan FR-Scan(\u00b7) to obtain scanned amplitude and phase features; invert back to the spatial domain via IDFT and modulate with SiLU(F_s^k) to obtain F_fre.\n\u2022 Concatenate the spatial and frequency global features: F_G = Cat(F_spa, F_fre).\n\nFinally, combine local and global outputs by concatenation to form the enhanced modality-agnostic representation for each modality: F_m^k = Cat(F_L, F_G), which is then fed to the subsequent cross-modal fusion module.",
      "source_document": "papers/2512.20921v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a two-modality image-fusion backbone that uses an SSM/Mamba-style \u201cscan\u201d operator for long-range modeling, how can a *cross-modal scanning* operator be designed to capture both spatial and channel dependencies *between* modalities, and how is this operator used inside a cross-modal Mamba expert block to produce the fused feature from the two modality feature maps (include the main fusion equation)?",
      "answer": "Cross-modal scanning can be defined to explicitly scan *across modalities* in both (i) the spatial dimension and (ii) the channel dimension: spatial scanning runs forward and reverse passes between the two modalities to model long-range spatial correlations, while channel scanning alternates across modalities to capture inter-modal channel dependencies (CM-Scan).\n\nInside a cross-modal Mamba expert, the two enhanced modality-agnostic features are first layer-normalized and linearly projected, then passed through a 1\u00d71 convolution with SiLU to obtain Fsilu1 and Fsilu2. Cross-modal scanning is applied in both directions:\n- Fcm1 = CM-Scan(Fsilu1, Fsilu2)\n- Fcm2 = CM-Scan(Fsilu2, Fsilu1)\nThe fused output feature is then computed by cross-gating with the other stream\u2019s (pre-conv) activation and summing:\nF_mf^N = Fcm1 \u2299 SiLU(Fln2) + Fcm2 \u2299 SiLU(Fln1).",
      "source_document": "papers/2512.20921v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a mixture-of-experts (MoE) cross-modal fusion block for two-modality image fusion, how can the gating weights be computed from the input feature maps using pooled global descriptors plus learnable noise and Top-K routing\u2014and what is the difference between training and inference in terms of which experts are executed?",
      "answer": "Concatenate the two modality features and extract a global gating descriptor with both global average pooling and global max pooling: Fmc = Cat(Fm1, Fm2), then Fg = GAP(Fmc) + GMP(Fmc). Inject learnable stochasticity by adding noise \u03b5 = N(0,1) \u00b7 Softplus(Fg \u00b7 Wnoise) to the gating logits for stable non-negative noise. Compute expert weights by routing to the Top-K experts and applying a softmax over those selected logits: Wexp = Softmax(TopK(Fg \u00b7 Wg + \u03b5)), so only the top-k experts (k=2) get nonzero weight. During training, all experts are run (their outputs are combined using Wexp to guide learning), while at inference only the top-k selected experts are actually executed for efficiency and task-adaptive computation.",
      "source_document": "papers/2512.20921v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an open-vocabulary 3D semantic segmentation pipeline that distills 2D CLIP features into 3D Gaussian features rendered with a sparse transmittance-aware renderer, how is the 2D supervision constructed from an input image, and what contrastive objective is minimized between the rendered feature and the CLIP targets (i.e., what constitutes the positive term vs. the negatives, and what similarity function is used)?",
      "answer": "The 2D supervision is built by first extracting a set of image patches/masks {m} using Grounded-SAM2, then running CLIP\u2019s vision encoder on each corresponding patch to obtain a 512-D target embedding f_CLIP. The network renders a feature vector \u007f~f^Q via the quantile renderer and trains by minimizing a contrastive (InfoNCE-style) loss using cosine similarity: \nL = -log( exp(sim(~f^Q, f_CLIP_i)) / sum_{j \\neq i} exp(sim(~f^Q, f_CLIP_j)) ),\nwhere the positive pair is (~f^Q, f_CLIP_i) for the matched mask/patch and the negatives are the other CLIP embeddings f_CLIP_j in the denominator, and sim(\u00b7,\u00b7) is cosine similarity.",
      "source_document": "papers/2512.20927v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an adaptive quantile-rendering setup where you don\u2019t want to fix the number of sampled Gaussians K a priori, how can K be selected dynamically at inference time, and what extra prediction head and training loss are used to make that possible (including what the loss regresses to and what inputs it uses)?",
      "answer": "K can be selected dynamically by training with a small set of candidate values (K\u2208{10,20,40}) and adding a similarity\u2011prediction head that takes the ray\u2019s transmittance profile as input and predicts the expected CLIP alignment (similarity) for each candidate K. During training, in addition to the original contrastive/distillation loss used to align the rendered feature with the target CLIP embedding, an extra regression loss is applied that penalizes the difference between the head\u2019s predicted similarity and the actual similarity computed between the rendered feature and the CLIP target (using cosine similarity). At inference, the model evaluates the predicted similarity scores for the candidate Ks and chooses the K with the highest expected similarity for that input.",
      "source_document": "papers/2512.20927v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When adapting 3D Gaussians (which have volumetric extent and can overlap) to sparse-voxel 3D backbones like MinkUNet/PTv3, how are the Gaussians voxelized for network input, and how are the network\u2019s voxel-level predictions de-voxelized back into per-Gaussian feature vectors (i.e., what coordinate is sampled for voxelization and what mapping/reduction operations are used to aggregate voxel predictions per Gaussian)?",
      "answer": "To make continuous, overlapping 3D Gaussians compatible with standard sparse-voxel 3D backbones, each Gaussian is voxelized by sampling a representative point\u2014specifically its center location \u03bc\u2014so the scene can be represented as sparse voxels with unique spatial locations that MinkUNet/PTv3 can process; the backbone predicts voxel features which are then converted back to Gaussian-level features via a de-voxelization step.\n\nIn de-voxelization, predictions are first expanded from the unique-voxel set back to the full (possibly duplicated) voxel list using an inverse index map (implemented with a scatter operation). Then, per-voxel predictions belonging to the same Gaussian are grouped using offsets formed from the cumulative sum of the number of voxels per Gaussian, and reduced into a single per-Gaussian prediction with a segment-wise reduction (segment_csr), using a reduction rule such as max (the provided implementation uses REDUCE=\"max\").",
      "source_document": "papers/2512.20927v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a transmittance-guided \u201cquantile rendering\u201d alternative to standard 3D-GS volume rendering, how are the K representative Gaussians along a ray selected from the rasterized Gaussian list, how are their alpha-blending weights computed, and why/how is the final rendered feature vector normalized when only a subset of Gaussians are blended?",
      "answer": "Quantile rendering walks through the ordered (front-to-back) list of rasterized Gaussians along a ray while tracking transmittance. It partitions the transmittance interval T\u2208[0,1] into K+1 evenly spaced segment boundaries (thresholds of the form 1\u2212(k+1)/(K+1)). For each Gaussian i it computes a tentative updated transmittance T_test = T\u00b7(1\u2212\u03b1\u2032_i). When T_test drops below the next boundary, that Gaussian is selected as a \u201cquantile Gaussian\u201d for the current interval (k\u2190k+1), and it is alpha-blended using weight w_Q = T_Q\u00b7\u03b1\u2032_i, accumulating f_Q \u2190 f_Q + w_Q\u00b7f_i and updating the quantile-render transmittance T_Q \u2190 T_Q\u00b7(1\u2212\u03b1\u2032_i). A while-loop increments k further if the same Gaussian crosses multiple boundaries. The traversal can early-stop once T_test < 1/(K+1). Because blending only K quantile Gaussians can leave a non-negligible remaining transmittance T_Q (unlike dense volume rendering where the final transmittance tends toward 0), the accumulated feature is normalized as f\u0303_Q = f_Q/(1\u2212T_Q) to approximate the zero-final-transmittance behavior of standard volume rendering.",
      "source_document": "papers/2512.20927v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When turning a set of volumetric 3D Gaussians into sparse voxels for a 3D backbone (e.g., MinkUNet/PTv3), what per-voxel feature vector is constructed for each sampled voxel, and how is the voxel\u2019s opacity/influence computed from the Gaussian parameters (include the distance metric and attenuation form)?",
      "answer": "Each sampled voxel is represented by concatenating its 3D coordinate, the Gaussian\u2019s RGB, and a voxel-opacity term: [voxel_xyz, voxel_rgb, voxel_opacity]. The voxel opacity is computed using a Mahalanobis-distance influence under the Gaussian: dist = (voxel_xyz \u2212 \u03bc)^T \u03a3^{-1} (voxel_xyz \u2212 \u03bc), where \u03bc is the Gaussian mean and \u03a3^{-1} its inverse 3D covariance matrix. The voxel opacity/influence applies Gaussian attenuation to the Gaussian\u2019s opacity factor \u03b1: voxel_opacity = \u03b1 \u00b7 exp(\u22120.5 \u00b7 dist).",
      "source_document": "papers/2512.20927v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an experience-driven visual programming agent for 3D spatial reasoning like TVP, how does the closed-loop pipeline expand the tool library from scratch: (i) how are prior solved examples retrieved and used to generate candidate programs for a new query, (ii) what gating rule determines whether the best program becomes a stored \u201cexperience\u201d example, (iii) how are stored examples clustered and turned into parameterized tools, and (iv) what validation and maintenance steps ensure newly abstracted or merged tools execute reliably and don\u2019t degrade answer correctness before rewriting past solutions to use them?",
      "answer": "TVP grows its capabilities with a program\u2192experience\u2192abstraction\u2192reuse loop built around two evolving libraries.\n\n(i) For each new (image, question), it retrieves up to kmax similar solved queries from the Example Library using text-embedding similarity (only those above a similarity threshold). These retrieved examples are provided as in-context demonstrations to the program generator, together with the current Tool Library (tool signatures/docstrings). The generator then samples multiple candidate programs, executes them, and keeps only those that successfully execute and return a non-None result.\n\n(ii) A VLM-based judge scores each successfully executed candidate using the program plus its full execution trace and visual evidence from the image. The highest-scoring candidate is selected; it is added to (or replaces the entry in) the Example Library only if its quality score meets a quality-threshold gate (and higher-quality or tool-different solutions can replace earlier ones for the same question).\n\n(iii) At regular abstraction intervals, TVP clusters eligible stored examples by embedding similarity. For clusters that are large enough and sufficiently similar, an LLM analyzes the cluster\u2019s solutions to identify a common computational pattern and an \u201cabstraction potential\u201d score. Clusters with high abstraction potential trigger tool creation: a tool abstractor produces a parameterized function that captures the shared logic across the cluster.\n\n(iv) Before adding a new tool, TVP performs two-stage validation: (1) execution validation, where every program in the cluster is rewritten to use the proposed tool and must execute successfully (any execution failure causes immediate rejection); and (2) correctness validation, where if rewritten programs produce results that differ from the originals, a judge checks whether the new results are still correct given the image (accepting the tool only if overall correctness across the cluster exceeds a required minimum). If validated, the tool is added to the Tool Library and the corresponding examples are rewritten/marked as abstracted to use the new tool.\n\nTo keep the library concise, a periodic maintenance step finds functionally similar learned tools and merges them into a more general tool; the merged tool is then validated with the same two-stage procedure on all examples that used any of the original tools, and, if successful, the old tools are deprecated and affected examples are updated to use the merged tool.",
      "source_document": "papers/2512.20934v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating 3D spatial reasoning systems on a benchmark that includes both discrete question types (yes/no, multiple-choice, counting) and continuous-valued numeric questions, how are floating-point answers scored (including the Mean Relative Accuracy setup and the additional tolerance-based metric), and what failure mode does this breakdown expose for monolithic VLMs compared to a compositional visual-programming approach like TVP?",
      "answer": "Floating-point (numeric) answers are evaluated with Mean Relative Accuracy (MRA), which computes accuracy under a sweep of relative-error thresholds: for a set of tolerances C = {0.5, 0.55, \u2026, 0.95}, a prediction is counted correct at threshold \u03b8 if |\u0177 \u2212 y|/y < 1 \u2212 \u03b8, and MRA averages this indicator over all \u03b8 in C. In addition, a simpler Float(\u00b110%) accuracy is reported, counting predictions correct if they fall within 10% relative error.\n\nUsing this per-type evaluation, monolithic VLMs are shown to be comparatively strong on perception-heavy discrete questions (e.g., yes/no) but to fail on precise multi-step geometric reasoning and arithmetic needed for numeric 3D measurements (very low float-calculation accuracy). A compositional visual-programming system like TVP mitigates this by decomposing the problem into tool calls (e.g., localization, depth, 2D-to-3D conversion) and explicit computations, yielding higher overall performance and better handling of the numeric/geometry-heavy portion of the benchmark.",
      "source_document": "papers/2512.20934v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For a closed-loop visual-programming agent that accumulates solved programs as examples and also abstracts recurring code into new reusable tools, how can you ablate the system to separate the benefit of (i) retrieving solved examples as in-context demonstrations from (ii) actually creating and using new abstracted tools\u2014and what kind of iteration- and difficulty-dependent performance pattern would demonstrate that tool abstraction adds value beyond example retrieval alone?",
      "answer": "Use an \u201cexample-library-only\u201d variant that still retrieves similar past solved examples to provide few-shot demonstrations to the program generator, but disables the evolving Tool Library so the generator is restricted to the initial small set of basic predefined tools. Compare this against the full closed-loop system that performs active tool abstraction.\n\nThe indicative pattern is:\n- The example-library-only variant stays essentially flat across iterations (about 31.7%\u219231.5%\u219231.5% overall), showing the in-context demonstrations alone don\u2019t yield progressive self-improvement.\n- The full system improves across iterations (about 31.3%\u219231.9%\u219233.3% overall), consistent with a program\u2192tool\u2192program feedback loop.\n- The benefit of tool abstraction is largest on the hardest questions: on the hard bucket the full system starts worse than example-only (\u22124.5% in iteration 1) but then improves enough to surpass it by iteration 3 (+6.7%), indicating that learned abstractions reduce complicated multi-step logic into simpler validated calls and help most when reasoning is complex.",
      "source_document": "papers/2512.20934v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a closed-loop visual-programming system that learns reusable tools from solved problems on-the-fly, what is the practical impact of processing the question stream in an easy-to-hard curriculum versus a random order on (i) how quickly the system accumulates retrievable examples and forms abstraction-eligible clusters early on, and (ii) the long-run diversity/coverage of the learned tool library and final accuracy\u2014and what underlying mechanism explains why random ordering can ultimately be better despite slower initial growth?",
      "answer": "Curriculum (easy-to-hard) ordering front-loads similar, simpler questions, which makes near-neighbor retrieval hit more often and causes eligible clusters to form sooner. As a result, the system accumulates high-quality examples earlier and triggers earlier tool creation (because similar examples are available to cluster and abstract from). However, random ordering interleaves different complexities and patterns throughout processing, which broadens exploration of the solution space; even if early retrieval/cluster formation is slower, the experience distribution is more diverse, so the abstraction/maintenance cycle can discover a wider variety of reusable patterns. Consequently, random ordering can end up with a more diverse set of created tools (more total tools generated) and gradually surpass or match curriculum ordering in overall accuracy by the end, demonstrating resilience to operating without dataset-specific curricula/prior knowledge.",
      "source_document": "papers/2512.20934v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When swapping the backbone LLM used to generate visual programs from a proprietary model to smaller open-source code LLMs, what performance trend should you expect as model capacity increases, and what does the comparison between a strong open-source backbone and a prior visual-programming baseline with a more capable proprietary backbone imply about whether the overall system is model-agnostic or reliant on a specific LLM?",
      "answer": "Accuracy increases monotonically as the backbone program-generator LLM gets larger (e.g., moving from ~7B to ~14B to ~32B parameters yields progressively better overall accuracy). With a sufficiently strong open-source backbone (the 32B model), the system\u2019s performance becomes close to the proprietary-backbone variant and even surpasses the prior best visual-programming baseline despite that baseline using a more capable proprietary backbone. This indicates the approach is largely model-agnostic and does not depend on any proprietary LLM to achieve strong results; stronger backbones mainly scale performance up rather than being a prerequisite.",
      "source_document": "papers/2512.20934v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "How does the proposed MLLM-based MAC-Score evaluate amodal completion outputs\u2014what inputs are given to the evaluator, what are the two sub-scores it produces, and along which dimensions (including point ranges) does MAC-Consistency grade the completion while explicitly ignoring which low-level visual factors?",
      "answer": "MAC-Score uses an MLLM configured as an expert evaluator via structured prompting. The evaluator is given (1) the original image, (2) the final completed object/result, and (3) the target object name. It produces two complementary scores:\n\n1) MAC-Completeness: a binary judgment of whether the object is structurally complete (\"Complete\" if the object\u2019s natural full structure is present; \"Incomplete\" if it is truncated, missing parts, or distorted), returned as a structured JSON decision with an explanation.\n\n2) MAC-Consistency: a 0\u201310 score of the intrinsic structural/semantic coherence of the completed object relative to its visible parts, designed to decouple object quality from low-level rendering artifacts. It explicitly instructs the evaluator to ignore low-level factors such as background blending, lighting shifts, and absolute position, and instead scores three high-level dimensions: Structural Continuity (0\u20134 points), Semantic Consistency (0\u20134 points), and Object Realism (0\u20132 points).",
      "source_document": "papers/2512.20936v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a planning-first, multi-agent amodal completion pipeline, how does the closed-loop verification mechanism catch (i) incorrect target segmentation and (ii) missed residual occluders before image synthesis\u2014specifically, what image \u201cwhite-out\u201d preprocessing is used for residual detection, what three-step programmatic Chain-of-Thought protocol is enforced to decide whether a candidate is a true occluder, and how are any newly found occluders incorporated back into the final inpainting mask used for synthesis?",
      "answer": "The verification stage uses a Verification Agent to audit the preliminary plan before inpainting. First it performs target grounding: if the target object is not successfully segmented, the Segmentation Agent is immediately triggered to re-segment the target from the original query. For missed-occluder detection it applies a \u201cwhite-out\u201d preprocessing: all currently identified occluders are masked with pure white in the input image (while preserving the rest of the scene) to remove visual redundancy; the verification agent then runs on this processed image conditioned on the query. Its residual check is constrained by a programmatic CoT protocol with three steps: (1) Candidate identification\u2014scan the processed image and list potential front-of-target objects that are not yet masked; (2) Sequential filtering\u2014test each candidate against exclusion rules and explicitly rule out environmental noise (e.g., dust/snow), surface artifacts (e.g., shadows/reflections), and self-occlusions; (3) Justified verdict\u2014only candidates that survive filtering are declared valid missed occluders, with a logical justification. The agent outputs a set of residual occluders \u0394O; if \u0394O is non-empty, the Segmentation Agent is recalled to segment these specific regions, and the occluder masks are added when re-forming the inpainting mask Minpaint (the union of the dilated occluder masks plus any canvas expansion region), producing the verified mask passed to the Inpainting Agent for single-pass synthesis.",
      "source_document": "papers/2512.20936v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a planning-first amodal completion pipeline that must handle targets truncated by the image frame, how is the inpainting region constructed before synthesis\u2014specifically, what does the boundary-analysis module predict to expand the canvas, how are occluder masks post-processed to reduce seam artifacts, and how are these pieces combined into the final binary mask used by the inpainting generator?",
      "answer": "The planning stage first uses a boundary-analysis agent to predict a 4D expansion ratio vector e = [e_t, e_b, e_l, e_r] for the top/bottom/left/right margins, defining a canvas expansion region M_exp so the full extrapolated geometry can fit. It then post-processes each occluder mask by morphologically dilating it (with structuring element B) so the synthesized area slightly overlaps the occlusion boundary, reducing interface artifacts. The final inpainting mask M_inpaint is formed by taking the union of all dilated occluder masks with the expansion region: M_inpaint = (\u22c3_i (M_occ^(i) \u2295 B)) \u222a M_exp, and this binary M_inpaint is passed to the inpainting model as the region to be filled (with visible target pixels preserved separately in the conditioning image).",
      "source_document": "papers/2512.20936v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In ambiguity-aware amodal completion, how can a system generate multiple *semantically distinct* completions (instead of just seed-level texture variation) while preventing \u201coccluder leakage\u201d into the inferred object\u2014specifically, how are the hypotheses represented and ranked, what constraint is imposed on the text descriptions, and how is diversity quantified to show an advantage over random-seed sampling?",
      "answer": "It uses an MLLM \u201cHypothesis Generator\u201d (the description agent) to explicitly model the latent semantics of the invisible region by proposing a set of K hypotheses H = {(T_k, w_k)}: each T_k is a holistic semantic description of the *entire* target (e.g., pose/attributes) and each w_k is an estimated confidence with the weights normalized (sum to 1). To prevent occluder leakage, each T_k is constrained to **exclude any mention of the occluding objects/relations** (e.g., avoid phrasing like \u201cbehind the chair\u201d), so the inpainting model is not encouraged to copy occluder textures into the target geometry. For automated inference a single hypothesis T* is selected by choosing the one with the highest confidence weight w_k and is then used to condition the inpainting generator along with the verified mask and preprocessed visual context. Diversity is quantified by generating multiple variations per input and computing average pairwise distances: **pairwise LPIPS** as a visual-diversity measure and **pairwise CLIP-distance** as a semantic-diversity measure, showing higher diversity than a baseline that only changes random seeds.",
      "source_document": "papers/2512.20936v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a training-free, decoupled multi-agent amodal completion system (semantic planning separated from visual synthesis), how does swapping out the proprietary MLLM reasoning backbone for an open-source one affect completion quality, and what does this backbone-swap experiment imply about whether the gains come from the collaborative agent design versus dependence on a specific closed-source model? Also, how does changing the visual synthesis backbone (e.g., from an SDXL-based inpainting model to a stronger inpainting model) relate to the framework\u2019s claimed \u201cplug-and-play\u201d extensibility?",
      "answer": "Because the reasoning and synthesis components are modular, the MLLM used for semantic planning can be replaced without changing the overall pipeline. When the proprietary GPT-4o reasoning backbone is replaced by the open-source Qwen3-VL-32B, MAC-Completeness drops modestly (reported about 65.45% \u2192 60.00%) but the system still clearly outperforms non-reasoning baselines. This indicates the main performance gains are attributable to the collaborative multi-agent reasoning/verification methodology rather than relying on a particular closed-source MLLM. Similarly, swapping the visual synthesis backbone to a stronger inpainting model (reported as upgrading from SDXL to FLUX) yields additional performance improvements, supporting the claim that the framework is \u201cplug-and-play\u201d: it can incorporate better foundation vision models as they emerge without redesigning the agentic planning architecture.",
      "source_document": "papers/2512.20936v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a real-centric AI-generated image detector that learns a boundary around the real-image manifold instead of generator artifacts, how are the \u201cnear-real\u201d samples constructed during training, and what are the specific loss terms used to (1) fit a smooth envelope boundary and (2) keep that boundary stable under real-world degradations (describe what each term encourages and how they are combined into the final objective)?",
      "answer": "Near-real samples are constructed by self-reconstruction with a VAE and then applying feature/latent-space perturbations: a real image xr is encoded to a latent vector z = E(xr), then controlled Gaussian noise is injected into a randomly selected subset of latent dimensions using a random mask M, i.e., z\u2032 = z + M \u2299 \u03b4 with \u03b4 ~ N(0, \u03b5^2 I). The perturbed latent is decoded to xf = D(z\u2032), producing semantically consistent but slightly off-manifold (near-real) samples that surround the real distribution.\n\nTo fit a smooth envelope boundary (Envelope Estimator), the detector is trained to separate real features hr = \u03c6(xr) from near-real features hf = \u03c6(xf) using: (i) a binary cross-entropy loss Lbce that pushes real samples to high \u201creal\u201d confidence (inside the envelope) and pushes near-real samples toward the boundary, and (ii) a tangency regularization Ltan that uses the top principal components of real features to approximate the local tangent space; it penalizes the component of the displacement \u0394h = hf \u2212 hr that is orthogonal to this tangent space, i.e., it minimizes \u2225(I \u2212 P)\u0394h\u2225^2 (with P the projection onto the tangent subspace), encouraging a geometry-aligned, continuous boundary rather than an irregular/overfit surface.\n\nTo keep the boundary stable under degradations (Cross-Domain Consistency), an image x and its degraded version x\u2032 = Degrade(x) are processed with a frozen DINOv3 anchor network f(\u00b7) and a fine-tuned learner backbone \u03c6(\u00b7). The learner features are linearly mapped (via W) to \u02c6h and \u02c6h\u2032, while the anchor produces a = f(x) and a\u2032 = f(x\u2032). Two L2 consistency terms are applied: (i) anchor consistency Lanc = \u2225a \u2212 \u02c6h\u2225^2 to keep learner features aligned to the anchor space and prevent semantic drift, and (ii) residual consistency Lres = \u2225(a \u2212 \u02c6h) \u2212 (a\u2032 \u2212 \u02c6h\u2032)\u2225^2 to preserve the residual (and thus the envelope geometry) across domains.\n\nThe final objective combines these pieces as L = (Lbce + \u03bb1 Ltan) + (\u03bb2 Lanc + \u03bb3 Lres), with \u03bb1\u2013\u03bb3 weighting the contributions of tangency and the two consistency constraints.",
      "source_document": "papers/2512.20937v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "A real-centric AI-generated image detector may be trained only for real-vs-fake, but can still be useful for attributing which generator produced a fake. How can real-centric envelope learning yield generator-discriminative features (even without attribution labels), and what evaluation protocols can be used for (i) closed-set and (ii) open-set forgery source attribution using those features?",
      "answer": "Real-centric envelope learning anchors representation learning around the real-image distribution; generator outputs then appear as different \u201cdirections\u201d/variations away from the real manifold, so fakes from different generators form distinct clusters in the learned feature space, enabling attribution without explicit attribution supervision. For evaluation: (i) in the closed-set setting, a classification head is retrained for each method to predict the generator classes; (ii) in the open-set setting, attribution is performed using feature distance (nearest/most similar generator cluster in feature space) rather than a retrained classifier.",
      "source_document": "papers/2512.20937v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When designing a real-world benchmark to evaluate AI-generated image detectors under social-media style \u201cchain degradations,\u201d how can the dataset be structured to separately measure (a) clean/ideal generalization to new generators and (b) robustness to multi-step propagation noise, and what concrete categories of degradation operations should the propagation chain include? Also, what mix of generator sources and generation modes helps reflect rapid generator evolution in such a benchmark?",
      "answer": "A practical structure is to create two parallel settings:\n\n1) **No Degradation (ND):** include directly captured real images and cleanly generated synthetic images. This setting is used to test detector generalization on advanced generators and diverse real-image sources (an upper-bound/ideal condition).\n\n2) **Chain Degradations (CD):** take those same real and fake images and pass each through a **random multi-step chain** that simulates realistic cross-platform/device propagation and user edits, including **compression and re-encoding** plus post-processing.\n\nThe degradation chain should include two categories of operations:\n- **Propagation operations:** uploading/downloading via PC apps; uploading/downloading via mobile apps; uploading on PC and downloading on mobile; uploading on mobile and downloading on PC.\n- **Post-processing operations:** applying filters; inserting stickers; cropping or changing aspect ratio; taking screenshots.\n\nTo reflect generator evolution, fake images should combine **open-source and commercial** generators (e.g., open-source diffusion-style models and commercial systems), and cover two generation modes:\n- **Text-to-image:** render a large set of diverse prompts across multiple generators.\n- **Image-to-image:** use real references to perform style transfer and local editing to mimic practical post-editing behavior.",
      "source_document": "papers/2512.20937v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In robustness evaluations for AI-generated image detection, some alignment-based detectors can show relatively flat performance across diverse perturbations (e.g., compression, blur, noise) yet underperform on clean/ideal data. What training-property tradeoff can explain this pattern, and why would it simultaneously increase apparent robustness while hurting clean-data discrimination?",
      "answer": "A plausible explanation is that these alignment-based methods do not explicitly model (or condition on) image format/quality factors during training. As a result, they tend to lose (or suppress) discriminative cues that are only helpful in high-quality conditions. This makes their performance look steady across perturbations\u2014because the detector is less sensitive to quality changes\u2014but it also weakens clean-data performance since those high-fidelity cues are no longer leveraged for discrimination.",
      "source_document": "papers/2512.20937v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a training-free image editing setup for a text-to-image diffusion model, how can you steer a generated sample toward a target concept/style using the model\u2019s internal representation space\u2014specifically, how is the steered denoiser constructed from encoder/decoder components, how is the target direction computed from a set of reference samples, and what qualitative difference should you expect between generalized vs. memorized samples as you increase the steering strength?",
      "answer": "Steering is done by modifying the model\u2019s intermediate representation (the encoder output) and then decoding it back: the denoiser is written as a decoder applied to an encoder, and the steered denoiser replaces h(x_t,t,c) with h(x_t,t,c)+a\u00b7v, where h_\u03b8 is the encoder, g_\u03b8 is the decoder, a\u2208R controls steering strength, c is the original text prompt, and \u001b\u001bc\u0304\u001b\u001b is the desired concept/style prompt. The steering direction v is the average (mean) encoder representation over a reference set S of samples from the target concept/style, i.e., v = (1/|S|) \u03a3_{x\u0303\u2208S} h_\u03b8(x\u0303_{t\u0303}, t\u0303, c\u0304). As steering strength a increases, generalized samples (with balanced representations) change smoothly and monotonically, enabling fine-grained controllable edits; memorized samples (with spiky representations) respond in a brittle, threshold-like way with limited fine-grained control.",
      "source_document": "papers/2512.20963v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "If you want a prompt-free way to tell whether a diffusion model output is a memorized training example versus a novel/generalized sample, how can you build a detector purely from the model\u2019s internal representations\u2014(i) what intermediate statistic serves as the memorization signal and why, (ii) what inference pipeline is used to extract the representation (including the role of adding forward noise, choosing a timestep, and any pooling/aggregation), and (iii) how does this representation-based detector compare to prior output-space baselines when evaluated with standard detection metrics (e.g., AUROC / TPR at fixed FPR) across different dataset\u2013model pairs?",
      "answer": "(i) Use \u201cspikiness\u201d of the intermediate activation vector as the memorization cue: memorized samples produce sharp, localized (nearly one\u2011hot / high-variance) representations, while generalized samples yield more balanced activations. A simple proxy is the standard deviation (variance) of the intermediate feature vector; high Std \u21d2 memorization, low Std \u21d2 generalization (alternatives like \u21134/\u21132 ratio, entropy, or max\u2013min also work similarly).\n\n(ii) Given a generated image x0, add forward diffusion noise to obtain x_t at a chosen timestep t (default used is a mid-noise DDPM timestep t=50, corresponding to an equivalent noise level \u22480.17). Run the diffusion denoiser without any conditioning (condition = \u2205) and take an intermediate layer activation h from inside the network\u2019s encoder/decoder computation. For compactness, apply global max pooling (spatial for U-Net\u2013style EDM/Stable Diffusion; token-wise for DiT) before computing Std(h). Classify as memorized if Std(h) exceeds a threshold.\n\n(iii) When benchmarked on multiple dataset\u2013model pairs (LAION\u2013Stable Diffusion v1.4, ImageNet\u2013DiT, CIFAR10\u2013EDM) and reported with AUROC and TPR at 1% FPR plus runtime, the Std-based representation detector achieves the strongest overall detection performance while remaining prompt-free, and is also more efficient than prior baselines that rely on prompts, scores/norm comparisons, attention anomalies, or output-space geometry.",
      "source_document": "papers/2512.20963v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a denoising-autoencoder view of diffusion training, suppose your training set is imbalanced because some images are duplicated many times while the rest of the data are well-sampled from several nondegenerate modes. What weight/feature structure should you expect a two-layer ReLU DAE to learn at a local minimizer (i.e., how does it simultaneously memorize the duplicates and generalize on the remaining modes), and what empirical signature does this hybrid behavior produce when tested on a diffusion model trained on CIFAR-10 with a duplicated subset?",
      "answer": "For an imbalanced dataset modeled as a mixture of clusters where the first m clusters are rank-1 duplicates (each cluster is repeated copies of a single vector x\u2113) and the remaining clusters contain distinct samples from nondegenerate modes, a two-layer ReLU DAE trained with input noise \u03c3>0 and weight decay \u03bb\u22650 admits a hybrid local minimizer with block structure: the encoder/decoder weights satisfy W1\u2605=W2\u2605, whose first m columns are proportional to the duplicated examples (r1 x1,\u2026, rm xm), i.e., those columns \u201cstore\u201d the duplicated training samples as in the pure memorization regime, while the remaining weight blocks (W_{X_{m+1}},\u2026,W_{X_K}) implement the generalization solution on the well-sampled clusters as in the underparameterized/generalization regime (learning local data statistics / low-rank denoisers per mode). Empirically, when training a diffusion model on CIFAR-10 with a duplicated subset (EDM setting), this produces bimodal behavior: duplicates induce a memorization mode while non-duplicated data continue to support generalization, and the intermediate representation statistics split accordingly (memorized samples show spiky/high-Std representations, generalized samples more balanced/lower-Std), yielding a clearly separated distribution of representation standard deviations.",
      "source_document": "papers/2512.20963v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a denoising-autoencoder (DAE) view of diffusion training, consider a two-layer ReLU DAE trained at a fixed noise level on data from a well-separated K-component mixture of Gaussians (MoG). If the network is underparameterized so that its latent width is split as p = \\sum_k p_k with p \\ll n (total samples), what is the resulting block structure of a local minimizer for the encoder/decoder weights, how does each block relate to learning local data statistics (mean/covariance) via a rank-p_k approximation of the optimal Gaussian denoiser (Wiener filter), and how is the expected test (population) loss bounded in terms of the eigenvalues of S_k = \\mu_k\\mu_k^\\top + \\Sigma_k, the noise level, the per-mode sample counts n_k, and p_k?",
      "answer": "In the underparameterized regime, there exists a local minimizer whose encoder and decoder share the same weight matrix and are block-partitioned by mixture component:\n\n- **Weight / representation structure:**\n  \\(W_2^\\star = W_1^\\star = [W_{X_1}\\; W_{X_2}\\; \\cdots \\; W_{X_K}] =: W_{\\mathrm{gen}}\\), where each block \\(W_{X_k} \\in \\mathbb{R}^{d\\times p_k}\\) is aligned with the principal directions (principal components) of the empirical Gram matrix \\(X_k X_k^\\top\\) of mode \\(k\\). This means samples from the same Gaussian mode activate a shared subset (\u201cblock\u201d) of neurons, yielding **balanced, distributed** representations rather than one-hot/spiky codes.\n\n- **Connection to learning local data statistics / Wiener filter:**\n  Each block acts like a low-rank (rank-\\(p_k\\)) approximation of the optimal denoiser for a Gaussian \\(\\mathcal{N}(\\mu_k,\\Sigma_k)\\):\n  \\[\n  W_{X_k} W_{X_k}^\\top \\;\\to\\; \\big[(S_k - \\tfrac{\\lambda}{\\rho_k} I)(S_k + \\sigma I)^{-1}\\big]_{\\text{rank-}p_k\\;\\text{approx}},\n  \\]\n  where \\(S_k = \\mu_k\\mu_k^\\top + \\Sigma_k\\) and \\(\\rho_k\\) is the mixture weight of component \\(k\\). Thus, instead of storing individual training samples in the weights, the model compresses each cluster into its leading covariance/mean structure.\n\n- **Expected test-loss bound (generalization error):**\n  As \\(\\lambda\\to 0\\), the expected population loss is bounded by a sum over modes of an eigenvalue-dependent term plus a finite-sample term:\n  \\[\n  \\mathbb{E}_{X\\sim p_{\\mathrm{gt}}}\\big[L_X(W_2^\\star,W_1^\\star)\\big]\n  \\;\\lesssim\\;\n  \\sum_{k=1}^K \\rho_k\\Bigg[\n  \\sum_{j\\le p_k} \\mathrm{eig}_j(S_k)\\,\\frac{\\sigma^4}{(\\mathrm{eig}_j(S_k)+\\sigma^2)^2}\n  \\;+\n  \\sum_{j>p_k} \\mathrm{eig}_j(S_k)\n  \\;+\n  C_k\\,p_k\\,\\frac{\\sigma^2}{n_k}\n  \\Bigg],\n  \\]\n  where \\(\\mathrm{eig}_j(S_k)\\) denotes the \\(j\\)-th eigenvalue of \\(S_k\\) and \\(C_k>0\\) depends only on the noise level and spectral properties of \\(S_k\\) (not on ambient dimension \\(d\\)).",
      "source_document": "papers/2512.20963v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a submap-based neural LiDAR mapping system where each newly created submap reinitializes its implicit (dense-grid) features, what mechanism can be used to prevent discontinuities in overlapping regions, and how is the cross-submap distillation loss computed on overlapping voxels?",
      "answer": "Use an overlap-alignment (distillation) mechanism: when switching to a new submap, copy the set of voxels from the previous submap\u2019s sparse grid into the new sparse grid to form an overlapping voxel set O. For each overlapping voxel, retrieve the well-trained implicit features from the previous submap and the newly initialized features from the current submap at the voxel\u2019s eight corner vertices, and penalize their discrepancy with an L1 feature-matching loss summed over all overlapping voxels and all 8 vertices: L_align = \u03bb_align * \u03a3_{v_o\u2208O} \u03a3_{j=1..8} ||h_j^(t) \u2212 h_j^(t+1)||_1. This distillation encourages consistent vertex features across consecutive submaps, improving continuity and convergence during transitions.",
      "source_document": "papers/2512.20976v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a hybrid explicit\u2013implicit LiDAR mapping setup that trains an implicit dense grid using narrow-band samples around measured ranges, how is a sample point\u2019s signed distance predicted from vertex features (including how features are fetched/interpolated), how is that signed distance converted into an occupancy probability, and what combined loss is optimized (name the terms and what each enforces)?",
      "answer": "Signed distance prediction is vertex-driven: for each sampled point ps, the method locates its containing voxel cell in the explicit sparse grid, retrieves the implicit dense-grid hash features at the cell\u2019s eight corner vertices {h(vj)} (j=1\u20268), trilinearly interpolates them to obtain the point feature f(ps)=Interp({h(vj)}), and feeds f(ps) through a shallow shared MLP F\u03b8 to predict the SDF value \u015d(ps)=F\u03b8(f(ps)).\n\nTo train with an occupancy-style objective, both the ground-truth SDF s(ps) and the predicted SDF \u015d(ps) are mapped to occupancy probabilities via a scaled sigmoid: o(s)=1/(1+exp(s/\u03c3t)), where \u03c3t is a temperature controlling truncation softness (denoting os=o(s(ps)) and \u00f4s=o(\u015d(ps))).\n\nThe optimized loss is a weighted sum of (1) a binary cross-entropy loss over sampled points, Lbce=\u2212(1/|P|)\u2211ps\u2208P[os log \u00f4s + (1\u2212os)log(1\u2212\u00f4s)], encouraging predicted occupancy to match the occupancy implied by the SDF targets; and (2) an Eikonal regularizer, Leik=(1/|P|)\u2211ps\u2208P(\u2016\u2207\u015d(ps)\u20162\u22121)^2, enforcing the SDF to have unit-norm gradients. The final objective is L=\u03bbbce Lbce + \u03bbeik Leik.",
      "source_document": "papers/2512.20976v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a submap-based hybrid-grid LiDAR mapper where each submap has a fixed physical size, how can the system decide when to start a new submap, and how should the new submap\u2019s placement be quantized so that voxel boundaries stay aligned across consecutive submaps?",
      "answer": "A new submap is triggered using an \u201centry rate\u201d criterion: for each incoming scan, compute the entry rate r as the fraction (ratio) of scan points that fall inside the current submap\u2019s bounding box; if r drops below a threshold r_min, create a new submap of the same fixed size l=(l_x,l_y,l_z). The new submap is first centered at the current scan center c_{t+1}, then its center is quantized to the voxel grid of the previous submap using the voxel size s_v so voxel edges align: c'_{t+1} = c_t + round((c_{t+1}\u2212c_t)/s_v)\u00b7s_v. With this aligned center, the submap\u2019s minimum corner is b_min = c'_{t+1} \u2212 (1/2)l, which defines its voxel-aligned bounding box.",
      "source_document": "papers/2512.20976v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an incremental neural LiDAR mapper that optimizes each submap online, how can catastrophic forgetting be mitigated with a replay mechanism, and what criterion is used to select \u201ckey-scans\u201d and manage them so memory doesn\u2019t grow unbounded as the mapped area expands?",
      "answer": "Use key-scan replay: for each submap, maintain a queue/set of key-scans that are periodically replayed during optimization to provide collective constraints and reduce catastrophic forgetting. Key-scans are selected by checking the distance between the current frame and the previous key-scan and adding the frame when this distance exceeds a threshold D_min. When a new submap is created, replay the previous submap using its key-scan set; because large scenes can accumulate too many key-scans, release (discard) that submap\u2019s key-scan list after replay to keep memory efficient.",
      "source_document": "papers/2512.20976v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In incremental neural LiDAR mapping that relies on a sparse voxel structure for ray-guided sampling, how can moving-object outliers be filtered without stepping along entire LiDAR rays, and after this filtering, what coordinate transforms and sparse-grid update are applied to the remaining scan points before training?",
      "answer": "Use a FreeDOM-style dynamic removal strategy that labels scan points directly as dynamic vs. static by starting from scan points as seeds and growing regions using a combination of raycasting checks and neighborhood expansion, rather than performing space-curving by marching along full rays. Given an input scan S={pi}, first transform points into the world frame with the pose Tw: Sw={Tw pi}. Then map them into the current submap\u2019s local coordinates by subtracting the submap\u2019s minimum corner bmin: Sloc=Sw\u2212bmin. The dynamic removal preprocessing outputs a static subset Sstatic\u2286Sloc, and only these static points are used to update the submap\u2019s sparse grid: \u03a9s\u2190Update(\u03a9s, Sstatic), after which narrow-band sampling/training proceeds.",
      "source_document": "papers/2512.20976v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In long-tailed multi-label chest X-ray classification with diffusion-based inpainting augmentation, how can a progressive incremental learning schedule mix the original training set with newly inpainted samples over training epochs to mitigate domain-gap-induced catastrophic forgetting, and what is the specific mixing rule used?",
      "answer": "Use a Progressive Incremental Learning (PIL) strategy that starts training with a low proportion of inpainted images and then gradually increases their contribution as epochs progress. Let the original dataset be Do={I1,\u2026,IN} and the inpainted dataset be Di={I\u20321,\u2026,I\u2032N}. The combined training set is formed as D = Do + Di(1 \u2212 e^{\u2212\u03b2 n}), where n is the current training epoch and \u03b2 controls how quickly the inpainted data is introduced. This progressive increase reduces the impact of the domain gap from generated images and helps retain (and sometimes improve) head-class performance instead of forgetting it when many tail-augmented samples are added at once.",
      "source_document": "papers/2512.20980v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a feed-forward multi-view inverse rendering system trained mostly on synthetic data, what self-supervised fine-tuning objective can be used on unlabeled real-world videos to reduce temporal flicker, and what additional constraint is needed to avoid degenerate \u201ccollapsed\u201d solutions during this fine-tuning?",
      "answer": "Use a temporal consistency objective built from optical-flow warping between adjacent frames: predict material maps for frames t and t+1, warp the t+1 prediction back to frame t using dense flow Ft+1\u2192t, and penalize the L2 difference between the warped map and the frame\u2011t prediction (a consistency loss). To prevent collapse to trivially smooth but meaningless outputs, add an anchor loss on a reference frame (frame 0) that forces the fine-tuned model\u2019s prediction at that frame to match a pretrained model\u2019s prediction, balancing anchor and consistency terms (with \u03bb_anchor set to 0.1 in practice).",
      "source_document": "papers/2512.21003v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In learning-based inverse rendering, albedo supervision often suffers from an overall scale ambiguity (the same scene can be explained by scaling albedo and compensating with shading). How can a scale-invariant albedo reconstruction loss be constructed in practice, and what simple training schedule can stabilize this loss early in training?",
      "answer": "Use a scale-invariant MSE by first solving for a per-channel scale vector s* that best aligns the predicted albedo A to the (pseudo) ground-truth albedo A* in least-squares sense: s* = arg min_{s\u2208R^3} ||A \u2299 s \u2212 A*||_2^2 (\u2299 is channel-wise multiplication). The albedo loss is then computed as L = (1/N)||A \u2299 s* \u2212 A*||_2^2. Because this scale-invariant formulation can be unstable at the beginning of training, warm up the network for several epochs using a vanilla (non-scale-invariant) MSE loss, then switch to the scale-invariant loss afterward.",
      "source_document": "papers/2512.21003v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a multi-view inverse-rendering network\u2019s *cross-view material consistency* (e.g., for albedo/roughness/metallicity), how can you turn per-view 2D predictions into a quantitative consistency score across viewpoints\u2014what geometric information is required, how are the predicted maps transferred between views, and how is the final error computed over valid regions?",
      "answer": "A practical consistency metric is to geometrically reproject predicted intrinsic maps between views and compare them in image space. Using known depth maps and camera poses, take a predicted material map from one view, back-project its pixels into 3D world coordinates, then reproject those 3D points into another view to obtain a warped/reprojected version of the first view\u2019s prediction on the second view\u2019s image plane. Finally, compute an error (RMSE is used) between this reprojected map and the material map predicted directly for the target view, restricting the computation to pixels/regions where the reprojection lands in overlapping valid areas. Repeating this across multiple view pairs yields an overall cross-view consistency RMSE per material channel (albedo, roughness, metallic).",
      "source_document": "papers/2512.21003v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a feed-forward multi-view inverse rendering transformer that uses an alternating-attention backbone, what are the two attention operations it alternates between, and how does the cross-view (global) attention encourage multi-view consistency without requiring camera pose supervision?",
      "answer": "The backbone alternates (1) frame-wise self-attention, which performs attention within a single image to refine tokens by capturing long-range spatial dependencies and local semantic structure, and (2) global self-attention, which performs attention across all input images so tokens from different views can mutually reference and reinforce each other. The global attention implicitly associates tokens that observe the same underlying 3D surface region, encouraging view-consistent features/material predictions without needing camera pose supervision.",
      "source_document": "papers/2512.21003v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a feed-forward inverse-rendering network that uses DPT-style dense prediction heads for albedo/BRDF maps, what failure mode typically appears if you use the DPT head alone, and what architectural add-on and integration strategy can be used to recover high-frequency texture details while keeping the overall head design intact?",
      "answer": "Using the DPT head alone tends to oversmooth the predicted material maps\u2014especially albedo\u2014producing blurry results that lose high-frequency texture patterns needed for accurate material recovery. A practical fix is to add an auxiliary multi-resolution convolutional encoder that extracts local high-frequency structures directly from the input image, then fuse these fine-grained features into the DPT-style prediction heads via feature fusion with skip connections. This injects local detail cues into the heads, preserving texture sharpness and improving spatial fidelity without changing the backbone/overall architecture.",
      "source_document": "papers/2512.21003v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a masked next-frame autoregressive video pretraining setup that uses a conditioned flow-matching decoder, how can you decouple semantic representation learning from the target-decoding process: specifically, what is the purpose of adding an MSE alignment term between the autoregressive predictor\u2019s next-frame representation and an EMA-updated reference encoder output (with stop-gradient), and how is this alignment combined with the flow-matching (velocity-regression) loss used to generate VAE-latent targets?",
      "answer": "Decoupling is achieved by splitting the model into (1) an encoder that produces frame-wise causal context features from masked multi-frame inputs, (2) an autoregressive predictor that uses causal cross-attention (Q-former style) to predict an implicit next-frame representation z_t from previous-frame context while keeping the context tokens fixed as keys/values, and (3) a conditioned flow-matching decoder that treats z_t purely as conditioning and generates the next frame\u2019s target in a separate domain (VAE latent space) via denoising/flow matching.\n\nThe MSE alignment regularizer is added to prevent the predicted representation z_t from becoming a mere substitute that implicitly reintroduces the historical context into decoding. A separate reference encoder that sees the full unmasked sequence produces a reference representation c\u2019_t for frame f_t; this reference encoder is updated by EMA for stability, and its output is stop-gradiented so it serves as a fixed target. The alignment loss is Lt_align = MSE(z_t, sg[c\u2019_t]), encouraging z_t to match the semantic reference representation while keeping the reference branch from being optimized to chase z_t.\n\nGeneration uses conditioned flow matching on VAE latents: the target h1 is the VAE-encoded latent of f_t, h0 is Gaussian noise, and the decoder is trained to regress the velocity field along a linear noise\u2013data interpolation path, conditioned on z_t. The per-time-step flow loss Lt_flow regresses g_\u03b8(\u03b3(h0,h1;\u03c4), z_t, \u03c4) toward (h1\u2212h0). The overall pretraining objective sums (Lt_flow + \u03b2 Lt_align) over t, where \u03b2 controls the strength of alignment regularization, so z_t is trained to be a semantically meaningful prediction while the decoder focuses on conditional latent generation without mixing z_t into its hidden-state transformations.",
      "source_document": "papers/2512.21004v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an autoregressive masked next-frame video pretraining setup that uses an encoder, a cross-attention autoregressive predictor, and a denoising-style flow-matching decoder, how should the attention masking be designed across these three modules to (i) enforce temporal causality, and (ii) prevent information leakage during latent-frame generation\u2014and what observed effect does allowing the noisy latent targets to self-attend (cross-self attention) have on downstream representation quality compared to restricting them to attend only to the conditioning representation (pure cross-attention)?",
      "answer": "Use three complementary masks: (1) a frame-wise causal mask in the encoder so each patch token can only attend within its own frame and earlier frames, yielding causal context features; (2) an autoregressive mask in the predictor\u2019s cross-attention blocks so the learnable query/mask tokens for frame t can attend only to the historical context tokens (as key/value) from frames up to t\u22121, enforcing next-frame causality while keeping the context isolated from being transformed; and (3) a frame-isolated mask in the flow-matching decoder so attention is full within each frame but blocked across frames, enabling per-frame latent generation without leaking information from other frames. Empirically, if the decoder uses cross-self attention so the noisy latent targets can self-attend, generation becomes easier and information can leak between target tokens; this reduces the difficulty of the objective and degrades representation quality versus a pure cross-attention design where the noisy targets attend only to the conditioning representation.",
      "source_document": "papers/2512.21004v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a masked next-frame autoregressive video pretraining framework that uses a conditioned flow-matching (DiT-style) decoder, what choice of generation target representation leads to the best downstream probe accuracy, how are these targets aligned with the predicted condition in space\u2013time, and why can video-VAE latents underperform compared to image-VAE latents or even direct pixel targets?",
      "answer": "The strongest downstream results come from generating frame-wise latent features produced by an image VAE (VAVAE). The VAE latents are reorganized into a target latent sequence with stacked channels so that each target token is spatially and temporally aligned with the corresponding predicted condition token (both represent the same spatiotemporal \u201ccube\u201d of the original video). Empirically, direct pixel targets are also competitive and SigLIP2 feature targets are similar, but Cosmos video-VAE latents perform substantially worse; a key reason is that the video VAE reconstructions are less precise, which degrades the supervision signal for representation learning.",
      "source_document": "papers/2512.21004v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For autoregressive masked next-frame pretraining that couples a large video encoder with a comparatively small conditioned flow-matching (DiT-style) decoder, what multi-stage optimization schedule can keep training stable, and what specific changes are made to (i) learning-rate control for the decoder versus the rest of the model, (ii) the flow-matching timestep (\u03c4) sampling procedure, and (iii) the number of input frames\u2014and what is the intended benefit of the final \u201ccool-down\u201d stage for downstream video semantics?",
      "answer": "A stable recipe is a four-stage schedule: (1) a warm-up stage where the learning rate is ramped up from small values so the model first learns basic patterns and stabilizes representations while the flow-matching decoder converges quickly; (2) a first stable stage where the learning rate is gradually decayed from its peak while the model \u201csearches\u201d for good representations and the autoregressive predictor progressively aligns to reference representations; (3) a second stable stage where the overall learning rate is reduced further but the flow-matching decoder is given its own separate fixed (kept large) learning rate, and the \u03c4-sampling rate is changed from 4 to 1 (single-step \u03c4 sampling), which improves robustness and yields stable updates during this non-stationary period; and (4) a cool-down stage where the model is fine-tuned with a smaller learning rate and the input clip length is increased to 64 frames, intended to consolidate semantic representations and improve understanding of longer videos, leading to markedly stronger video semantic understanding downstream.",
      "source_document": "papers/2512.21004v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In masked next-frame autoregressive video pretraining with a ViT encoder that processes multiple frames at once, what masking strategy can reduce the \u201ccomplementary effect\u201d from highly correlated patches across adjacent frames, and how is it applied during pretraining?",
      "answer": "Use a temporally consistent masking scheme: for a given video clip, randomly select spatial patch locations to mask and then mask those same spatial locations simultaneously across multiple frames in the clip. Because patches at identical spatial positions across frames are highly correlated, masking them together prevents the model from trivially filling in missing content from redundant information in other frames. In practice, multiple mask strategies are applied simultaneously during pretraining on these multi-frame inputs.",
      "source_document": "papers/2512.21004v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "How does Granular-ball Guided Masking (GBGM) generate a structure-aware binary mask in its hierarchical coarse-to-fine pipeline\u2014specifically, what \u201cpurity\u201d score is computed on blocks, how are blocks selected/refined across the first two stages, and how does the final importance map plus stochastic thresholding decide which regions are kept vs masked?",
      "answer": "GBGM builds the mask in three stages:\n\n1) Coarse masking: The input image X (H\u00d7W) is partitioned into S1\u00d7S1 blocks Pi,j. Each block is scored by a local purity value defined as the average absolute deviation of a small central patch C (e.g., 2\u00d72 pixels) from the block mean intensity \u03bc(Pi,j):\nPurity(Pi,j) = (1/|C|) * \u03a3_(u,v\u2208C) |Pi,j(u,v) \u2212 \u03bc(Pi,j)|.\nThe top\u2011k1 blocks with the highest purity (equivalently, those above a threshold \u03c41 given by the k1-th largest purity) are marked as retained in a coarse binary mask M1.\n\n2) Finer masking: Every block rejected by M1 is subdivided into 2\u00d72 sub-blocks Qi\u2032,j\u2032 (so S2 = S1/2 and the fine grid has 4\u00d7 as many blocks). Purity is computed on these fine blocks the same way, and the top\u2011k2 purest fine blocks (above threshold \u03c42) form a finer binary mask M2. This recovers smaller structures (e.g., edges/small objects) that may have been missed at the coarse scale.\n\n3) Importance refinement + stochastic binarization: To incorporate spatial context, M2 is convolved with a 3\u00d73 all-ones kernel to produce an importance map I = Conv3\u00d73(M2), where higher values indicate blocks in heterogeneous/boundary regions. I is normalized to \u0128 in [0,1] using (I\u2212min(I))/(max(I)\u2212min(I)+\u03b5). A random matrix R is sampled elementwise from U(\u03b5, 1\u2212\u03b5), and the final low-res binary mask is computed as M_final(lowres)(i)=1 if \u0128(i) < R(i), else 0, then upsampled to H\u00d7W.\nIn the resulting mask, 0 corresponds to high-purity (typically homogeneous/background) blocks that are masked out, while 1 corresponds to low-purity, target-relevant blocks that are retained, with randomness preventing deterministic, rigid patterns.",
      "source_document": "papers/2512.21011v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a one-shot text-driven video editing system that adapts a pretrained Stable Diffusion U-Net, how can a parameter-efficient \u201cbypass attention\u201d module replace the original query/key projections in (sparse causal) temporal attention while keeping the attention-map dimensionality unchanged, and which parameters remain trainable versus frozen during fine-tuning (including how the bypass attention output is combined with the original attention)?",
      "answer": "A bypass attention module introduces low-rank substitutes for the original query/key projection weights in the temporal (causal) attention. Concretely, it replaces the full-rank projection matrices WQ, WK \u2208 R^{d\u00d7d} with low-rank matrices W\u2032Q, W\u2032K \u2208 R^{d\u00d7k} (k < d), and computes a new attention map\n\nA\u2032\u03d5(K,Q) = Q W\u2032Q W\u2032T_K K^T,\n\ndesigned to have the same dimensionality as the original attention map A\u03d5 = Q WQ W^T_K K K^T. During fine-tuning, only the low-rank matrices W\u2032Q and W\u2032K are updated; all other pretrained diffusion-model parameters are kept frozen. The module then forms the effective attention used by the network as a weighted combination of the bypass and original maps:\n\nA_full\u03d5(K,Q) = (1 \u2212 \u03c6) \u00b7 A\u2032\u03d5(K,Q) + \u03c6 \u00b7 A\u03d5(K,Q),\n\nso the model can retain the pretrained attention behavior initially (via initialization that keeps A\u2032 close to A) while learning task-specific edits with far fewer trainable parameters.",
      "source_document": "papers/2512.21015v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When replacing transformer-style temporal self-attention with an SSM-based Mamba block for video editing, what design changes make the Mamba module explicitly \u201ctemporal-aware\u201d (so it can distinguish frames), and how are the multiple scan-direction outputs transformed and fused back into a single feature sequence for each frame?",
      "answer": "The Mamba block is made temporal-aware by (1) padding each frame with a learnable, frame-specific embedding so different timesteps become distinguishable to a model that is otherwise biased toward local/neighboring tokens, and (2) using four directional scans implemented via flip operations to cover spatial forward/reverse and temporal forward/reverse orders. Concretely, each frame feature map x_t is padded with a learnable embedding \u03b8_frame to form a slightly larger map x\u2032_t; then three flip operations (plus the unflipped input) produce four variants that are passed through the same Conv \u2192 activation \u2192 SSM pipeline to produce z_t^(i). The outputs from the flipped scans are inverse-flipped to restore the original temporal/spatial ordering (so they align with the unflipped scan), and then summed with the restored unflipped output to yield a single fused feature z_{t,final} for that frame/time index.",
      "source_document": "papers/2512.21015v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In text-driven video editing where pixel-level ground truth for the edited video is unavailable, what evaluation protocol can be used to jointly measure (i) prompt faithfulness and (ii) temporal/frame consistency\u2014specifically, how are CLIP-based \u201ctext\u201d and \u201cframe\u201d scores and a preference-based PickScore computed, and how can a complementary user study be set up to assess editing fidelity vs temporal consistency?",
      "answer": "A practical protocol is to use the LOVEU-TGVE metrics plus a user study: \n\n\u2022 CLIP Score (text): compute the average cosine similarity between each generated video frame\u2019s embedding and the text prompt embedding using a pretrained CLIP model (ViT\u2011L/14), then average over frames to measure text\u2013video alignment.\n\n\u2022 CLIP Score (frame): compute average cosine similarity between CLIP embeddings of different frames within the same generated video (excluding self-similarity) to quantify internal frame/temporal consistency.\n\n\u2022 PickScore: use a CLIP-like model trained on human preference data to score how well the generated frames align with the given prompt from a human-preference perspective.\n\n\u2022 User study: ask human evaluators to choose (a) which video has higher editing fidelity to the target prompt and (b) which has better temporal consistency; have many users (e.g., 150) each evaluate a fixed number of randomly selected videos (e.g., 30), and report the final score as the percentage of users preferring each method for each criterion.",
      "source_document": "papers/2512.21015v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When crafting adversarial perturbations to defend a portrait reference video against 3D neural-field talking-face generation, what is the full optimization objective used during PGD, and how do its components jointly enforce (i) identity/face information disruption and (ii) high perceptual fidelity of the protected frames across scale changes?",
      "answer": "The perturbations are optimized with a PGD procedure on a combined adversarial objective\n\n\u2022 Overall objective:  L_vdf_adv(x,y) = L_sem(x,y) \u2212 L_mseg(x).\n\n(i) Disrupting facial/identity information (defense effectiveness + scale robustness):\n\u2022 L_mseg is a multiscale segmentation-based loss computed over a masked region of interest M and a set of \u201csuppressed\u201d target classes C_tgt.\n\u2022 For each scale s in S = {2.0, 1.5, 1.0, 0.75, 0.5}, the perturbed image (x+\u03b4) is resized by R_s(\u00b7), passed through a segmentation model f_\u03b8, and the loss penalizes high predicted probability for the target facial classes inside M using a log(1 \u2212 \u03a3_{c\u2208C_tgt} P_\u03b8(c|R_s(x+\u03b4))_{i,j} + \u03b5) term averaged over pixels (i,j) in M.\n\u2022 Summing over scales makes the learned perturbation remain effective under resizing and other scale changes.\n\n(ii) Preserving perceptual fidelity (video quality/temporal coherence):\n\u2022 L_sem is a VGG16 feature-space alignment loss between the defended frame and the original frame: it sums L2 distances between activation maps \u03d5_l(\u00b7) at layers L = {3, 8, 15, 22}, normalized by N_l and weighted by w_l = [1.0, 0.75, 0.5, 0.25].\n\u2022 This constrains the perturbation so visual content stays close to the original even while the segmentation-based term pushes the face region toward misclassification.\n\nPGD then iteratively updates the input by taking the sign of the gradient of L_vdf_adv and projecting back into an L_\u221e-bounded perturbation set.",
      "source_document": "papers/2512.21019v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When defending a portrait reference video against 3D-field personalized talking-face generation, how can temporal redundancy between consecutive frames be leveraged to speed up adversarial perturbation optimization, and how is an inter-frame similarity score used to initialize the perturbation for the next frame?",
      "answer": "Use a similarity-guided parameter sharing strategy: instead of optimizing each frame\u2019s perturbation from scratch, initialize the perturbation for frame t+1 by inheriting the optimized perturbation from frame t, scaled by the similarity between the two frames, and add a small random term. Concretely, the initialization is\n\n\u03b4_{t+1}^{init} = SSIM(x_{t+1}, x_t) \u00b7 \u03b4_t + \u03b4_random,\n\nso highly similar adjacent frames start from nearly the same perturbation, reducing optimization effort while maintaining visual quality.",
      "source_document": "papers/2512.21019v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a video-level adversarial defense that perturbs the *reference-video processing* of 3D-field talking-face generators, how can a dual-domain (frequency\u2192spatial) perturbation module be constructed to (i) inject perturbations in the Fourier domain but (ii) suppress visually redundant global artifacts in the spatial domain, and how does the accompanying *multiscale* segmentation-based loss explicitly enforce robustness to resizing by aggregating objectives across multiple scale factors and a facial ROI mask?",
      "answer": "A dual-domain module can add the learnable perturbation directly in the frequency domain and then convert it back to the image while modulating the resulting spatial-domain noise with a learned spatial attention map to suppress redundant artifacts:\n\n- Frequency\u2192spatial reconstruction with spatial attention:\n  x\u2032 = F^{-1}(F(x) + \u03b4) \u2299 A_spatial + x,\n  where F and F^{-1} are the Fourier / inverse Fourier transforms, \u03b4 is the perturbation in the frequency domain, \u2299 is element-wise multiplication, and A_spatial adaptively suppresses redundant spatial noise (important because frequency-domain changes induce global spatial perturbations).\n\nTo make the defense robust to common video-transmission scaling operations, the optimization uses a multiscale objective that sums a segmentation-driven misclassification loss over several resized versions of the perturbed frame:\n\n- Multiscale segmentation loss:\n  L_m^seg(x) = \\sum_{s\u2208S} E_{(x,y)\u223cD}\\Big[ \\frac{1}{|M|}\\sum_{(i,j)\u2208M} \\log\\big(1 \u2212 \\sum_{c\u2208C_tgt} P_\u03b8(c\\mid R_s(x+\u03b4))_{i,j} + \u03f5\\big) \\Big],\n  where S = {2.0, 1.5, 1.0, 0.75, 0.5} are the scale factors, R_s(\u00b7) resizes the perturbed image x+\u03b4 to scale s, P_\u03b8(\u00b7) is the segmentation model\u2019s per-pixel class probability, C_tgt is the set of target (to-be-suppressed) facial classes, M is a binary mask defining the facial region of interest, and \u03f5 is for numerical stability.\n\nThis loss penalizes high target-class probability inside the masked region at multiple scales, encouraging misclassification in the face ROI in a way that generalizes across resizing/compression-induced resolution changes.",
      "source_document": "papers/2512.21019v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When testing a portrait-video protection method against common \u201cpurification\u201d defenses, what experimental protocol is used to measure robustness, and what design reasons explain why a frequency-domain + multi-scale video defense can outperform prior methods under JPEG/resize purification but be slightly less effective under diffusion-based purification (DiffPure/FreqPure)?",
      "answer": "Robustness is evaluated by applying purification operations to the protected reference videos (JPEG compression, resizing, DiffPure, and FreqPure) and then training and testing the 3D-field talking-face generator (SyncTalk) on these purified videos to see whether privacy protection still holds. The frequency-domain perturbations combined with multi-scale optimization are difficult to remove with JPEG and resizing, so the defense performs best under those purifications. It can be slightly worse under DiffPure/FreqPure because some baseline methods inject much stronger facial perturbations that diffusion-based purifiers cannot fully eliminate, whereas the proposed method uses smaller/less aggressive perturbations to preserve visual quality, creating a trade-off.",
      "source_document": "papers/2512.21019v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building an iterative adversarial video-protection method against 3D-field personalized talking-face generators, what practical trade-off arises between optimizing perturbations directly in the spatial domain versus the frequency domain, and how do (i) restricting the perturbation with a facial/ROI spatial mask, (ii) adding a VGG-feature (semantic/perceptual) alignment loss, and (iii) inheriting the previous frame\u2019s optimized noise change the efficiency and visual quality of the defended video?",
      "answer": "Spatial-domain perturbations tend to preserve protected-frame image quality better, but they are much more computationally expensive to optimize. Frequency-domain perturbations are markedly more efficient (reported as about a threefold efficiency gain over spatial noise), but they can introduce more visible distortion unless additional constraints are used. Adding a spatial ROI mask further improves protection efficiency while only slightly affecting visual quality. Adding a VGG-based semantic/perceptual loss helps maintain visual quality by limiting the distortion introduced by frequency-domain perturbations. Finally, removing inter-frame noise inheritance makes optimization prohibitively slow (e.g., hundreds of minutes even on a small subset), whereas inheriting the previous frame\u2019s optimized perturbation significantly accelerates adversarial-example generation, enabling protection of full-length videos in practice.",
      "source_document": "papers/2512.21019v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a thermal-to-visible latent diffusion face translation system that tries to preserve identity-related attributes (e.g., skin tone, age, gender), how can a multi-attribute classifier be trained using paired visible/thermal images so that its predictions are consistent across modalities, and how are the resulting attribute predictions then used to condition the diffusion model during generation?",
      "answer": "Train the attribute classifier in two stages: (1) pre-train a classifier on visible RGB faces (using datasets such as FairFace and UTKFace) so it can predict attributes like ethnicity/skin tone, gender, and age; then freeze this visible-domain network. (2) Create a trainable infrared/thermal copy with the same architecture and train it on paired (x_vis, x_IR) samples by enforcing prediction consistency with the frozen visible network, minimizing an MSE consistency loss \\(\\|f_{vis}(x_{vis}) - f_{IR}(x_{IR})\\|_2^2\\) over the paired dataset. After training, the infrared classifier can infer those attributes from thermal inputs. At inference/training time for T2V translation, convert the predicted attribute labels into text-like prompts and encode them with a (fine-tuned/customized) CLIP text encoder; the resulting prompt embeddings are used as conditioning inputs (via cross-attention) to the latent diffusion model\u2019s denoising UNet to guide generation toward the predicted attributes and reduce attribute/identity feature loss.",
      "source_document": "papers/2512.21032v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a latent-diffusion thermal-to-visible face translation pipeline, how can identity preservation be enforced during training using an identity verification module\u2014specifically, what embeddings are compared (which images are used), and what form does the identity loss take?",
      "answer": "Identity preservation is enforced by adding an identity verification module that uses a pre-trained face-recognition network F to extract identity embeddings from (i) the synthesized visible output \\(\\hat I_v\\) and (ii) a reference visible image \\(I_{ref}\\) of the same subject. The identity loss is defined as a cosine-distance objective: \\(L_{ID} = 1 - \\cos\\big(F(\\hat I_v),\\, F(I_{ref})\\big)\\), and it is included in training to encourage the generated image to match the subject\u2019s identity-specific features (e.g., facial structure and key features).",
      "source_document": "papers/2512.21032v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When adapting a latent-diffusion UNet for thermal-to-visible face translation, how can the standard multi-head self-attention blocks be replaced to keep global cross-modal feature modeling while accelerating inference\u2014what is the replacement module\u2019s core modeling idea (e.g., its state-space/bidirectional design and complexity behavior), and what *order-of-magnitude* reductions in model size/memory and per-image inference latency are achieved at 1000 diffusion steps compared with attention-based DDPM/LDM baselines?",
      "answer": "Replace the UNet\u2019s multi-head self-attention (MHSA) with the proposed Self-attn Mamba block, which uses a Selective State Space Model (SSM) (with lightweight convolutions) to model both local and global dependencies with *linear* complexity in sequence length. It can be extended with bidirectional Mamba (forward + backward passes) and a residual connection to improve global modeling while keeping computation low. Empirically at T=1000 steps, swapping attention for Self-attn Mamba substantially cuts parameter count and memory footprint and speeds up inference: compared with an attention-based T2V-DDPM baseline it reduces parameters by roughly three quarters and brings single-image inference down to a few\u00d710 ms (about 36 ms in the reported setup), while also reducing memory usage markedly; similar reductions are reported for an LDM baseline when replacing attention with Self-attn Mamba.",
      "source_document": "papers/2512.21032v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a thermal-to-visible latent diffusion pipeline that uses attribute prompting for identity-preserving translation, how can skin-tone information be represented and injected as a conditioning signal\u2014specifically, how many fine-grained skin-tone labels are defined, what external standard is used to define them, and what datasets are used to fine-tune the CLIP-based encoder that turns the predicted attributes (age/gender/skin tone) into prompt embeddings for cross-attention conditioning?",
      "answer": "Skin tone is discretized into 19 fine-grained labels defined using the PANTONE SkinTone Guide standard. A CLIP-based encoder is customized/fine-tuned using UTKFace and FairFace so that the classifier\u2019s predicted attributes (gender, age, and the 19-way skin-tone label) can be converted into prompt embeddings, which are then used as conditioning inputs (via cross-attention) to guide the latent diffusion denoising process.",
      "source_document": "papers/2512.21032v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a thermal-to-visible latent diffusion face translation system that proposes (1) swapping self-attention for a state-space \u201cSelf-attn Mamba\u201d block, (2) conditioning generation on a multi-attribute classifier (gender/age/skin tone), and (3) feeding those classifier outputs through a CLIP encoder for semantically richer conditioning, what set of ablation variants would you run against an LDM baseline to isolate each component\u2019s contribution\u2014and what overall trend in image-quality and identity-verification metrics should these variants demonstrate as you progressively add classifier conditioning, CLIP embeddings, and Mamba together?",
      "answer": "A clear ablation is to start from a baseline LDM conditioned on the thermal input (diffusion/denoising in VQ-VAE latent space, using a visible-image VQ-VAE encoder and a conditional thermal encoder), then compare: (A) baseline + Self-attn Mamba to test whether replacing the original attention improves global feature modeling and image quality at lower cost; (B) baseline + multi-attribute classifier to test whether adding explicit attribute-based conditioning (skin tone, gender, age) improves reconstruction fidelity; (C) baseline + classifier + CLIP encoder to test whether mapping classifier outputs into CLIP\u2019s semantic embedding space further strengthens conditional generation and thermal\u2013visible alignment; and (D) baseline + Mamba + classifier + CLIP to test complementarity when all components are combined. The expected metric trend is monotonic improvement as components are added: Mamba improves image quality over the baseline; classifier conditioning further improves perceptual/fidelity metrics (lower FID/LPIPS and higher PSNR/SSIM); adding CLIP improves these further by better semantic alignment; and the full combination yields the best overall results, including the strongest identity preservation (higher Rank-1 and VR@FAR) in addition to the best image-quality metrics.",
      "source_document": "papers/2512.21032v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a cross-scale self-supervised denoising setup that trains a blind-spot network to map a noise-decorrelated low-resolution input to a higher-resolution target from the same noisy image, how can the training pairs be constructed so that (i) spatially correlated noise is blocked from leaking across scales, while (ii) fine structural detail is still learnable from the targets\u2014and what kind of target pixel selection strategy is favored to preserve those details compared with purely random selection?",
      "answer": "Construct training pairs patch-wise from a single noisy image by (1) splitting the image into non-overlapping s\u00d7s patches; (2) in each patch, sampling t\u00d7t pixels to form the higher-scale target patch; (3) distributing the remaining (s\u00b2\u2212t\u00b2) pixels one-to-one into separate 1\u00d71 \u201csub-image\u201d inputs (so neighboring, noise-correlated pixels are assigned to different inputs, which breaks/blocks spatial noise correlation); and (4) pairing each decorrelated sub-image input with the same high-scale target (and optionally repeating sampling to create multiple targets and take the Cartesian product of inputs\u00d7targets).\n\nTo make the targets informative for detail reconstruction, the target sampling should preserve the relative spatial arrangement of the selected pixels within the s\u00d7s patch. Strategies that perfectly preserve relative positions\u2014such as selecting pixels at intersections of t sampled rows and t sampled columns, or selecting a consecutive t\u00d7t patch\u2014perform better than purely random selection; the consecutive t\u00d7t patch strategy is used as the default because it maintains structural consistency while still allowing diverse random pairing.",
      "source_document": "papers/2512.21038v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When fitting an SMPL-X avatar to monocular sign-language video, what additional regularizers and constraints does DexAvatar add beyond the standard 2D keypoint reprojection and interpenetration penalties to improve stability under self-occlusion and in one-handed signing, and how are irrelevant joints/limbs handled during optimization?",
      "answer": "DexAvatar keeps the SMPLify-X-style 2D keypoint reprojection term (L_joint) and the interpenetration penalty (L_pen), but replaces/augments the generic pose prior with sign-language priors and adds temporal and biomechanical regularization. Its full objective is:\n\nL = L_joint + \u03bb1 L_bprior + \u03bb2 L_hprior + \u03bb3 L_pen + \u03bb4 L_temp + \u03bb5 L_bbiomech + \u03bb6 L_hbiomech.\n\n\u2022 L_bprior integrates the SignBPoser body prior to regularize infeasible body poses and is formulated with a robust penalty to stay near an initial body pose estimate \u03b8\u0302_b plus a latent regularizer on the low-dimensional body embedding (\u03b6\u0304).\n\u2022 L_hprior integrates the SignHPoser hand prior similarly, using initial hand pose estimates \u03b8\u0302_h and low-dimensional latent codes for left/right hands (\u03b5_l, \u03b5_r).\n\u2022 L_temp enforces temporal consistency by penalizing changes in body pose parameters relative to the previous frame (\u03b8_b vs \u03b8_b^pre).\n\u2022 L_bbiomech and L_hbiomech add biomechanical constraint losses for body and hands as additional supervision.\n\nTo avoid optimizing parts that are typically irrelevant/noisy for signing, DexAvatar sets the keypoint weights \u03c9_i=0 for all lower-body joints (excluding them from L_joint). It also uses a one-vs-two-handed sign decision; for one-handed signing it disables optimization of the non-dominant arm (shoulder, elbow, wrist) and the non-dominant hand by assigning \u03c9_i=0 to those joints, preventing spurious updates and focusing optimization on the active limb.",
      "source_document": "papers/2512.21054v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For training a sign-language-specific body/hand pose prior using a VAE over SMPL-X/MANO joint rotations, what loss components are optimized, and what constraint does each term impose (latent regularization, pose reconstruction, mesh-level fidelity, rotation validity, parameter regularization, and biomechanical joint-angle limits)?",
      "answer": "The pose priors are trained with a weighted sum of six losses:\n1) KL-divergence loss L_KL that regularizes the latent code toward a standard normal N(0,I).\n2) Reconstruction loss L_recon that penalizes squared \u21132 error between the input joint rotations (axis\u2013angle) and the decoded rotations.\n3) Mesh loss L_mesh that enforces vertex-level fidelity by penalizing squared \u21132 error between the mesh produced by the SMPL-X layer from the decoded pose and the reference mesh.\n4) Orthogonality loss L_orth that encourages valid rotations by penalizing deviation of each decoded rotation matrix from being orthonormal with unit determinant (via \u2016R R^T \u2212 I\u2016_2^2).\n5) Regularization loss L_reg that discourages overfitting by applying an \u21132 penalty on trainable parameters.\n6) Biomechanical loss L_biomech that enforces anatomical joint-angle limits by applying a quadratic penalty when any joint angle falls outside per-joint lower/upper bounds (applied to a set of body joints for the body prior and hand joints for the hand prior).",
      "source_document": "papers/2512.21054v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training sign-language-specific body and hand pose priors for monocular SMPL-X fitting, what kind of biomechanical preprocessing is applied to the training motion data (which joints/DoFs are constrained and how poses are filtered vs corrected), and what do the ablations show about the effect of (i) training on biomechanically filtered/corrected data versus raw data and (ii) additionally adding a biomechanical penalty during training/optimization?",
      "answer": "Biomechanical preprocessing is used to remove or fix physically implausible motion before learning the priors.\n\n\u2022 Body (SignBPoser): the 3D body poses (pseudo-ground truth from How2Sign-derived data) are *filtered* by enforcing plausible joint ranges of motion and signer-space constraints, focusing on the joints most relevant to signing\u2014shoulders, elbows/forearms, and wrists. Frames whose joint angles fall outside the defined physiological ranges (e.g., overly elevated/retracted or fully outstretched arms inconsistent with real signing) are discarded.\n\n\u2022 Hands (SignHPoser): the glove-based mocap hand poses are *corrected* with a biomechanical \u201chand rectifier\u201d that enforces per-joint angular limits on three DoFs described by Euler angles\u2014bending, splaying, and twisting\u2014across the hand joints (with an axis-alignment step because MANO\u2019s rotation coordinates differ from the biomechanical convention). Rather than discarding frames, implausible joint configurations are adjusted to satisfy these limits.\n\nAblation trends:\n\u2022 Using a prior trained on biomechanically filtered/corrected data (BPf/HPf) consistently improves reconstruction accuracy compared with training on unfiltered/uncorrected data (BPu/HPu), demonstrating that cleaning the training distribution helps the downstream fitting.\n\u2022 Adding an explicit biomechanical loss during prior training (BPf+bio) does not necessarily help further; for the body prior it slightly degrades performance, suggesting mild over-regularization.\n\u2022 Adding biomechanical penalties during the *fitting/optimization* stage (while using the filtered priors) yields the best overall results for the body; for hands, adding biomechanical regularization provides small gains in some regions but can slightly worsen one hand, indicating a trade-off between physical regularization and fitting flexibility.",
      "source_document": "papers/2512.21054v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating monocular 3D signing-avatar reconstruction on the SGNify motion-capture benchmark, what is the main quantitative metric used to compare against prior whole-body mesh recovery baselines, how is it restricted spatially, which body regions are reported, and what additional metrics are used to assess the body/hand pose priors independently?",
      "answer": "The main metric is mean vertex-to-vertex error (TR-V2V). It is computed only on vertices above the pelvis, and results are reported for three regions: Upper Body (excluding the face), Left Hand, and Right Hand. To evaluate the pose priors independently, MPJPE and MPVPE are computed on the recovered joints and meshes.",
      "source_document": "papers/2512.21054v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an optimization-based SMPL-X fitting pipeline for sign-language videos that uses learned VAE pose priors, how can the body and hand prior terms be formulated so that optimizing the low-dimensional latent codes is well-conditioned, and which external pose estimators are used as supervisory signals for the body and hands in those prior losses?",
      "answer": "A practical formulation is to couple each VAE prior with a robust supervision term toward an off-the-shelf regressor estimate, plus an explicit latent-code regularizer. Concretely, the body prior loss is\n\n\u2022 L_bprior = \u03c8(\u03b8_b \u2212 \u03b8\u0302_b) + \u03bb_{\\bar{\u03b6}} L_{\\bar{\u03b6}},\n\nwhere \u03b8_b is the optimized body pose, \u03b8\u0302_b is a supervisory body pose estimate, \u03c8 is a robust penalty (same type used to reduce the effect of noisy supervision), and L_{\\bar{\u03b6}} regularizes the low-dimensional body embedding \\bar{\u03b6}.\n\nThe hand prior loss follows the same idea for both hands:\n\n\u2022 L_hprior = \u03c8(\u03b8_h \u2212 \u03b8\u0302_h) + \u03bb_{\u03f5_l} L_{\u03f5_l} + \u03bb_{\u03f5_r} L_{\u03f5_r},\n\nwhere \u03b8_h is the optimized hand pose (left/right), \u03b8\u0302_h is a supervisory hand pose estimate, and L_{\u03f5_l}, L_{\u03f5_r} regularize the low-dimensional embeddings for the left and right hands.\n\nThe supervisory signals come from SMPLerX for the body pose (\u03b8\u0302_b) and HaMeR for the left/right hand poses (\u03b8\u0302_h).",
      "source_document": "papers/2512.21054v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For a pathology text-to-image model that uses a prototype bank to enable component-level morphological control, what hybrid prototype-retrieval pipeline can be used to pick conditioning prototypes from the bank (including how the prompt is embedded and how global vs. local retrieval are combined), and what ablation finding supports using the combined global+local strategy instead of relying on only global dense retrieval or only local keyword-based retrieval?",
      "answer": "A workable hybrid pipeline is:\n\n\u2022 Embed the user prompt with a pathology vision\u2013language encoder (CONCH) to obtain an L2-normalized query vector capturing the prompt\u2019s core semantics.\n\u2022 Global semantic retrieval (dense): use the query to do cosine-similarity Top\u2011k retrieval against two dense indices built over the prototype bank\u2014one index of prototype texts encoded by CONCH\u2019s text encoder and one index of prototype images encoded by CONCH\u2019s vision encoder\u2014and take the union of retrieved prototype IDs.\n\u2022 Local fine-grained retrieval (sparse): parse the prompt into morphological keywords from a curated pathology vocabulary and use an inverted index over prototype texts to recall prototype IDs matching each keyword.\n\u2022 Combine global and local: take the union of the global and local prototype IDs, clip to a fixed maximum length, fetch the corresponding prototype feature vectors (precomputed UNI2-h features for real prototype instances), project them into conditioning tokens, and inject them into the generator via the model\u2019s conditioning mechanism.\n\nThe ablation evidence is that using only local sparse keyword retrieval produces the weakest fidelity (keywords alone don\u2019t provide enough generative context), while combining global dense context with local fine-grained retrieval is best overall; within the global module, a hybrid (text+vision) global retrieval also outperforms single-modality (text-only or vision-only) retrieval, showing that both complementary global context and fine-grained sparse guidance are needed for the strongest fidelity/alignment trade-off.",
      "source_document": "papers/2512.21058v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a latent Diffusion Transformer for controllable pathology text-to-image synthesis, how can a Flow Matching objective be set up to replace the standard DDPM noise-prediction loss\u2014specifically, how are the corrupted latent and the target \u201cvelocity\u201d constructed from a clean VAE latent and Gaussian noise, what regression loss is used, and how does a two-stage schedule that first trains on a large-scale corpus and then fine-tunes on a smaller high-quality subset improve semantic alignment and fine-grained controllability relative to using only the large corpus?",
      "answer": "Use Flow Matching by working in VAE latent space: encode an image to a clean latent z0, sample a timestep t uniformly in [0,1] and sample Gaussian noise z1. Construct the corrupted/interpolated latent by linear interpolation zt = t\u00b7z0 + (1\u2212t)\u00b7z1. The analytically defined target vector field (velocity) is vt = dzt/dt = z0 \u2212 z1. Train the DiT to regress this velocity, conditioned on (zt, t, and the fused multi-stream condition Ccomp), using an L2 (MSE) regression loss between the predicted velocity and vt. For training, first pre-train the DiT and conditioning (MSC) modules on the large image\u2013text corpus to learn broad visual\u2013text/semantic alignment, then fine-tune on a curated high-quality subset with a smaller learning rate to boost visual fidelity and strengthen fine-grained, pathology-relevant controllability beyond what training on the noisy large corpus alone provides.",
      "source_document": "papers/2512.21058v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating whether a pathology text-to-image generator provides *learnable* fine-grained semantic control (beyond matching pixel statistics), how can a \u201cTrain-on-Synth, Test-on-Real\u201d protocol be set up\u2014what feature backbone, classifier type, and data split are used\u2014and what does this evaluation reveal about how close the synthetic images get to a Real2Real ceiling, including the effect of augmenting each prompt with multiple generated samples?",
      "answer": "Use a Train-on-Synth, Test-on-Real (Gen2Real) setup because metrics like FID don\u2019t guarantee that generated morphology is learnable. Concretely, split the 10K high-quality test set into 60/20/20 (train/val/test), extract frozen CONCH features, train a linear-probe classifier on the synthetic (train) split, and evaluate on the real test split, reporting F1 and AUC. Real Data\u2013Image serves as the Real2Real (R2R) ceiling, and a Real Data\u2013Text baseline shows that key diagnostic factors are encoded in prompts. Under this protocol, the method\u2019s synthetic data is closest to the R2R benchmark among Gen2Real models (e.g., cytology-type F1 within ~2 points of R2R, and hemorrhage surpassing the next-best baseline). Further, generating multiple samples per prompt (5 images per prompt) improves results to about 98.7% (hemorrhage) and 97.9% (cytology type) of the real-image AUC, nearly closing the R2R gap.",
      "source_document": "papers/2512.21058v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a pathology generator that is natively image-conditioned (rather than text-conditioned) against text-to-image diffusion baselines, how can you adapt it so it can be driven by a text prompt for a fair T2I comparison, and what key limitation does this adaptation introduce when interpreting the results?",
      "answer": "Adapt the image-conditioned model by turning the text prompt into an image condition via retrieval: encode the prompt with CONCH and use it to retrieve a prompt-relevant real image from the prototype bank, then feed that retrieved image as the model\u2019s generation condition. The limitation is that this is only a convenience workaround\u2014because the model is not truly text-conditioned, this retrieved-image conditioning may not reflect the architecture\u2019s optimal text-conditioning performance, so the comparison can underestimate (or otherwise misrepresent) what the method could do under its native setup.",
      "source_document": "papers/2512.21058v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an embedding-fusion (single-backbone) self-supervised framework for multimodal skeleton action understanding, how can training be designed so that fused multimodal representations (1) retain recoverable unimodal information, (2) are directly optimized to match the multimodal representation you would have obtained via late fusion, and (3) become viewpoint-invariant when multi-camera captures are available\u2014what loss terms implement each of these goals, and what is the resulting accuracy/efficiency trade-off compared to late-fusion-style multimodal baselines (e.g., methods that run separate backbones per modality)?",
      "answer": "A practical way is to add three complementary self-supervised signals on top of the shared spatial\u2013temporal encoder used for all modalities:\n\n1) **Retain unimodal information inside the fused (embedding-fused) feature via Decomposition (UFD):** after encoding, the fused multimodal feature is *decomposed* into modality-specific features and each decomposed feature is aligned to the corresponding unimodal feature from the unimodal branch. This is done **separately for temporal and spatial streams** using an MSE reconstruction/alignment loss:\n- temporal: \\(L_d^t = \\frac{1}{N}\\sum_i\\sum_k \\|z^k_{i,t}-\\tilde z^k_{i,t}\\|_2^2\\)\n- spatial: \\(L_d^s = \\frac{1}{N}\\sum_i\\sum_k \\|z^k_{i,s}-\\tilde z^k_{i,s}\\|_2^2\\)\nwith overall decomposition loss \\(L_d=L_d^t+L_d^s\\). This enforces that the fused representation preserves modality-specific (unimodal) characteristics.\n\n2) **Directly optimize fused multimodal features via Composition (MFC):** since embedding fusion alone does not explicitly ensure that the fused multimodal feature matches the stronger late-fusion multimodal feature, a *composed* late-fusion multimodal target is built during training by averaging unimodal features (per stream) and projecting them; the fused feature projection is then aligned to this late-fusion target (again using MSE across temporal and spatial streams). This yields a composition loss \\(L_c\\) that reduces the discrepancy between \u201cmultimodal-from-fused-embeddings\u201d and \u201cmultimodal-from-late-fusion.\u201d\n\n3) **Encourage viewpoint invariance using multi-camera captures:** beyond standard data-augmentation positives, positives are also formed from the **same action recorded simultaneously from different camera viewpoints**. Unordered viewpoint pairs \\((x_i,x_j)\\) are used as positives (yielding \\((V^2+V)/2\\) positive pairs for \\(V\\) cameras), providing an additional unsupervised signal complementary to augmentations and encouraging the learned representation to be both augmentation-invariant and viewpoint-invariant.\n\nThese terms are combined with variance\u2013covariance (VC) regularization applied to unimodal and multimodal features in both spatial and temporal streams, giving a final objective of the form \\(L=\\alpha L_d+\\beta L_c+L_{reg}\\).\n\n**Trade-off vs late fusion:** because inference uses the shared backbone with embedding fusion (rather than running separate backbones per modality and fusing outputs), prediction-time compute remains low (comparable to a single-backbone baseline) while accuracy improves substantially; empirically it outperforms mainstream multimodal late-fusion-style approaches on NTU-60/NTU-120/PKU-MMD II, with especially notable gains on the more challenging NTU-120 setting, achieving a strong balance of efficiency and effectiveness.",
      "source_document": "papers/2512.21064v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When deploying a multimodal skeleton-based action recognition model that was pretrained with a unified (embedding-fusion) backbone, how should you expect downstream performance to change as you vary the modality subset used at inference (joint vs bone vs motion vs their combinations), and what dataset-dependent pitfall can make adding the motion modality harmful?",
      "answer": "Downstream linear-evaluation performance generally improves as more modalities are included at inference, because additional modalities add both extra and complementary information. Among single modalities, the joint modality is the strongest; among multimodal subsets, combinations that include the joint modality tend to perform best. A notable caveat is that on the more complex PKU-MMD II dataset, motion representations are substantially weaker than the other modalities and can negatively impact performance when motion is included in a multimodal feature set. The model\u2019s joint-only performance can even reach the level of a baseline that uses all three modalities (joint+motion+bone), indicating strong transfer from the unified pretraining even without using all modalities at test time.",
      "source_document": "papers/2512.21064v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In self-supervised multimodal skeleton action pretraining with a two-stream (spatial/temporal) encoder, how can VICReg-style variance\u2013covariance regularization be used to prevent representation collapse, and what is the key change when extending it from a unimodal-only baseline to a spatial\u2013temporal decoupled multimodal training setup?",
      "answer": "Collapse is prevented with a VICReg/VC regularizer that (1) enforces per-feature-dimension variance across the batch to be above a threshold via a hinge on the standard deviation/variance term V(Z)= (1/D)\u2211_j max(0,\u03b3\u2212S(z_j,\u03b5)), and (2) decorrelates feature dimensions by penalizing off-diagonal covariance C(Z)= (1/D)\u2211_i\u2211_{j\u2260i} Cov(Z)_{i,j}^2. In the unimodal-only baseline this VC regularization is applied only to global unimodal projected features (and their decomposed estimates) for each modality. In the spatial\u2013temporal decoupled multimodal setup, the regularizer is expanded to cover not just global features but also temporal-stream and spatial-stream unimodal features (both true and decomposed) and the multimodal temporal/spatial features (including the composed/late-fusion supervision versions), yielding a total loss of the form L = \u03b1\u00b7L_d + \u03b2\u00b7L_c + L_reg with L_reg summing Lvc over all those spatial/temporal and multimodal feature matrices.",
      "source_document": "papers/2512.21064v1.pdf",
      "mode": "textual",
      "content_refs": [
        "lines 414-456 (VC regularization definition and baseline usage)",
        "lines 734-784 (extended VC regularization over spatial/temporal and multimodal features; final loss)"
      ]
    },
    {
      "question": "In a language-guided generative grasp detector that first produces coarse mask/grasp maps and then refines them, how can the training objective be constructed so that (i) both coarse and refined stages receive supervision and (ii) grasp regression is explicitly tied to the language-referred object region? Describe the loss terms used for segmentation and grasp prediction, including any weighting/masking strategies and the grasp-parameter encoding used to make angle regression stable.",
      "answer": "Use a multi-stage, multi-task objective with deep supervision: the total loss is the refined-stage loss plus an auxiliary coarse-stage loss, and each stage\u2019s loss is the sum of a segmentation-mask term and a grasp-regression term.\n\n\u2022 Deep supervision across stages: supervise both the coarse prediction branch and the refinement branch, combining them as L_total = L_refine + \u03bb\u00b7L_coarse, where each stage decomposes into L_stage = L_mask + L_grasp.\n\n\u2022 Segmentation loss: predict a binary logit mask for the language-referred region and train it with a weighted binary cross-entropy loss with logits. To address foreground/background imbalance, apply foreground reweighting w_uv = \u03b2\u00b7M_uv + 1 so foreground pixels (M_uv=1) contribute more than background.\n\n\u2022 Grasp regression loss coupled to the referred region: represent each pixel\u2019s grasp with a 4D vector {q, cos(2\u03b8), sin(2\u03b8), w} (quality, double-angle encoding for a continuous angle space, and gripper width). Compute the regression loss only over valid spatial locations inside the ground-truth referred mask (i \u2208 V where M_i=1), using a Smooth L1 loss summed over the four components. This masking ties grasp supervision to the language-targeted object and avoids penalizing background pixels.",
      "source_document": "papers/2512.21065v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a language-guided dense grasp detector that predicts (i) a referring-object mask and (ii) per-pixel grasp parameters, how can an instruction-adaptive prediction head be designed using a mixture-of-experts dynamic convolution while still supporting efficient batched inference? Describe how the shared feature map is split into task-specific groups, how sentence-level language features produce per-sample expert weights, how expert kernels/biases are mixed into language-conditioned parameters, and how grouped convolution is used to apply different kernels to each sample in a batch.",
      "answer": "Use a language-conditioned dynamic convolution head (LDCH) that turns sentence-level embeddings into per-sample mixtures over a small bank of learnable convolution \u201cexperts,\u201d and apply the resulting per-sample kernels via grouped convolution:\n\n- **Task-wise feature preparation:** From the decoder feature map, first expand channels with a 1\u00d71 conv to produce a tensor whose channels are partitioned into **five task-specific feature groups** corresponding to the outputs: **mask**, **grasp quality q**, **angle sin(2\u03b8)**, **angle cos(2\u03b8)**, and **gripper width w**. Each group Ft has shape B\u00d7C\u00d7H\u00d7W.\n\n- **Per-sample expert weights from language:** For each sample i, take a **sentence-level embedding** si and pass it through a small MLP followed by **softmax** to obtain mixture weights \u03c0i over **K experts** (\u03c0i\u2208R^K).\n\n- **Mix expert kernels/biases into language-conditioned parameters:** For each task t, maintain K learnable 3\u00d73 conv expert kernels and biases {W_t^(k), b_t^(k)}. Form the sample-specific kernel and bias by a **weighted sum** with \u03c0i: Wi,t=\u03a3_k \u03c0i,k W_t^(k) and bi,t=\u03a3_k \u03c0i,k b_t^(k). Stacking across the batch yields per-task dynamic kernel/bias banks (one kernel per sample).\n\n- **Efficient batched application via grouped convolution:** Reshape Ft for grouped convolution and apply a **GroupConv2D with groups equal to the batch size B**, so each sample\u2019s feature slice is convolved with its own mixed kernel/bias in one batched operation. This produces language-conditioned coarse prediction maps for each of the five outputs while remaining efficient in batch inference.",
      "source_document": "papers/2512.21065v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a referring-expression (language-guided) grasp detection benchmark that outputs grasp rectangles and a referred-object mask, what evaluation protocol should be used to (i) decide whether a predicted grasp rectangle is correct, (ii) compute the top-N success metric J@N for language-conditioned grasping, and (iii) evaluate referring segmentation quality? State the geometric thresholds/criteria involved and the metrics reported.",
      "answer": "(i) A predicted grasp rectangle is counted correct if it meets both geometric criteria used in rectangle-based grasp evaluation: the absolute orientation (angle) difference between the predicted rectangle and a ground-truth rectangle is < 30\u00b0 and the Jaccard index (IoU) between the two rectangles exceeds 0.25.\n\n(ii) J@N measures whether the top-N ranked predicted grasp contains a successful grasp for the referring expression: success requires that the grasp refers to the correct object specified by the language instruction and also satisfies the same two geometric conditions (angle difference < 30\u00b0 and rectangle IoU > 0.25) with respect to a ground-truth grasp on that referred object.\n\n(iii) Referring segmentation is evaluated with mask Intersection-over-Union (IoU) averaged over test instances, and Precision@X (Pr@X): the fraction of test instances whose mask IoU exceeds a threshold X, reported for X \u2208 {0.5, 0.6, 0.7, 0.8, 0.9}.",
      "source_document": "papers/2512.21065v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a language-guided dense grasp detector that fuses a CLIP image feature map with CLIP word-level tokens, how can a *bidirectional* cross-attention bottleneck be designed so that (i) image features are grounded by language cues and (ii) language tokens are grounded by relevant image regions, while preserving spatial structure? Describe the sequence of operations (including the use of positional encoding and self-attention before fusion), the two cross-attention paths, and how the two enhanced streams are merged into a final multimodal representation for decoding.",
      "answer": "Use a dual cross vision\u2013language fusion (DCVLF) bottleneck built from attention blocks:\n\n1) **Preserve spatial structure + strengthen visual queries before fusion**: add sinusoidal positional encoding to the image features and pass them through a multi-head self-attention (MHSA) block. Positional encoding is needed because self-attention is permutation-invariant; encoding coordinates makes attention depend on both content and position, maintaining topological sensitivity and helping the later cross-attention focus on semantic alignment rather than doing spatial inference.\n\n2) **Bidirectional cross-attention (two paths)** using standard Q/K/V projections. For a sequence X\u2208R^{n\u00d7d}, project to Q=XW_Q, K=XW_K, V=XW_V and compute Attention(Q,K,V)=SoftMax((QK^T)/\u221ad) V. Then fuse symmetrically:\n   \u2022 **Image\u2192Text path** (visual features attend to language cues): \\(\\tilde I = \\mathrm{SoftMax}(((\\hat I W_Q)(T W_K)^T)/\\sqrt{d_k})\\, T W_V\\), enriching visual semantics with word-level cues.\n   \u2022 **Text\u2192Image path** (text features attend to visual regions): \\(\\tilde T = \\mathrm{SoftMax}(((T W_Q)(\\hat I W_K)^T)/\\sqrt{d_k})\\, \\hat I W_V\\), improving visual grounding of the language tokens.\n   Each path is followed by **residual connections, layer normalization, and its own FFN** to stabilize training and increase nonlinearity.\n\n3) **Merge the two enhanced streams**: concatenate \\(\\tilde I\\) and \\(\\tilde T\\), then apply a **1\u00d71 convolution + ReLU**: \\(R=\\mathrm{ReLU}(\\mathrm{Conv}_{1\\times1}(\\mathrm{Concat}(\\tilde I,\\tilde T)))\\).\n\n4) **Final integration**: apply another MHSA and FFN with residual+LN to obtain the final multimodal representation (e.g., \\(H=\\mathrm{LN}(R+\\mathrm{MHSA}(R))\\), \\(O=\\mathrm{LN}(H+\\mathrm{FFN}(H))\\)) that is passed to the decoder for mask/grasp prediction.",
      "source_document": "papers/2512.21065v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a coarse-to-fine, language-guided dense grasp detector that first predicts coarse segmentation and grasp-parameter maps, what kind of refinement module can be added to sharpen the mask and stabilize grasp outputs, and how should it be applied to the coarse predictions (e.g., residual vs. direct prediction, separate branches, where in the pipeline/logit space it operates, and the core encoder\u2013decoder structure)?",
      "answer": "Use a lightweight U-Net\u2013style residual refinement module applied after the coarse heads. Instead of predicting final outputs directly, the module learns a residual correction \u0394 over the coarse logits, which stabilizes training and preserves coarse-stage information. Implement two refinement branches: (1) a mask refinement branch that takes the coarse mask logit map and outputs a 1-channel residual, and (2) a grasp refinement branch that takes the 4-channel coarse grasp logit tensor (quality, sin(2\u03b8), cos(2\u03b8), width) and outputs a 4-channel residual. The refined outputs are computed by addition: Y_mask = Y_mask^coarse + Mask_refine(Y_mask^coarse) and Y_grasp = Y_grasp^coarse + Grasp_refine(Y_grasp^coarse). Each branch uses a symmetric encoder\u2013decoder with four convolutional blocks and max-pooling in the encoder for context aggregation, and mirrored convolutional blocks with bilinear upsampling and skip connections in the decoder to recover spatial details. Refinement is performed on logits (before activation) to keep gradient flow through both coarse and refined paths for stable end-to-end optimization, improving precision/semantic coherence especially under occlusion or irregular shapes.",
      "source_document": "papers/2512.21065v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-view visual place recognition system built on a geometry-grounded transformer that outputs both intermediate 2D tokens and multi-view 3D tokens, how can you construct a viewpoint-robust global place descriptor for (i) single-image retrieval and (ii) variable-length sequence retrieval\u2014specifically, which token types are kept vs. discarded, how are register/CLS tokens aggregated differently from patch tokens (including the role of Sinkhorn/optimal transport and any \u201cdustbin\u201d handling), and what supervision/loss and training protocol are used to train the descriptor?",
      "answer": "Single-image retrieval: the descriptor is built by jointly using (a) intermediate 2D tokens from the visual encoder\u20142D CLS, 2D register tokens, and 2D patch tokens\u2014and (b) 3D tokens from the geometry-grounded backbone, but with the 3D camera token discarded to improve viewpoint invariance. Only 3D register tokens and 3D patch tokens are kept. Because CLS/register token sets are small, they are aggregated with GeM pooling (paired with MLP blocks for projection/dimensionality reduction) to form compact global components. Patch tokens are aggregated via an optimal-transport assignment: learned score matrices produce soft correspondences that are solved with the Sinkhorn algorithm, and a dustbin entry is added so non-informative patch features can be assigned away; the resulting weighted patch aggregates form the patch descriptor.\n\nVariable-length sequence retrieval: an anchor frame plus multiple support frames are used. Patch tokens across frames are aggregated with a SALAD-style optimal-transport scheme (after clustering patch tokens from different frames, then computing assignments with Sinkhorn). For 2D CLS/register and 3D register tokens, a multi-frame aggregation combines GeM pooling across frames with an MLP-based projector that aligns feature dimensions across modalities/frames, enabling arbitrary-length sequences at inference.\n\nTraining/supervision: for single-frame training, the model is trained on GSV-Cities with the multi-similarity metric-learning loss and AdamW. For sequence-level training (since GSV-Cities lacks sequences), training is done on MSLS. A two-stage protocol is used: Stage 1 freezes the feature-extraction backbone and trains only the aggregation/assignment networks (GeM and the optimal-transport module); Stage 2 unfreezes feature-extraction blocks but (to avoid instability of the weakly supervised contrastive objective) only fine-tunes the last few blocks rather than fully fine-tuning all global-attention and DINOv2 blocks. Training uses mixed-precision (FP16) and a warmup+cosine learning-rate schedule (peak LR 1e\u22126), with LoRA used for fine-tuning the 3D backbone blocks.",
      "source_document": "papers/2512.21078v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a visual place recognition system on standard benchmarks with Recall@k, how is a retrieval judged \u201ccorrect\u201d in (i) single-frame matching and (ii) sequence matching across MSLS, Nordland, and Oxford RobotCar\u2014specifically, what spatial distance thresholds (e.g., 25 m vs a stricter 2 m) and what frame-window criteria are used, and why is Oxford commonly evaluated with a 2 m threshold in addition to 25 m?",
      "answer": "The evaluation uses Recall@k (R@k), where a query is counted as correctly retrieved if at least one ground-truth database item appears in the top\u2011k results under dataset-specific proximity criteria.\n\n(i) Single-frame matching: a retrieval is correct if at least one database image within 25 meters of the query location is in the top\u2011k; for Nordland (which is treated in terms of temporal proximity), correctness is defined as being within two frames of the query.\n\n(ii) Sequence matching: for Oxford RobotCar, performance is reported under both 25 m and a stricter 2 m threshold because using 25 m often saturates results (close to 100%) and the 2 m threshold provides a more fine-grained, non-saturated comparison. For Nordland sequence matching, correctness is defined by a temporal window: the retrieved frame being within ten frames of the query (and sequence length is set to 5 in that setup).",
      "source_document": "papers/2512.21078v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When fine-tuning a multi-view, geometry-grounded transformer backbone for weakly supervised visual place recognition, what staged fine-tuning strategy can prevent the contrastive objective from becoming unstable, and which model components are trained vs. frozen in each stage (including how LoRA is used in the attention blocks)?",
      "answer": "Use a two-stage training protocol with selective fine-tuning. Stage 1: freeze the feature-extraction backbone and train only the descriptor/aggregation and assignment head\u2014i.e., the GeM-pooling based token aggregation layers and the optimal-transport (Sinkhorn) assignment module. Stage 2: unfreeze the feature-extraction blocks but do not fully fine-tune everything; apply LoRA-based fine-tuning to refine the VGGT frame-attention and global-attention blocks, and only fine-tune the last few global-attention and DINOv2 blocks because fully fine-tuning all global-attention blocks and all DINOv2 blocks makes the weakly supervised contrastive objective unstable.",
      "source_document": "papers/2512.21078v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-view visual place recognition model that builds its global descriptor by concatenating 2D \u201ctexture\u201d tokens (CLS/register/patch) with 3D \u201cgeometric\u201d tokens (register/patch) and uses a dustbin-capable optimal-transport assignment for patch aggregation, what do ablations reveal about which token groups contribute most to recall and *why* (e.g., 2D patch vs 3D patch vs CLS/register), and what qualitative evidence indicates that 2D and 3D features attend to complementary image regions\u2014including which regions are typically treated as non-informative and effectively sent to the dustbin?",
      "answer": "Ablations show that the biggest gains come from the patch-token components: both 2D patch tokens and 3D patch tokens are critical for performance. The 2D patch tokens contribute by capturing fine-grained, texture-rich details and help especially when structural cues are sparse, while the 3D patch tokens contribute by encoding geometric/structural information and are more robust under illumination and weather changes. The 2D CLS and 2D/3D register tokens provide an additional but smaller improvement by adding semantic/category-level cues on top of the patch-level descriptors. Qualitatively, inspecting which tokens are not assigned to the dustbin indicates the model discards non-informative regions such as sky, road, and dynamic objects. The remaining attention patterns are complementary: 2D features focus on texture-rich elements (e.g., posters, kiosks, bicycles), whereas 3D features focus more on geometric/structural elements (e.g., walls and buildings), reflecting stronger spatial understanding.",
      "source_document": "papers/2512.21078v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-task end-to-end table recognition system that predicts HTML structure and cell text, how can bidirectional mutual learning be used to train the HTML decoder (i.e., what is the loss that couples the left-to-right and right-to-left predictions), and what specific changes make it possible to decode all cell contents in parallel with local attention without leaking information across cells while still preserving inter-cell context? Also, what training-time data augmentation policy is used?",
      "answer": "Bidirectional mutual learning trains two \u201cstudent\u201d HTML predictions in opposite directions (left-to-right and right-to-left) and couples them with a loss that combines (1) standard negative log-likelihood / cross-entropy on the ground-truth directional token sequence and (2) a KL-divergence term that makes one student match the other student\u2019s prediction distribution after reversing its output (i.e., an NLL term \u2212p(x\u2192)log q(x\u2192) plus a mutual-learning term of the form q(x\u2190)\u00b7log(q(x\u2190)/q(x\u2192), with the analogous loss for the opposite direction). The implementation reuses a single decoder for both directions by injecting an LtoR or RtoL directional vector into the token embeddings.\n\nTo decode all cell contents in parallel, the cell decoder treats the contents of all cells as one concatenated sequence separated by SEP tokens: it starts from an initial sequence consisting of SOS followed by one SEP per detected cell, then at each step inserts each cell\u2019s newly predicted next token immediately before that cell\u2019s following SEP; when a cell outputs SEP, that cell stops growing, and the procedure repeats until all cells have produced SEP. Because token indices shift at every step, positional encoding must be cell-wise (positions are encoded relative to the previous SEP, not as absolute positions in the full concatenation). With local attention, many cells may initially fall inside the same window, so an additional mask is introduced so tokens from one cell cannot attend to tokens from other cells (preventing cross-cell leakage). Since this cell-wise masking would otherwise remove useful inter-cell context, an HTML refiner is added between the structure decoder and cell decoder: it \u201cfetches\u201d the <td>-related structural features and applies non-causal global attention so all cells can refer to each other; this sharing is non-autoregressive and adds negligible inference-time cost.\n\nFor training-time augmentation, none is used (images are normalized, padded to preserve aspect ratio, and resized, but no data augmentation is applied for fair comparison).",
      "source_document": "papers/2512.21083v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multi-task end-to-end table recognition system where the cell-text decoder can be conditioned either (i) only on predicted cell bounding boxes or (ii) on dense per-cell structural features passed through a refiner (optionally with a non-causal attention block), what does the ablation on a text-heavy dataset versus a many-cells dataset show about whether accuracy gains come primarily from richer structural conditioning versus improved cell localization, and how does the size of these gains vary as cell text length increases?",
      "answer": "The ablation comparing a bbox-only cell decoder to a full decoder that additionally consumes dense structural features from the refiner indicates two different dominant benefits depending on the dataset characteristics: on the text-heavy dataset (FinTabNet, which has more characters per cell), conditioning on the refiner\u2019s structural features\u2014capturing within-cell structure and relationships to other cells\u2014drives most of the improvement in total TEDS, whereas on the many-cells dataset (PTN250, which has many cells per table), the refiner\u2019s main benefit is improving positional accuracy/box refinement, which is more important than within-cell structural richness. Across both datasets, the advantage of using structural features and/or refiner-based refinement grows as the minimum characters-per-cell threshold increases (i.e., gains are larger for longer cell text).",
      "source_document": "papers/2512.21083v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When objectively evaluating text-to-audio-video generators for cross-modal alignment, what specific embedding models/metrics can be used to score (1) text\u2013audio semantic alignment, (2) text\u2013video semantic alignment, (3) audio\u2013video semantic alignment independent of the text prompt, and (4) temporal synchronization\u2014and what additional synchronization metric is used specifically for talking-face (speech\u2013lip) scenarios?",
      "answer": "A comprehensive objective cross-modal alignment suite can be built from four parts:\n1) Text\u2013Audio (T\u2013A) alignment: compute cosine similarity between text and audio embeddings from CLAP.\n2) Text\u2013Video (T\u2013V) alignment: compute cosine similarity between text and video features from VideoCLIP-XL-V2.\n3) Audio\u2013Video (A\u2013V) alignment (text-independent): compute A\u2013V semantic similarity using ImageBind.\n4) Temporal synchronization: measure audio\u2013visual onset alignment using DeSync (DS) from Synchformer, defined as the absolute time offset between audio and visual onsets averaged over the video (lower DS is better).\nFor talking-face cases, additionally report LatentSync (LS), a SyncNet-based lip-sync metric for speech\u2013lip synchronization (higher is better).",
      "source_document": "papers/2512.21094v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When constructing a challenging prompt set to evaluate text-to-audio-video generators, what end-to-end pipeline can be used to (i) achieve broad semantic coverage without being dominated by redundant \u201ccommon\u201d prompts, and (ii) inject physically plausible, real-world dynamics\u2014specifically, how are prompts sourced, embedded/deduplicated and sampled, then rewritten/audited, and how is a video-to-text \u201cinversion\u201d stream incorporated?",
      "answer": "A three-stage pipeline is used:\n\n1) Multi-source prompt collection for coverage: aggregate raw prompts from diverse high-quality sources (e.g., VidProM, a T2V community prompt pool such as Kling\u2019s, LMArena, and Shot2Story).\n\n2) Semantic deduplication + long-tail-friendly sampling, then prompt densification: encode prompts with a sentence-embedding model (all-mpnet-base-v2), remove near-duplicates with cosine-similarity threshold 0.8, cluster the remaining prompts, and apply square-root sampling (sampling probability inversely proportional to \u221acluster size) to prevent frequent topics from dominating while preserving semantic distinctiveness. Then use an LLM (Gemini-2.5-Pro) to restructure/enrich prompts by adding dense visual subject descriptions, motion dynamics, and acoustic events while enforcing cinematography constraints (e.g., camera/lighting), followed by manual auditing to filter static or illogical compositions (yielding a curated set of complex prompts).\n\n3) Real-world \u201cvideo inversion\u201d for physical plausibility: select a set of diverse, high-fidelity short YouTube clips (4\u201310s), generate dense temporally aligned captions/prompts with Gemini-2.5-Pro, and resolve mismatches against the source video via human-in-the-loop verification, producing prompts anchored in real-world dynamics.\n\nThis yields a prompt suite combining a large curated set (400) plus a physically grounded inversion set (100).",
      "source_document": "papers/2512.21094v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a text-to-audio-video generator\u2019s *realism independent of the prompt*, what five complementary realism metrics can be used, and what specific artifact or consistency failure mode is each metric designed to detect in the generated audio-video output?",
      "answer": "A prompt-independent realism evaluation can be decomposed into five complementary metrics:\n\n- **MSS (Motion Smoothness Score)**: penalizes *unnatural jitter and discontinuities* in video motion.\n- **OIS (Object Integrity Score)**: detects *anatomical/structural distortions and visual artifacts* that break object integrity.\n- **TCS (Temporal Coherence Score)**: evaluates *object permanence over time* and whether *occlusions remain plausible* (i.e., no illogical disappear/appear).\n- **AAS (Acoustic Artifacts Score)**: targets *audio noise and unnatural/mechanical-sounding artifacts*.\n- **MTC (Material\u2013Timbre Consistency)**: checks whether *sound timbre matches the physical properties of visible materials* (sound-to-material realism).",
      "source_document": "papers/2512.21094v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking text-to-audio-video generators with automated (reference-free) *video-quality* metrics, how can you separate low-level technical fidelity from high-level aesthetic appeal\u2014i.e., what are the two scores used, which pretrained evaluators compute them, and how are frame-level/keyframe scores aggregated into a single video score?",
      "answer": "Use two complementary video-quality scores:\n\n1) **Video Technological Score (VT)** for low-level visual integrity (penalizes noise/blur/compression artifacts). It is computed with **DOVER++**, which scores representative frames and then **aggregates the frame-level predictions into a holistic video score** (higher VT = cleaner/sharper/more photorealistic).\n\n2) **Video Aesthetic Score (VA)** for high-level perceptual aesthetics (composition, lighting, color harmony). It is computed with the **LAION-Aesthetic Predictor V2.5** run on **extracted keyframes**, and the **keyframe scores are averaged** to yield the final VA.",
      "source_document": "papers/2512.21094v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building a lightweight unified OCR model that must recognize both plain text and LaTeX-style formulas, how can the tokenizer be designed to prevent \u201csemantic entanglement\u201d between the two modalities, and what empirical behavior indicates this design especially benefits formula recognition (including any noted trade-off)?",
      "answer": "Use a semantics-decoupled tokenizer: train two independent tokenizers, one on plain text and one on mathematical formulas, then merge by inserting the formula-tokenizer\u2019s tokens into the text tokenizer as special tokens (excluding tokens already present in the text vocabulary). This keeps tokens that can appear in both contexts (e.g., \u201csum\u201d, \u201cinfty\u201d, \u201cleft\u201d, \u201cfrac\u201d, \u201cright\u201d) from sharing embeddings across modalities, reducing representational ambiguity for small models. Ablations show that removing this SDT causes the largest degradation on formula recognition relative to text/mixed content, indicating the decoupling particularly helps formulas; a small trade-off is that SDT can slightly hurt performance in some domains (e.g., a minor drop on Textbook), though combining SDT with hierarchical supervision yields gains across domains on average.",
      "source_document": "papers/2512.21095v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a unified OCR model that must read text, standalone formulas, and mixed blocks at multiple granularities (character\u2192multi-paragraph), how can hierarchical layout information be injected into the sequence supervision so the decoder learns line/paragraph structure, and what kinds of accuracy gains does this provide\u2014particularly at higher-level text blocks and in complex-layout domains?",
      "answer": "Inject hierarchy by inserting dedicated supervision tokens into the target sequence during data construction: a line-break token <|ln|> to mark line breaks within a paragraph and a paragraph-end token <|pn|> to mark paragraph boundaries. The model is trained autoregressively with cross-entropy over all token positions on these augmented label sequences, so it must predict both content tokens and the structural tokens. At inference, <|ln|> is removed and <|pn|> is converted to two newline characters to reconstruct paragraph formatting.\n\nThis hierarchical supervision training yields consistent edit-distance improvements: about 1.2% on text, 1.0% on formulas, and 1.5% on mixed text\u2013formula blocks. For multi-level text, the five levels improve by roughly 0.5%, 0.8%, 0.4%, 1.7%, and 1.2%, with the largest gains at paragraph and multi-paragraph levels. The strongest domain gains occur on complex-layout subsets such as PPT2PDF (+2.8%) and Exam Paper (+2.9%), where modeling structural complexity matters most.",
      "source_document": "papers/2512.21095v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a two-stage PDF document parsing pipeline that first performs layout analysis and then recognizes cropped regions, how can a lightweight unified text+formula recognizer be integrated to improve both end-to-end parsing accuracy and latency, and what evaluation signals demonstrate that this swap is effective (i.e., what gets replaced, what metric improves, and what kind of speedup is observed)?",
      "answer": "Integrate the lightweight unified recognizer by keeping the first-stage layout analysis unchanged, but replacing the second-stage text and formula recognition modules (the components that recognize each cropped text/formula region) with the unified model. Effectiveness is demonstrated by (1) improved full-page parsing accuracy measured by edit distance on OmniDocBench when plugged into existing two-stage parsers (e.g., MinerU2.5 and PaddleOCR-VL), and (2) large reductions in end-to-end parsing time\u2014multi-fold speedups on both block-level recognition and full-page parsing (on the order of ~5\u00d7 or more, and nearly ~7\u00d7 reported in the MinerU2.5 integration)\u2014showing the unified model is both accurate and much faster as a drop-in recognizer.",
      "source_document": "papers/2512.21095v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When creating a large-scale training set for unified recognition of plain text, standalone formulas, and text\u2013formula mixtures at multiple granularities (word/line/paragraph) without manual labeling, what automatic pipeline can be used to derive these multi-level labels from TeX sources, and how are paragraph-level labels obtained in addition to word/line labels?",
      "answer": "Collect TeX sources (e.g., from arXiv) and convert other sources (e.g., Wikipedia HTML) into TeX, then insert LaTeX color commands so every valid text or formula token is rendered in a unique color. Render the TeX into PDFs and perform color-based alignment between the rendered PDF and the TeX source to recover word- and line-level labels. Paragraph-level labels are obtained by further parsing the LaTeX grammars/structure on top of the aligned content.",
      "source_document": "papers/2512.21095v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a lightweight encoder\u2013decoder model to jointly recognize plain text, mathematical formulas, and their mixtures as an autoregressive token sequence, what training objective is used and what optimizer/schedule plus data augmentations are applied to make the model robust to real-world document artifacts?",
      "answer": "The model is trained as an autoregressive sequence generator with a token-level cross-entropy loss summed over all output positions (predicting the next token conditioned on previous tokens and the image features). Training uses the AdamW optimizer (with weight decay) and a one-cycle learning-rate schedule with a brief linear warm-up over a 10-epoch run. To improve robustness, image augmentations are randomly applied, including rotation, geometric distortion, motion blur, and Gaussian noise; training is performed from scratch (no pretrained weights).",
      "source_document": "papers/2512.21095v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a tuning-free diffusion inpainting method that applies inference-time guidance, how can you decompose the inpainting posterior so that text alignment, visual rationality, and human preference are guided separately, and what differentiable reward models and denoising update are used to implement these three guidance terms (including the time-dependent modulation choice)?",
      "answer": "The posterior over latents for text-guided inpainting is written as p(z_t|c,z_m) and decomposed via Bayes into p(z_t|c,z_m) \u221d p(c,z_m|z_t)\u00b7p(z_t) \u2248 p(c|z_t)\u00b7p(z_m|z_t)\u00b7p(z_t) by assuming conditional independence between the text c and masked latent z_m given z_t. A further condition q is added to model human preference, yielding p(z_t|c,z_m,q) \u221d p(c|z_t)\u00b7p(z_m|z_t)\u00b7p(q|z_t)\u00b7p(z_t), with p(q|z_t) proportional to exp(r_q(z_t)). During denoising, three separate guidance terms are implemented using off-the-shelf differentiable rewards: r_c for text alignment (instantiated with local CLIPScore between the prompt and masked-region content), r_m for visual rationality/coherence (InpaintReward), and r_q for human preference (ImageReward). The predicted noise is refined by subtracting the weighted reward gradients, i.e., \u00160\u00170\u00175\u00177\u00174\u00176\u00173\u00160\u00174 = \u03b5_\u03b8(z_t,t,c,z_m,M') \u2212 \u03b3_c\u00b7\u221a(\u03b1\u0304_t)\u00b7\u2207_{z_t} r_c(z_t,c) \u2212 \u03b3_m\u00b7\u221a(\u03b1\u0304_t)\u00b7\u2207_{z_t} r_m(z_t,z_m) \u2212 \u03b3_q\u00b7\u221a(\u03b1\u0304_t)\u00b7\u2207_{z_t} r_q(z_t,c). The modulation uses \u221a(\u03b1\u0304_t) (monotonically increasing) rather than the conventional \u221a(1\u2212\u03b1\u0304_t) to down-weight unreliable early noisy steps and emphasize later denoising steps.",
      "source_document": "papers/2512.21104v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In text-guided diffusion inpainting, how can inference-time optimization of the *initial* noise latent be used to improve prompt alignment by steering attention into the masked region: (i) what are the cross-attention and self-attention loss terms defined from the attention maps and a downsampled mask, (ii) how is the noise latent distribution parameterized and constrained during optimization, and (iii) why is the attention loss evaluated only at the first denoising step rather than averaged over all steps?",
      "answer": "(i) The method defines an attention-steering objective that penalizes attention outside the mask and encourages attention inside it, separately for cross-attention Ac (text\u2013patch relevance) and self-attention As (patch\u2013patch relevance). Using the downsampled mask M\u2032\u2208R^{h\u2032\u00d7w\u2032}, the losses sum over spatial locations:\n- Cross-attention loss: Lc = \u03a3_{i=1..h\u2032} \u03a3_{j=1..w\u2032} [ (1\u2212M\u2032_{ij})\u00b7Ac_{ij} \u2212 M\u2032_{ij}\u00b7Ac_{ij} ].\n- Self-attention loss: Ls = \u03a3_{i=1..h\u2032} \u03a3_{j=1..w\u2032} [ (1\u2212M\u2032_{ij})\u00b7As_{ij} \u2212 M\u2032_{ij}\u00b7As_{ij} ].\nThese terms push attention mass toward masked pixels.\n\n(ii) Instead of directly optimizing zT, it optimizes the parameters of a Gaussian noise distribution zT~N(\u03bc,\u03c3^2), initialized with \u03bc=0 and \u03c3=1, updating \u03bc and \u03c3 by gradient descent to minimize a joint objective Ljoint = \u03bb1 Lc + \u03bb2 Ls + \u03bb3 LKL, where LKL is a KL divergence term that constrains the optimized distribution to stay close to N(0,1). After optimization, a new noise sample is drawn as z\u2032T = \u03bc\u2032 + \u03c3\u2032 zT.\n\n(iii) Computing attention maps averaged across all denoising steps is deemed computationally prohibitive; empirically, the attention maps at the initial denoising step t=tini closely resemble the full-step average, so the method computes Lc and Ls only at this first step for efficiency while retaining effectiveness.",
      "source_document": "papers/2512.21104v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a two-stage precipitation nowcasting model that uses a deterministic conditioning network to provide a coarse forecast and a conditional latent diffusion model to refine it, what extra training loss is used to keep the conditioning forecast aligned with the true spatio-temporal motion, how is that loss computed, and what happens to forecasting skill (e.g., CSI/HSS) when this loss is removed?",
      "answer": "The model adds an explicit constraint loss on the conditioning network\u2019s first-estimation forecast so that the diffusion refinement stays consistent with the correct large-scale motion. Concretely, the reconstruction objective is augmented with a term L_C that is an MSE between the decoded first-estimation sequence from the conditioning network (Y\u0304_{1:N}) and the ground-truth future sequence (Y_{1:N}); i.e., L_C = ||Y_{1:N} \u2212 Y\u0304_{1:N}||^2 (added alongside the usual VAE reconstruction MSE on the concatenated input+target frames). If L_C is omitted, the conditioning network is only indirectly constrained by its KL regularizer and the diffusion-related term, producing a noisy/poorly constrained first estimation that leads the final prediction to over-predict and reduces nowcasting skill scores\u2014CSI and HSS drop noticeably\u2014while perceptual scores (SSIM/LPIPS) change much less.",
      "source_document": "papers/2512.21118v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a conditional latent-diffusion precipitation nowcasting system where a deterministic translator produces a first-estimate latent sequence to condition generation, how can classifier-free guidance be implemented (both in training and in the denoising network\u2019s conditioning pathway at inference), and what qualitative change does enabling CFG produce in perceptual quality (SSIM/LPIPS) and forecasting skill scores (CSI/HSS) on a benchmark evaluation?",
      "answer": "Classifier-free guidance (CFG) is implemented by training the denoiser on a mixture of conditional and unconditional cases: the conditioning signal (the translator\u2019s first-estimate latent sequence) is randomly dropped with probability 15%, so the model learns both conditional (85%) and unconditional (15%) denoising. At inference, each denoising step runs two forward passes to obtain conditional and unconditional noise predictions, which are then combined using the CFG rule with guidance strength w = 1.0.\n\nIn the denoiser\u2019s conditioning pathway, the conditional case injects the translator latent \\bar{z}_{1:N} by concatenating it with the noisy latent z^t_{1:N} along the channel dimension at the first ResBlock of each Downsampling Block; the unconditional case replaces \\bar{z}_{1:N} with a same-shaped zero tensor (null embedding).\n\nEnabling CFG consistently improves all reported metrics on SEVIR: SSIM increases and LPIPS decreases (better perceptual quality), while the forecasting skill scores CSI-m/CSI4-m/CSI16-m and HSS all increase compared to running without CFG (e.g., SSIM 0.7041\u21920.7183, LPIPS 0.2129\u21920.1929, CSI16-m 0.5614\u21920.6178, HSS 0.4898\u21920.5024).",
      "source_document": "papers/2512.21118v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a two-stage precipitation nowcasting system that uses a latent diffusion denoiser to \u201cenhance\u201d a deterministic first estimate, what trade-off arises if you simplify the enhancement stage to be purely spatial (i.e., enhance each predicted frame independently by removing temporal attention) instead of doing spatio-temporal enhancement, and what does this indicate about temporal consistency in the generated radar sequence?",
      "answer": "Making the enhancement stage purely spatial (frame-wise, with temporal attention removed) slightly improves the per-frame perceptual similarity metric (LPIPS) and reduces sampling/inference time, but it substantially degrades the spatio-temporal quality metric (FVD). In contrast, spatio-temporal enhancement is a bit slower and can be slightly worse on LPIPS, yet it yields much better FVD and produces steadier, more coherent motion over time\u2014indicating markedly superior temporal consistency of the generated radar sequence.",
      "source_document": "papers/2512.21118v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a precipitation-nowcasting pipeline that combines (1) a VAE to map radar frames to/from a latent space, (2) a deterministic translator/conditioning network for a first latent forecast, and (3) a conditional latent diffusion denoiser to refine that forecast, what practical trade-off emerges between freezing versus jointly fine-tuning these components during denoiser training\u2014specifically, which joint-training setup gives the best perceptual quality (SSIM/LPIPS), how do the forecasting skill scores (CSI/HSS) compare across setups, and what does this imply about end-to-end component \u201ccooperation\u201d versus training efficiency (e.g., GPU memory/convergence speed)?",
      "answer": "Three setups are compared: (A) train only the diffusion denoiser while keeping both the VAE and translator fixed (pre-trained), (B) jointly train the denoiser and translator while keeping the VAE fixed, and (C) end-to-end tune VAE + translator + denoiser together. Freezing pre-trained modules (A, and partly B) is more training-efficient (lower GPU-memory demand) and the fully frozen setup (A) converges fastest, but it yields the worst perceptual metrics. End-to-end tuning (C) yields the best perceptual quality (highest SSIM and lowest LPIPS), while all three strategies have very similar forecasting skill scores; among them, jointly tuning denoiser+translator with a fixed VAE (B) is marginally best on CSI/HSS. Overall, this indicates that end-to-end fine-tuning improves perceptual fidelity by encouraging better cooperation between the conditioning network and denoiser, whereas freezing components mainly helps efficiency and convergence speed without materially improving skill scores.",
      "source_document": "papers/2512.21118v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a latent diffusion nowcasting model end-to-end (i.e., jointly tuning the VAE encoder/decoder along with the conditioning/translator and the denoising network), what extra \u201cprior\u201d term needs to be included in the ELBO-style objective to keep the terminal noisy latent sequence consistent with the forward diffusion assumption, why is this term typically omitted in DDPM/LDM setups that use a fixed (pre-trained) VAE, and how is this prior loss written mathematically (as a KL between which two Gaussians)?",
      "answer": "An additional prior loss (Term D) is included to ensure the disrupted/terminal latent sequence at diffusion step T follows the forward diffusion distribution (so that z_T is consistent with the assumed standard normal prior). In common DDPM or latent-diffusion setups with a fixed denoiser and a pre-trained/frozen VAE, this KL term is usually dropped because it is not relevant to optimizing the denoising network itself; but with end-to-end tuning it matters because the encoder/latent distribution can drift, breaking the diffusion prior assumption. The term is formulated as\nDKL(q(z^T_{1:N}|x,z_x,\\bar z_{1:N}) || p(z^T_{1:N}|z_x,\\bar z_{1:N})) = DKL( N(\\sqrt{\\bar\\alpha_T}\\, z_{1:N},\\, (1-\\bar\\alpha_T)) \\;||\\; N(0,1) ),\nwhere z_{1:N} is the encoded latent of the ground-truth future radar frames.",
      "source_document": "papers/2512.21118v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When adapting a frozen CLIP text encoder for text-guided medical image segmentation, how can you bridge the \u201csemantic gap\u201d from long, terminology-rich clinical descriptions while keeping the prompt representation compact\u2014specifically, how are a primary prompt and an LLM-generated auxiliary prompt combined (what are the Query/Key/Value roles and the residual formulation), and what ablation outcome supports injecting auxiliary knowledge into the primary prompt rather than replacing it?",
      "answer": "Use two prompts: a concise primary prompt (Tp) that provides the direct referring description, and a domain-rich auxiliary prompt (Ta) generated by an LLM to add high-level clinical knowledge. Encode both with the same frozen text encoder to get embeddings Fp=\u03a6T(Tp) and Fa=\u03a6T(Ta). Then perform cross-attention to \u201cinfuse\u201d knowledge by using the auxiliary embedding as the Query and the primary embedding as both Key and Value: Finfused = CrossAttn(Q=Fa, K=Fp, V=Fp). Finally, merge with a residual connection and normalize to keep the representation compact while enriched: FT = LayerNorm(Fp + Finfused). In the text-encoder ablation, this auxiliary-text injection strategy is the best-performing option on both chest datasets, while using only the main text is worse and strategies that replace the main prompt (auxiliary-only) or generically expand it with an LLM perform even worse, indicating that targeted injection contextualizes the primary prompt without introducing noise or losing task-specific guidance.",
      "source_document": "papers/2512.21135v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When adapting a pretrained CLIP feature space for text-guided medical image segmentation, how can you correct the domain-specific \u201calignment gap\u201d without introducing heavy fusion blocks\u2014specifically, how does a calibration module build a shared cross-modal context and then calibrate both modalities (what are the Query/Key/Value assignments in each cross-attention), and what ablation evidence shows that a gated shared-context design is more effective than (i) having no explicit alignment module and (ii) using standard single or bidirectional cross-attention?",
      "answer": "A parameter-efficient way is to first generate a shared cross-modal context feature using a gated network, then use that context as the common Key/Value for cross-attention calibration in both directions:\n\n\u2022 Shared context generation: Fctx = G(FV, FT), where G(\u00b7) is a gated network that distills informative semantic cues from both visual features FV and text features FT and suppresses noise.\n\n\u2022 Bidirectional calibration via shared context:\n  \u2013 Visual calibration: F\u2032V = CrossAttn(Q = FV, K = Fctx, V = Fctx)\n  \u2013 Text calibration: F\u2032T = CrossAttn(Q = FT, K = Fctx, V = Fctx)\n  Here each modality keeps its own features as the Query (to preserve intrinsic structure/semantics), while the shared context provides aligned guidance as Key/Value.\n\nAblation results support the gated shared-context design: \u201cGated Global Alignment\u201d achieves 90.54% Dice on QaTa-COV19 and 80.94% Dice on MosMedData+, outperforming (i) \u201cNo Prior Alignment\u201d (89.95%, 79.62%) and also outperforming standard cross-attention variants: (ii-a) \u201cSingle Cross-Attention\u201d (90.25%, 80.03%) and (ii-b) \u201cBi-directional Cross-Attention\u201d (90.39%, 80.11%).",
      "source_document": "papers/2512.21135v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When adapting a pretrained CLIP vision encoder to text-guided medical image segmentation, how can you close the structural gap (boundary/detail loss) without destroying CLIP\u2019s pretrained vision\u2013language alignment\u2014specifically, how does a dual-branch semantic\u2013structural encoder fuse CLIP-ViT global tokens with a CNN\u2019s multi-scale feature pyramid (including the projection/add-norm fusion and the role of multi-scale deformable attention), and what ablation finding motivates keeping the CLIP ViT frozen rather than fully fine-tuning it?",
      "answer": "A structure-aware adaptation can be done with a dual-branch Semantic\u2013Structural Synergy Encoder (SSE):\n\n- Global-semantic branch: pass the image through the pretrained CLIP ViT (kept frozen) to obtain a high-level semantic feature/token sequence F_clip.\n- Local-structural branch: in parallel, a lightweight CNN extracts a multi-scale feature hierarchy {F_cnn^{1/8}, F_cnn^{1/16}, F_cnn^{1/32}} that preserves boundary/detail cues.\n- Deep fusion (semantic injection into structure): linearly project F_clip with P_V and the deepest CNN feature F_cnn^{1/32} with P_C to match channels, then fuse by element-wise addition followed by LayerNorm:\n  F_fused^{1/32} = LayerNorm( P_V(F_clip) + P_C(F_cnn^{1/32}) ).\n- Hybrid pyramid + multi-scale refinement: build a hybrid pyramid P_hybrid = {F_cnn^{1/8}, F_cnn^{1/16}, F_fused^{1/32}} and feed it to a Multi-Scale Deformable Attention module, which adaptively samples informative regions across scales to produce the final hierarchical visual features {F_V^{1/8}, F_V^{1/16}, F_V^{1/32}} used for later vision\u2013language alignment and decoding.\n\nThe motivation for freezing CLIP comes from ablations showing that (i) full fine-tuning can distort CLIP\u2019s pretrained multimodal alignment\u2014yielding instability/accuracy drops on a noisier dataset even if it slightly improves on an easier one\u2014and (ii) a version that updates the CLIP image encoder still underperforms the final SSE design, consistent with overfitting weakening the pretrained cross-modal priors. Keeping the CLIP ViT frozen while adding the CNN+fusion+MSDeformAttn branch gives the most accurate and stable results (a \u201c1+1>2\u201d synergy).",
      "source_document": "papers/2512.21135v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a CLIP-based text-guided medical segmentation model that avoids heavy feature-fusion decoders, how can a cost-aggregation decoder leverage the pre-aligned vision\u2013language space to produce a high-resolution mask\u2014specifically, how is the initial pixel\u2013text cost volume constructed (which feature scales are used), what two refinement operations are applied to denoise and enforce semantic consistency, and how is the final mask recovered using multi-scale features from the visual encoder?",
      "answer": "A cost-aggregation decoder can operate directly in the pixel\u2013text similarity (cost) space: it first computes an initial cost volume C_init from the calibrated low-resolution visual feature at the 1/32 scale (F\u2032_V) and the calibrated text feature (F\u2032_T). The cost volume is then iteratively refined by (1) a Spatial Aggregation module to smooth noisy spatial responses and (2) a Class Aggregation module to reinforce inter-class semantic relationships. To recover a high-resolution segmentation mask, the decoder performs multi-stage upsampling of the aggregated cost feature and fuses it with higher-resolution visual features from the encoder\u2014specifically the 1/16 and 1/8 scale features produced by the semantic\u2013structural encoder\u2014so that semantic consistency from the 1/32 cost volume and fine-grained spatial detail from the higher-resolution maps jointly determine the final mask M.",
      "source_document": "papers/2512.21135v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a text-guided medical image segmentation method across both chest imaging (X-ray/CT) and abdominal CT datasets, how should the evaluation metrics differ by dataset type\u2014specifically, which datasets are evaluated with both mean Dice and mean IoU versus Dice-only, and what is the rationale for that split?",
      "answer": "Chest datasets (QaTa-COV19 and MosMedData+) are evaluated with both mean Dice Similarity Coefficient (mDice) and mean Intersection-over-Union (mIoU) to comprehensively assess segmentation accuracy. For the abdominal CT datasets (MSD-Spleen, WORD, and AbdomenCT-1k), evaluation uses mDice as the primary metric, following prior works\u2019 standard protocol to ensure consistency and fair comparison.",
      "source_document": "papers/2512.21135v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When fine-tuning a general-purpose vision\u2013language model like MiniGPT-4 to generate instance-level captions for fine-grained marine species, which component is updated while the language model remains frozen, and how does this design choice explain why some metrics (e.g., METEOR) may not improve even if BLEU-4/ROUGE/CIDEr increase?",
      "answer": "The fine-tuning updates only MiniGPT-4\u2019s linear projection layer (the visual-to-LLM interface), while keeping the underlying language model frozen. Because the LLM\u2019s semantic vocabulary and generation space do not expand when it is frozen, METEOR can remain unchanged even though the model learns better visual\u2013text alignment on the dataset, which shows up as higher n-gram and relevance-oriented scores such as BLEU-4, ROUGE, and CIDEr.",
      "source_document": "papers/2512.21150v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an open-vocabulary detector benchmarked on fine-grained marine species with taxonomic evaluation splits (Class-Level vs. Intra-Class/Inter-Class), what failure mode causes accuracy to drop as the split becomes more fine-grained, and what architectural mechanism is identified as giving a language-conditioned detector a clear advantage for recognizing these visually similar species?",
      "answer": "As the evaluation moves from Class-Level to Intra-Class and Inter-Class, performance drops mainly because morphological overlap between closely related species confuses species identification\u2014models can localize organisms but misclassify them when fine-grained distinctions rely on subtle visual cues. The language-conditioned detector\u2019s advantage is attributed to its language-conditioned query selection (i.e., using category/label text to condition which detection queries are selected), which helps it recognize fine-grained marine species beyond purely visual cues.",
      "source_document": "papers/2512.21150v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a marine-species visual grounding benchmark where each object is paired with an expert-verified instance caption, how is grounding accuracy evaluated (metric and IoU criterion), what dataset construction choice is made to stay compatible with models that only accept one caption per image, and why does grounding avoid the large performance drop seen in fine-grained (Intra-/Inter-Class) object detection?",
      "answer": "Grounding is evaluated as top-1 bounding-box accuracy, counting a prediction correct if its box has Intersection-over-Union (IoU) \u2265 0.5 with the ground-truth box. Because some grounding evaluators (notably the GroundingDINO evaluation setup) allow only one caption per image, the benchmark uses the first annotation/caption of each image when building the test set. Grounding does not show the pronounced degradation across Intra-/Inter-Class splits seen in object detection because the detailed, morphology-rich instance captions provide additional discriminative cues that facilitate species identification and make the models more robust even when species are visually similar.",
      "source_document": "papers/2512.21150v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In few-shot GAN adaptation, how can an equivariant feature-rotation strategy be implemented so the learned rotation stays orthogonal, and what specific instance-level and distribution-level alignment losses are combined with the standard GAN losses to align source and target domains in the resulting proxy feature space?",
      "answer": "The strategy builds a proxy feature space where target intermediate generator features are rotated by a learnable orthogonal matrix before alignment. Orthogonality is enforced by parameterizing the rotation in the Lie group SO(d) and optimizing in its Lie algebra so(d): instead of directly updating R\u2208SO(d), an unconstrained matrix \\(\\hat R\\) is learned and mapped to an orthogonal rotation via the exponential map \\(R=\\exp(\\hat R-\\hat R^\\top)\\), which guarantees \\(R\\in SO(d)\\). In practice, rotating both domains is equivalent to rotating one, so rotation is applied only to the target features for efficiency.\n\nAlignment is done at two levels in this rotated proxy space:\n1) Instance-wise alignment uses a contrastive loss with cosine similarity, encouraging each source feature \\(I_i^s\\) to match its corresponding rotated target feature \\(R I_i^t\\) (temperature-scaled softmax over negatives in the minibatch).\n2) Distribution-level alignment matches global distributional structure using optimal transport: it constructs intra-domain similarity graphs over intermediate features \\(G_s=\\{\\mathrm{sim}(I_i^s,I_j^s)\\}\\) and \\(G_t=\\{\\mathrm{sim}(I_k^t,I_l^t)\\}\\), then penalizes discrepancy between \\(G_s\\) and the rotated target graph via a (sliced) Gromov\u2013Wasserstein distance with a coupling matrix between source and target samples.\n\nThe full optimization objective combines these with the standard target GAN generator and discriminator losses:\n\\(\\mathcal L = \\mathcal L_{G_t} + \\mathcal L_D + \\lambda_1\\mathcal L_{ins} + \\lambda_2\\mathcal L_{dis}.\\)",
      "source_document": "papers/2512.21174v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a few-shot GAN adaptation method trained with only a small target set, how can evaluation be set up to measure both sample quality and diversity, and how should the number of generated samples be chosen relative to the (much larger) target dataset to make metrics like FID meaningful?",
      "answer": "Evaluation uses two quantitative metrics: (1) Fr\u00e9chet Inception Distance (FID) to measure the divergence between Gaussian fits of real vs. generated samples (image quality), and (2) intra-cluster LPIPS (intra-LPIPS) to measure variation among generated images (diversity). Even though training is done in a 10-shot setting, the target benchmark datasets contain many images (e.g., thousands), so for evaluation the adapted generator is used to synthesize an equivalent number of images to the target set size; the full target dataset can then be used as the real distribution when computing FID (and intra-LPIPS is computed on the generated set).",
      "source_document": "papers/2512.21174v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For a memory-efficient multiscale learned primal\u2013dual CBCT reconstruction model that jointly performs scatter correction and reconstruction, how is the training objective constructed across (i) the image domain and (ii) the projection domain, including how it treats full-field-of-view vs partial-field-of-view voxels and the loss weights/schedule; and what geometric/data augmentations are applied during training?",
      "answer": "The objective combines an image-domain reconstruction loss applied to each multiscale reconstruction output and a projection-domain scatter-correction loss.\n\n\u2022 Image-domain loss Lp(x, \u03bc): a weighted sum of MAE and SSIM terms computed separately on two voxel masks:\n  \u2013 FullFoV = voxels visible in at least half of projections.\n  \u2013 PartFoV = voxels visible in at least one projection.\n  \n  Lp(x, \u03bc) = \u2016x \u2212 \u03bc\u2016FullFoV + \u03b11(1 \u2212 SSIMFullFoV(x, \u03bc)) + \u03b12\u2016x \u2212 \u03bc\u2016PartFoV + \u03b12\u03b11(1 \u2212 SSIMPartFoV(x, \u03bc)).\n  Here \u03b11 = 0.5, and \u03b12 is set to 0.1 initially then reduced to 0.01 after the first learning-rate decay to prioritize FullFoV reconstruction.\n\n\u2022 Projection-domain loss Ld(y, yprimary): weighted MAE between the network\u2019s scatter-corrected projections y and the simulated primary signal yprimary:\n  Ld(y, yprimary) = 10\u2016y \u2212 yprimary\u2016.\n\nLosses are computed for all reconstructed volumes in the multiscale output list and summed.\n\nTraining augmentations:\n\u2022 Random flips along the left\u2013right axis and the head\u2013foot axis.\n\u2022 Random isocenter shift: add an offset sampled from an isotropic Gaussian with mean 0 mm and standard deviation 100 mm to the volume center.",
      "source_document": "papers/2512.21180v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a learned primal\u2013dual CBCT reconstructor that is both multiscale and rotationally equivariant, how can the unrolled update be organized so that (i) a full\u2011resolution FDK initialization is progressively refined by additive corrections computed at multiple coarser-to-finer scales, including how the projection data are reduced at each scale, and (ii) the image\u2011domain (primal) update becomes equivariant to in\u2011plane rotations\u2014what symmetry group is used and how is the extra group dimension handled at the block output; and how do these design choices affect the quality\u2013speed trade\u2011off compared with single\u2011scale U\u2011Net/\u2202U\u2011Net and earlier (non\u2011multiscale and/or non\u2011equivariant) learned primal\u2013dual baselines on synthetic thorax/pelvic CBCT tests?",
      "answer": "(i) The reconstruction is initialized once at full resolution with an FDK reconstruction computed from scatter\u2011corrected projections (with redundancy weighting for an offset detector). Rather than reconstructing at low resolution and then upsampling, the method keeps the reconstruction volume at full resolution and refines it by adding learned correction terms computed at three resolutions (25%, 50%, and 100%). At each scale \u03b1, a downsampled version of the current reconstruction and auxiliary tensors (FoV tensor V and redundancy weights w) is used, and the projection data are also reduced via a scale-specific projection downsampling operator ProjDown\u03b1 that both downsamples detector resolution and subsamples the view angles by keeping only every 1/\u03b1\u2011th projection; correspondingly, scale-specific projector/backprojector operators P\u03b1 and P*\u03b1 are used.\n\n(ii) Rotational equivariance is built into the primal (image\u2011space) update by using P4\u2011equivariant 3D convolutions inside the primal U\u2011Net cell, making the block equivariant to 90\u00b0 rotations about the z\u2011axis. Because group convolutions introduce an additional group dimension (here of size 4), the last layer produces channels with this extra group dimension; the block output is then averaged over the group dimension to return a standard-channel tensor while preserving P4 equivariance.\n\nEffect on quality\u2013speed trade\u2011off: the multiscale correction strategy reduces compute (fewer expensive full\u2011resolution iterations) and yields substantially faster inference than earlier single\u2011scale learned invertible primal\u2013dual variants, while the use of equivariant convolutions improves parameter efficiency/robustness. On synthetic thorax and pelvic/abdominal CBCT tests, the multiscale equivariant model achieves the best overall image quality among the reported learned baselines (higher PSNR/SSIM and lower HU MAE than U\u2011Net/\u2202U\u2011Net and prior LIRE variants) while running much faster than the older non\u2011multiscale learned primal\u2013dual baseline and far faster than iterative TV reconstruction.",
      "source_document": "papers/2512.21180v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When training a CBCT reconstructor on purely synthetic data, what end\u2011to\u2011end forward model can be used to generate realistic *raw* projection inputs from a planning CT, including (i) how the CT is converted to a simple material model, (ii) how the *polychromatic primary* signal is simulated (energy discretization, detector response, noise model), (iii) how *scatter* is simulated efficiently with quasi\u2011Monte\u2011Carlo path sampling (what is sampled and how it is scaled/used across projection angles), and (iv) what computational shortcuts and acquisition randomizations are applied so the simulated data match clinical variability (e.g., scatter resolution/projection subsampling, varying projection count and photon flux during training vs evaluation)?",
      "answer": "A complete synthetic forward model is built as follows.\n\n(i) **CT \u2192 water/bone material model.** A single\u2011energy CT volume (effective attenuation at 60 keV) is converted from HU to modified CT numbers \u03c1\u0302 = 0.001\u00b7HU + 1, and then mapped to per\u2011voxel *relative water and bone densities* \u03c1w(x), \u03c1b(x) using a piecewise definition with constants \u03c61=1.2, \u03c62=1.6 and \u03baab=0.409. These densities are used to form energy\u2011dependent attenuation fields for each material and energy bin, yielding e\u03bc (water and bone components per energy).\n\n(ii) **Polychromatic primary projection simulation.** The X\u2011ray spectrum is discretized into 10 keV bins from 20\u2013120 keV (bin centers E={25,35,\u2026,115} keV). For each energy bin e, an unattenuated intensity map I0,e(\u03c3) is obtained by binning a stored source \u201cphase file,\u201d and a detector response function resp(e) is approximated as piecewise linear with key points r20=5, r60=20, r120=10. The *noisy primary* detector signal is then simulated as a sum over energies with **Poisson noise**:\n\n\\[ \\tilde P_p(\\mu)= \\sum_{e\\in E} \\mathrm{resp}(e)\\, \\mathrm{Poisson}(I_{0,e}\\, e^{-P(\\mu_e)}) \\]\n\n(where P is the cone\u2011beam projection operator applied to the energy\u2011dependent attenuation).\n\n(iii) **Scatter via quasi\u2011Monte\u2011Carlo path sampling.** Scatter is expressed as an integral over photon path space and approximated by sampling photon paths (interaction distances, interaction material/type, scatter direction/energy) and then explicitly aggregating their expected contributions at detector pixels. Efficiency comes from using **Sobol low\u2011discrepancy sequences** (quasi\u2011Monte\u2011Carlo) instead of i.i.d. random numbers for the sampling step. For projection i, source photon initial conditions are rotated to the view angle (Rot\u03c6i), paths are sampled and integrated, and the estimated scatter is scaled from a small subset of source photons Src to the full source flux I\u03a3 by the factor I\u03a3/|Src|:\n\n\\[ \\tilde P_s(\\mu)(i)= \\frac{I_\\Sigma}{|\\mathrm{Src}|}\\, \\mathrm{Integrate}(\\mathrm{SamplePath}(\\mathrm{Rot}_{\\varphi_i}(\\mathrm{Src}), e\\mu), e\\mu). \\]\n\n(iv) **Raw projection formation, shortcuts, and training\u2011time variability.** The synthetic *raw* network input is a normalized negative log transform using an air scan for normalization:\n\n\\[ y_{\\mathrm{raw}}(\\mu)= -\\log\\, \\min\\Big(\\frac{\\tilde P_s(\\mu)+\\tilde P_p(\\mu)}{\\tilde P_p(\\mathrm{air})},\\,1\\Big), \\]\n\nand the corresponding primary\u2011only target is computed similarly without scatter. To reduce computation, scatter is simulated at **one quarter** of the primary pixel pitch and only for **every eighth** projection; it is then upscaled to full detector resolution and full projection count with **linear interpolation**. To mimic clinical acquisition variability, during training the **projection count is sampled uniformly from 432 to 944**, while evaluation uses **720** projections; the photon flux (photons/mm\u00b2) is also randomized **from 16,000 to 66,000** during training, while evaluation fixes it to **16,000 for thorax** and **66,000 for pelvic** cases (with intensity maps and total flux scaled accordingly).",
      "source_document": "papers/2512.21180v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating CBCT reconstruction methods on real patient scans where no true ground-truth CBCT volume exists, how can HU accuracy be quantified against a planning CT\u2014specifically, how is an \u201cFDK (calibrated)\u201d baseline obtained, what registration/alignment is applied between planning CT and CBCT for metric computation, and what ROI-based measurements are used in addition to a global MAE (including how the ROIs are defined)?",
      "answer": "HU accuracy is quantified by using the planning CT as a reference after rigidly registering it to the CBCT. A calibrated FDK baseline is produced by applying a single global affine intensity transform to the FDK HU values, with the affine parameters fit by linear regression that matches central slices of the FDK reconstructions to the corresponding planning-CT slices. For metrics, MAE in HU is computed in the central full\u2013field-of-view region, and additionally four spherical ROIs (about 2\u20134 cm in diameter) that are well aligned between planning CT and CBCT are selected; mean HU within each ROI is computed and the mean differences between planning CT and each reconstruction are reported for these ROIs.",
      "source_document": "papers/2512.21180v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When adapting a memory\u2011efficient learned primal\u2013dual CBCT reconstructor that was trained at 2\u202fmm voxel pitch to produce 1\u202fmm reconstructions, what modification to the unrolled network can be made starting from the 2\u202fmm model, what data can this extension be fine\u2011tuned on, and what concrete memory-management measures make training and inference feasible at 1\u202fmm resolution?",
      "answer": "A 1\u202fmm version can be obtained by taking the pretrained 2\u202fmm model and appending an additional primal/dual update block on top of it, then fine\u2011tuning the entire network on simulated pelvic CBCT data. To keep the 1\u202fmm setting feasible, training is done with smaller patch sizes and CPU\u2013GPU streaming of the latent (primal/dual) vectors so it fits on 48\u202fGB GPUs, while inference is arranged to keep GPU memory below about 24\u202fGB.",
      "source_document": "papers/2512.21180v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a two-stage image-conditioned 3D diffusion system that first generates a coarse mesh and then refines it, what specific mechanism lets the refinement-stage diffusion model avoid having to learn global point placement (i.e., decouple spatial localization from geometric detail synthesis), and how is the shape VAE modified/trained so it can decode SDFs at the voxel query locations used during refinement (including any query-point augmentation and how spatial/image conditioning enters the DiT)?",
      "answer": "The refinement stage is run on voxel-based queries defined on a fixed-resolution voxel grid rather than on free-form surface point queries. Voxel queries are derived from the Stage-1 coarse geometry, so the refinement DiT is conditioned on explicit, fixed spatial locations (positional anchors) instead of having to generate positions and geometry jointly. The voxel-query coordinates are encoded with rotary positional embeddings (RoPE) and injected into each transformer layer, giving explicit spatial awareness while reducing the diffusion solution space and stabilizing convergence.\n\nTo support voxel queries (which can be off the surface), the shape VAE is adapted so its decoder can predict valid geometry beyond surface-only locations: during training, the original surface query points are augmented with random spatial perturbations within a bounded range, enabling the decoder to produce valid SDF values at off-surface locations. At inference, voxel queries sampled from the coarse geometry are subsampled to a token budget, denoised by the refinement diffusion process, decoded by the VAE into an SDF field queried on a regular grid, and the final mesh is extracted with marching cubes.\n\nFor image conditioning in refinement, the DiT uses cross-attention over DINOv2 image features, and applies image-token masking to keep only informative image tokens so background tokens do not interfere with geometry generation.",
      "source_document": "papers/2512.21185v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an image-only 4-way local patch-completion benchmark, how can distractor options be constructed to control task difficulty, and how should distractors be handled when image-level augmentations (e.g., blur/brightness/rotation/edges) are applied so that the task still tests perceptual completion rather than coordinate mismatch?",
      "answer": "Two distractor-generation strategies are used:\n\n\u2022 Random Sampling (RS): incorrect patches are sampled from unrelated regions within the same image.\n\n\u2022 Similarity-based distractors (DS): a set of 64 patches is uniformly sampled from the image, embedded with a DINOv2-large encoder, and the top-3 most similar patches (by cosine similarity) to the ground-truth patch are selected as distractors, making the options perceptually confusable.\n\nWhen image-level augmentations (blur, brightness/gamma correction, rotation, edges) are applied, distractor patches are re-extracted/resampled at matching coordinates after augmentation to preserve spatial consistency\u2014so the model must resolve perceptual ambiguity rather than being helped/hurt by misalignment. The exception is the edge-image variant, where similarity search is performed directly in edge space after converting the image to an edge map.",
      "source_document": "papers/2512.21194v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a supervised fine-tuning experiment that trains a VLM specifically on image-only local-completion/occlusion (Level-1) multiple-choice tasks, which types of subtasks tend to benefit the most from fine-tuning versus least, and what does this pattern imply about which visual cues are more learnable under direct supervision?",
      "answer": "Fine-tuning yields consistent gains across Level-1, but the biggest improvements occur on rotation-sensitive and global-context subtasks, while gains on pixel-level perturbations like blur, brightness, and edge-only conditions are smaller. This suggests geometric cues and broader contextual/structural information are more learnable under direct supervision than robustness to low-level pixel variations, which remains harder to acquire.",
      "source_document": "papers/2512.21194v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When a vision\u2013language model performs poorly on image-only Raven-style grid tasks, what controlled comparison can be used to disentangle whether the failure is due to weak logical reasoning vs. weak visual feature extraction, and what does a large accuracy gap between the two conditions imply about the dominant bottleneck?",
      "answer": "A controlled way to separate reasoning from perception is to evaluate the same Level-2/Level-3 grid problems in a text-only (symbolic) form: replace each grid cell image with a symbolic description of its attributes (e.g., \u201c3 blue globes\u201d, \u201c2 red mugs\u201d) and present the answer options as text, keeping the underlying rule structure unchanged while eliminating vision. In this setting, much higher accuracy than in the visual condition (e.g., GPT-5 achieving 85% on Level-2 and 66% on Level-3 text-only vs. about 50% and 37% when using images) indicates that the model\u2019s reasoning machinery is relatively strong, and the dominant bottleneck is perceptual grounding/visual feature extraction\u2014i.e., translating visual input into the symbolic representations needed for rule inference\u2014rather than the logical inference itself.",
      "source_document": "papers/2512.21194v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a Raven-style image-grid benchmark intended to test compositional (multi-attribute) visual reasoning rather than shortcut learning, how can you set the missing-cell position and generate multiple-choice distractors so that (a) positional priors don\u2019t help and (b) wrong options correspond to specific rule violations across attributes like color, count, orientation, and object identity?",
      "answer": "Use two complementary controls.\n\n1) Missing-cell placement to avoid positional shortcuts: keep the missing cell fixed when the goal is to isolate single-attribute effects (the benchmark fixes it at position (2,2) for Level-2 single-attribute grids), but vary the missing-cell position across the 3\u00d73 grid for multi-attribute tasks (Level-3) so models cannot rely on a constant blank location and so grid-wise/row-wise/spiral variants are comparable.\n\n2) Distractor construction as targeted rule violations: for Level-3, generate three distractors procedurally so that each wrong option systematically violates the governing rule rather than being unrelated. For color/orientation rules, distractors modify only the target attribute while preserving the spatial structure (e.g., swapping in a color from another row). For count-based rules, distractors change one operand or the operation itself (e.g., reverse a progression, swap addition vs. subtraction, or substitute an incorrect min/max extreme).",
      "source_document": "papers/2512.21194v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multimodal egocentric motion-capture setup that trains a high-quality teacher using denser IMU coverage and distills to a student that only has sparse, noisy consumer IMUs, what training objectives are used for (a) the teacher and (b) the student? Describe the loss terms (pose vs translation, and output- vs feature-level distillation) and how they are combined/weighted.",
      "answer": "(a) Teacher: it is trained with a two-task objective comprising a local pose loss and a global translation loss, combined using learned task-uncertainty weighting. The local pose loss is a per-joint weighted mean-squared error over the 6D rotation representation for the 24 SMPL joints. The translation loss is an L2 loss between predicted and ground-truth root positions. These are summed with learnable uncertainty parameters that adaptively weight pose vs translation.\n\n(b) Student: it is trained with a multi-objective loss that adds two distillation terms to a motion-estimation loss. The motion-estimation loss uses the same pose+translation objective as the teacher. Output-level distillation penalizes the squared error between the teacher\u2019s and student\u2019s predicted 6D joint rotations. Feature-level distillation aligns the teacher and student IMU latent features via an L2 loss between their extracted IMU feature vectors. The total student loss is a weighted sum of motion loss, output distillation loss, and feature distillation loss (with coefficients that are treated as balancing weights during training).",
      "source_document": "papers/2512.21209v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a multimodal egocentric motion-capture system that fuses three camera streams with sparse consumer IMUs, how is a monocular SLAM estimate integrated to improve global localization/head pose, and what does ablating the forward-facing camera show about its different contributions to (a) SLAM-based global root localization and (b) body motion estimation accuracy?",
      "answer": "The pipeline runs an off-the-shelf monocular SLAM module (MASt3R-SLAM) on the forward-facing camera stream to estimate global camera poses over time, then applies a rigid transformation to convert those camera poses into head poses that are concatenated with visual+IMU features for temporal fusion and motion prediction. Because monocular SLAM and IMUs can be in different coordinate systems (notably global translation scale), the teacher\u2013student model is trained to implicitly align these frames.\n\nCamera ablations separate the forward camera\u2019s roles: removing the forward camera from SLAM causes the largest degradation in global localization (Root Position Error increases by 4.56 cm), indicating the forward view provides crucial environmental context for SLAM. In contrast, removing the forward camera only from the motion-estimation feature stream mainly hurts pose/mesh accuracy (MPJPE increases by 1.23 cm), consistent with that camera directly observing the upper body.",
      "source_document": "papers/2512.21209v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an egocentric motion-capture dataset that records three cameras plus multiple consumer wearables and uses a MoCap system as ground truth, the different devices don\u2019t share timestamps. What practical procedure can be used to temporally synchronize (1) the wearables to MoCap and (2) the cameras to MoCap using simple alignment gestures, and after alignment how are the different modality sampling rates unified for training?",
      "answer": "A two-gesture alignment procedure is used. (1) Wearables\u2013MoCap alignment: the user extends the right arm, places the smartphone on the palm, and raises the arm vertically three times while keeping the phone horizontal; the z-axis acceleration peaks from the smartphone IMU are temporally aligned to the peaks from the MoCap right-hand sensor to compute the time offset. Because the smartphone, smartwatch, and earbuds share a synchronized clock (via the same mobile device), syncing the smartphone to MoCap synchronizes all consumer wearables. (2) Cameras\u2013MoCap alignment: the user stands upright, quickly turns the head to the right and returns forward; the camera frame where the field of view is maximally rightward (maximum head rotation) is matched to the extremum of the z-axis orientation from the MoCap head sensor to get the camera\u2013MoCap offset. After alignment, all streams are downsampled to 25 Hz (the earbuds\u2019 native and lowest rate) using nearest-neighbor sampling to obtain uniform sampling for training.",
      "source_document": "papers/2512.21209v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When fine-tuning a teacher\u2013student multimodal motion estimator from dense MoCap IMUs (teacher) to sparse, noisy consumer IMUs (student), what parameter initialization and optimization strategy helps the student adapt to missing/noisy inertial observations without destroying the teacher\u2019s learned vision\u2013temporal fusion mapping\u2014specifically: which submodule is randomly initialized vs copied from the teacher, what learning rates are used for that submodule versus the rest of the network, and how are the distillation-loss weights scheduled over training?",
      "answer": "The student is initialized from the teacher\u2019s pretrained weights for the shared network, except the student IMU feature encoder (the MLP IMU encoder), which is randomly initialized. During student training, the IMU encoder is optimized with a higher learning rate (1\u00d710\u22123) to learn representations that compensate for missing dense IMU observations, while the remaining parameters are fine-tuned with a lower learning rate (1\u00d710\u22124) to preserve the teacher\u2019s learned multimodal feature-to-motion mapping. The student loss weights are initialized as \u03bbmotion=1.0, \u03bboutput=0.5, and \u03bbfeat=0.5, and the two distillation weights (\u03bboutput and \u03bbfeat) are decayed by a factor of 0.8 every 10 epochs to gradually shift emphasis toward direct motion prediction.",
      "source_document": "papers/2512.21209v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For an egocentric full-body motion estimator that fuses three head-mounted camera streams with on-body IMU signals, how can temporal fusion be implemented so that the model jointly reasons over vision, inertial cues, and global head motion\u2014specifically: what per-time-step feature streams are extracted from the images and IMUs, what additional signal can be injected to anchor global head pose, and how are these fused over a sliding time window to predict the motion sequence?",
      "answer": "Temporal fusion is implemented as a sequence-to-sequence model over a sliding window. At each time step, visual features are extracted independently from the three RGB views (forward, left-downward, right-downward) using a ResNet-18 image encoder. Inertial signals from the available IMUs are encoded by an MLP-based IMU feature encoder into a unified latent IMU representation U_t. To provide a global anchor for head motion, an off-the-shelf monocular SLAM system run on the forward camera estimates global camera poses, which are rigidly transformed into per-frame head poses H_t; this SLAM-derived head pose is injected alongside the learned features. The per-step features {F^f_t, F^l_t, F^r_t, U_t, H_t} are concatenated across the N-frame window and passed through a bidirectional LSTM temporal fusion module, which outputs the motion sequence M (global root translation and 6D joint rotations).",
      "source_document": "papers/2512.21209v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an LMM augmented with learnable latent \u201cvisual reasoning\u201d tokens, how can attention masking be set up so that the model\u2019s answer cannot directly attend to image tokens (i.e., all visual information must pass through the latents), and what two-stage training objective is used to first make the latents carry useful visual information and then let the model jointly use both latents and the original image tokens?",
      "answer": "Attention masking is configured to create a strict visual bottleneck: during the bottleneck stage, answer tokens are allowed to attend only to the text prompt tokens and the K appended latent tokens, but not to any image tokens; additionally, the prompt tokens are also prevented from attending to image tokens to avoid indirect \u201cleakage\u201d of visual information to the answer via the prompt. Training then uses two stages with the same end-task loss: (1) Stage 1 trains under this bottleneck mask using the standard autoregressive negative log-likelihood (NLL) objective, computed only on the answer tokens, which directly forces the latent tokens (whose embedding rows are learnable) to encode the visual information needed to solve the task. (2) Stage 2 switches back to a standard attention mask where answer tokens may attend to both the original image tokens and the now-enriched latent tokens, while still optimizing the same NLL computed only on answer tokens, so the model learns to integrate information from both pathways when producing the answer.",
      "source_document": "papers/2512.21218v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an event-centric text-to-image retrieval pipeline that first narrows candidates with entity-aware lexical retrieval and then performs multimodal reranking, what are the two complementary BEiT-3 reranking configurations used, what parts of the model are updated vs. frozen during fine-tuning, and how are (i) the BEiT-3 similarity scores and (ii) the first-stage article ranks combined and then fused across the two rerankers to produce the final image ranking?",
      "answer": "The reranking stage uses two BEiT-3 Base configurations with complementary alignment behavior:\n1) An event-aligned BEiT-3 model that is fine-tuned from a Flickr30k-pretrained checkpoint on OpenEvents v1 training data to emphasize literal alignment between long event queries and the image-associated text (especially named entities, temporal references, and factual descriptions).\n2) A BEiT-3 ITC (image\u2013text contrastive) configuration that leverages BEiT-3\u2019s contrastive-pretrained weights to better capture higher-level/latent visual semantics beyond direct text overlap (e.g., emotional or symbolic cues).\n\nFine-tuning is \u201cdecoupled\u201d: the visual encoder is frozen to preserve general visual representations and avoid catastrophic forgetting, while text/language-processing components are selectively updated to adapt to event-query characteristics (complex entity relations, temporal expressions, domain terminology), improving computational efficiency by reducing the number of trainable parameters.\n\nFor scoring, each reranker applies a sigmoid-based boosting that combines the BEiT-3 raw similarity score s_i with the first-stage article rank r_i for the image\u2019s originating article:\nboost_score(i) = \u03c3(\u03b1\u00b7s_i \u2212 \u03b2\u00b7log(r_i)) \u00b7 \u03b3, and final_score(i) = s_i + boost_score(i), where \u03c3 is sigmoid and \u03b1, \u03b2, \u03b3 control similarity weight, rank weight, and maximum boost.\n\nAfter boosting within each BEiT-3 configuration, the two reranked lists are merged with Reciprocal Rank Fusion so images ranked highly by both models receive the strongest final ranking.",
      "source_document": "papers/2512.21221v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an event-based text-to-image retrieval system that uses a first-stage text/article retriever before image reranking, how should a long-form event query be transformed and searched to robustly find candidate articles\u2014specifically: what entity types are extracted, what two parallel retrieval branches are run, how are entity types prioritized and expanded to handle aliasing, and what rank-aggregation method is used to fuse the two branches into a top-K article set for downstream image retrieval?",
      "answer": "The query is first passed through NER (spaCy) to extract salient event-defining entities such as locations/geo-political entities, temporal expressions, persons, and organizations. Retrieval then runs in parallel via (1) an entity-matching branch that looks for documents containing semantically related mentions of the recognized entities, and (2) a standard full-text BM25 branch over the raw query text for strong lexical matching. To make entity matching more event-relevant, different entity types are given domain-specific weights (with PERSON and numeric/quantity entities emphasized most, followed by ORG and GPE, and less informative types down-weighted). The extracted entities are also expanded with synonym variants using WordNet to improve robustness to alternative terminology. Finally, the two ranked lists are fused using Reciprocal Rank Fusion, and the top-K fused articles (each linked to one or more images) are forwarded as the candidate pool for the image-level reranking stage.",
      "source_document": "papers/2512.21221v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking an event-based text-to-image retrieval system on OpenEvents v1, which competing baseline achieves the best mAP among the provided comparisons, how large is the mAP gain of the two-stage entity-filtering + dual BEiT-3 reranking pipeline relative to that baseline, and what three design factors are identified as driving this improvement?",
      "answer": "The strongest baseline in the comparison is SBERT + Bart + CLIP (mAP = 0.3232). The proposed pipeline reaches mAP = 0.559, which is reported as a 73% relative improvement in mAP over that baseline. The three cited drivers are: (1) an entity-driven first-stage filtering that combines weighted entity matching with BM25 for efficient, precise candidate selection; (2) BEiT-3\u2019s ability to process long text inputs (up to 512 tokens) to better capture the context of complex event queries; and (3) a dual-model BEiT-3 configuration (event-aligned fine-tuned model plus an image-text-contrastive/ITC model) that captures complementary semantic cues for better multimodal matching.",
      "source_document": "papers/2512.21221v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an arbitrary keyframe\u2013guided one-shot video diffusion system, how can a tailored DPO stage be set up to specifically reduce (a) abrupt cuts between adjacent conditioning frames and (b) physically implausible subject motion\u2014i.e., how are the positive/negative preference pairs constructed for each issue, what is used as the reference policy, and what DPO objective is optimized?",
      "answer": "The tailored DPO stage targets two artifact types by constructing issue-specific preference pairs and then optimizing a standard DPO loss against an SFT-initialized reference model.\n\n\u2022 Abrupt cuts: First build a cut-severity dataset by generating ~10k first\u2013last-frame-conditioned clips and labeling them into five cut-severity levels (initially via GPT-4o, then refined by human annotators). Fine-tune a vision-language model as an \u201cabrupt cut discriminator.\u201d Next, for each prompt (same text + same boundary conditions), generate a group of candidate videos with different random seeds; apply the discriminator to select the \u201cbest\u201d (least cut) and \u201cworst\u201d (most cut) videos to form positive/negative pairs (non-cut vs cut).\n\n\u2022 Subject motion rationality: Identify common subjects (e.g., humans/vehicles) and problematic actions; create image-to-video prompts consisting of first frame, last frame, and descriptive text using a text-to-image generator plus a VLM-based rephraser; generate seed-varied video groups and then (since VLMs struggle to judge motion plausibility) use human annotators to pick high-contrast positive/negative pairs (plausible vs implausible motion).\n\n\u2022 Reference policy and objective: Use \u03c0ref initialized from the SFT weights; directly optimize the policy \u03c0\u03b8 with the standard DPO loss\nLDPO = \u2212E(c,vw,vl)~D [ log \u03c3( \u03b2 log(\u03c0\u03b8(vw|c)/\u03c0ref(vw|c)) \u2212 \u03b2 log(\u03c0\u03b8(vl|c)/\u03c0ref(vl|c)) ) ],\nwhere c is the visual+text conditioning, vw is the preferred (plausible/non-cut) sample, vl is the dispreferred (artifact) sample, and \u03b2 controls deviation from \u03c0ref. This maximizes the likelihood margin of preferred over artifact-prone samples while preserving diversity from SFT.",
      "source_document": "papers/2512.21252v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When generating a one-shot video longer than what fits in a single diffusion pass, how does a segment-wise autoregressive (SAR) latent-space strategy (i) decide where to cut the video into segments, (ii) condition each new segment to enforce continuity with the previous segment, and (iii) merge the segment outputs back into one coherent latent sequence before decoding?",
      "answer": "The SAR strategy partitions the target video in latent space using a variable-length sliding window, treating user-provided conditioning items (images/video clips at specified times) as candidate boundaries; as the window slides, the current segment is ended at the latest boundary once the window would exceed a preset maximum length. Segments are then generated autoregressively: the n-th segment is produced by conditioning the generator on the tail latents of the previous segment via a temporal operator \u03c4(sn\u22121), together with the set of local guidance conditions Cn that fall inside the current window, i.e., sn = G\u03b8(\u03c4(sn\u22121), Cn), which enforces pixel-level continuity and aligns the new segment with the prior boundary context. After all segments are generated, overlapping latent frames between adjacent segments are fused to form the final latent sequence, which is then decoded by the VAE; doing all of this in latent space yields smoother transitions and helps avoid flicker/abrupt jumps.",
      "source_document": "papers/2512.21252v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a two-stage latent diffusion video generator that uses a separate super-resolution DiT conditioned on high-res reference frames, how can the positional-encoding/conditioning scheme be designed so the SR model does not treat small low-res vs high-res discrepancies as semantic conflicts\u2014i.e., how are conditional frames injected into the token sequence and how are their rotary position embeddings assigned\u2014and what failure modes does this mitigate?",
      "answer": "Use a \u201cShared-RoPE\u201d sequence-wise conditioning in the SR DiT: beyond channel-wise concatenation, append the conditional frame tokens to the tail of the SR input sequence, and assign each condition frame the same rotary positional embedding (RoPE) value as the target frame it is meant to guide (for video conditions, apply this sharing only to the first conditioned frame). This enforces alignment between the SR generation and the conditioning signal, preventing the SR model from interpreting minor low-res/high-res differences as semantic conflicts, which otherwise leads to artifacts such as severe temporal flickering and color shifts; sharing RoPE improves temporal stability and eliminates these artifacts.",
      "source_document": "papers/2512.21252v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When adapting a DiT video diffusion model that uses a causal, temporally downsampling VideoVAE to support arbitrary intermediate image/video conditioning, what is the training-time strategy for constructing conditioning latents so they match what will be available at inference time\u2014especially for (i) single-frame conditions and (ii) multi-frame (clip) conditions\u2014and what additional condition-sampling is used to make the model robust to diverse intermediate constraints?",
      "answer": "Because the causal VideoVAE\u2019s temporal downsampling makes an \u201cintermediate\u201d latent aggregate information from multiple frames (so it is not a clean condition for a specific timestamp), the adaptation aligns the training distribution with inference by:\n\n(i) **Single-frame (image) conditions:** choosing conditioning frames (e.g., action boundary frames) and **re-encoding each conditioning frame in single-image mode with the VAE encoder** so the latent corresponds to that exact frame (matching inference-time encoding).\n\n(ii) **Video-clip conditions:** sampling latent segments of varying lengths from precomputed video latents, but since these segments are not a typical encoding result under causal encoding, using an approximation where **the first frame of each segment is re-encoded**, and **the subsequent frames are re-sampled from the latent distribution**.\n\nTo improve robustness and input diversity beyond boundary-frame conditions, the training also **samples random intermediate frames within each video as supplementary conditions** (and re-encodes them with the single-image VAE mode).",
      "source_document": "papers/2512.21252v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When curating training data to adapt a DiT-based video diffusion model for one-shot, arbitrary-frame guidance, what concrete filtering criteria/tools can be used to distill a generic video corpus into a \u201cone-shot subset\u201d with (i) single-shot structure, (ii) large visual variation, (iii) high aesthetic quality, and (iv) strong motion\u2014 and how can the resulting videos be further annotated to support action-aware conditioning-frame selection?",
      "answer": "A practical adaptive-tuning recipe is to filter the base corpus into a one-shot subset by: (1) using a VLM-based scene detection model to exclude multi-shot videos; (2) computing cosine similarity between CLIP features of the first and last frame and discarding videos with high similarity so the retained clips exhibit large visual variation; (3) using Q-Align aesthetic scoring to remove low-aesthetic videos; (4) estimating motion strength with a 2D optical-flow predictor to keep clips with adequate motion intensity; and (5) applying RTMPose to retain high-quality human-centric videos with clear pose structure. After this filtering, action-aware annotations can be produced by running an action recognizer to identify action intervals and then using a VLM captioner to generate dense descriptions per action, yielding structured, action-wise video annotations that can guide selection of conditioning frames (e.g., action-boundary frames).",
      "source_document": "papers/2512.21252v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an any-modality (missing-sequence) multi-sequence MRI anomaly detection setting, how can a model explicitly align the feature distribution of incomplete-modality inputs to the full-modality feature space during training, and how is this alignment term combined with prototype-consistency and reconstruction objectives in the overall loss?",
      "answer": "One approach is to pre-compute, on full-modality (complete) inputs, the channel-wise mean and variance of the fused encoder feature maps and cache them as a reference distribution. During training, for each (possibly masked/incomplete) input, compute the current feature distribution (mean and variance) and penalize its deviation from the cached full-modality statistics with a distribution alignment loss:\n\nL_dist = MSE(\u03bc_current, \u03bc_full) + MSE(\u03c3_current^2, \u03c3_full^2).\n\nThis alignment term is trained jointly with (i) a prototype consistency loss that encourages image features to be close to their nearest intrinsic normal prototype (INP) in cosine distance, and (ii) an adaptive reconstruction loss that measures cosine distance between encoder features and the INP-guided decoder features, weighted to emphasize hard-to-reconstruct regions. The total training objective is a weighted sum:\n\nL_total = L_rec + \u03bb1\u00b7L_con + \u03bb2\u00b7L_dist,\n\nwith \u03bb1 set to 0.2 by default and \u03bb2 selected via tuning (0.2 giving the best performance in the reported experiments). The intent is to reduce feature shift caused by missing modalities while retaining prototype-guided normal reconstruction.",
      "source_document": "papers/2512.21264v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a prototype-guided reconstruction anomaly detector for multi-sequence MRI, how can the reconstruction objective be made \u201cadaptive\u201d so it emphasizes regions that are hard to reconstruct\u2014i.e., how is the per-sample (or per-position) difficulty weight computed from distances to the intrinsic normal prototypes (INPs), and how is that weight used when aggregating cosine distances between encoder features and decoder features across decoding layers?",
      "answer": "The adaptive reconstruction loss assigns a higher weight to features that are farther from the learned intrinsic normal prototypes (INPs). For each feature position i, it first computes the minimum cosine distance to the prototype set (nearest INP), d_i = min_{n\u2208{1,\u2026,N}} CosSim(F_Q(i), p_n). It then computes the average cosine distance \u0003d within the current batch and converts the relative difficulty into a weight using a temperature hyperparameter \u03b3: \u03c9_i = (\\bar d / d_i)^\u03b3 (with \u03b3 set to 3 by default). The reconstruction loss is then formed by taking the cosine distance between corresponding encoder features and INP-guided decoder features at multiple decoding layers, multiplying each distance by its weight \u03c9_i, summing over channels/spatial locations and layers, and averaging over the batch (expressed as L_rec = (1/2)\u00b7\u2211_{i=0}^{L} (CosSim(F_en_i, F_de_{i\u22121})\u00b7\u03c9_i)).",
      "source_document": "papers/2512.21264v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an any-modality multi-sequence MRI anomaly detector that uses two fused encoder feature maps (a shallower fusion and a deeper/semantic fusion), where is it more effective to apply the feature-distribution alignment loss\u2014on the shallow fused feature map or on the deep fused feature map\u2014and what does that indicate about how missing-modality \u201cfeature shift\u201d should be corrected?",
      "answer": "Applying the distribution alignment loss to the deeper fused feature map (En1, built from the encoder\u2019s deeper feature hierarchy) is more effective than applying it to the shallow fused feature map (En0), and both are better than omitting the loss. This indicates that correcting missing-modality feature shift at higher, more semantic feature levels yields stronger cross-modal consistency and generalization than trying to align only lower-level/shallow features.",
      "source_document": "papers/2512.21264v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a video diffusion model that aims for direct controllability from a sparse 3D object layout, how can attention supervision be applied inside a Diffusion Transformer to enforce the layout semantics during training, and how is the supervision target constructed from the layout-derived mask?",
      "answer": "Attention supervision is imposed by running two parallel inputs through (shared-parameter) DiT blocks: (1) a masked version of the video latent (obtained by applying a binary mask M derived from the sparse 3D layout and encoding it) and (2) the original noisy video latent at timestep t. Tokens from the masked latent act as queries (Q_mask) and tokens from the noisy latent act as keys (K), producing a cross-attention map via softmax(Q_mask K^T / sqrt(d)). The cross-attention map is averaged along the query dimension to obtain a 1D response map over tokens, and an attention loss L_attn minimizes an MSE between this response map (across attention layers) and a target map M_target. The target map M_target is built directly from the mask M by applying temporal average pooling and nearest-neighbor downsampling (and pooling) to match the token resolution. Training uses a weighted sum of the standard diffusion/flow-matching loss and the attention loss: L = \u03bb_diff L_diff + \u03bb_attn L_attn.",
      "source_document": "papers/2512.21268v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When adapting a pre-trained image-to-video Diffusion Transformer to accept a sparse 3D-aware layout condition, how can a dedicated ControlNet branch be constructed and merged with the frozen backbone so that (i) the conditioning path starts with zero influence and (ii) progressively learns to inject layout information into the generation process?",
      "answer": "Construct the sparse-layout ControlNet by copying the first N Diffusion Transformer (DiT) blocks from the pre-trained video model and making these copied blocks trainable while keeping the original backbone blocks frozen. For each corresponding block i, pass the ControlNet block\u2019s output through a zero-initialized linear layer (so its initial contribution is zero and does not perturb the pre-trained model at the start of training), then add this transformed output element-wise to the output of the matching frozen block in the base I2V model. Training then learns non-zero residual injections that encode the layout conditioning.",
      "source_document": "papers/2512.21268v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a camera-trajectory\u2013controlled video diffusion model, how can camera-guidance accuracy be quantified from the generated videos (i.e., how are the rotation and translation errors computed, and what steps are taken to make the comparison fair when different methods output different video lengths)?",
      "answer": "Camera-guidance accuracy is measured with rotation error (Rerr) and translation error (Terr) computed from estimated camera trajectories of the generated videos. The trajectories are recovered using COLMAP, then all recovered poses are normalized to a consistent scale and errors are computed relative to the first frame. Because baselines may produce videos of different lengths, the reported Rerr/Terr (and other metrics) are averaged across all frames for fair comparison.",
      "source_document": "papers/2512.21268v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using sparse depth/layout signals to control a video diffusion model, how can a classifier-based guidance baseline be implemented at the latent level (i.e., what auxiliary model is trained, what loss is used, how are gradients obtained and applied during denoising, and what fairness constraint should be imposed on the video backbone), and what failure mode does this baseline tend to exhibit compared to attention-supervised conditioning?",
      "answer": "A classifier-based guidance baseline can be built by training an auxiliary video depth estimation model (fine-tuned on the constructed layout-depth dataset) that takes the video latents as input; in practice the input is formed by channel-wise concatenation of the diffusion model\u2019s video latents with depth-prediction latents. The auxiliary model is trained with a mean-squared error loss to predict the target layout depth. At inference, the MSE loss is backpropagated through the depth model to obtain gradients with respect to the video latents, and these gradients are used as guidance signals to adjust the diffusion denoising trajectory. For a fair comparison, the guided baseline uses the same video diffusion backbone architecture as the attention-supervised method. This classifier-guided approach tends to introduce adversarial artifacts that reduce visual fidelity and weaken structural/layout control, whereas attention-supervised conditioning produces cleaner synthesis with stronger alignment to the intended layout signals.",
      "source_document": "papers/2512.21268v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a spike-driven masked-video pretraining setup for surgical scene segmentation, what training objective is used to learn the encoder\u2019s spatiotemporal representations, and how is semantic knowledge transferred from a strong ANN encoder despite the representational gap between spiking membrane-potential features and ANN activations?",
      "answer": "The encoder is pretrained with a masked autoencoding objective that reconstructs only the masked regions using an MSE reconstruction loss over masked pixel indices \u03a9: L_pretrain = (1/|\u03a9|) \u03a3_{i\u2208\u03a9} ||I(i) \u2212 \u00ce(i)||\u00b2. To inject stronger semantic priors, semantic knowledge distillation is added from the SAM2 encoder: because spike-driven features (membrane-potential-based) and SAM2 features (activation-based) are not directly aligned, a convolutional adapter is used to map SAM2 features into the spike feature space, and the distillation loss is L_kd = ||U\u2084 \u2212 Conv(U_sam)||\u00b2, where U\u2084 is the spike encoder feature and U_sam is the SAM2 encoder feature. The adapter (and the ViT decoder used for reconstruction) is discarded after pretraining, keeping only the spike-driven encoder for downstream finetuning.",
      "source_document": "papers/2512.21284v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a spike-driven video segmentation system designed for real-time surgical scenes, how does the downstream segmentation head incorporate temporal information while remaining computation-efficient, and what loss functions and data augmentations are used during finetuning to handle class imbalance and improve robustness?",
      "answer": "The segmentation head is kept lightweight and has two main parts: (1) a memory read-and-fusion module that retrieves semantic features from previous frames and fuses them with the current frame using spike-driven Hamming attention to model temporal dependencies while preserving event-driven SNN efficiency; and (2) a spike-driven Feature Pyramid Network (SpikeFPN) that builds multi-scale features by applying a convolution after spiking normalization at each encoder stage and adding an upsampled higher-level pyramid feature (Ol = Conv(SN(Ul)) + Upsample(Ol+1)), then upsamples/aggregates these pyramid features to produce per-pixel logits.\n\nFor finetuning, training uses a combination of cross-entropy loss and focal loss to mitigate the severe class imbalance typical in surgical datasets (focal loss down-weights easy examples and emphasizes hard pixels, with focusing parameter typically set to 2). Data augmentation during training includes random resizing (scale range 0.3\u20130.7), cropping to a fixed input size (512\u00d7640 for EndoVis18 and 384\u00d7640 for SurgBleed), random horizontal flipping, and photometric distortion; clips use 4 frames (T=4).",
      "source_document": "papers/2512.21284v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a benchmark comparison between a task-specific surgical video segmentation network and prompt-based SAM/SAM2-style baselines, what prompt setting is used to make the comparison fair, and how are runtime efficiency metrics (latency and energy/power) measured/estimated including the evaluation hardware assumptions?",
      "answer": "A single-point prompt is used for all prompt-based baselines to keep the auxiliary input/prior consistent. Segmentation accuracy is evaluated with IoU (including class-wise IoU and mIoU on EndoVis18). Efficiency is reported as (1) power/energy in mJ estimated from the number of required floating-point operations\u2014accumulate (AC) ops for SNNs or multiply-and-accumulate (MAC) ops for ANNs\u2014and (2) inference time (latency) in ms estimated on an AMD Xilinx ZCU104 embedded platform, explicitly excluding GPU support to reflect non-GPU deployment conditions.",
      "source_document": "papers/2512.21284v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In masked-video pretraining for spike-driven encoders, how can you construct a multi-scale \u201ctube\u201d mask that stays aligned with each intermediate feature map, and why is it important to apply this mask before the temporal spiking process rather than only masking the input frames once?",
      "answer": "A 2D random mask is first sampled at the patch grid resolution (H/16 \u00d7 W/16) with masking ratio \u03b1. For each encoder layer l, this same 2D mask is (1) repeated along the temporal dimension T to form a tube through time, and (2) spatially expanded so each masked unit covers the corresponding patch size at that layer (2^(4\u2212l) \u00d7 2^(4\u2212l) pixels), yielding a layer-specific tube mask Ml that matches the spatial resolution of the l-th intermediate feature map (Ml \u2208 {0,1}^{T \u00d7 H/2^l \u00d7 W/2^l}). The mask is applied to the feature maps before the temporal spiking step so masked regions generate no spike activity; doing this hierarchically at every layer reduces distribution shift and prevents information leakage that can occur if convolution/attention is applied on partially masked inputs (because operations could mix masked and unmasked regions and inadvertently reveal masked content).",
      "source_document": "papers/2512.21284v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In masked-video pretraining for a spike-driven surgical video segmentation backbone, how does downstream segmentation quality change as you (1) increase the fraction of the available training set used for pretraining and (2) add extra unlabeled surgical video, and what qualitative failure mode tends to appear when the pretraining data is insufficient?",
      "answer": "Downstream mIoU improves consistently as pretraining data scale increases: using a larger portion of the available training set for masked-video pretraining yields higher mIoU on both EndoVis18 and SurgBleed, with gains becoming more pronounced as the pretraining portion approaches the full dataset. Adding extra unlabeled surgical video during pretraining further boosts mIoU in a monotonic manner on both datasets. When pretraining data is insufficient (e.g., none or only part of the available data), the model tends to produce fragmented object boundaries and temporally unstable/inconsistent predictions.",
      "source_document": "papers/2512.21284v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When using a mask-based table structure model that predicts column masks (e.g., a VGG-19\u2013encoder TableNet trained with a binary cross-entropy segmentation loss), how can the predicted column mask be post-processed into explicit column boundary coordinates without modifying the original image, and what evaluation setup/metric demonstrates that this improves OCR-based table extraction compared to using the raw mask predictions directly?",
      "answer": "The mask is treated as a spatial signal rather than a visual filter: pixel-wise transitions in the predicted binary column mask are aggregated across rows to form a 1D density signal along the x-axis. Column boundary candidates are then obtained by an iterative threshold\u2013convolution procedure that alternates (i) thresholding/truncation to zero out low-amplitude components regarded as noise and (ii) Gaussian convolution to smooth the remaining mass; Gaussian smoothing acts as a low-pass filter and, across iterations with increasing smoothing scale, stabilizes dominant peaks while suppressing spurious fluctuations. After the final iteration, local maxima of the smoothed 1D signal are selected as boundary locations and mapped/rescaled back to image coordinates. Row separation is inferred separately via OCR by analyzing vertical spacing between recognized text lines in the extracted column regions.\n\nEffectiveness is evaluated on PubTable-1M using the Content-/Cell-Aware Segmentation Accuracy (CASA) metric, which counts a word as correct only if it is both correctly recognized and placed in the correct ground-truth cell. In this setup, applying OCR directly to the raw predicted column masks yields 67% CASA, while using the post-processed column boundaries increases CASA to 76%.",
      "source_document": "papers/2512.21287v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "If you want to use a pretrained slide-level tile contextualizer with embeddings from a new (previously unseen) tile encoder, what parts of the model should be frozen vs newly trained, what self-supervised objective is used for this adaptation, and which components are actually used at inference to produce contextualized tile embeddings for that new encoder?",
      "answer": "Freeze the shared transformer slide encoder and the cross-decoder, and add/train only lightweight encoder-specific projection modules for the new tile encoder: a new input projector that maps the new encoder\u2019s native embedding dimension into the shared space, and a new output projector head. These new projectors are optimized with a single-target masked self-reconstruction objective\u2014mask some of the new encoder\u2019s tile embeddings and train the model to reconstruct the masked embeddings of that same encoder. At inference, contextualization for the new encoder is done by passing its embeddings through the newly trained input projector and then the frozen shared slide encoder to output contextualized tile embeddings (the decoder is not needed for inference contextualization).",
      "source_document": "papers/2512.21331v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a memory-based video object segmentation pipeline like SAM2, how can you prune post-encoder tokens in a prompt-aware way without changing the segmentation architecture\u2014specifically, what signals are used to score token importance, how is the text signal produced and aligned to the ViT token space when no ground-truth mask is available, how is per-token uncertainty estimated and aligned, and how are these signals fused to select the top\u2011k tokens that get propagated through the memory/decoder?",
      "answer": "Token pruning is done *after* SAM2\u2019s frozen ViT-Hiera image encoder and *before* tokens are written to the memory and used by the mask decoder, leveraging the fact that SAM2\u2019s memory accepts variable-length token sequences.\n\n\u2022 **Signals for token importance:** each token is ranked using (1) its **visual embedding** from the ViT encoder, (2) **semantic relevance** from an object-centric text prompt, and (3) **predictive uncertainty** so that ambiguous/boundary regions are less likely to be pruned.\n\n\u2022 **Text prompt without ground truth + alignment:** if the user does not provide text, a coarse region of interest is obtained in the first frame via a **class-agnostic object proposal** approach (Faster R-CNN/RPN), taking the highest-objectness proposal. A vision\u2013language model (LLaVA) generates a coarse caption for that region, and a small language model (BERT) refines it into a concise phrase. The phrase is embedded with a **frozen CLIP text encoder** to get a 512-D text embedding, then **aligned to the 768-D ViT token space** using a *training-free least-squares projection* computed on the encoder tokens (Wt = argmin_W ||X_ViT W \u2212 e_text||^2), producing an aligned semantic vector e\u2032_text \u2208 R^768 (broadcast across tokens).\n\n\u2022 **Uncertainty estimation + alignment:** uncertainty is computed per token using **Monte Carlo Dropout** applied to intermediate ViT-Hiera layers (layers 3\u20135). Over T stochastic forward passes, the method extracts each token\u2019s **pre-softmax attention logits**, computes the **variance** across passes as uncertainty, and min\u2013max normalizes it. The resulting uncertainty feature is then also **projected into the 768-D token space** via another *training-free least-squares projection* (Wu) to obtain U_i.\n\n\u2022 **Fusion and top\u2011k selection:** for each token i, the method concatenates the three aligned 768-D signals into a fused vector h_i = [X_ViT,i ; e\u2032_text ; U_i] \u2208 R^2304. A lightweight two-layer **router MLP** (2304 \u2192 256 \u2192 1) predicts a scalar score s_i, scores are softmax-normalized to \u03b1_i, and the **TopK(\u03b1, k)** tokens are retained as X_pruned, which replaces the full 196-token grid for SAM2\u2019s memory writing and mask decoding.",
      "source_document": "papers/2512.21333v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an interactive video object segmentation setup that adds post-encoder token pruning to SAM2 without changing the decoder/memory design, how is the initial point prompt chosen (especially for irregular or holed shapes), what condition is used to trigger additional refinement clicks during propagation, and how does pruning affect the average number of simulated clicks per sequence compared with unpruned SAM2?",
      "answer": "The system is initialized with a single positive point prompt for SAM2\u2019s prompt encoder; this point can be a user click, but for irregular/holed objects it is chosen as a distance-transform\u2013based representative point rather than a geometric centroid to avoid bad initialization. During propagation, additional (synthetic or user) clicks are added only if segmentation quality drops below a J&F threshold (kept above ~80% in the experiments); pruning reduces drift and thus reduces refinement calls. Empirically, averaged across datasets, pruning lowers the required interaction from about 4.2 simulated clicks per sequence for the SAM2 baseline to about 2.6 clicks per sequence with pruning.",
      "source_document": "papers/2512.21333v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In post-encoder token pruning for memory-based video object segmentation (e.g., pruning ViT tokens before SAM2\u2019s memory/decoder), how do you pick the token budget and the number of Monte Carlo Dropout passes so that you get a good accuracy\u2013efficiency trade-off, and what empirical behavior motivates using roughly 30% token retention with about 5 stochastic passes as the default operating point on a challenging benchmark like UVO?",
      "answer": "Token budget: Sweeping the retained-token fraction shows that keeping a moderate subset (around 30% of the 14\u00d714 tokens) provides the best balance\u2014accuracy stays essentially on par with the unpruned/barely pruned settings while speed increases substantially. Retaining more tokens (e.g., 50% or 100%) yields little or no accuracy benefit but sacrifices FPS, while very aggressive pruning (e.g., ~10%) increases FPS further but starts to reduce J&F because too few tokens remain to preserve semantic coverage and fine details.\n\nMonte Carlo Dropout passes: Increasing the number of stochastic passes used to estimate token uncertainty improves the stability/quality of uncertainty estimates and can slightly improve or maintain segmentation quality up to a point, but additional passes quickly incur extra compute with diminishing accuracy returns. Empirically, about 5 passes provides the best overall quality\u2013speed trade-off (better J&F than fewer passes, without the notable FPS drop seen when increasing beyond this).",
      "source_document": "papers/2512.21333v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When post-encoder token pruning makes a ViT-based VOS pipeline miss fine-grained local details (e.g., thin structures or small parts), what kind of auxiliary visual backbone can be added to complement the ViT features, how is it incorporated relative to the frozen encoder, and what accuracy\u2013latency trade-off does this add-on produce compared with using the ViT features alone?",
      "answer": "A frozen ConvNeXt backbone can be appended alongside the frozen ViT-Hiera image encoder to capture finer local spatial patterns that ViT features may under-represent. This dual-backbone setup yields a small but consistent improvement in segmentation quality on UVO (J&F increases slightly, from 85.8 to 86.0) at the cost of additional runtime overhead (about +18 ms per frame) compared with using the ViT features alone.",
      "source_document": "papers/2512.21333v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a real-time streaming video LLM that must decide *when* to speak as frames arrive, how can response-timing decisions be learned end-to-end within the same next-token prediction sequence (rather than using a separate controller), and what specific reweighted training objective can be used to handle the heavy class imbalance between \u201cstay silent\u201d and \u201crespond\u201d states?",
      "answer": "Response timing is learned by reformulating each video into a temporally segmented, multi-turn dialogue where each segment is tagged with explicit time markers (e.g., <2s-3s>) and the model is trained to predict a discrete response-state token at every turn as part of normal next-token prediction. The three integrated states are <Silence> (keep processing), <Standby> (relevant content detected but insufficient to answer), and <Response> (sufficient information; trigger immediate answer generation). Because <Silence> dominates in streaming data, the training objective reweights the cross-entropy loss only for these three special state tokens using (i) focal weighting based on token hardness, wfocal(i) = (1 \u2212 p_ci)^\\gamma, and (ii) a per-batch inverse-frequency \u201calpha\u201d weight for each state k, \\alpha_k = (1 / (|S| \\sum_{j\\in S} n_j)) \\cdot (\\sum_{j\\in S} n_j / n_k) = 1/(|S|\\, n_k) up to the shared batch-normalization factor, where n_k is the count of state k in the batch and |S|=3. The final per-token loss is Li = \\alpha_{t_i} wfocal(i) LCE(i,t_i) for state tokens (t_i \\in S), and Li = LCE(i,t_i) otherwise, with the total loss computed as the average over non-masked positions to avoid dependence on sequence length.",
      "source_document": "papers/2512.21334v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When curating streaming-video supervision for event-level captioning, what pipeline can be used to produce temporally grounded segment captions with sharp end boundaries (including how noisy samples are filtered), and how is the corresponding action-caption task generated differently to provide step-level supervision?",
      "answer": "A workable pipeline is to (1) generate segment-level captions with a video model (here ARC-Hunyuan-Video-7B), (2) temporally ground each generated caption with the same model, and then (3) filter the data by retaining only videos whose segment captions yield mutually consistent, overlapping grounded time spans that align with the original outputs\u2014this removes erroneous/noisy samples and yields sharper, more explicit event boundaries for clearer supervision. The action-caption task reuses this event-caption pipeline but adds action-oriented prompts plus targeted filtering so the supervision focuses on discrete actions/procedural steps, producing cleaner step-level annotations with sharper action delineation.",
      "source_document": "papers/2512.21334v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "For time-sensitive video QA in a streaming setting (where the correct answer can change as the video progresses), how can you evaluate a model so that it is penalized for answering at the wrong moment as well as for wrong content\u2014specifically, how are ground-truth and predicted answers represented, what temporal tolerance is used to decide a match, and how are accuracy vs. recall computed from the per-timestamp matching indicators?",
      "answer": "Represent each TSQA question q with multiple ground-truth time-stamped answers G_q = {(a_i^q, t_i^q)}_{i=1..m_q} (answer content plus the time when it becomes correct). The model outputs a set of predicted pairs P_q = {(\u00e2_j^q, \u007ft_j^q)}_{j=1..n_q}.\n\nA prediction (\u00e2_j^q, \u007ft_j^q) is counted as matching a ground-truth point (a_i^q, t_i^q) only if BOTH conditions hold: (1) content matches, via an indicator C(\u00e2_j^q, a_i^q)=1, and (2) its timestamp is within a tolerance \u03b4t of the ground-truth timestamp, via T(\u007ft_j^q, t_i^q; \u03b4t)=1 when | \u007ft_j^q \u2212 t_i^q | \u2264 \u03b4t. In the evaluation setting, \u03b4t is 3 seconds.\n\nDefine a per-ground-truth indicator I_i^q that is 1 if there exists at least one prediction j satisfying C=1 and T=1 for that ground-truth point i, and 0 otherwise.\n\nAccuracy averages these indicators over all ground-truth time-stamped answers across all questions: Accuracy = (1 / (\u2211_{q\u2208Q} m_q)) \u00b7 \u2211_{q\u2208Q} \u2211_{i=1..m_q} I_i^q.\n\nRecall is computed by first averaging I_i^q over the m_q ground-truth points within each question, then averaging over questions: Recall = (1/|Q|) \u00b7 \u2211_{q\u2208Q} (1/m_q) \u00b7 \u2211_{i=1..m_q} I_i^q.",
      "source_document": "papers/2512.21334v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When evaluating a streaming video assistant on (i) temporal event grounding queries that may be asked before an event (forward) or after it (backward), and (ii) open-ended real-time narration / dense captioning where there is no single \u201cexact match\u201d reference, what evaluation metrics can be used for each task type, and how are those metrics computed?",
      "answer": "For grounding (both forward and backward queries), use mean Intersection-over-Union (mIoU) between the predicted temporal interval and the ground-truth interval. For each sample i, with predicted interval t_i^pred=[s_i^pred,e_i^pred] and ground truth t_i^gt=[s_i^gt,e_i^gt], compute IoU_i as the intersection length divided by union length:\nIoU_i = max(0, min(e_i^pred,e_i^gt) \u2212 max(s_i^pred,s_i^gt)) / (max(e_i^pred,e_i^gt) \u2212 min(s_i^pred,s_i^gt)),\nthen average over N samples: mIoU = (1/N) * sum_i IoU_i.\n\nFor narration and captioning (open-ended generation), evaluate quality via pairwise comparison against a strong baseline model (Qwen2.5-VL-72B) following an LLM-as-a-judge protocol (Chatbot Arena/StreamingVLM style). Report the win rate, defined as the proportion of cases where the model\u2019s output is judged better than the baseline\u2019s output.",
      "source_document": "papers/2512.21334v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When building instruction-tuning data for **time-sensitive video QA** in a streaming setting (where the correct answer should be updated as the video evolves), how can you automatically generate supervision from raw videos\u2014specifically, what kinds of **temporal change points** should be detected, and how are these changes converted into **question\u2013answer labels over time** (i.e., one question with multiple time-stamped answers)?",
      "answer": "A practical way to construct time-sensitive QA supervision is to first run each video through a strong vision-language model (here, GLM-4.5V) to detect **change points** over multiple aspects of the stream: **object attributes** (e.g., color/size/state), **spatial positions**, **actions and interactions**, **counts**, and broader **scene/context shifts**. Then, use these detected variations to create QA supervision by asking a **single unified question** and providing **different time-specific answers** at the corresponding change-point timestamps, so the label explicitly teaches the model to update its answer as the video changes.",
      "source_document": "papers/2512.21334v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When designing a CLIP-based ordinal regressor to predict a building\u2019s construction year from an image with optional GPS coordinates, what mechanism can be used to fuse the location embedding with the image embedding without manually tuning a fusion weight, and how does the training objective enforce ordinal consistency via a ranking-based contrastive loss (including how negative samples are weighted by label distance)?",
      "answer": "Use a learnable \u201czero convolution\u201d layer after the location encoder to fuse GPS and image embeddings so the model learns the effective fusion weight automatically during training.\n\nTrain with a fine-grained cross-modal ranking-based contrastive loss (FCRC) that encourages ordinal consistency:\n\nL_FCRC^z = \u2212 \\sum_{i=1}^M (1/M) \\log \\Big( f(z_i,w_i) / ( f(z_i,w_i) + \\sum_{j\\ne i} \\lambda_{i,j}^z f(z_i,w_j) ) \\Big),\n\nwhere f(z_i,w_j)=exp(cos(z_i,w_j)/\\tau) is the image\u2013text similarity term, and the regularization/negative weight \\lambda_{i,j} is set from label distance: \\lambda_{i,j}=Norm(\\beta\\cdot d_{i,j}), with d_{i,j}=|y_i\u2212y_j| and Norm making the weights sum to 1.",
      "source_document": "papers/2512.21337v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a CLIP-based coarse-to-fine building-year estimator that also outputs human-verifiable explanations, how are (1) coarse architectural style classes and (2) a bank of attribute \u201creason\u201d prompts converted into features for the regressor, how is the final continuous year prediction computed from the regressor\u2019s period outputs, and how are the most influential reasons selected to form the textual rationale?",
      "answer": "The model encodes two sets of texts with a (frozen) text encoder: (i) seven coarse architectural style/period class prompts and (ii) a bank of \u201creason\u201d prompts decomposed into reasons (e.g., roof type, wall/material, height, etc.) with multiple subcategories per reason. Given an input image (and optional GPS-fused embedding), it computes cosine-similarity scores between the input embedding and each class embedding and each reason-subcategory embedding. These similarities are concatenated into a single regressor input vector containing all seven class similarities followed by all reason-subcategory similarities: s = [sim_c1,\u2026,sim_c7, sim_r11, sim_r12,\u2026,sim_rmn].\n\nThe regressor maps s to probabilities over the seven historical periods. The final predicted year is then computed as a weighted average over the periods\u2019 midpoints, using the predicted period probabilities and a small learnable stability/confidence term per period: \u0177 = \\sum_i p_i \u00b7 (b_i / (1+\u03b4_i)), where p_i is the probability of period i, b_i is that period\u2019s midpoint, and \u03b4_i is a small learnable parameter.\n\nFor explainability, an importance score is computed for each reason subcategory by combining its similarity score with the regressor\u2019s attention to the corresponding historical period. For each reason (e.g., roof), the subcategory with the highest importance is selected, and subcategory scores are also summed to get an overall importance per reason. The rationale is formed by taking the top five reasons with the highest overall importance (e.g., \u201croof type: dome\u201d, \u201cmaterial: stone\u201d).",
      "source_document": "papers/2512.21337v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In an ordinal regression benchmark for predicting a building\u2019s construction year from an image (optionally with GPS), how can \u201cpopularity bias\u201d be quantified at evaluation time using Wikipedia page views\u2014specifically, how is interval accuracy defined, how is it stratified by popularity, and how is the bias summarized as a single \u201cGain\u201d number?",
      "answer": "Popularity bias is measured by combining an interval-accuracy metric with popularity strata defined by Wikipedia page views. Interval Accuracy is defined as\n\nIA_k = (1/N) \\u2211_i 1[|y_i \\u2212 \\u0177_i| \\u2264 k],\n\ni.e., the fraction of test examples whose predicted year \\u0177_i falls within \\u00b1k years of the ground-truth year y_i (with k commonly taken as 5, 20, 50, or 100 years; IA5 rewards near-exact dating while IA20 approximates getting the correct architectural period).\n\nTo probe memorization/popularity effects, IA5 is then computed separately on popularity bins formed from Wikipedia page-view counts; in the headline popularity analysis, \u201clow popularity\u201d is defined as page views < 10^2 and \u201chigh popularity\u201d as page views > 10^5.\n\nThe popularity bias is summarized by \u201cGain,\u201d defined as the difference between IA5 on the high-popularity bin and IA5 on the low-popularity bin (Gain = IA5_high \u2212 IA5_low). A large positive Gain indicates much better performance on highly viewed (popular) buildings, consistent with memorization-driven bias.",
      "source_document": "papers/2512.21337v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When creating a large-scale benchmark for predicting a building\u2019s construction year from Wikipedia facade images, how can the dataset be deduplicated and split to (a) prevent train/test leakage of the same building and (b) keep the temporal and geographic distributions balanced across splits?",
      "answer": "Deduplicate by retaining a single image per Wikipedia page title (so each sample corresponds to a distinct building), then construct the train/validation/test partitions by stratifying on both construction decade and continent before assigning buildings to splits (using a 60%/20%/20% allocation). Enforce a leakage constraint that no building\u2014and likewise no associated caption or image\u2014appears in more than one split.",
      "source_document": "papers/2512.21337v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When benchmarking a building construction-year estimator that can optionally take GPS coordinates, what evaluation protocol can be used to expose how much accuracy comes from geographic priors (and potential spatial leakage) rather than visual cues, and how is result variability summarized across runs?",
      "answer": "Evaluate all models on the same fixed test split, and for any method that uses GPS, report the metrics twice\u2014once with location enabled and once without location\u2014to explicitly show the benefit (and possible leakage) of spatial priors. Repeat each experiment with three random seeds and report the mean and standard deviation of the metrics.",
      "source_document": "papers/2512.21337v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a DMD2-style distribution-matching distillation pipeline used to fine-tune a text-to-video diffusion transformer for fast/high-resolution generation, what are the roles of the three models (teacher/real diffusion, student/final generator, and fake diffusion), and which objective or gradient is used to update each during training?",
      "answer": "The training uses three Wan2.1-architecture models: (1) a frozen large teacher that serves as the \u201creal diffusion model\u201d; (2) an updatable 1.3B \u201cfinal generator\u201d student that will be used at inference; and (3) a second updatable 1.3B \u201cfake diffusion model\u201d used to help compute gradients. During training, the fake diffusion model is updated with the standard diffusion loss, while the final generator is updated using a distribution-matching gradient computed from the divergence between the real (teacher) and fake diffusion models; the teacher itself remains frozen.",
      "source_document": "papers/2512.21338v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a chunk-wise autoregressive video diffusion Transformer, how can you design the attention/KV-cache so inference cost stays constant as video length grows while still preserving (a) smooth local motion and (b) long-range scene consistency? Specify what frames/tokens are kept in the sliding window and the roles of the \u201clocal history/temporal decay\u201d frames versus the single \u201canchor/attention sink\u201d frame.",
      "answer": "Use a fixed-size Anchor-Guided Sliding Window cache during chunked generation. When generating the current chunk of M frames, restrict attention context to: (1) the current chunk tokens (M frames), (2) a local history cache consisting of the most recent M\u22121 past frames (the \u201ctemporal decay contribution\u201d), and (3) a single global anchor frame (the first frame) that is always included as a \u201ctemporal attention sink\u201d. This caps the maximum attention length to 2M frames, so compute/memory do not grow with total video length. The M\u22121 neighboring frames provide the short-range motion/state information needed for fluid transitions, while the always-cached first frame grounds long-range object/scene consistency across all subsequent chunks without attending to the full history.",
      "source_document": "papers/2512.21338v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "In a few-step autoregressive video diffusion/flow-matching model that denoises the first part of each chunk at low spatial resolution and then refines at high resolution, why does naively reusing the low-resolution KV cache to condition the next chunk hurt temporal consistency, and what dual-resolution caching update rule can keep the cache spatially aligned with the final high-resolution output?",
      "answer": "Reusing the KV features produced during the low-resolution denoising steps to condition the next chunk is harmful because those cached states correspond to an intermediate, low-res (and not-yet-final) representation; after high-resolution refinement, the chunk\u2019s final content has shifted, so the low-res cache becomes misaligned with the final high-res output, creating structural conflicts and degrading temporal consistency.\n\nTo keep conditioning information aligned, maintain two KV caches (KVhigh and KVlow) and update them from the *final* refined output of each chunk: after completing high-resolution refinement, cache (1) high-resolution KV computed from the final high-res output into KVhigh, and (2) downsample the final high-res output back to low resolution and compute/cache its KV into KVlow. Passing this downsampled-from-final result as the low-res cache guarantees the low-res cache is spatially consistent with the high-resolution video that was actually generated, while KVhigh supports high-res refinement for subsequent chunks.",
      "source_document": "papers/2512.21338v1.pdf",
      "mode": "textual",
      "content_refs": []
    },
    {
      "question": "When a diffusion-transformer video generator is fine-tuned at a lower spatial resolution but is expected to sample at much higher (e.g., 1080p) resolution, what inference-time adjustments to the diffusion schedule/positional encoding can help preserve quality across this resolution jump, and why would you boost attention strength only for the initial chunk but disable that boost for later chunks in a chunk-wise autoregressive sampler?",
      "answer": "Use an \u201cHD\u201d inference setting that increases the timestep-shift used for sampling and enlarges the RoPE extrapolation (NTK-RoPE scaling) so the model can handle the larger token grid at higher resolution. Additionally, apply an attention-scaling boost only on the first chunk to force the model to generate strong, detailed high-resolution anchor frames; then turn the boost off for subsequent chunks because those chunks can inherit/propagate the high-resolution details via the cached states, and keeping attention boosted would tend to create redundant artifacts rather than improving fidelity.",
      "source_document": "papers/2512.21338v1.pdf",
      "mode": "textual",
      "content_refs": []
    }
  ],
  "timestamp": "2026-01-06T14:45:32.677799+00:00"
}